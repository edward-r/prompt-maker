This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
docs/
  context-templates.md
  cookbook.md
  fzf-commands.txt
  neovim-plugin-integration.md
  pmc-tutorial.md
  prompt-maker-cli-tui-encyclopedia.md
  sidekick.nvim-prompt-maker-jsonl-fixtures.md
  sidekick.nvim-prompt-maker-tui-integration.md
  tui-design.md
  tui-styling-guide.md
  tui-updates.md
exports/
  cli.json
  feeset-changes-prompt-export.yaml
  gemini-pdf-prompt-export.yaml
  loantek-publication-downloader-prompt-export.yaml
  pdf-2-prompt-export.yaml
  pdf-3-prompt-export.yaml
  pdf-4-prompt-export.yaml
  pdf-5-prompt-export.yaml
  pdf-6-prompt-export.yaml
  pdf-7-prompt-export.yaml
  pdf-document-prompt-export.yaml
  pdf-issue.yaml
  prompt-export.json
packages/
  core/
    src/
      __tests__/
        llm.test.ts
      lib/
        llm.ts
      index.ts
runs/
  prompt-export.yaml
scripts/
  build.mjs
  dev.mjs
src/
  __tests__/
    __fixtures__/
      themes/
        adapt/
          opencode.json
        global/
          invalid.json
          ocean.json
          pm-dark.json
        project/
          .prompt-maker-cli/
            themes/
              broken-theme.json
              forest.json
          a/
            .prompt-maker-cli/
              themes/
                pm-dark.json
    generate/
      interactive.test.ts
      payload-io.test.ts
      resume-from-file.test.ts
      resume-loaded-stream-event.test.ts
    history/
      generate-history.test.ts
    tui/
      budgets-persistence.test.ts
      budgets-settings.test.ts
      buffered-history-writer.test.ts
      command-screen-popup-bindings-contract.test.ts
      context-popup-glue-contract.test.ts
      context-popup-state-mutations.test.ts
      generation-history-formatters.test.ts
      generation-pipeline-reducer.test.ts
      help-command.test.ts
      list-popup-model.test.ts
      list-windowing.test.ts
      markdown-highlight.test.ts
      model-popup-model.test.ts
      notifier-dedupe.test.ts
      popup-manager-command-mapping.test.ts
      popup-reducer.test.ts
      popup-scan-orchestrator.test.ts
      popup-shortcuts.test.ts
      theme-adapter.test.ts
      theme-loader.test.ts
      theme-persistence.test.ts
      theme-provider.test.ts
      theme-registry.test.ts
      theme-resolver.test.ts
      theme-validate.test.ts
      tui-exit-clear-screen.test.ts
      url-args.test.ts
      useDroppedFileDetection.test.ts
    app-container-keymap.test.ts
    command-filter.test.ts
    command-menu-keymap.test.ts
    command-screen-reducer.test.ts
    compose-command.test.ts
    config.test.ts
    context-budget.test.ts
    context-smart-root.test.ts
    drag-drop-path.test.ts
    export-command.test.ts
    file-context.test.ts
    file-suggestions.test.ts
    generate-args.test.ts
    generate-command.test.ts
    generate-stream-dispatcher.test.ts
    generation-pipeline-reducer.test.ts
    github-context.test.ts
    help-config.test.ts
    help-overlay-scroll.test.ts
    history-logger.test.ts
    image-loader.test.ts
    index.test.ts
    input-bar.test.ts
    intent-source.test.ts
    io.test.ts
    list-window.test.ts
    media-loader.test.ts
    message-builders.test.ts
    model-filter.test.ts
    model-manager.test.ts
    model-options.test.ts
    model-popup-options.test.ts
    model-session.test.ts
    multiline-text-buffer.test.ts
    new-command.test.ts
    notifier.test.ts
    paste-snippet.test.ts
    pdf-parts.test.ts
    popup-reducer.test.ts
    prompt-generator-service.test.ts
    provider-chip.test.ts
    provider-status.test.ts
    series-path.test.ts
    smart-context-service.test.ts
    soft-wrap.test.ts
    status-indicators-layout.test.ts
    string-filter.test.ts
    test-command.test.ts
    test-runner-reducer.test.ts
    text-input-keys.test.ts
    token-counter.test.ts
    token-usage-store.test.ts
    tui-command-descriptors.test.ts
    tui-command-history.test.ts
    url-context.test.ts
    useCommandHistory.test.ts
    useGenerationPipeline.test.ts
    usePopupManager.test.ts
  export/
    export-generate-payload.ts
  generate/
    actions.ts
    args.ts
    command.ts
    context-budget.ts
    context-output.ts
    context-templates.ts
    debug.ts
    display.ts
    fs-utils.ts
    intent.ts
    interactive-transport.ts
    interactive.ts
    models.ts
    payload-io.ts
    pipeline.ts
    polish.ts
    progress.ts
    stream.ts
    token-telemetry.ts
    types.ts
  history/
    generate-history.ts
  prompt-generator/
    message-builders.ts
    model-credentials.ts
    parse-llm-json.ts
    pdf-parts.ts
    prompts.ts
    series-repair.ts
    series-validation.ts
    target-model-guidance.ts
    types.ts
    video-parts.ts
  rag/
    vector-store.ts
  testing/
    evaluator.ts
    test-schema.ts
  tui/
    components/
      core/
        BackgroundFill.tsx
        bracketed-paste.ts
        command-menu-keymap.ts
        CommandMenu.tsx
        help-overlay-scroll.ts
        HelpOverlay.tsx
        input-bar-layout.ts
        input-bar-presentation.ts
        InputBar.tsx
        multiline-text-buffer.ts
        MultilineTextInput.tsx
        ScrollableOutput.tsx
        single-line-text-input-keymap.ts
        SingleLineTextInput.tsx
        soft-wrap.ts
        status-indicators-layout.ts
        text-input-keys.ts
        Toast.tsx
        ToastOverlay.tsx
        tokenized-text.ts
      popups/
        BudgetsPopup.tsx
        ExportPopup.tsx
        InstructionsPopup.tsx
        IntentFilePopup.tsx
        list-popup-layout.ts
        list-popup-model.ts
        list-window.ts
        list-windowing.ts
        ListPopup.tsx
        model-popup-model.ts
        ModelPopup.tsx
        PopupSheet.tsx
        ReasoningPopup.tsx
        ResumePopup.tsx
        SeriesIntentPopup.tsx
        SettingsPopup.tsx
        SmartPopup.tsx
        TestPopup.tsx
        ThemeModePopup.tsx
        ThemePickerPopup.tsx
        TogglePopup.tsx
        TokenUsagePopup.tsx
      OpencodeSpinner.tsx
    hooks/
      popup-manager/
        command-mapping.ts
        scan-orchestrator.ts
      buffered-history-writer.ts
      generation-history-formatters.ts
      popup-scans.ts
      series-artifacts-io.ts
      useCommandHistory.ts
      useDebouncedValue.ts
      useGenerationPipeline.ts
      useLatestRef.ts
      usePersistentCommandHistory.ts
      usePopupManager.ts
      useStableCallback.ts
    markdown/
      markdown-highlight.ts
    screens/
      command/
        components/
          CommandInput.tsx
          CommandMenuPane.tsx
          HistoryPane.tsx
          PopupArea.tsx
        hooks/
          context-popup-glue/
            popup-state-mutations.ts
            useFilePopupGlue.ts
            useImagePopupGlue.ts
            usePdfPopupGlue.ts
            useSmartPopupGlue.ts
            useUrlPopupGlue.ts
            useVideoPopupGlue.ts
          popup-shortcuts/
            budgets-popup-shortcuts.ts
            escape-only-popup-shortcuts.ts
            export-popup-shortcuts.ts
            history-popup-shortcuts.ts
            intent-popup-shortcuts.ts
            model-popup-shortcuts.ts
            reasoning-popup-shortcuts.ts
            resume-popup-shortcuts.ts
            selection.ts
            smart-popup-shortcuts.ts
            suggested-input.ts
            suggested-selected-list-popup-shortcuts.ts
            suggested-selected-list.ts
            theme-popup-shortcuts.ts
            toggle-popup-shortcuts.ts
            types.ts
            url-popup-shortcuts.ts
          useCommandGenerationPipeline.ts
          useCommandMenuManager.ts
          useCommandScreenBindings.ts
          useCommandScreenChips.ts
          useCommandScreenContextPopupBindings.ts
          useCommandScreenController.ts
          useCommandScreenHistoryAndTests.ts
          useCommandScreenHistoryIntentPopupBindings.ts
          useCommandScreenInputState.ts
          useCommandScreenLayout.ts
          useCommandScreenModelGeneration.ts
          useCommandScreenPasteBindings.ts
          useCommandScreenPopupAndView.ts
          useCommandScreenPopupAndView.types.ts
          useCommandScreenPopupBindings.ts
          useCommandScreenPopupManager.ts
          useCommandScreenPopupVisibility.ts
          useCommandScreenShell.ts
          useCommandScreenSubmitBindings.ts
          useCommandScreenViewModel.ts
          useContextPopupGlue.ts
          useDroppedFileDetection.ts
          useDroppedFilePath.ts
          useHistoryPopupGlue.ts
          useHistoryScrollKeys.ts
          useIntentPopupGlue.ts
          useIntentSubmitHandler.ts
          useMiscPopupDraftHandlers.ts
          useModelPopupData.ts
          useModelProviderState.ts
          usePasteManager.ts
          usePopupKeyboardShortcuts.ts
          usePopupSelectionClamp.ts
          usePromptTestRunner.ts
          useReasoningPopup.ts
          useSessionCommands.ts
          useTerminalEffects.ts
          useThemeModePopupGlue.ts
          useThemePopupGlue.ts
        utils/
          control-key.ts
          debug-keys.ts
          paste-tokens.ts
          submit-plan.ts
          url-args.ts
        command-screen-reducer.ts
        CommandScreen.tsx
        CommandScreenImpl.tsx
        useCommandScreen.ts
      test-runner/
        components/
          TestList.tsx
          TestRunnerActions.tsx
          TestRunnerError.tsx
          TestRunnerFileInput.tsx
          TestRunnerLogs.tsx
          TestRunnerSummary.tsx
        test-runner-reducer.ts
        TestRunnerScreen.tsx
        useTestRunnerScreen.ts
    theme/
      builtins/
        pm-dark.ts
        pm-light.ts
      examples/
        ocean-example.json
      terminal-appearance.ts
      theme-adapter.ts
      theme-discovery.ts
      theme-loader.ts
      theme-parse.ts
      theme-provider.tsx
      theme-registry.ts
      theme-resolver.ts
      theme-settings-service.ts
      theme-types.ts
      theme-validate.ts
    app-container-keymap.ts
    AppContainer.tsx
    budget-settings.ts
    command-filter.ts
    command-history.ts
    CommandScreen.tsx
    config.ts
    context-store.ts
    context.tsx
    DEVELOPER_NOTE.md
    drag-drop-path.ts
    file-suggestions.ts
    generation-pipeline-reducer.ts
    help-config.ts
    index.tsx
    intent-source.ts
    model-filter.ts
    model-options.ts
    model-popup-options.ts
    model-session.ts
    new-command.ts
    notifier.ts
    paste-snippet.ts
    popup-reducer.ts
    provider-chip.ts
    provider-status.ts
    resume-history.ts
    string-filter.ts
    TestRunnerScreen.tsx
    toast-constants.ts
    toast-state.ts
    token-usage-store.ts
    types.ts
    useLogBuffer.ts
  types/
    jsdom.d.ts
  utils/
    model-manager.ts
    series-path.ts
  compose-command.ts
  config.ts
  export-command.ts
  file-context.ts
  generate-command.ts
  github-context.ts
  history-logger.ts
  image-loader.ts
  index.ts
  io.ts
  media-loader.ts
  model-providers.ts
  prompt-generator-service.ts
  smart-context-service.ts
  test-command.ts
  token-counter.ts
  url-context.ts
tests/
  mocks/
    boxen.ts
    chalk.ts
    ora.ts
    yargs.ts
.188ab3ffff7bafcd-00000000.bun-build
.188ab3fffffdb13d-00000000.bun-build
.eslintrc.json
.gitignore
.nvmrc
.prettierignore
.prettierrc
AGENTS.md
jest.config.cjs
package.json
prompt-maker
prompt-maker-mac
prompt-tests.yaml
README.md
repomix-output.xml
session-ses_4602.md
tsconfig.jest.json
tsconfig.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="scripts/build.mjs">
import path from 'node:path'
import fs from 'node:fs/promises'
import { fileURLToPath } from 'node:url'
import { build, context } from 'esbuild'

const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)
const repoRoot = path.resolve(__dirname, '..')

const cliArgs = process.argv.slice(2)
const watch = cliArgs.includes('--watch')
const minify = cliArgs.includes('--minify')

const srcEntry = path.join(repoRoot, 'src/index.ts')
const outFile = path.join(repoRoot, 'dist/index.js')
const metaFile = path.join(repoRoot, 'dist/meta.json')

const coreSrcRoot = path.join(repoRoot, 'packages/core/src')

const promptMakerCoreAliasPlugin = {
  name: 'prompt-maker-core-alias',
  setup(pluginBuild) {
    pluginBuild.onResolve({ filter: /^@prompt-maker\/core$/ }, () => ({
      path: path.join(coreSrcRoot, 'index.ts'),
    }))

    pluginBuild.onResolve({ filter: /^@prompt-maker\/core\/.+$/ }, (args) => {
      const subPath = args.path.slice('@prompt-maker/core/'.length)
      return {
        path: path.join(coreSrcRoot, subPath),
      }
    })
  },
}

const buildOptions = {
  entryPoints: [srcEntry],
  outfile: outFile,
  bundle: true,
  packages: 'external',
  platform: 'node',
  target: 'node18',
  format: 'esm',
  sourcemap: true,
  minify,
  plugins: [promptMakerCoreAliasPlugin],
  metafile: true,
  logLevel: 'info',
}

await fs.mkdir(path.dirname(outFile), { recursive: true })

if (watch) {
  const ctx = await context(buildOptions)
  await ctx.watch()
  console.log('esbuild is watching for changes...')
} else {
  const result = await build(buildOptions)
  await fs.writeFile(metaFile, JSON.stringify(result.metafile, null, 2), 'utf8')
}
</file>

<file path="scripts/dev.mjs">
import path from 'node:path'
import { fileURLToPath } from 'node:url'
import { spawn } from 'node:child_process'
import { context } from 'esbuild'

const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)
const repoRoot = path.resolve(__dirname, '..')

const srcEntry = path.join(repoRoot, 'src/index.ts')
const outFile = path.join(repoRoot, 'dist/index.js')

const coreSrcRoot = path.join(repoRoot, 'packages/core/src')

const cliArgs = process.argv.slice(2)

let child = null

const killChild = () => {
  if (!child) return
  child.kill('SIGTERM')
  child = null
}

const startChild = () => {
  killChild()

  child = spawn('node', [outFile, ...cliArgs], {
    stdio: 'inherit',
  })
}

const promptMakerCoreAliasPlugin = {
  name: 'prompt-maker-core-alias',
  setup(pluginBuild) {
    pluginBuild.onResolve({ filter: /^@prompt-maker\/core$/ }, () => ({
      path: path.join(coreSrcRoot, 'index.ts'),
    }))

    pluginBuild.onResolve({ filter: /^@prompt-maker\/core\/.+$/ }, (args) => {
      const subPath = args.path.slice('@prompt-maker/core/'.length)
      return {
        path: path.join(coreSrcRoot, subPath),
      }
    })
  },
}

const restartPlugin = {
  name: 'restart-on-rebuild',
  setup(pluginBuild) {
    pluginBuild.onEnd((result) => {
      if (result.errors.length > 0) {
        return
      }

      startChild()
    })
  },
}

const ctx = await context({
  entryPoints: [srcEntry],
  outfile: outFile,
  bundle: true,
  packages: 'external',
  platform: 'node',
  target: 'node18',
  format: 'esm',
  sourcemap: true,
  plugins: [promptMakerCoreAliasPlugin, restartPlugin],
  logLevel: 'info',
})

const shutdown = async () => {
  killChild()
  await ctx.dispose()
}

process.on('SIGINT', () => {
  void shutdown().finally(() => process.exit(0))
})
process.on('SIGTERM', () => {
  void shutdown().finally(() => process.exit(0))
})

await ctx.watch()
await ctx.rebuild()
</file>

<file path="src/__tests__/__fixtures__/themes/global/invalid.json">
{ this is not valid json }
</file>

<file path="src/__tests__/__fixtures__/themes/global/ocean.json">
{
  "theme": {
    "background": "#001219",
    "text": "#e9d8a6",
    "mutedText": "#94d2bd",
    "border": "#005f73",
    "accent": "#0a9396",
    "accentText": "#001219",
    "warning": "#ee9b00",
    "error": "#ae2012",
    "success": "#2a9d8f",
    "panelBackground": "#0b2a32",
    "popupBackground": "panelBackground",
    "selectionBackground": "#005f73",
    "selectionText": "#e9d8a6",
    "chipBackground": "panelBackground",
    "chipText": "text",
    "chipMutedText": "mutedText"
  }
}
</file>

<file path="src/__tests__/__fixtures__/themes/global/pm-dark.json">
{
  "defs": {
    "bg": "#111111",
    "text": "#eeeeee",
    "muted": "#777777",
    "border": "#444444",
    "accent": "#ff00ff",
    "warning": "#ffff00",
    "error": "#ff0000",
    "success": "#00ff00",
    "panelBg": "#1a1a1a",
    "selectionBg": "#333333"
  },
  "theme": {
    "background": "bg",
    "text": "text",
    "mutedText": "muted",
    "border": "border",
    "accent": "accent",
    "accentText": "bg",
    "warning": "warning",
    "error": "error",
    "success": "success",
    "panelBackground": "panelBg",
    "popupBackground": "panelBackground",
    "selectionBackground": "selectionBg",
    "selectionText": "text",
    "chipBackground": "panelBackground",
    "chipText": "text",
    "chipMutedText": "mutedText"
  }
}
</file>

<file path="src/__tests__/__fixtures__/themes/project/.prompt-maker-cli/themes/broken-theme.json">
{
  "theme": {
    "background": "#000000"
  }
}
</file>

<file path="src/__tests__/__fixtures__/themes/project/.prompt-maker-cli/themes/forest.json">
{
  "theme": {
    "background": "#0b1f0f",
    "text": "#e7f6e7",
    "mutedText": "#96b69a",
    "border": "#2f4f38",
    "accent": "#3fb950",
    "accentText": "#0b1f0f",
    "warning": "#d29922",
    "error": "#f85149",
    "success": "#3fb950",
    "panelBackground": "#0f2a14",
    "popupBackground": "panelBackground",
    "selectionBackground": "#2f4f38",
    "selectionText": "#e7f6e7",
    "chipBackground": "panelBackground",
    "chipText": "text",
    "chipMutedText": "mutedText"
  }
}
</file>

<file path="src/__tests__/__fixtures__/themes/project/a/.prompt-maker-cli/themes/pm-dark.json">
{
  "defs": {
    "bg": "#000000",
    "text": "#ffffff",
    "accent": "#00ffff",
    "panelBg": "#101010"
  },
  "theme": {
    "background": "bg",
    "text": "text",
    "mutedText": "#888888",
    "border": 240,
    "accent": "accent",
    "accentText": "bg",
    "warning": 220,
    "error": 196,
    "success": 46,
    "panelBackground": "panelBg",
    "popupBackground": { "dark": "panelBackground", "light": "#f6f8fa" },
    "selectionBackground": "accent",
    "selectionText": "bg",
    "chipBackground": "panelBackground",
    "chipText": "text",
    "chipMutedText": "mutedText"
  }
}
</file>

<file path="src/__tests__/tui/theme-loader.test.ts">
import path from 'node:path'
import { loadThemes } from '../../tui/theme/theme-loader'
const FIXTURES_ROOT = path.join(__dirname, '..', '__fixtures__', 'themes')
const GLOBAL_DIR = path.join(FIXTURES_ROOT, 'global')
const PROJECT_ROOT = path.join(FIXTURES_ROOT, 'project')
const PROJECT_CWD = path.join(PROJECT_ROOT, 'a')
describe('theme loader', () => {
  test('merges built-in + global + project with correct precedence', async () => {
    const { themes, errors } = await loadThemes({
      cwd: PROJECT_CWD,
      stopAt: PROJECT_ROOT,
      globalThemesDir: GLOBAL_DIR,
    })
    expect(errors).not.toHaveLength(0)
    const names = themes.map((theme) => theme.name)
    expect(names).toEqual(['pm-dark', 'pm-light', 'forest', 'ocean'])
    const pmDark = themes.find((theme) => theme.name === 'pm-dark')
    expect(pmDark?.source).toBe('project')
    expect(pmDark?.filePath).toContain(
      path.join('a', '.prompt-maker-cli', 'themes', 'pm-dark.json'),
    )
    const ocean = themes.find((theme) => theme.name === 'ocean')
    expect(ocean?.source).toBe('global')
    // Project-local `pm-dark` fixture uses ANSI numbers.
    expect(pmDark?.theme.theme.border).toBe(240)
  })
  test('invalid themes surface errors but do not prevent other themes from loading', async () => {
    const { themes, errors } = await loadThemes({
      cwd: PROJECT_CWD,
      stopAt: PROJECT_ROOT,
      globalThemesDir: GLOBAL_DIR,
    })
    expect(themes.map((theme) => theme.name)).toContain('ocean')
    expect(themes.map((theme) => theme.name)).toContain('forest')
    const errorFiles = errors.map((error) => path.basename(error.filePath))
    expect(errorFiles).toContain('invalid.json')
    expect(errorFiles).toContain('broken-theme.json')
  })
  test('listing is deterministic across calls', async () => {
    const first = await loadThemes({
      cwd: PROJECT_CWD,
      stopAt: PROJECT_ROOT,
      globalThemesDir: GLOBAL_DIR,
    })
    const second = await loadThemes({
      cwd: PROJECT_CWD,
      stopAt: PROJECT_ROOT,
      globalThemesDir: GLOBAL_DIR,
    })
    expect(first.themes.map((theme) => theme.name)).toEqual(
      second.themes.map((theme) => theme.name),
    )
    expect(first.themes.map((theme) => theme.source)).toEqual(
      second.themes.map((theme) => theme.source),
    )
  })
})
</file>

<file path="src/__tests__/tui/theme-persistence.test.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
const readJson = async (filePath: string): Promise<unknown> => {
  const contents = await fs.readFile(filePath, 'utf8')
  return JSON.parse(contents) as unknown
}
const writeJson = async (filePath: string, value: unknown): Promise<void> => {
  await fs.mkdir(path.dirname(filePath), { recursive: true })
  await fs.writeFile(filePath, `${JSON.stringify(value, null, 2)}\n`, 'utf8')
}
describe('theme persistence', () => {
  const envBefore = { ...process.env }
  afterEach(() => {
    process.env = { ...envBefore }
  })
  test('defaults theme + mode when config lacks fields', async () => {
    const tempRoot = await fs.mkdtemp(path.join(os.tmpdir(), 'pm-theme-persist-'))
    const configPath = path.join(tempRoot, 'config.json')
    const globalThemesDir = path.join(tempRoot, 'global-themes')
    await writeJson(configPath, {})
    process.env.PROMPT_MAKER_CLI_CONFIG = configPath
    jest.resetModules()
    const { loadThemeSelection } = await import('../../tui/theme/theme-settings-service')
    const { DEFAULT_THEME_NAME } = await import('../../tui/theme/theme-registry')
    const result = await loadThemeSelection({
      themeLoadOptions: {
        cwd: tempRoot,
        stopAt: tempRoot,
        globalThemesDir,
      },
    })
    expect(result.selection.themeName).toBe(DEFAULT_THEME_NAME)
    expect(result.selection.themeMode).toBe('dark')
    expect(result.warnings).toEqual([])
  })
  test('invalid theme name falls back deterministically with warning', async () => {
    const tempRoot = await fs.mkdtemp(path.join(os.tmpdir(), 'pm-theme-persist-'))
    const configPath = path.join(tempRoot, 'config.json')
    const globalThemesDir = path.join(tempRoot, 'global-themes')
    await writeJson(configPath, { theme: 'not-a-theme' })
    process.env.PROMPT_MAKER_CLI_CONFIG = configPath
    jest.resetModules()
    const { loadThemeSelection } = await import('../../tui/theme/theme-settings-service')
    const { DEFAULT_THEME_NAME } = await import('../../tui/theme/theme-registry')
    const result = await loadThemeSelection({
      themeLoadOptions: {
        cwd: tempRoot,
        stopAt: tempRoot,
        globalThemesDir,
      },
    })
    expect(result.selection.themeName).toBe(DEFAULT_THEME_NAME)
    expect(result.warnings).toHaveLength(1)
    expect(result.warnings[0]?.kind).toBe('unknown-theme')
    expect(result.warnings[0]?.requested).toBe('not-a-theme')
    expect(result.warnings[0]?.fallback).toBe(DEFAULT_THEME_NAME)
  })
  test('auto themeMode is treated as system', async () => {
    const tempRoot = await fs.mkdtemp(path.join(os.tmpdir(), 'pm-theme-persist-'))
    const configPath = path.join(tempRoot, 'config.json')
    const globalThemesDir = path.join(tempRoot, 'global-themes')
    await writeJson(configPath, { themeMode: 'auto' })
    process.env.PROMPT_MAKER_CLI_CONFIG = configPath
    jest.resetModules()
    const { loadThemeSelection } = await import('../../tui/theme/theme-settings-service')
    const result = await loadThemeSelection({
      themeLoadOptions: {
        cwd: tempRoot,
        stopAt: tempRoot,
        globalThemesDir,
      },
    })
    expect(result.selection.themeMode).toBe('system')
  })
  test('saving updates config without rewriting other fields', async () => {
    const tempRoot = await fs.mkdtemp(path.join(os.tmpdir(), 'pm-theme-persist-'))
    const configPath = path.join(tempRoot, 'config.json')
    await writeJson(configPath, {
      openaiApiKey: 'keep-me',
      theme: 'pm-dark',
      themeMode: 'dark',
      promptGenerator: { defaultModel: 'gpt-4o' },
    })
    process.env.PROMPT_MAKER_CLI_CONFIG = configPath
    jest.resetModules()
    const { saveThemeSelection } = await import('../../tui/theme/theme-settings-service')
    await saveThemeSelection({ themeName: 'pm-light', themeMode: 'light' })
    const updated = await readJson(configPath)
    expect(updated).toMatchObject({
      openaiApiKey: 'keep-me',
      theme: 'pm-light',
      themeMode: 'light',
      promptGenerator: { defaultModel: 'gpt-4o' },
    })
  })
})
</file>

<file path="src/__tests__/tui/theme-provider.test.ts">
import React from 'react'
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import type { ThemeDescriptor } from '../../tui/theme/theme-loader'
import { ThemeProvider, useTheme } from '../../tui/theme/theme-provider'
import { loadThemeSelection, saveThemeSelection } from '../../tui/theme/theme-settings-service'
const pmDarkTheme: ThemeDescriptor = {
  name: 'pm-dark',
  label: 'Prompt Maker Dark',
  source: 'builtin',
  theme: {
    theme: {
      background: { dark: '#000000', light: '#ffffff' },
      text: '#ffffff',
      mutedText: '#888888',
      border: '#444444',
      accent: '#00ffff',
      accentText: '#000000',
      warning: '#ffff00',
      error: '#ff0000',
      success: '#00ff00',
      panelBackground: '#111111',
      popupBackground: 'panelBackground',
      selectionBackground: '#333333',
      selectionText: '#ffffff',
      chipBackground: '#222222',
      chipText: '#ffffff',
      chipMutedText: '#aaaaaa',
    },
  },
}
const pmLightTheme: ThemeDescriptor = {
  name: 'pm-light',
  label: 'Prompt Maker Light',
  source: 'builtin',
  theme: {
    theme: {
      background: '#ffffff',
      text: '#000000',
      mutedText: '#555555',
      border: '#dddddd',
      accent: '#0000ff',
      accentText: '#ffffff',
      warning: '#ff8800',
      error: '#ff0000',
      success: '#00aa00',
      panelBackground: '#f6f8fa',
      popupBackground: 'panelBackground',
      selectionBackground: '#ddddff',
      selectionText: '#000000',
      chipBackground: '#f6f8fa',
      chipText: '#000000',
      chipMutedText: '#555555',
    },
  },
}
jest.mock('../../tui/theme/theme-settings-service')
const dom = new JSDOM('<!doctype html><html><body></body></html>')
type GlobalDom = { window: Window; document: Document }
beforeAll(() => {
  const target = globalThis as unknown as GlobalDom
  target.window = dom.window as unknown as Window
  target.document = dom.window.document
})
afterAll(() => {
  const target = globalThis as unknown as Partial<GlobalDom>
  delete target.window
  delete target.document
})
describe('ThemeProvider', () => {
  beforeEach(() => {
    const mockedLoad = jest.mocked(loadThemeSelection)
    mockedLoad.mockResolvedValue({
      themes: [pmDarkTheme, pmLightTheme],
      loadErrors: [],
      selection: { themeName: 'pm-dark', themeMode: 'dark' },
      warnings: [],
    })
    const mockedSave = jest.mocked(saveThemeSelection)
    mockedSave.mockResolvedValue()
  })
  test('previewTheme swaps without persisting', async () => {
    const wrapper: React.FC<{ children: React.ReactNode }> = ({ children }) =>
      React.createElement(ThemeProvider, null, children)
    const { result } = renderHook(() => useTheme(), { wrapper })
    await act(async () => {
      await Promise.resolve()
    })
    expect(result.current.activeThemeName).toBe('pm-dark')
    act(() => {
      const ok = result.current.previewTheme('pm-light')
      expect(ok).toBe(true)
    })
    expect(result.current.activeThemeName).toBe('pm-light')
    expect(jest.mocked(saveThemeSelection)).not.toHaveBeenCalled()
    act(() => {
      const ok = result.current.previewTheme('pm-dark')
      expect(ok).toBe(true)
    })
    expect(result.current.activeThemeName).toBe('pm-dark')
    expect(jest.mocked(saveThemeSelection)).not.toHaveBeenCalled()
  })
  test('setTheme re-resolves and persists', async () => {
    const wrapper: React.FC<{ children: React.ReactNode }> = ({ children }) =>
      React.createElement(ThemeProvider, null, children)
    const { result } = renderHook(() => useTheme(), { wrapper })
    await act(async () => {
      await Promise.resolve()
    })
    await act(async () => {
      const ok = await result.current.setTheme('pm-light')
      expect(ok).toBe(true)
    })
    expect(result.current.activeThemeName).toBe('pm-light')
    expect(result.current.theme.background).toBe('#ffffff')
    expect(jest.mocked(saveThemeSelection)).toHaveBeenCalledWith({ themeName: 'pm-light' })
  })
  test('setMode re-resolves and persists', async () => {
    const wrapper: React.FC<{ children: React.ReactNode }> = ({ children }) =>
      React.createElement(ThemeProvider, null, children)
    const { result } = renderHook(() => useTheme(), { wrapper })
    await act(async () => {
      await Promise.resolve()
    })
    expect(result.current.mode).toBe('dark')
    await act(async () => {
      const ok = await result.current.setMode('light')
      expect(ok).toBe(true)
    })
    expect(result.current.mode).toBe('light')
    expect(result.current.theme.background).toBe('#ffffff')
    expect(jest.mocked(saveThemeSelection)).toHaveBeenCalledWith({ themeMode: 'light' })
  })
  test('system mode uses env detection with dark fallback', async () => {
    const envBefore = { ...process.env }
    process.env = { ...envBefore, COLORFGBG: '15;7' }
    const wrapper: React.FC<{ children: React.ReactNode }> = ({ children }) =>
      React.createElement(ThemeProvider, null, children)
    const { result } = renderHook(() => useTheme(), { wrapper })
    await act(async () => {
      await Promise.resolve()
    })
    await act(async () => {
      const ok = await result.current.setMode('system')
      expect(ok).toBe(true)
    })
    expect(result.current.mode).toBe('system')
    expect(result.current.theme.background).toBe('#ffffff')
    expect(jest.mocked(saveThemeSelection)).toHaveBeenCalledWith({ themeMode: 'system' })
    process.env = envBefore
  })
})
</file>

<file path="src/__tests__/tui/theme-registry.test.ts">
import { DEFAULT_THEME_NAME, getTheme, listThemes } from '../../tui/theme/theme-registry'
describe('theme registry', () => {
  test('returns built-in themes deterministically', () => {
    const first = listThemes().map((theme) => theme.name)
    const second = listThemes().map((theme) => theme.name)
    expect(first).toEqual(second)
    expect(first).toEqual(['pm-dark', 'pm-light'])
  })
  test('default theme exists in registry', () => {
    const names = listThemes().map((theme) => theme.name)
    expect(names).toContain(DEFAULT_THEME_NAME)
    expect(getTheme(DEFAULT_THEME_NAME)).toBeDefined()
  })
})
</file>

<file path="src/__tests__/command-filter.test.ts">
import { filterCommandDescriptors, resolveCommandMenuSearchState } from '../tui/command-filter'
import { COMMAND_DESCRIPTORS } from '../tui/config'
describe('command filtering', () => {
  it('returns all commands for an empty query', () => {
    const result = filterCommandDescriptors({ query: '   ', commands: COMMAND_DESCRIPTORS })
    expect(result.map((command) => command.id)).toEqual(COMMAND_DESCRIPTORS.map((c) => c.id))
  })
  it('matches aliases case-insensitively', () => {
    const result = filterCommandDescriptors({ query: 'MeTa', commands: COMMAND_DESCRIPTORS })
    expect(result.map((command) => command.id)).toEqual(['instructions'])
  })
  it('matches by description tokens when not a command prefix', () => {
    const state = resolveCommandMenuSearchState({
      commandQuery: 'prompt tests',
      commands: COMMAND_DESCRIPTORS,
    })
    expect(state.treatRemainderAsArgs).toBe(false)
    const result = filterCommandDescriptors({
      query: state.filterQuery,
      commands: COMMAND_DESCRIPTORS,
    })
    expect(result.map((command) => command.id)).toContain('test')
  })
  it('treats the remainder as args when there is a prefix match', () => {
    const state = resolveCommandMenuSearchState({
      commandQuery: 'test prompt-tests.yaml',
      commands: COMMAND_DESCRIPTORS,
    })
    expect(state).toEqual({ filterQuery: 'test', treatRemainderAsArgs: true })
    const result = filterCommandDescriptors({
      query: state.filterQuery,
      commands: COMMAND_DESCRIPTORS,
    })
    expect(result.map((command) => command.id)).toContain('test')
  })
  it('prefers id/alias/label prefix matches', () => {
    const result = filterCommandDescriptors({ query: 'why', commands: COMMAND_DESCRIPTORS })
    expect(result.map((command) => command.id)[0]).toBe('reasoning')
  })
  it('matches substrings in label and description', () => {
    const result = filterCommandDescriptors({ query: 'switch', commands: COMMAND_DESCRIPTORS })
    expect(result.map((command) => command.id)).toContain('model')
  })
})
</file>

<file path="src/__tests__/command-screen-reducer.test.ts">
import {
  commandScreenReducer,
  createInitialCommandScreenState,
} from '../tui/screens/command/command-screen-reducer'
describe('commandScreenReducer', () => {
  it('sets terminal size and avoids needless changes', () => {
    const initial = createInitialCommandScreenState({ terminalRows: 24, terminalColumns: 80 })
    const same = commandScreenReducer(initial, {
      type: 'set-terminal-size',
      rows: 24,
      columns: 80,
    })
    expect(same).toBe(initial)
    const next = commandScreenReducer(initial, {
      type: 'set-terminal-size',
      rows: 40,
      columns: 120,
    })
    expect(next).toEqual({ ...initial, terminalRows: 40, terminalColumns: 120 })
  })
  it('supports functional input updates', () => {
    const initial = createInitialCommandScreenState({ terminalRows: 24, terminalColumns: 80 })
    const next = commandScreenReducer(initial, {
      type: 'set-input',
      next: (prev) => `${prev}hello`,
    })
    expect(next.inputValue).toBe('hello')
  })
  it('supports functional command selection updates', () => {
    const initial = createInitialCommandScreenState({ terminalRows: 24, terminalColumns: 80 })
    const next = commandScreenReducer(initial, {
      type: 'set-command-selection',
      next: (prev) => prev + 2,
    })
    expect(next.commandSelectionIndex).toBe(2)
  })
})
</file>

<file path="src/__tests__/context-smart-root.test.ts">
import React from 'react'
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import { ContextProvider } from '../tui/context'
import { useContextDispatch, useContextState } from '../tui/context-store'
const dom = new JSDOM('<!doctype html><html><body></body></html>')
const globalScope = globalThis as typeof globalThis & {
  window: Window & typeof globalThis
  document: Document
  navigator: Navigator
}
globalScope.window = dom.window
globalScope.document = dom.window.document
globalScope.navigator = dom.window.navigator
const wrapper: React.FC<{ children: React.ReactNode }> = ({ children }) =>
  React.createElement(ContextProvider, null, children)
describe('smart context root', () => {
  it('overwrites instead of accumulating roots', () => {
    const { result } = renderHook(
      () => ({ state: useContextState(), dispatch: useContextDispatch() }),
      { wrapper },
    )
    act(() => {
      result.current.dispatch.setSmartRoot('src')
    })
    expect(result.current.state.smartContextRoot).toBe('src')
    act(() => {
      result.current.dispatch.setSmartRoot('apps')
    })
    expect(result.current.state.smartContextRoot).toBe('apps')
  })
  it('clears the root when passed empty input', () => {
    const { result } = renderHook(
      () => ({ state: useContextState(), dispatch: useContextDispatch() }),
      { wrapper },
    )
    act(() => {
      result.current.dispatch.setSmartRoot('src')
    })
    expect(result.current.state.smartContextRoot).toBe('src')
    act(() => {
      result.current.dispatch.setSmartRoot('')
    })
    expect(result.current.state.smartContextRoot).toBeNull()
  })
})
</file>

<file path="src/__tests__/drag-drop-path.test.ts">
import { isCommandInput, parseAbsolutePathFromInput } from '../tui/drag-drop-path'
describe('drag-drop-path', () => {
  describe('parseAbsolutePathFromInput', () => {
    it('parses a plain posix absolute path', () => {
      expect(parseAbsolutePathFromInput('/Users/alice/file.txt')).toBe('/Users/alice/file.txt')
    })
    it('parses a quoted absolute path with spaces', () => {
      expect(parseAbsolutePathFromInput('"/Users/alice/My File.md"')).toBe(
        '/Users/alice/My File.md',
      )
    })
    it('parses a backslash-escaped absolute path with spaces', () => {
      expect(parseAbsolutePathFromInput('/Users/alice/My\\ File.md')).toBe(
        '/Users/alice/My File.md',
      )
    })
    it('returns null when additional tokens exist', () => {
      expect(parseAbsolutePathFromInput('/file arg')).toBeNull()
    })
    it('returns null for relative paths', () => {
      expect(parseAbsolutePathFromInput('src/index.ts')).toBeNull()
    })
    it('preserves Windows-style absolute paths', () => {
      expect(parseAbsolutePathFromInput('C:\\Users\\alice\\file.txt')).toBe(
        'C:\\Users\\alice\\file.txt',
      )
    })
    it('parses pasted paths with bracketed paste markers', () => {
      expect(parseAbsolutePathFromInput("[200~'/Users/alice/My File.md'[201~")).toBe(
        '/Users/alice/My File.md',
      )
    })
    it('parses pasted paths with stray sgr fragments', () => {
      expect(parseAbsolutePathFromInput("[200~'/Users/alice/My File.md'[7m")).toBe(
        '/Users/alice/My File.md',
      )
    })
  })
  describe('isCommandInput', () => {
    const existsSync = (candidate: string): boolean => candidate === '/Users'
    it('treats / (slash alone) as a command opener', () => {
      expect(isCommandInput('/', existsSync)).toBe(true)
    })
    it('treats command keywords as command input', () => {
      expect(isCommandInput('/files', existsSync)).toBe(true)
    })
    it('treats nested absolute paths as non-command input', () => {
      expect(isCommandInput('/tmp/foo', existsSync)).toBe(false)
    })
    it('treats existing root-level paths as non-command input', () => {
      expect(isCommandInput('/Users', existsSync)).toBe(false)
    })
    it('treats /keyword with args as command input', () => {
      expect(isCommandInput('/file src/index.ts', existsSync)).toBe(true)
    })
  })
})
</file>

<file path="src/__tests__/file-context.test.ts">
import { resolveFileContext, formatContextForPrompt } from '../file-context'
import fg from 'fast-glob'
jest.mock('fast-glob')
const globMock = fg as unknown as jest.MockedFunction<typeof fg>
jest.mock('node:fs/promises', () => ({ readFile: jest.fn() }))
const fs = jest.requireMock('node:fs/promises') as { readFile: jest.Mock }
describe('file-context', () => {
  beforeEach(() => {
    jest.clearAllMocks()
  })
  it('returns empty array when patterns list is empty', async () => {
    const result = await resolveFileContext([])
    expect(result).toEqual([])
    expect(fg).not.toHaveBeenCalled()
  })
  it('resolves matching files into FileContext entries', async () => {
    globMock.mockResolvedValue(['a.txt', 'b.txt'])
    fs.readFile.mockResolvedValueOnce('alpha').mockResolvedValueOnce('beta')
    const result = await resolveFileContext(['*.txt'])
    expect(result).toEqual([
      { path: 'a.txt', content: 'alpha' },
      { path: 'b.txt', content: 'beta' },
    ])
  })
  it('warns and returns [] when no files matched', async () => {
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    globMock.mockResolvedValue([])
    const result = await resolveFileContext(['none.txt'])
    expect(result).toEqual([])
    expect(warn).toHaveBeenCalledWith(
      expect.stringContaining('No files matched the context patterns'),
    )
    warn.mockRestore()
  })
  it('skips files that fail to read', async () => {
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    globMock.mockResolvedValue(['ok.txt', 'bad.txt'])
    fs.readFile.mockResolvedValueOnce('good').mockRejectedValueOnce(new Error('boom'))
    const result = await resolveFileContext(['*.txt'])
    expect(result).toEqual([{ path: 'ok.txt', content: 'good' }])
    expect(warn).toHaveBeenCalledWith('Warning: Failed to read context file bad.txt')
    warn.mockRestore()
  })
  it('formats context with XML-like wrappers', () => {
    const text = formatContextForPrompt([
      { path: 'src/a.ts', content: 'const a = 1' },
      { path: 'src/b.ts', content: 'const b = 2' },
    ])
    expect(text).toContain('<file path="src/a.ts">')
    expect(text).toContain('</file>\n\n<file path="src/b.ts">')
  })
  it('returns empty string when no files provided to formatter', () => {
    expect(formatContextForPrompt([])).toBe('')
  })
})
</file>

<file path="src/__tests__/generation-pipeline-reducer.test.ts">
import {
  generationPipelineReducer,
  INITIAL_GENERATION_PIPELINE_STATE,
  type GenerationPipelineState,
} from '../tui/generation-pipeline-reducer'
describe('generationPipelineReducer', () => {
  const reduce = (
    state: GenerationPipelineState,
    action: Parameters<typeof generationPipelineReducer>[1],
  ): GenerationPipelineState => generationPipelineReducer(state, action)
  it('starts generation and clears interactive state', () => {
    const next = reduce(
      { ...INITIAL_GENERATION_PIPELINE_STATE, awaitingInteractiveMode: 'transport' },
      { type: 'generation-start', statusMessage: 'Preparing…' },
    )
    expect(next.isGenerating).toBe(true)
    expect(next.statusMessage).toBe('Preparing…')
    expect(next.awaitingInteractiveMode).toBeNull()
    expect(next.isAwaitingRefinement).toBe(false)
    expect(next.latestTelemetry).toBeNull()
  })
  it('stops generation and clears awaiting flags', () => {
    const next = reduce(
      {
        ...INITIAL_GENERATION_PIPELINE_STATE,
        isGenerating: true,
        statusMessage: 'Working…',
        awaitingInteractiveMode: 'tty',
        isAwaitingRefinement: true,
      },
      { type: 'generation-stop', statusMessage: 'Complete' },
    )
    expect(next.isGenerating).toBe(false)
    expect(next.statusMessage).toBe('Complete')
    expect(next.awaitingInteractiveMode).toBeNull()
    expect(next.isAwaitingRefinement).toBe(false)
  })
  it('updates interactive awaiting mode and status together', () => {
    const next = reduce(INITIAL_GENERATION_PIPELINE_STATE, {
      type: 'set-awaiting-interactive',
      awaitingInteractiveMode: 'transport',
      statusMessage: 'Waiting…',
    })
    expect(next.awaitingInteractiveMode).toBe('transport')
    expect(next.statusMessage).toBe('Waiting…')
  })
  it('does not overwrite status when updating awaiting mode without a message', () => {
    const next = reduce(
      { ...INITIAL_GENERATION_PIPELINE_STATE, statusMessage: 'Keep me' },
      { type: 'set-awaiting-interactive', awaitingInteractiveMode: null },
    )
    expect(next.awaitingInteractiveMode).toBeNull()
    expect(next.statusMessage).toBe('Keep me')
  })
})
</file>

<file path="src/__tests__/github-context.test.ts">
import { resolveGithubUrl } from '../github-context'
describe('resolveGithubUrl', () => {
  const originalFetch = global.fetch
  const mockFetch = jest.fn() as jest.MockedFunction<typeof fetch>
  beforeEach(() => {
    mockFetch.mockReset()
    global.fetch = mockFetch
  })
  afterAll(() => {
    global.fetch = originalFetch
  })
  it('fetches blob URLs via raw content', async () => {
    mockFetch.mockResolvedValueOnce(createBufferResponse(Buffer.from('console.log("hi")')))
    const files = await resolveGithubUrl(
      new URL('https://github.com/org/repo/blob/main/src/index.ts'),
    )
    expect(mockFetch).toHaveBeenCalledWith(
      expect.stringContaining('raw.githubusercontent.com'),
      expect.any(Object),
    )
    expect(files).toEqual([
      {
        path: 'github:org/repo/src/index.ts',
        content: 'console.log("hi")',
      },
    ])
  })
  it('fetches tree URLs and filters ignored files', async () => {
    mockFetch.mockImplementation(async (input) => {
      const url = typeof input === 'string' ? input : input.toString()
      if (url.includes('/git/trees/HEAD')) {
        return createJsonResponse({
          tree: [
            { path: 'README.md', type: 'blob', size: 1000 },
            { path: 'node_modules/pkg/index.js', type: 'blob', size: 10 },
            { path: 'src/app.ts', type: 'blob', size: 500 },
          ],
        })
      }
      if (url.includes('/contents/README.md')) {
        return createJsonResponse({
          content: Buffer.from('# Readme').toString('base64'),
          encoding: 'base64',
        })
      }
      if (url.includes('/contents/src/app.ts')) {
        return createJsonResponse({
          content: Buffer.from('export const hi = true').toString('base64'),
          encoding: 'base64',
        })
      }
      throw new Error(`Unexpected fetch ${url}`)
    })
    const files = await resolveGithubUrl(new URL('https://github.com/org/repo'))
    expect(files).toEqual([
      { path: 'github:org/repo/README.md', content: '# Readme' },
      { path: 'github:org/repo/src/app.ts', content: 'export const hi = true' },
    ])
  })
})
const createBufferResponse = (buffer: Buffer, status = 200): Response =>
  ({
    ok: status >= 200 && status < 300,
    status,
    headers: new Headers(),
    arrayBuffer: async () => buffer,
  }) as unknown as Response
const createJsonResponse = (body: unknown, status = 200): Response =>
  ({
    ok: status >= 200 && status < 300,
    status,
    headers: new Headers({ 'content-type': 'application/json' }),
    json: async () => body,
  }) as unknown as Response
</file>

<file path="src/__tests__/help-overlay-scroll.test.ts">
import {
  clampHelpOverlayScrollOffset,
  getHelpOverlayContentRows,
  getHelpOverlayMaxScroll,
  scrollHelpOverlayBy,
} from '../tui/components/core/help-overlay-scroll'
describe('help-overlay-scroll', () => {
  it('computes content rows accounting for header/footer spacing', () => {
    expect(getHelpOverlayContentRows(10)).toBe(5)
    expect(getHelpOverlayContentRows(5)).toBe(1)
  })
  it('clamps scroll offset to valid range', () => {
    const contentRows = 5
    expect(getHelpOverlayMaxScroll(5, contentRows)).toBe(0)
    expect(getHelpOverlayMaxScroll(8, contentRows)).toBe(3)
    expect(clampHelpOverlayScrollOffset(-1, 8, contentRows)).toBe(0)
    expect(clampHelpOverlayScrollOffset(0, 8, contentRows)).toBe(0)
    expect(clampHelpOverlayScrollOffset(3, 8, contentRows)).toBe(3)
    expect(clampHelpOverlayScrollOffset(4, 8, contentRows)).toBe(3)
  })
  it('scrolls by delta and clamps at edges', () => {
    const contentRows = 5
    const lineCount = 8
    expect(scrollHelpOverlayBy(0, 1, lineCount, contentRows)).toBe(1)
    expect(scrollHelpOverlayBy(0, -1, lineCount, contentRows)).toBe(0)
    expect(scrollHelpOverlayBy(3, 1, lineCount, contentRows)).toBe(3)
    expect(scrollHelpOverlayBy(0, contentRows, lineCount, contentRows)).toBe(3)
  })
})
</file>

<file path="src/__tests__/history-logger.test.ts">
import { appendToHistory } from '../history-logger'
jest.mock('node:fs/promises', () => ({
  mkdir: jest.fn(),
  appendFile: jest.fn(),
}))
const fs = jest.requireMock('node:fs/promises') as { mkdir: jest.Mock; appendFile: jest.Mock }
describe('history-logger', () => {
  beforeEach(() => {
    jest.clearAllMocks()
  })
  it('creates the history directory and appends a line', async () => {
    fs.mkdir.mockResolvedValue(undefined)
    fs.appendFile.mockResolvedValue(undefined)
    await appendToHistory({ intent: 'demo' })
    expect(fs.mkdir).toHaveBeenCalledWith(expect.stringContaining('.config'), { recursive: true })
    expect(fs.appendFile).toHaveBeenCalledWith(
      expect.stringContaining('history.jsonl'),
      expect.stringContaining('"intent":"demo"'),
      'utf8',
    )
  })
  it('logs a warning when append fails but does not throw', async () => {
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    fs.mkdir.mockResolvedValue(undefined)
    fs.appendFile.mockRejectedValue(new Error('disk full'))
    await appendToHistory({ intent: 'demo' })
    expect(warn).toHaveBeenCalledWith('Failed to write history entry:', expect.any(Error))
    warn.mockRestore()
  })
})
</file>

<file path="src/__tests__/image-loader.test.ts">
import { resolveImageParts } from '../image-loader'
jest.mock('node:fs/promises', () => ({
  readFile: jest.fn(),
}))
const fs = jest.requireMock('node:fs/promises') as { readFile: jest.Mock }
describe('image-loader', () => {
  beforeEach(() => {
    jest.clearAllMocks()
  })
  it('skips unsupported extensions with a warning', async () => {
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    const parts = await resolveImageParts(['doc.txt'])
    expect(parts).toEqual([])
    expect(warn).toHaveBeenCalledWith('Skipping unsupported image type: doc.txt')
    warn.mockRestore()
  })
  it('returns base64 encoded part for supported image', async () => {
    const buffer = Buffer.from('image-bytes')
    fs.readFile.mockResolvedValue(buffer)
    const onUpload = jest.fn()
    const parts = await resolveImageParts(['photo.png'], onUpload)
    expect(onUpload).toHaveBeenNthCalledWith(1, 'start', { kind: 'image', filePath: 'photo.png' })
    expect(onUpload).toHaveBeenLastCalledWith('finish', { kind: 'image', filePath: 'photo.png' })
    expect(parts).toEqual([
      {
        type: 'image',
        mimeType: 'image/png',
        data: buffer.toString('base64'),
      },
    ])
  })
  it('skips images larger than limit', async () => {
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    const buffer = Buffer.alloc(25 * 1024 * 1024)
    fs.readFile.mockResolvedValue(buffer)
    const parts = await resolveImageParts(['large.jpg'])
    expect(parts).toEqual([])
    expect(warn).toHaveBeenCalledWith(expect.stringContaining('Skipping image too large'))
    warn.mockRestore()
  })
  it('logs and skips unreadable files', async () => {
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    fs.readFile.mockRejectedValue(new Error('nope'))
    const parts = await resolveImageParts(['broken.png'])
    expect(parts).toEqual([])
    expect(warn).toHaveBeenCalledWith('Failed to read image broken.png:', expect.any(Error))
    warn.mockRestore()
  })
})
</file>

<file path="src/__tests__/input-bar.test.ts">
import { estimateInputBarRows } from '../tui/components/core/input-bar-layout'
import { resolveInputBarPresentation } from '../tui/components/core/input-bar-presentation'
describe('InputBar presentation', () => {
  it('renders intent mode styling by default', () => {
    expect(resolveInputBarPresentation('intent')).toEqual({
      borderTone: 'default',
      label: 'Intent / Command',
      labelTone: 'muted',
      labelBold: false,
    })
  })
  it('renders refinement mode with prominent styling', () => {
    expect(resolveInputBarPresentation('refinement')).toEqual({
      borderTone: 'warning',
      label: 'Refinement (Enter to submit · empty to finish)',
      labelTone: 'warning',
      labelBold: true,
    })
  })
  it('estimates rows without border padding', () => {
    expect(
      estimateInputBarRows({
        value: 'one\ntwo\nthree',
      }),
    ).toBe(5)
    expect(
      estimateInputBarRows({
        value: Array.from({ length: 10 }, (_, index) => `line-${index}`).join('\n'),
      }),
    ).toBe(12)
  })
})
</file>

<file path="src/__tests__/intent-source.test.ts">
import { resolveIntentSource } from '../tui/intent-source'
describe('resolveIntentSource', () => {
  it('returns text intent when no file path is provided', () => {
    const result = resolveIntentSource('  draft prompt ', '   ')
    expect(result).toEqual({ kind: 'text', intent: 'draft prompt' })
  })
  it('prefers the file path when both are provided', () => {
    const result = resolveIntentSource('typed intent', ' ./intent.md ')
    expect(result).toEqual({ kind: 'file', intentFile: './intent.md' })
  })
  it('returns empty when both sources are blank', () => {
    const result = resolveIntentSource('   ', '')
    expect(result).toEqual({ kind: 'empty' })
  })
})
</file>

<file path="src/__tests__/io.test.ts">
import { EventEmitter } from 'node:events'
import { readFromStdin } from '../io'
describe('readFromStdin', () => {
  const originalStdin = process.stdin
  afterEach(() => {
    Object.defineProperty(process, 'stdin', { value: originalStdin, configurable: true })
  })
  it('returns null when stdin is a TTY', async () => {
    Object.defineProperty(process, 'stdin', {
      value: { isTTY: true } as NodeJS.ReadStream,
      configurable: true,
    })
    await expect(readFromStdin()).resolves.toBeNull()
  })
  it('collects data chunks when stdin is piped', async () => {
    class MockStdin extends EventEmitter {
      isTTY = false
      on(event: string, listener: (...args: unknown[]) => void): this {
        super.on(event, listener)
        return this
      }
    }
    const mock = new MockStdin()
    Object.defineProperty(process, 'stdin', { value: mock, configurable: true })
    const promise = readFromStdin()
    mock.emit('data', 'hello ')
    mock.emit('data', Buffer.from('world'))
    mock.emit('end')
    await expect(promise).resolves.toBe('hello world')
  })
  it('rejects when stdin emits an error', async () => {
    class MockStdin extends EventEmitter {
      isTTY = false
      on(event: string, listener: (...args: unknown[]) => void): this {
        super.on(event, listener)
        return this
      }
    }
    const mock = new MockStdin()
    Object.defineProperty(process, 'stdin', { value: mock, configurable: true })
    const promise = readFromStdin()
    mock.emit('error', new Error('boom'))
    await expect(promise).rejects.toThrow('boom')
  })
})
</file>

<file path="src/__tests__/model-filter.test.ts">
import { filterModelOptions, resolveModelPopupQuery } from '../tui/model-filter'
import type { ModelOption } from '../tui/types'
const createOption = (id: string, label: string): ModelOption => ({
  id,
  label,
  provider: 'openai',
  description: `${label} description`,
  capabilities: [],
  source: 'builtin',
})
describe('model-filter helpers', () => {
  it('resets to empty query when input is blank', () => {
    expect(resolveModelPopupQuery('', 'stale')).toBe('')
    expect(resolveModelPopupQuery('   ', 'stale')).toBe('')
  })
  it('keeps debounced query when input is non-empty', () => {
    expect(resolveModelPopupQuery('g', 'gpt')).toBe('gpt')
  })
  it('returns all models when query is blank', () => {
    const options = [createOption('gpt-4o-mini', 'GPT-4o Mini'), createOption('gpt-4o', 'GPT-4o')]
    expect(filterModelOptions('', options).map((option) => option.id)).toEqual([
      'gpt-4o-mini',
      'gpt-4o',
    ])
  })
  it('filters models by substring match', () => {
    const options = [
      createOption('gpt-4o-mini', 'GPT-4o Mini'),
      createOption('gemini-1.5-pro', 'Gemini 1.5 Pro'),
    ]
    expect(filterModelOptions('gem', options).map((option) => option.id)).toEqual([
      'gemini-1.5-pro',
    ])
  })
  it('caps results when a limit is provided', () => {
    const options = [
      createOption('model-a', 'Model A'),
      createOption('model-b', 'Model B'),
      createOption('model-c', 'Model C'),
    ]
    expect(filterModelOptions('model', options, 2).map((option) => option.id)).toEqual([
      'model-a',
      'model-b',
    ])
  })
  it('supports fuzzy subsequence matching', () => {
    const options = [
      createOption('gpt-4o-mini', 'GPT-4o Mini'),
      createOption('gemini-1.5-pro', 'Gemini 1.5 Pro'),
    ]
    expect(filterModelOptions('gmp', options).map((option) => option.id)).toEqual([
      'gemini-1.5-pro',
    ])
  })
  it('supports multi-token matching', () => {
    const options = [
      createOption('gpt-4o-mini', 'GPT-4o Mini'),
      createOption('gemini-1.5-pro', 'Gemini 1.5 Pro'),
    ]
    expect(filterModelOptions('gem pro', options).map((option) => option.id)).toEqual([
      'gemini-1.5-pro',
    ])
  })
})
</file>

<file path="src/__tests__/model-options.test.ts">
const mockLoadCliConfig = jest.fn()
jest.mock('../config', () => ({
  loadCliConfig: mockLoadCliConfig,
}))
describe('model-options helper', () => {
  beforeEach(() => {
    jest.resetModules()
    mockLoadCliConfig.mockReset()
  })
  it('merges config-defined models with built-ins', async () => {
    mockLoadCliConfig.mockResolvedValue({
      promptGenerator: {
        models: [
          { id: 'custom-model', label: 'Custom', provider: 'openai', capabilities: ['fast'] },
          { id: 'gpt-4o-mini', notes: 'override builtin description' },
        ],
      },
    })
    const module = await import('../tui/model-options')
    const result = await module.loadModelOptions()
    const custom = result.options.find((option) => option.id === 'custom-model')
    expect(custom).toBeDefined()
    expect(custom?.source).toBe('config')
    expect(custom?.capabilities).toContain('fast')
    const overridden = result.options.find((option) => option.id === 'gpt-4o-mini')
    expect(overridden?.notes).toBe('override builtin description')
  })
  it('falls back to built-ins when config load fails', async () => {
    mockLoadCliConfig.mockRejectedValue(new Error('bad config'))
    const module = await import('../tui/model-options')
    const result = await module.loadModelOptions()
    expect(result.warning).toMatch(/Failed to load CLI model entries/)
    expect(result.options.length).toBeGreaterThan(0)
  })
  it('selects preferred model id with fallback logic', async () => {
    mockLoadCliConfig.mockResolvedValue({ promptGenerator: { models: [] } })
    const module = await import('../tui/model-options')
    const builtIns = module.getBuiltInModelOptions()
    const requested = module.getPreferredModelId(builtIns, 'does-not-exist')
    expect(builtIns.map((option) => option.id)).toContain(requested)
  })
})
</file>

<file path="src/__tests__/model-popup-options.test.ts">
import { buildModelPopupOptions } from '../tui/model-popup-options'
import type { ModelOption } from '../tui/types'
const createOption = (id: string, provider: ModelOption['provider']): ModelOption => ({
  id,
  label: id,
  provider,
  description: id,
  capabilities: [],
  source: 'builtin',
})
describe('buildModelPopupOptions', () => {
  it('prepends recent models when query is empty', () => {
    const modelOptions: ModelOption[] = [
      createOption('gpt-4o-mini', 'openai'),
      createOption('gemini-1.5-pro', 'gemini'),
      createOption('other-model', 'other'),
    ]
    const result = buildModelPopupOptions({
      query: '',
      modelOptions,
      recentModelIds: ['gemini-1.5-pro', 'missing-model', 'gpt-4o-mini'],
    })
    expect(result.recentCount).toBe(2)
    expect(result.options.map((option) => option.id)).toEqual([
      'gemini-1.5-pro',
      'gpt-4o-mini',
      'other-model',
    ])
  })
  it('groups filtered options by provider while preserving intra-provider score order', () => {
    const modelOptions: ModelOption[] = [
      createOption('gpt-4o-mini', 'openai'),
      createOption('gpt-4o', 'openai'),
      createOption('gemini-1.5-pro', 'gemini'),
      createOption('gemini-1.5-flash', 'gemini'),
    ]
    const result = buildModelPopupOptions({ query: 'gpt mini', modelOptions })
    expect(result.recentCount).toBe(0)
    expect(result.options.map((option) => option.id)).toEqual(['gpt-4o-mini'])
  })
})
</file>

<file path="src/__tests__/model-session.test.ts">
import {
  getLastSessionModel,
  getRecentSessionModels,
  recordRecentSessionModel,
  resetLastSessionModelForTests,
  resetRecentSessionModelsForTests,
  setLastSessionModel,
} from '../tui/model-session'
describe('model-session helpers', () => {
  afterEach(() => {
    resetLastSessionModelForTests()
  })
  it('stores and retrieves the last session model id', () => {
    resetLastSessionModelForTests()
    expect(getLastSessionModel()).toBeNull()
    setLastSessionModel('gpt-4o-mini')
    expect(getLastSessionModel()).toBe('gpt-4o-mini')
  })
  it('clears the session value when set to empty', () => {
    setLastSessionModel('   ')
    expect(getLastSessionModel()).toBeNull()
  })
})
describe('model-session recent models', () => {
  beforeEach(() => {
    resetRecentSessionModelsForTests()
  })
  it('records recent models most-recent first', () => {
    recordRecentSessionModel('gpt-4o-mini')
    recordRecentSessionModel('gemini-1.5-pro')
    expect(getRecentSessionModels()).toEqual(['gemini-1.5-pro', 'gpt-4o-mini'])
  })
  it('deduplicates when recording the same model', () => {
    recordRecentSessionModel('gpt-4o-mini')
    recordRecentSessionModel('gemini-1.5-pro')
    recordRecentSessionModel('gpt-4o-mini')
    expect(getRecentSessionModels()).toEqual(['gpt-4o-mini', 'gemini-1.5-pro'])
  })
  it('caps the list length', () => {
    recordRecentSessionModel('a')
    recordRecentSessionModel('b')
    recordRecentSessionModel('c')
    recordRecentSessionModel('d')
    recordRecentSessionModel('e')
    recordRecentSessionModel('f')
    expect(getRecentSessionModels()).toEqual(['f', 'e', 'd', 'c', 'b'])
  })
})
</file>

<file path="src/__tests__/multiline-text-buffer.test.ts">
import {
  backspace,
  deleteForward,
  getCursorCoordinates,
  getLineCount,
  insertText,
  moveCursorLeft,
  moveCursorRight,
  type MultilineTextBufferState,
} from '../tui/components/core/multiline-text-buffer'
describe('multiline-text-buffer', () => {
  it('counts lines based on newlines', () => {
    expect(getLineCount('')).toBe(1)
    expect(getLineCount('hello')).toBe(1)
    expect(getLineCount('a\nb')).toBe(2)
    expect(getLineCount('a\nb\n')).toBe(3)
  })
  it('inserts text at cursor and advances cursor', () => {
    const initial: MultilineTextBufferState = { value: 'ace', cursor: 1 }
    expect(insertText(initial, 'b')).toEqual({ value: 'abce', cursor: 2 })
  })
  it('supports inserting newlines', () => {
    const initial: MultilineTextBufferState = { value: 'hello', cursor: 5 }
    expect(insertText(initial, '\nworld')).toEqual({ value: 'hello\nworld', cursor: 11 })
  })
  it('strips bracketed paste markers during insert', () => {
    const initial: MultilineTextBufferState = { value: '', cursor: 0 }
    expect(insertText(initial, '\u001b[200~hello\u001b[201~')).toEqual({
      value: 'hello',
      cursor: 5,
    })
    expect(insertText(initial, '[200~hello[201~')).toEqual({ value: 'hello', cursor: 5 })
  })
  it('backspace removes the previous character', () => {
    const initial: MultilineTextBufferState = { value: 'abc', cursor: 2 }
    expect(backspace(initial)).toEqual({ value: 'ac', cursor: 1 })
  })
  it('deleteForward removes the next character', () => {
    const initial: MultilineTextBufferState = { value: 'abc', cursor: 1 }
    expect(deleteForward(initial)).toEqual({ value: 'ac', cursor: 1 })
  })
  it('moves cursor left and right with clamping', () => {
    const start: MultilineTextBufferState = { value: 'abc', cursor: 0 }
    expect(moveCursorLeft(start).cursor).toBe(0)
    expect(moveCursorRight(start).cursor).toBe(1)
    expect(moveCursorRight({ value: 'abc', cursor: 3 }).cursor).toBe(3)
  })
  it('reports cursor coordinates across lines', () => {
    expect(getCursorCoordinates('a\nbc\n', 0)).toEqual({ row: 0, column: 0 })
    expect(getCursorCoordinates('a\nbc\n', 1)).toEqual({ row: 0, column: 1 })
    expect(getCursorCoordinates('a\nbc\n', 2)).toEqual({ row: 1, column: 0 })
    expect(getCursorCoordinates('a\nbc\n', 4)).toEqual({ row: 1, column: 2 })
    expect(getCursorCoordinates('a\nbc\n', 5)).toEqual({ row: 2, column: 0 })
  })
})
</file>

<file path="src/__tests__/new-command.test.ts">
import { planSessionCommand } from '../tui/new-command'
describe('session command planner', () => {
  it('plans /new as reset-only', () => {
    expect(planSessionCommand({ commandId: 'new', lastGeneratedPrompt: 'ignored' })).toEqual({
      type: 'reset-only',
      message: '[new] Session reset.',
    })
  })
  it('plans /reuse with no prompt as reset-only', () => {
    expect(planSessionCommand({ commandId: 'reuse', lastGeneratedPrompt: null })).toEqual({
      type: 'reset-only',
      message: '[reuse] Session reset · no previous prompt to reuse.',
    })
    expect(planSessionCommand({ commandId: 'reuse', lastGeneratedPrompt: '   ' })).toEqual({
      type: 'reset-only',
      message: '[reuse] Session reset · no previous prompt to reuse.',
    })
  })
  it('plans /reuse with prompt as reset-and-load-meta', () => {
    expect(planSessionCommand({ commandId: 'reuse', lastGeneratedPrompt: ' hello ' })).toEqual({
      type: 'reset-and-load-meta',
      message: '[reuse] Session reset · loaded last prompt into meta instructions.',
      metaInstructions: 'hello',
    })
  })
})
</file>

<file path="src/__tests__/notifier.test.ts">
import React from 'react'
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import { ToastProvider, useNotifier } from '../tui/notifier'
const dom = new JSDOM('<!doctype html><html><body></body></html>')
const globalEnv = globalThis as typeof globalThis & {
  window: Window & typeof globalThis
  document: Document
  navigator: Navigator
}
globalEnv.window = dom.window as typeof globalEnv.window
globalEnv.document = dom.window.document as Document
globalEnv.navigator = dom.window.navigator
const createWrapper = (options: Omit<React.ComponentProps<typeof ToastProvider>, 'children'>) => {
  const Wrapper: React.FC<{ children: React.ReactNode }> = ({ children }) =>
    React.createElement(ToastProvider, { ...options, children })
  return Wrapper
}
describe('toast provider notifier', () => {
  beforeEach(() => {
    jest.useFakeTimers()
  })
  afterEach(() => {
    jest.useRealTimers()
  })
  it('auto-dismisses after timeout, then removes after exit animation', () => {
    const wrapper = createWrapper({ exitAnimationMs: 40 })
    const { result } = renderHook(() => useNotifier({ autoDismissMs: 50 }), { wrapper })
    act(() => {
      result.current.notify('Hello')
    })
    expect(result.current.toasts).toHaveLength(1)
    expect(result.current.toasts[0]?.message).toBe('Hello')
    expect(result.current.toasts[0]?.isExiting).toBe(false)
    act(() => {
      jest.advanceTimersByTime(50)
    })
    expect(result.current.toasts).toHaveLength(1)
    expect(result.current.toasts[0]?.isExiting).toBe(true)
    act(() => {
      jest.advanceTimersByTime(40)
    })
    expect(result.current.toasts).toHaveLength(0)
  })
  it('does not let an old timer dismiss a newer toast', () => {
    const wrapper = createWrapper({ exitAnimationMs: 10 })
    const { result } = renderHook(() => useNotifier({ autoDismissMs: 100 }), { wrapper })
    act(() => {
      result.current.notify('First')
    })
    act(() => {
      jest.advanceTimersByTime(60)
    })
    act(() => {
      result.current.notify('Second')
    })
    act(() => {
      jest.advanceTimersByTime(49)
    })
    expect(result.current.toasts.map((toast) => toast.message)).toEqual(['First', 'Second'])
    expect(result.current.toasts[0]?.isExiting).toBe(true)
    expect(result.current.toasts[1]?.isExiting).toBe(false)
    act(() => {
      jest.advanceTimersByTime(1)
    })
    expect(result.current.toasts.map((toast) => toast.message)).toEqual(['Second'])
    act(() => {
      jest.advanceTimersByTime(50)
    })
    expect(result.current.toasts).toHaveLength(1)
    expect(result.current.toasts[0]?.message).toBe('Second')
    expect(result.current.toasts[0]?.isExiting).toBe(true)
    act(() => {
      jest.advanceTimersByTime(10)
    })
    expect(result.current.toasts).toHaveLength(0)
  })
  it('enforces max toast cap by exiting the oldest active toast', () => {
    const wrapper = createWrapper({ maxToasts: 2, exitAnimationMs: 20 })
    const { result } = renderHook(() => useNotifier({ autoDismissMs: 100_000 }), { wrapper })
    act(() => {
      result.current.notify('One')
      result.current.notify('Two')
      result.current.notify('Three')
    })
    expect(result.current.toasts.map((toast) => toast.message)).toEqual(['One', 'Two', 'Three'])
    expect(result.current.toasts[0]?.isExiting).toBe(true)
    expect(result.current.toasts[1]?.isExiting).toBe(false)
    expect(result.current.toasts[2]?.isExiting).toBe(false)
    act(() => {
      jest.advanceTimersByTime(20)
    })
    expect(result.current.toasts.map((toast) => toast.message)).toEqual(['Two', 'Three'])
  })
  it('dismiss-by-id only affects the intended toast', () => {
    const wrapper = createWrapper({ exitAnimationMs: 15 })
    const { result } = renderHook(() => useNotifier({ autoDismissMs: 100_000 }), { wrapper })
    let firstToastId: number | null = null
    act(() => {
      firstToastId = result.current.showToast('First')
      result.current.notify('Second')
    })
    if (firstToastId === null) {
      throw new Error('Expected first toast id')
    }
    const firstId = firstToastId
    act(() => {
      result.current.dismissToast(firstId)
    })
    expect(result.current.toasts.map((toast) => toast.message)).toEqual(['First', 'Second'])
    expect(result.current.toasts[0]?.isExiting).toBe(true)
    expect(result.current.toasts[1]?.isExiting).toBe(false)
    act(() => {
      jest.advanceTimersByTime(15)
    })
    expect(result.current.toasts.map((toast) => toast.message)).toEqual(['Second'])
  })
})
</file>

<file path="src/__tests__/paste-snippet.test.ts">
import {
  BRACKETED_PASTE_END,
  BRACKETED_PASTE_START,
  consumeBracketedPasteChunk,
  createBracketedPasteState,
  createPastedSnippet,
  detectPastedSnippetFromInputChange,
} from '../tui/paste-snippet'
describe('paste-snippet', () => {
  describe('consumeBracketedPasteChunk', () => {
    it('returns no completions for normal typing', () => {
      const state = createBracketedPasteState()
      const result = consumeBracketedPasteChunk(state, 'hello')
      expect(result.completed).toEqual([])
      expect(result.state).toEqual(state)
      expect(result.didSeeBracketedPaste).toBe(false)
    })
    it('captures a complete bracketed paste in a single chunk', () => {
      const state = createBracketedPasteState()
      const chunk = `${BRACKETED_PASTE_START}hello\nworld${BRACKETED_PASTE_END}`
      const result = consumeBracketedPasteChunk(state, chunk)
      expect(result.state.isActive).toBe(false)
      expect(result.completed).toEqual(['hello\nworld'])
      expect(result.didSeeBracketedPaste).toBe(true)
    })
    it('captures a bracketed paste across multiple chunks', () => {
      const state = createBracketedPasteState()
      const first = consumeBracketedPasteChunk(state, `${BRACKETED_PASTE_START}hello`)
      expect(first.state.isActive).toBe(true)
      expect(first.state.buffer).toBe('hello')
      expect(first.completed).toEqual([])
      const second = consumeBracketedPasteChunk(first.state, `\nworld${BRACKETED_PASTE_END}`)
      expect(second.state.isActive).toBe(false)
      expect(second.state.buffer).toBe('')
      expect(second.completed).toEqual(['hello\nworld'])
    })
    it('captures multiple pastes from a single chunk', () => {
      const state = createBracketedPasteState()
      const chunk = `${BRACKETED_PASTE_START}one${BRACKETED_PASTE_END}${BRACKETED_PASTE_START}two${BRACKETED_PASTE_END}`
      const result = consumeBracketedPasteChunk(state, chunk)
      expect(result.completed).toEqual(['one', 'two'])
      expect(result.state.isActive).toBe(false)
    })
  })
  describe('createPastedSnippet', () => {
    it('returns null for text below the threshold', () => {
      expect(createPastedSnippet('hello world')).toBeNull()
      expect(createPastedSnippet('line1\nline2\nline3')).toBeNull()
    })
    it('returns a snippet for text above the threshold', () => {
      const raw = `${'x'.repeat(80)}\nline2\nline3`
      const snippet = createPastedSnippet(raw)
      expect(snippet).not.toBeNull()
      expect(snippet?.lineCount).toBe(3)
      expect(snippet?.label).toBe('[Pasted ~3 lines]')
    })
    it('strips bracketed paste markers from snippet text', () => {
      const rawWithEsc = `\u001b[200~${'x'.repeat(80)}\u001b[201~`
      const snippetWithEsc = createPastedSnippet(rawWithEsc)
      expect(snippetWithEsc).not.toBeNull()
      expect(snippetWithEsc?.text).toBe('x'.repeat(80))
      const rawNoEsc = `[200~${'x'.repeat(80)}[201~`
      const snippetNoEsc = createPastedSnippet(rawNoEsc)
      expect(snippetNoEsc).not.toBeNull()
      expect(snippetNoEsc?.text).toBe('x'.repeat(80))
    })
  })
  describe('detectPastedSnippetFromInputChange', () => {
    it('ignores small multi-line edits', () => {
      expect(detectPastedSnippetFromInputChange('a', 'a\nb')).toBeNull()
    })
    it('ignores small single-line edits', () => {
      expect(detectPastedSnippetFromInputChange('hello', 'hello there')).toBeNull()
    })
    it('detects a large multi-line paste', () => {
      const large = `${'x'.repeat(90)}\n${'y'.repeat(10)}`
      const detection = detectPastedSnippetFromInputChange('', large)
      expect(detection).not.toBeNull()
      expect(detection?.snippet.lineCount).toBe(2)
      expect(detection?.snippet.charCount).toBe(101)
      expect(detection?.range).toEqual({ start: 0, end: large.length })
    })
    it('detects a large single-line paste', () => {
      const large = 'x'.repeat(500)
      const detection = detectPastedSnippetFromInputChange('', large)
      expect(detection).not.toBeNull()
      expect(detection?.snippet.charCount).toBe(500)
      expect(detection?.snippet.lineCount).toBe(1)
      expect(detection?.range).toEqual({ start: 0, end: large.length })
    })
  })
})
</file>

<file path="src/__tests__/provider-chip.test.ts">
import { formatProviderStatusChip } from '../tui/provider-chip'
import type { ProviderStatusMap } from '../tui/types'
describe('formatProviderStatusChip', () => {
  it('formats the selected provider chip for OpenAI models', () => {
    const statuses: ProviderStatusMap = {
      openai: { provider: 'openai', status: 'ok', message: 'ready' },
      gemini: { provider: 'gemini', status: 'ok', message: 'ready' },
      other: { provider: 'other', status: 'ok', message: 'ready' },
    }
    expect(formatProviderStatusChip('gpt-4o-mini', statuses)).toBe('[openai:ok]')
  })
  it('formats missing credentials as missing-key', () => {
    const statuses: ProviderStatusMap = {
      openai: { provider: 'openai', status: 'ok', message: 'ready' },
      gemini: { provider: 'gemini', status: 'missing', message: 'missing' },
      other: { provider: 'other', status: 'ok', message: 'ready' },
    }
    expect(formatProviderStatusChip('gemini-1.5-pro', statuses)).toBe('[gemini:missing-key]')
  })
  it('formats errors as error', () => {
    const statuses: ProviderStatusMap = {
      openai: { provider: 'openai', status: 'error', message: 'boom' },
      gemini: { provider: 'gemini', status: 'ok', message: 'ready' },
      other: { provider: 'other', status: 'ok', message: 'ready' },
    }
    expect(formatProviderStatusChip('gpt-4o-mini', statuses)).toBe('[openai:error]')
  })
  it('formats unknown providers as other', () => {
    const statuses: ProviderStatusMap = {
      openai: { provider: 'openai', status: 'ok', message: 'ready' },
      gemini: { provider: 'gemini', status: 'ok', message: 'ready' },
      other: { provider: 'other', status: 'ok', message: 'Custom provider (not validated)' },
    }
    expect(formatProviderStatusChip('my-local-model', statuses)).toBe('[other:ok]')
  })
})
</file>

<file path="src/__tests__/provider-status.test.ts">
const mockResolveOpenAiCredentials = jest.fn()
const mockResolveGeminiCredentials = jest.fn()
jest.mock('../config', () => ({
  resolveOpenAiCredentials: mockResolveOpenAiCredentials,
  resolveGeminiCredentials: mockResolveGeminiCredentials,
}))
describe('provider-status utility', () => {
  beforeEach(() => {
    jest.resetModules()
    mockResolveOpenAiCredentials.mockReset()
    mockResolveGeminiCredentials.mockReset()
  })
  it('returns ok status when credentials resolve', async () => {
    mockResolveOpenAiCredentials.mockResolvedValue({ apiKey: 'ok' })
    const { checkProviderStatus, invalidateProviderStatus } = await import('../tui/provider-status')
    invalidateProviderStatus()
    const status = await checkProviderStatus('openai')
    expect(status).toEqual({ provider: 'openai', status: 'ok', message: 'Credentials available' })
  })
  it('returns missing status when credentials are absent', async () => {
    mockResolveGeminiCredentials.mockRejectedValue(new Error('Missing GEMINI_API_KEY'))
    const { checkProviderStatus, invalidateProviderStatus } = await import('../tui/provider-status')
    invalidateProviderStatus('gemini')
    const status = await checkProviderStatus('gemini')
    expect(status.status).toBe('missing')
    expect(status.message).toContain('Missing GEMINI_API_KEY')
  })
  it('caches provider lookups', async () => {
    mockResolveOpenAiCredentials.mockResolvedValue({ apiKey: 'cached' })
    const { checkProviderStatus, invalidateProviderStatus } = await import('../tui/provider-status')
    invalidateProviderStatus('openai')
    await checkProviderStatus('openai')
    await checkProviderStatus('openai')
    expect(mockResolveOpenAiCredentials).toHaveBeenCalledTimes(1)
  })
})
</file>

<file path="src/__tests__/series-path.test.ts">
import {
  buildSeriesOutputDirName,
  sanitizeForPathSegment,
  sanitizeForPathSegmentWithHash,
} from '../utils/series-path'
describe('series-path', () => {
  test('sanitizeForPathSegment slugifies and truncates', () => {
    expect(sanitizeForPathSegment('Hello, World!', 'fallback')).toBe('hello-world')
    expect(sanitizeForPathSegment('---', 'fallback')).toBe('fallback')
    expect(sanitizeForPathSegment('Hello, World!', 'fallback', 5)).toBe('hello')
  })
  test('sanitizeForPathSegmentWithHash keeps within maxLength', () => {
    const longValue = 'a'.repeat(500)
    const result = sanitizeForPathSegmentWithHash(longValue, 'fallback', 40)
    expect(result.length).toBeLessThanOrEqual(40)
    expect(result).toMatch(/-[0-9a-f]{8}$/)
  })
  test('buildSeriesOutputDirName caps intent slug length', () => {
    const intent =
      'We need to replace the DataTable component in the Pricing Rules section with the same library component in the Fee Manager'
    const date = new Date(2025, 11, 27, 11, 46, 55)
    const dirName = buildSeriesOutputDirName(intent, date)
    expect(dirName.startsWith('20251227-114655-')).toBe(true)
    const intentSlug = dirName.replace(/^\d{8}-\d{6}-/, '')
    expect(intentSlug.length).toBeLessThanOrEqual(80)
    expect(intentSlug).not.toMatch(/\s/)
  })
})
</file>

<file path="src/__tests__/smart-context-service.test.ts">
import path from 'node:path'
import fg from 'fast-glob'
import { resolveSmartContextFiles } from '../smart-context-service'
jest.mock('fast-glob')
jest.mock('node:fs/promises', () => ({
  stat: jest.fn(),
  readFile: jest.fn(),
}))
jest.mock('../rag/vector-store', () => ({
  indexFiles: jest.fn(),
  search: jest.fn(),
}))
const globMock = fg as unknown as jest.MockedFunction<typeof fg>
const fs = jest.requireMock('node:fs/promises') as { stat: jest.Mock; readFile: jest.Mock }
const vectorStore = jest.requireMock('../rag/vector-store') as {
  indexFiles: jest.Mock
  search: jest.Mock
}
describe('smart-context-service', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    globMock.mockResolvedValue([])
    fs.stat.mockResolvedValue({ size: 1000 })
    fs.readFile.mockResolvedValue('contents')
    vectorStore.search.mockResolvedValue([])
  })
  it('returns empty array when no files match glob patterns', async () => {
    const onProgress = jest.fn()
    const result = await resolveSmartContextFiles('intent', [], onProgress)
    expect(result).toEqual([])
    expect(onProgress).toHaveBeenCalledWith('No smart context files found')
  })
  it('filters by size, indexes, searches, and returns new files', async () => {
    globMock.mockResolvedValue(['/repo/a.md', '/repo/b.md'])
    vectorStore.search.mockResolvedValue(['/repo/a.md', '/repo/b.md'])
    const result = await resolveSmartContextFiles(
      'intent',
      [{ path: '/repo/a.md', content: '' }],
      undefined,
    )
    expect(vectorStore.indexFiles).toHaveBeenCalledWith(['/repo/a.md', '/repo/b.md'])
    expect(result).toEqual([{ path: '/repo/b.md', content: 'contents' }])
  })
  it('scans the provided smart context root when supplied', async () => {
    globMock.mockResolvedValue([])
    await resolveSmartContextFiles('intent', [], undefined, './packages')
    expect(globMock).toHaveBeenCalledWith(
      expect.any(Array),
      expect.objectContaining({ cwd: path.resolve('./packages') }),
    )
  })
  it('logs warning and returns [] when indexing fails', async () => {
    globMock.mockResolvedValue(['/repo/a.md'])
    vectorStore.indexFiles.mockRejectedValue(new Error('fail'))
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    const result = await resolveSmartContextFiles('intent', [], undefined)
    expect(result).toEqual([])
    expect(warn).toHaveBeenCalledWith('Smart context indexing failed: fail')
    warn.mockRestore()
  })
  it('skips files that fail to read while still returning others', async () => {
    globMock.mockResolvedValue(['/repo/a.md', '/repo/b.md'])
    vectorStore.indexFiles.mockResolvedValue(undefined)
    vectorStore.search.mockResolvedValue(['/repo/a.md', '/repo/b.md'])
    fs.readFile.mockResolvedValueOnce('content-a').mockRejectedValueOnce(new Error('nope'))
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    const result = await resolveSmartContextFiles('intent', [], undefined)
    expect(result).toEqual([{ path: '/repo/a.md', content: 'content-a' }])
    expect(warn).toHaveBeenCalledWith('Warning: Failed to read smart context file /repo/b.md: nope')
    warn.mockRestore()
  })
})
</file>

<file path="src/__tests__/string-filter.test.ts">
import { filterStringsByQuery } from '../tui/string-filter'
describe('filterStringsByQuery', () => {
  it('returns all items unchanged for an empty query', () => {
    expect(filterStringsByQuery(['b', 'a', 'c'], '   ')).toEqual(['b', 'a', 'c'])
  })
  it('matches case-insensitively and preserves input order', () => {
    expect(filterStringsByQuery(['Readme.md', 'src/app.ts', 'docs/guide.md'], 'MD')).toEqual([
      'Readme.md',
      'docs/guide.md',
    ])
  })
  it('prefers prefix matches over substring matches', () => {
    expect(
      filterStringsByQuery(['packages/readme.md', 'docs/readme.md', 'src/readme.ts'], 'src'),
    ).toEqual(['src/readme.ts'])
  })
  it('returns prefix matches first, then substring matches', () => {
    expect(filterStringsByQuery(['abc', 'xabc', 'ab', 'zab'], 'ab')).toEqual([
      'abc',
      'ab',
      'xabc',
      'zab',
    ])
  })
})
</file>

<file path="src/__tests__/test-command.test.ts">
import { runTestCommand } from '../test-command'
jest.mock('node:fs/promises', () => ({ readFile: jest.fn() }))
jest.mock('js-yaml', () => ({ load: jest.fn() }))
jest.mock('../testing/test-schema', () => ({ parsePromptTestSuite: jest.fn() }))
jest.mock('../testing/evaluator', () => ({ evaluatePrompt: jest.fn() }))
jest.mock('../prompt-generator-service', () => ({
  createPromptGeneratorService: jest.fn(),
  resolveDefaultGenerateModel: jest.fn().mockResolvedValue('gpt-4o-mini'),
}))
jest.mock('../file-context', () => ({ resolveFileContext: jest.fn().mockResolvedValue([]) }))
jest.mock('../smart-context-service', () => ({
  resolveSmartContextFiles: jest.fn().mockResolvedValue([]),
}))
const fs = jest.requireMock('node:fs/promises') as { readFile: jest.Mock }
const yaml = jest.requireMock('js-yaml') as { load: jest.Mock }
const schema = jest.requireMock('../testing/test-schema') as { parsePromptTestSuite: jest.Mock }
const evaluator = jest.requireMock('../testing/evaluator') as { evaluatePrompt: jest.Mock }
const { createPromptGeneratorService, resolveDefaultGenerateModel } = jest.requireMock(
  '../prompt-generator-service',
) as {
  createPromptGeneratorService: jest.Mock
  resolveDefaultGenerateModel: jest.Mock
}
const smartContext = jest.requireMock('../smart-context-service') as {
  resolveSmartContextFiles: jest.Mock
}
const service = {
  generatePrompt: jest.fn(),
}
createPromptGeneratorService.mockResolvedValue(service)
describe('runTestCommand', () => {
  const originalStdoutTty = process.stdout.isTTY
  const originalExitCode = process.exitCode
  beforeEach(() => {
    jest.clearAllMocks()
    createPromptGeneratorService.mockResolvedValue(service)
    resolveDefaultGenerateModel.mockResolvedValue('gpt-4o-mini')
    service.generatePrompt.mockResolvedValue('generated prompt')
    evaluator.evaluatePrompt.mockResolvedValue({ pass: true, reason: 'ok' })
    process.exitCode = undefined
    Object.defineProperty(process.stdout, 'isTTY', { value: false, configurable: true })
  })
  afterAll(() => {
    Object.defineProperty(process.stdout, 'isTTY', { value: originalStdoutTty })
    process.exitCode = originalExitCode
  })
  it('loads default prompt-tests.yaml when no file provided', async () => {
    fs.readFile.mockResolvedValue('suite')
    yaml.load.mockReturnValue({ tests: [] })
    schema.parsePromptTestSuite.mockReturnValue({ tests: [] })
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runTestCommand([])
    expect(fs.readFile).toHaveBeenCalledWith(expect.stringContaining('prompt-tests.yaml'), 'utf8')
    log.mockRestore()
  })
  it('executes tests and evaluates prompts', async () => {
    fs.readFile.mockResolvedValue('suite')
    const tests = [{ name: 'Case 1', intent: 'intent', expect: { contains: 'result' } }]
    yaml.load.mockReturnValue({ tests })
    schema.parsePromptTestSuite.mockReturnValue({ tests })
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runTestCommand(['custom.yaml'])
    expect(service.generatePrompt).toHaveBeenCalledWith(
      expect.objectContaining({ intent: 'intent', model: 'gpt-4o-mini' }),
    )
    expect(evaluator.evaluatePrompt).toHaveBeenCalledWith('generated prompt', {
      contains: 'result',
    })
    log.mockRestore()
  })
  it('invokes smart context resolution when enabled for a test', async () => {
    fs.readFile.mockResolvedValue('suite')
    const tests = [{ name: 'Smart', intent: 'intent', expect: {}, smartContext: true }]
    yaml.load.mockReturnValue({ tests })
    schema.parsePromptTestSuite.mockReturnValue({ tests })
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    smartContext.resolveSmartContextFiles.mockResolvedValue([
      { path: 'smart.md', content: 'smart' },
    ])
    await runTestCommand([])
    expect(smartContext.resolveSmartContextFiles).toHaveBeenCalledWith(
      'intent',
      [],
      expect.any(Function),
      undefined,
    )
    log.mockRestore()
  })
  it('passes smartContextRoot from test definitions when provided', async () => {
    fs.readFile.mockResolvedValue('suite')
    const tests = [
      {
        name: 'Smart Root',
        intent: 'intent',
        expect: {},
        smartContext: true,
        smartContextRoot: './src',
      },
    ]
    yaml.load.mockReturnValue({ tests })
    schema.parsePromptTestSuite.mockReturnValue({ tests })
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    smartContext.resolveSmartContextFiles.mockResolvedValue([])
    await runTestCommand([])
    expect(smartContext.resolveSmartContextFiles).toHaveBeenCalledWith(
      'intent',
      [],
      expect.any(Function),
      './src',
    )
    log.mockRestore()
  })
  it('sets process.exitCode when failures occur', async () => {
    fs.readFile.mockResolvedValue('suite')
    const tests = [{ name: 'Case', intent: 'intent', expect: {} }]
    yaml.load.mockReturnValue({ tests })
    schema.parsePromptTestSuite.mockReturnValue({ tests })
    evaluator.evaluatePrompt.mockResolvedValue({ pass: false, reason: 'bad' })
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runTestCommand([])
    expect(process.exitCode).toBe(1)
    log.mockRestore()
  })
})
</file>

<file path="src/__tests__/test-runner-reducer.test.ts">
import {
  INITIAL_TEST_RUNNER_STATE,
  testRunnerReducer,
} from '../tui/screens/test-runner/test-runner-reducer'
describe('testRunnerReducer', () => {
  it('updates the file path', () => {
    const next = testRunnerReducer(INITIAL_TEST_RUNNER_STATE, {
      type: 'set-file-path',
      next: 'other.yaml',
    })
    expect(next.filePath).toBe('other.yaml')
  })
  it('starts a run and clears transient state', () => {
    const seeded = {
      ...INITIAL_TEST_RUNNER_STATE,
      tests: [{ name: 'a', status: 'fail' as const, reason: 'nope' }],
      error: 'boom',
      summary: { passed: 1, failed: 2 },
    }
    const next = testRunnerReducer(seeded, { type: 'run-start' })
    expect(next.status).toBe('running')
    expect(next.tests).toEqual([])
    expect(next.error).toBeNull()
    expect(next.summary).toBeNull()
  })
  it('loads suite tests as pending', () => {
    const afterLoad = testRunnerReducer(INITIAL_TEST_RUNNER_STATE, {
      type: 'suite-loaded',
      loadedPath: '/tmp/prompt-tests.yaml',
      testNames: ['t1', 't2'],
    })
    expect(afterLoad.lastRunFile).toBe('/tmp/prompt-tests.yaml')
    expect(afterLoad.tests).toEqual([
      { name: 't1', status: 'pending', reason: null },
      { name: 't2', status: 'pending', reason: null },
    ])
  })
  it('updates a test status by ordinal', () => {
    const withSuite = testRunnerReducer(INITIAL_TEST_RUNNER_STATE, {
      type: 'suite-loaded',
      loadedPath: '/tmp/prompt-tests.yaml',
      testNames: ['t1', 't2'],
    })
    const running = testRunnerReducer(withSuite, { type: 'test-start', ordinal: 2, name: 't2' })
    expect(running.tests[1]).toEqual({ name: 't2', status: 'running', reason: null })
    const failed = testRunnerReducer(running, {
      type: 'test-complete',
      ordinal: 2,
      name: 't2',
      pass: false,
      reason: 'bad',
    })
    expect(failed.tests[1]).toEqual({ name: 't2', status: 'fail', reason: 'bad' })
  })
  it('completes run with summary and returns to idle', () => {
    const running = testRunnerReducer(INITIAL_TEST_RUNNER_STATE, { type: 'run-start' })
    const next = testRunnerReducer(running, { type: 'run-complete', passed: 3, failed: 1 })
    expect(next.status).toBe('idle')
    expect(next.summary).toEqual({ passed: 3, failed: 1 })
  })
  it('stores errors and returns to idle', () => {
    const running = testRunnerReducer(INITIAL_TEST_RUNNER_STATE, { type: 'run-start' })
    const next = testRunnerReducer(running, { type: 'run-error', message: 'boom' })
    expect(next.status).toBe('idle')
    expect(next.error).toBe('boom')
  })
})
</file>

<file path="src/__tests__/text-input-keys.test.ts">
import type { Key } from 'ink'
import { resolveSingleLineKeyAction } from '../tui/components/core/single-line-text-input-keymap'
import { isBackspaceKey } from '../tui/components/core/text-input-keys'
const createKey = (overrides: Partial<Key> = {}): Key => overrides as Key
describe('isBackspaceKey', () => {
  it('recognizes ink backspace flag', () => {
    expect(isBackspaceKey('', createKey({ backspace: true }))).toBe(true)
  })
  it('recognizes DEL and BS characters', () => {
    expect(isBackspaceKey('\u007f', createKey())).toBe(true)
    expect(isBackspaceKey('\b', createKey())).toBe(true)
  })
  it('recognizes Ctrl+H and Ctrl+?', () => {
    expect(isBackspaceKey('h', createKey({ ctrl: true }))).toBe(true)
    expect(isBackspaceKey('?', createKey({ ctrl: true }))).toBe(true)
  })
  it('recognizes Kitty CSI-u style sequences', () => {
    expect(isBackspaceKey('\u001b[127u', createKey())).toBe(true)
    expect(isBackspaceKey('\u001b[8u', createKey())).toBe(true)
    expect(isBackspaceKey('\u001b[51u', createKey())).toBe(true)
  })
  it('recognizes Kitty tilde backspace sequences', () => {
    expect(isBackspaceKey('\u001b[127~', createKey())).toBe(true)
    expect(isBackspaceKey('\u001b[8~', createKey())).toBe(true)
    expect(isBackspaceKey('\u001b[51~', createKey())).toBe(true)
  })
  it('treats delete-with-empty-input as backspace', () => {
    expect(isBackspaceKey('', createKey({ delete: true }))).toBe(true)
  })
  it('does not match regular input', () => {
    expect(isBackspaceKey('a', createKey())).toBe(false)
  })
})
describe('resolveSingleLineKeyAction', () => {
  it('applies Kitty CSI-u backspace to the buffer', () => {
    const action = resolveSingleLineKeyAction({
      input: '\u001b[127u',
      key: createKey({}),
      state: { value: 'abc', cursor: 3 },
    })
    expect(action).toEqual({ type: 'change', nextState: { value: 'ab', cursor: 2 } })
  })
  it('treats delete-with-empty-input as a backspace edit', () => {
    const action = resolveSingleLineKeyAction({
      input: '',
      key: createKey({ delete: true }),
      state: { value: 'abc', cursor: 3 },
    })
    expect(action).toEqual({ type: 'change', nextState: { value: 'ab', cursor: 2 } })
  })
})
</file>

<file path="src/__tests__/token-counter.test.ts">
describe('token-counter', () => {
  afterEach(() => {
    jest.resetModules()
  })
  it('counts tokens using the encoder path when available', async () => {
    const module = await import('../token-counter')
    const tokens = module.countTokens('abcd')
    expect(tokens).toBeGreaterThan(0)
  })
  it('falls back to heuristic when encoder fails', async () => {
    jest.doMock('js-tiktoken', () => ({
      getEncoding: () => ({
        encode: () => {
          throw new Error('boom')
        },
      }),
    }))
    const module = await import('../token-counter')
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    expect(module.countTokens('abcd')).toBe(Math.ceil('abcd'.length / 4))
    expect(warn).toHaveBeenCalledWith('Token counting failed, defaulting to character heuristic.')
    warn.mockRestore()
  })
  it('formats token counts with severity tiers', async () => {
    const module = await import('../token-counter')
    expect(module.formatTokenCount(120000)).toBe('⚠️ 120,000 tokens (High)')
    expect(module.formatTokenCount(40000)).toBe('40,000 tokens (Medium)')
    expect(module.formatTokenCount(100)).toBe('100 tokens')
  })
})
</file>

<file path="src/__tests__/token-usage-store.test.ts">
import { createTokenUsageStore } from '../tui/token-usage-store'
describe('token-usage-store', () => {
  it('computes totals and estimates cost when supported', () => {
    const store = createTokenUsageStore()
    const runId = store.startRun('gpt-4o-mini')
    store.recordTelemetry(runId, {
      files: [],
      intentTokens: 100,
      fileTokens: 200,
      systemTokens: 50,
      totalTokens: 350,
    })
    store.recordIteration(runId, { iteration: 1, promptTokens: 75, reasoningTokens: 25 })
    const breakdown = store.getLatestBreakdown()
    expect(breakdown).not.toBeNull()
    expect(breakdown?.input.total).toBe(350)
    expect(breakdown?.output.total).toBe(100)
    expect(breakdown?.totals.tokens).toBe(450)
    expect(breakdown?.totals.estimatedCostUsd).toBe(0.000113)
  })
  it('deduplicates iterations by iteration number', () => {
    const store = createTokenUsageStore()
    const runId = store.startRun('gpt-4o-mini')
    store.recordTelemetry(runId, {
      files: [],
      intentTokens: 1,
      fileTokens: 1,
      systemTokens: 1,
      totalTokens: 3,
    })
    store.recordIteration(runId, { iteration: 1, promptTokens: 5, reasoningTokens: 0 })
    store.recordIteration(runId, { iteration: 1, promptTokens: 6, reasoningTokens: 2 })
    const run = store.getLatestRun()
    expect(run?.iterations).toEqual([{ iteration: 1, promptTokens: 6, reasoningTokens: 2 }])
  })
})
</file>

<file path="src/__tests__/tui-command-history.test.ts">
import fs from 'node:fs/promises'
import {
  readCommandHistory,
  updateCommandHistory,
  writeCommandHistory,
  type CommandHistoryRecord,
} from '../tui/command-history'
jest.mock('node:os', () => ({ homedir: jest.fn(() => '/home/tester') }))
jest.mock('node:fs/promises', () => ({
  readFile: jest.fn(),
  mkdir: jest.fn(),
  rename: jest.fn(),
  writeFile: jest.fn(),
}))
const fsMock = fs as unknown as {
  readFile: jest.Mock
  mkdir: jest.Mock
  rename: jest.Mock
  writeFile: jest.Mock
}
describe('tui command history', () => {
  beforeEach(() => {
    jest.clearAllMocks()
  })
  it('adds new entries to the front and dedupes consecutive repeats', () => {
    const start: CommandHistoryRecord[] = []
    const first = updateCommandHistory({ previous: start, nextValue: ' hello ', maxEntries: 3 })
    expect(first[0]?.value).toBe('hello')
    const repeated = updateCommandHistory({ previous: first, nextValue: 'hello', maxEntries: 3 })
    expect(repeated).toBe(first)
    const second = updateCommandHistory({ previous: first, nextValue: 'world', maxEntries: 3 })
    expect(second.map((entry) => entry.value)).toEqual(['world', 'hello'])
  })
  it('limits stored entries to maxEntries', () => {
    const first = updateCommandHistory({ previous: [], nextValue: 'one', maxEntries: 2 })
    const second = updateCommandHistory({ previous: first, nextValue: 'two', maxEntries: 2 })
    const third = updateCommandHistory({ previous: second, nextValue: 'three', maxEntries: 2 })
    expect(third.map((entry) => entry.value)).toEqual(['three', 'two'])
  })
  it('returns an empty list when the history file is missing', async () => {
    const enoent = Object.assign(new Error('missing'), { code: 'ENOENT' })
    fsMock.readFile.mockRejectedValueOnce(enoent)
    await expect(readCommandHistory()).resolves.toEqual([])
  })
  it('returns an empty list when the history file is empty', async () => {
    fsMock.readFile.mockResolvedValueOnce('   \n')
    await expect(readCommandHistory()).resolves.toEqual([])
  })
  it('repairs corrupt JSON history files', async () => {
    fsMock.readFile.mockResolvedValueOnce('[')
    fsMock.mkdir.mockResolvedValueOnce(undefined)
    fsMock.rename.mockResolvedValueOnce(undefined)
    fsMock.writeFile.mockResolvedValueOnce(undefined)
    await expect(readCommandHistory()).resolves.toEqual([])
    expect(fsMock.rename).toHaveBeenCalledWith(
      '/home/tester/.config/prompt-maker-cli/tui-history.json',
      expect.stringContaining('/home/tester/.config/prompt-maker-cli/tui-history.corrupt-'),
    )
    expect(fsMock.writeFile).toHaveBeenCalledWith(
      '/home/tester/.config/prompt-maker-cli/tui-history.json',
      '[]\n',
      'utf8',
    )
  })
  it('writes history entries to the config directory', async () => {
    fsMock.mkdir.mockResolvedValueOnce(undefined)
    fsMock.writeFile.mockResolvedValueOnce(undefined)
    fsMock.rename.mockResolvedValueOnce(undefined)
    await writeCommandHistory([{ value: 'cmd', timestamp: 'now' }])
    expect(fsMock.mkdir).toHaveBeenCalledWith('/home/tester/.config/prompt-maker-cli', {
      recursive: true,
    })
    const tempFile = `/home/tester/.config/prompt-maker-cli/tui-history.json.${process.pid}.tmp`
    expect(fsMock.writeFile).toHaveBeenCalledWith(
      tempFile,
      expect.stringContaining('"value": "cmd"'),
      'utf8',
    )
    expect(fsMock.rename).toHaveBeenCalledWith(
      tempFile,
      '/home/tester/.config/prompt-maker-cli/tui-history.json',
    )
  })
})
</file>

<file path="src/__tests__/url-context.test.ts">
import { resolveUrlContext } from '../url-context'
import { resolveGithubUrl } from '../github-context'
jest.mock('../github-context', () => ({
  resolveGithubUrl: jest.fn().mockResolvedValue([]),
}))
describe('resolveUrlContext', () => {
  const originalFetch = global.fetch
  const mockFetch = jest.fn() as jest.MockedFunction<typeof fetch>
  beforeEach(() => {
    mockFetch.mockReset()
    global.fetch = mockFetch
  })
  afterAll(() => {
    global.fetch = originalFetch
  })
  it('returns text content for generic HTML pages', async () => {
    const html = '<html><body><h1>Title</h1><p>Paragraph</p></body></html>'
    mockFetch.mockResolvedValueOnce(createBufferResponse(Buffer.from(html)))
    const results = await resolveUrlContext(['https://example.com/page'])
    expect(mockFetch).toHaveBeenCalledWith('https://example.com/page', expect.any(Object))
    expect(results).toHaveLength(1)
    const file = results[0]
    if (!file) {
      throw new Error('Expected resolveUrlContext to return one file')
    }
    expect(file.path).toBe('url:https://example.com/page')
    expect(file.content.toLowerCase()).toContain('title')
    expect(file.content).toContain('Paragraph')
  })
  it('deduplicates URLs and warns on invalid entries', async () => {
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    const html = '<html><body>Doc</body></html>'
    mockFetch.mockResolvedValue(createBufferResponse(Buffer.from(html)))
    const results = await resolveUrlContext([
      '',
      'notaurl',
      'https://example.com',
      'https://example.com',
    ])
    expect(results).toHaveLength(1)
    expect(mockFetch).toHaveBeenCalledTimes(1)
    expect(warn).toHaveBeenCalled()
    warn.mockRestore()
  })
  it('delegates GitHub URLs to the GitHub resolver', async () => {
    const githubResult = [{ path: 'github:owner/repo/file.ts', content: 'code' }]
    ;(resolveGithubUrl as jest.Mock).mockResolvedValueOnce(githubResult)
    const results = await resolveUrlContext(['https://github.com/owner/repo'])
    expect(resolveGithubUrl).toHaveBeenCalledWith(
      expect.objectContaining({ href: 'https://github.com/owner/repo' }),
      undefined,
    )
    expect(results).toEqual(githubResult)
  })
})
const createBufferResponse = (buffer: Buffer, status = 200): Response =>
  ({
    ok: status >= 200 && status < 300,
    status,
    headers: new Headers({ 'content-length': buffer.byteLength.toString() }),
    arrayBuffer: async () => buffer,
  }) as unknown as Response
</file>

<file path="src/__tests__/useCommandHistory.test.ts">
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import { useCommandHistory } from '../tui/hooks/useCommandHistory'
import type { HistoryEntry } from '../tui/types'
const dom = new JSDOM('<!doctype html><html><body></body></html>')
const globalEnv = globalThis as typeof globalThis & {
  window: Window & typeof globalThis
  document: Document
  navigator: Navigator
}
globalEnv.window = dom.window as typeof globalEnv.window
globalEnv.document = dom.window.document as Document
globalEnv.navigator = dom.window.navigator
describe('useCommandHistory', () => {
  it('resets history back to initial entries', () => {
    const initialEntries: HistoryEntry[] = [
      { id: 'seed-0', content: 'Welcome', kind: 'system' },
      { id: 'seed-1', content: 'Tip', kind: 'system' },
    ]
    const { result } = renderHook(() =>
      useCommandHistory({
        initialEntries,
        visibleRows: 10,
      }),
    )
    act(() => {
      result.current.pushHistory('User message', 'user')
    })
    expect(result.current.history).toHaveLength(3)
    act(() => {
      result.current.resetHistory()
    })
    expect(result.current.history).toEqual(initialEntries)
  })
  it('clears history to an empty buffer', () => {
    const initialEntries: HistoryEntry[] = [
      { id: 'seed-0', content: 'Welcome', kind: 'system' },
      { id: 'seed-1', content: 'Tip', kind: 'system' },
    ]
    const { result } = renderHook(() =>
      useCommandHistory({
        initialEntries,
        visibleRows: 10,
      }),
    )
    act(() => {
      result.current.pushHistory('User message', 'user')
    })
    act(() => {
      result.current.clearHistory()
    })
    expect(result.current.history).toEqual([])
  })
})
</file>

<file path="src/generate/actions.ts">
import chalk from 'chalk'
import clipboard from 'clipboardy'
import open from 'open'
import { shouldTraceCopy } from './debug'
export const maybeCopyToClipboard = async (
  shouldCopy: boolean,
  prompt: string,
  showFeedback: boolean,
): Promise<void> => {
  const traceEnabled = shouldTraceCopy()
  const trace = (message: string): void => {
    if (traceEnabled) {
      console.error(chalk.dim(`[pmc:copy] ${message}`))
    }
  }
  if (!shouldCopy) {
    trace('Skipping clipboard write (flag not provided).')
    return
  }
  trace(`Attempting clipboard write (${prompt.length.toLocaleString()} chars).`)
  try {
    await clipboard.write(prompt)
    if (showFeedback) {
      console.log(chalk.green('✓ Copied prompt to clipboard.'))
    } else {
      trace('Copied prompt to clipboard (feedback suppressed).')
    }
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown clipboard error.'
    trace(`Clipboard write failed: ${message}`)
    console.warn(chalk.yellow(`Failed to copy prompt to clipboard: ${message}`))
  }
}
export const maybeOpenChatGpt = async (
  shouldOpen: boolean,
  prompt: string,
  showFeedback: boolean,
): Promise<void> => {
  if (!shouldOpen) {
    return
  }
  const url = `https://chatgpt.com/?q=${encodeURIComponent(prompt)}`
  try {
    await open(url)
    if (showFeedback) {
      console.log(chalk.green('✓ Opened ChatGPT with the generated prompt.'))
    }
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown browser error.'
    console.warn(chalk.yellow(`Failed to open ChatGPT: ${message}`))
  }
}
</file>

<file path="src/generate/command.ts">
import { parseGenerateArgs } from './args'
import { runGeneratePipeline } from './pipeline'
export const runGenerateCommand = async (argv: string[]): Promise<void> => {
  const { args, showHelp } = parseGenerateArgs(argv)
  if (args.help) {
    showHelp()
    return
  }
  await runGeneratePipeline(args)
}
</file>

<file path="src/generate/context-output.ts">
import fs from 'node:fs/promises'
import chalk from 'chalk'
import type { FileContext } from '../file-context'
export const displayContextFiles = (
  files: FileContext[],
  format: 'text' | 'json',
  writeLine: (line: string) => void,
): void => {
  if (format === 'json') {
    writeLine(serializeContextAsJson(files))
    return
  }
  writeLine(`\n${chalk.bold.cyan('Context Files')}`)
  writeLine(chalk.dim('──────────────'))
  if (files.length === 0) {
    writeLine(chalk.dim('(none)'))
    return
  }
  files.forEach((file, index) => {
    writeLine(`<file path="${file.path}">`)
    writeLine(file.content)
    writeLine('</file>')
    if (index < files.length - 1) {
      writeLine('')
    }
  })
}
export const writeContextFile = async (
  filePath: string,
  format: 'text' | 'json',
  files: FileContext[],
): Promise<void> => {
  const payload = format === 'json' ? serializeContextAsJson(files) : serializeContextAsText(files)
  await fs.writeFile(filePath, payload, 'utf8')
}
export const serializeContextAsJson = (files: FileContext[]): string =>
  JSON.stringify(
    files.map(({ path, content }) => ({ path, content })),
    null,
    2,
  )
export const serializeContextAsText = (files: FileContext[]): string => {
  if (files.length === 0) {
    return '(none)'
  }
  return files
    .map((file) => [`<file path="${file.path}">`, file.content, '</file>'].join('\n'))
    .join('\n\n')
}
</file>

<file path="src/generate/context-templates.ts">
import { loadCliConfig } from '../config'
const CONTEXT_TEMPLATE_PLACEHOLDER = '{{prompt}}'
const BUILT_IN_CONTEXT_TEMPLATES: Record<string, string> = {
  nvim: [
    '## NeoVim Prompt Buffer',
    'Paste this block into a scratch buffer (e.g., :enew) so you can keep prompts beside your work.',
    CONTEXT_TEMPLATE_PLACEHOLDER,
  ].join('\n\n'),
}
export const renderContextTemplate = (template: string, prompt: string): string => {
  if (template.includes(CONTEXT_TEMPLATE_PLACEHOLDER)) {
    return template.split(CONTEXT_TEMPLATE_PLACEHOLDER).join(prompt)
  }
  const trimmedTemplate = template.trimEnd()
  if (!trimmedTemplate) {
    return prompt
  }
  return `${trimmedTemplate}\n\n${prompt}`
}
export const resolveContextTemplate = async (name: string): Promise<string> => {
  const builtIn = BUILT_IN_CONTEXT_TEMPLATES[name]
  if (builtIn) {
    return builtIn
  }
  const config = await loadCliConfig()
  const fromConfig = config?.contextTemplates?.[name]
  if (fromConfig) {
    return fromConfig
  }
  const available = [
    ...Object.keys(BUILT_IN_CONTEXT_TEMPLATES),
    ...(config?.contextTemplates ? Object.keys(config.contextTemplates) : []),
  ]
  const availableList = available.length > 0 ? available.join(', ') : 'none'
  throw new Error(`Unknown context template "${name}". Available templates: ${availableList}.`)
}
</file>

<file path="src/generate/debug.ts">
const envFlagEnabled = (value: string | undefined): boolean => {
  if (!value) {
    return false
  }
  const normalized = value.trim().toLowerCase()
  return normalized !== '0' && normalized !== 'false'
}
export const shouldTraceFlags = (): boolean => envFlagEnabled(process.env.PROMPT_MAKER_DEBUG_FLAGS)
export const shouldTraceCopy = (): boolean =>
  envFlagEnabled(process.env.PROMPT_MAKER_COPY_TRACE) || shouldTraceFlags()
</file>

<file path="src/generate/display.ts">
import boxen from 'boxen'
import chalk from 'chalk'
import { formatTokenCount } from '../token-counter'
export const displayPrompt = (prompt: string, iteration: number, tokenCount?: number): void => {
  const label = iteration === 1 ? 'Generated Prompt' : `Iteration ${iteration}`
  const meta = typeof tokenCount === 'number' ? chalk.dim(` · ${formatTokenCount(tokenCount)}`) : ''
  const title = chalk.bold.green(`${label}${meta}`)
  const boxed = boxen(prompt, {
    padding: { left: 1, right: 1, top: 0, bottom: 0 },
    borderColor: 'green',
    borderStyle: 'round',
    title,
    titleAlignment: 'left',
  })
  console.log(`\n${boxed}`)
}
export const displayPolishedPrompt = (prompt: string, model: string): void => {
  const title = chalk.bold.magenta(`Polished Prompt · ${model}`)
  const boxed = boxen(prompt, {
    padding: { left: 1, right: 1, top: 0, bottom: 0 },
    borderColor: 'magenta',
    borderStyle: 'round',
    title,
    titleAlignment: 'left',
  })
  console.log(`\n${boxed}`)
}
export const displayContextTemplatePrompt = (prompt: string, templateName: string): void => {
  const title = chalk.bold.blue(`Context Template · ${templateName}`)
  const boxed = boxen(prompt, {
    padding: { left: 1, right: 1, top: 0, bottom: 0 },
    borderColor: 'blue',
    borderStyle: 'round',
    title,
    titleAlignment: 'left',
  })
  console.log(`\n${boxed}`)
}
</file>

<file path="src/generate/fs-utils.ts">
export const isFsNotFoundError = (error: unknown): boolean =>
  Boolean(
    error &&
    typeof error === 'object' &&
    'code' in error &&
    typeof (error as { code?: unknown }).code === 'string' &&
    (error as { code: string }).code === 'ENOENT',
  )
</file>

<file path="src/generate/intent.ts">
import fs from 'node:fs/promises'
import chalk from 'chalk'
import type { GenerateArgs } from './types'
import { readFromStdin } from '../io'
import { isFsNotFoundError } from './fs-utils'
const MAX_INTENT_FILE_BYTES = 512 * 1024
export const resolveIntent = async (args: GenerateArgs): Promise<string> => {
  if (args.intent && args.intentFile) {
    throw new Error('Provide either an inline intent argument or --intent-file, not both.')
  }
  if (args.intentFile) {
    return await readIntentFile(args.intentFile)
  }
  const inlineIntentFromInteractiveFlag = await maybeResolveInlineIntentFile(args)
  if (inlineIntentFromInteractiveFlag) {
    return inlineIntentFromInteractiveFlag
  }
  if (args.intent?.trim()) {
    return args.intent.trim()
  }
  const piped = await readFromStdin()
  if (piped?.trim()) {
    return piped.trim()
  }
  throw new Error(
    'Intent text is required. Provide a quoted argument, use --intent-file, or pipe text via stdin.',
  )
}
const maybeResolveInlineIntentFile = async (args: GenerateArgs): Promise<string | null> => {
  if (!args.inlineIntentAfterInteractive || !args.intent) {
    return null
  }
  const candidatePath = args.intent.trim()
  if (!candidatePath) {
    return null
  }
  try {
    const content = await readIntentFile(candidatePath)
    console.warn(
      chalk.yellow(
        [
          `Detected "${candidatePath}" immediately after -i/--interactive.`,
          'Treating it as an intent file. Use -f/--intent-file (optionally alongside --interactive) for clearer commands and restored progress feedback.',
        ].join(' '),
      ),
    )
    return content
  } catch (error) {
    if (isFsNotFoundError(error)) {
      return null
    }
    throw error
  }
}
const readIntentFile = async (filePath: string): Promise<string> => {
  const stats = await fs.stat(filePath)
  if (stats.size > MAX_INTENT_FILE_BYTES) {
    const sizeKb = (stats.size / 1024).toFixed(1)
    throw new Error(`Intent file ${filePath} is too large (${sizeKb} KB).`)
  }
  const buffer = await fs.readFile(filePath)
  if (buffer.includes(0)) {
    throw new Error(`Intent file ${filePath} appears to be binary. Provide a UTF-8 text file.`)
  }
  const trimmed = buffer.toString('utf8').trim()
  if (!trimmed) {
    throw new Error(`Intent file ${filePath} is empty.`)
  }
  return trimmed
}
</file>

<file path="src/generate/interactive-transport.ts">
import fs from 'node:fs/promises'
import net from 'node:net'
import { isFsNotFoundError } from './fs-utils'
import type { TransportLifecycleEventInput } from './types'
import type { StreamWriter } from './stream'
type InteractiveCommand = { type: 'refine'; instruction: string } | { type: 'finish' }
const isWindowsNamedPipePath = (target: string): boolean => target.startsWith('\\\\.\\pipe\\')
export class InteractiveTransport {
  private server: net.Server | null = null
  private client: net.Socket | null = null
  private buffer = ''
  private commandQueue: InteractiveCommand[] = []
  private pendingResolvers: Array<(command: InteractiveCommand | null) => void> = []
  private stopped = false
  private lifecycleEmitter?: (event: TransportLifecycleEventInput) => void
  constructor(private readonly socketPath: string) {}
  setEventEmitter(callback: (event: TransportLifecycleEventInput) => void): void {
    this.lifecycleEmitter = callback
  }
  async start(): Promise<void> {
    if (!isWindowsNamedPipePath(this.socketPath)) {
      try {
        await fs.unlink(this.socketPath)
      } catch (error) {
        if (!isFsNotFoundError(error)) {
          throw error
        }
      }
    }
    await new Promise<void>((resolve, reject) => {
      const server = net.createServer((socket) => this.handleConnection(socket))
      const onError = (error: Error): void => {
        server.close()
        reject(error)
      }
      server.once('error', onError)
      server.listen(this.socketPath, () => {
        server.off('error', onError)
        this.server = server
        this.emitLifecycle({ event: 'transport.listening', path: this.socketPath })
        resolve()
      })
    })
  }
  async stop(): Promise<void> {
    if (this.stopped) {
      return
    }
    this.stopped = true
    if (this.client) {
      this.client.removeAllListeners()
      this.client.destroy()
      this.client = null
    }
    if (this.server) {
      await new Promise<void>((resolve) => {
        this.server?.close(() => resolve())
      })
      this.server = null
    }
    if (!isWindowsNamedPipePath(this.socketPath)) {
      try {
        await fs.unlink(this.socketPath)
      } catch (error) {
        if (!isFsNotFoundError(error)) {
          throw error
        }
      }
    }
    this.flushPending()
  }
  getEventWriter(): StreamWriter {
    return (chunk) => {
      if (this.client && !this.client.destroyed) {
        this.client.write(chunk)
      }
    }
  }
  async nextCommand(): Promise<InteractiveCommand | null> {
    if (this.commandQueue.length > 0) {
      return this.commandQueue.shift() ?? null
    }
    if (this.stopped) {
      return null
    }
    return await new Promise<InteractiveCommand | null>((resolve) => {
      this.pendingResolvers.push(resolve)
    })
  }
  private handleConnection(socket: net.Socket): void {
    if (this.client && !this.client.destroyed) {
      this.client.destroy()
    }
    this.client = socket
    this.buffer = ''
    this.emitLifecycle({ event: 'transport.client.connected', status: 'connected' })
    socket.setEncoding('utf8')
    socket.on('data', (data: string) => {
      this.handleData(data)
    })
    socket.on('close', () => {
      if (this.client === socket) {
        this.client = null
      }
      this.emitLifecycle({ event: 'transport.client.disconnected', status: 'disconnected' })
      this.flushPending()
    })
    socket.on('error', () => {
      if (this.client === socket) {
        this.client = null
        this.emitLifecycle({ event: 'transport.client.disconnected', status: 'disconnected' })
      }
    })
  }
  private handleData(data: string): void {
    this.buffer += data
    let newlineIndex = this.buffer.indexOf('\n')
    while (newlineIndex >= 0) {
      const raw = this.buffer.slice(0, newlineIndex).trim()
      this.buffer = this.buffer.slice(newlineIndex + 1)
      if (raw) {
        this.processRawCommand(raw)
      }
      newlineIndex = this.buffer.indexOf('\n')
    }
  }
  private processRawCommand(raw: string): void {
    try {
      const parsed = JSON.parse(raw) as { type?: string; instruction?: unknown }
      if (parsed.type === 'refine' && typeof parsed.instruction === 'string') {
        const instruction = parsed.instruction.trim()
        if (!instruction) {
          this.sendTransportError('Refinement instruction must be non-empty.')
          return
        }
        this.enqueueCommand({ type: 'refine', instruction })
        return
      }
      if (parsed.type === 'finish') {
        this.enqueueCommand({ type: 'finish' })
        return
      }
      this.sendTransportError('Unknown interactive command.')
    } catch {
      this.sendTransportError('Invalid command payload; expected JSON.')
    }
  }
  private enqueueCommand(command: InteractiveCommand): void {
    if (this.pendingResolvers.length > 0) {
      const resolve = this.pendingResolvers.shift()
      resolve?.(command)
      return
    }
    this.commandQueue.push(command)
  }
  private flushPending(): void {
    while (this.pendingResolvers.length > 0) {
      const resolve = this.pendingResolvers.shift()
      resolve?.(null)
    }
    this.commandQueue = []
  }
  private sendTransportError(message: string): void {
    if (this.client && !this.client.destroyed) {
      this.client.write(`${JSON.stringify({ event: 'transport.error', message })}\n`)
    }
  }
  private emitLifecycle(event: TransportLifecycleEventInput): void {
    this.lifecycleEmitter?.(event)
  }
}
</file>

<file path="src/generate/polish.ts">
import { callLLM } from '@prompt-maker/core'
import {
  ensureModelCredentials,
  sanitizePromptForTargetModelLeakage,
} from '../prompt-generator-service'
const POLISH_SYSTEM_PROMPT =
  'You refine prompt contracts for language models. Preserve headings, bullet ordering, and constraints. Only tighten wording and fix inconsistencies.'
export const polishPrompt = async (
  originalIntent: string,
  prompt: string,
  model: string,
  targetModel?: string,
): Promise<string> => {
  await ensureModelCredentials(model)
  const normalizedTargetModel = targetModel?.trim() ?? ''
  const targetGuidance = normalizedTargetModel
    ? [
        'Internal Optimization Target (do not include in output):',
        `- targetRuntimeModel: ${normalizedTargetModel}`,
        '',
        'Rules (non-negotiable):',
        '- Use the target runtime model only to tune compliance, clarity, and formatting expectations.',
        '- Do NOT mention or output the target runtime model id/label/name anywhere in the polished prompt text.',
        '- Do NOT include phrases like "Target runtime model" / "Target Runtime Model" in the polished prompt text.',
        '- Only include the target model id/label/name if the user intent explicitly asks to mention it.',
      ].join('\n')
    : ''
  const messages = [
    { role: 'system' as const, content: POLISH_SYSTEM_PROMPT },
    ...(targetGuidance ? [{ role: 'system' as const, content: targetGuidance }] : []),
    {
      role: 'user' as const,
      content: [
        'Intent:',
        originalIntent,
        '---',
        'Generated prompt candidate:',
        prompt,
        '---',
        'Return the polished prompt text, preserving exact sections.',
      ].join('\n'),
    },
  ]
  const raw = await callLLM(messages, model)
  return sanitizePromptForTargetModelLeakage({
    prompt: raw,
    intent: originalIntent,
    targetModel: normalizedTargetModel,
  })
}
</file>

<file path="src/generate/progress.ts">
import chalk from 'chalk'
import ora from 'ora'
import type { UploadStateChange } from '../prompt-generator-service'
import type { StreamDispatcher } from './stream'
export type ProgressHandle = {
  stop: (finalMessage?: string) => void
  setLabel: (label: string) => void
}
export const startProgress = (label: string, options: { showSpinner: boolean }): ProgressHandle => {
  const spinner = options.showSpinner
    ? ora({
        text: chalk.dim(label),
        color: 'cyan',
        spinner: 'dots',
      }).start()
    : null
  let stopped = false
  const stop = (finalMessage?: string): void => {
    if (stopped) {
      return
    }
    stopped = true
    if (spinner) {
      if (finalMessage) {
        spinner.succeed(finalMessage)
      } else {
        spinner.succeed(chalk.green(`${label} ✓`))
      }
    }
  }
  const setLabel = (nextLabel: string): void => {
    if (stopped) {
      return
    }
    if (spinner) {
      spinner.text = chalk.dim(nextLabel)
    }
  }
  return { stop, setLabel }
}
export const createUploadStateTracker = (
  progress: ProgressHandle | null,
  defaultLabel: string,
  stream?: StreamDispatcher,
): UploadStateChange => {
  let uploadsInFlight = 0
  const uploadLabel = 'Uploading...'
  return (state, detail) => {
    if (state === 'start') {
      uploadsInFlight += 1
      if (uploadsInFlight === 1) {
        progress?.setLabel(uploadLabel)
      }
    } else {
      uploadsInFlight = Math.max(0, uploadsInFlight - 1)
      if (uploadsInFlight === 0) {
        progress?.setLabel(defaultLabel)
      }
    }
    if (stream) {
      stream.emit({ event: 'upload.state', state, detail })
    }
  }
}
</file>

<file path="src/generate/stream.ts">
import { stdout as output } from 'node:process'
import type { StreamEventInput, StreamMode } from './types'
export type StreamWriter = (chunk: string) => void
export type StreamDispatcher = {
  mode: StreamMode
  emit: (event: StreamEventInput) => void
}
type StreamDispatcherOptions = {
  writer?: StreamWriter
  taps?: StreamWriter[]
}
export const createStreamDispatcher = (
  mode: StreamMode,
  options: StreamDispatcherOptions = {},
): StreamDispatcher => {
  const writer =
    options.writer ??
    ((chunk: string): void => {
      output.write(chunk)
    })
  const taps = options.taps ?? []
  const emitToTaps = (serialized: string): void => {
    taps.forEach((tap) => {
      tap(serialized)
    })
  }
  if (mode !== 'jsonl') {
    return {
      mode,
      emit: (event) => {
        if (taps.length === 0) {
          return
        }
        const payload = { ...event, timestamp: new Date().toISOString() }
        const serialized = `${JSON.stringify(payload)}\n`
        emitToTaps(serialized)
      },
    }
  }
  return {
    mode,
    emit: (event) => {
      const payload = { ...event, timestamp: new Date().toISOString() }
      const serialized = `${JSON.stringify(payload)}\n`
      writer(serialized)
      emitToTaps(serialized)
    },
  }
}
</file>

<file path="src/generate/token-telemetry.ts">
import { stdout as output } from 'node:process'
import boxen from 'boxen'
import chalk from 'chalk'
import Table from 'cli-table3'
import type { FileContext } from '../file-context'
import { GEN_SYSTEM_PROMPT } from '../prompt-generator-service'
import { countTokens, formatTokenCount } from '../token-counter'
import type { TokenTelemetry } from './types'
export const buildTokenTelemetry = (
  intentText: string,
  files: FileContext[],
  metaInstructions?: string,
): TokenTelemetry => {
  const fileSummaries = files.map((file) => ({
    path: file.path,
    tokens: countTokens(file.content),
  }))
  const fileTokens = fileSummaries.reduce((acc, file) => acc + file.tokens, 0)
  const intentTokens = countTokens(intentText)
  const metaTokens = metaInstructions?.trim() ? countTokens(metaInstructions) : 0
  const systemTokens = countTokens(GEN_SYSTEM_PROMPT) + metaTokens
  return {
    files: fileSummaries,
    intentTokens,
    fileTokens,
    systemTokens,
    totalTokens: intentTokens + fileTokens + systemTokens,
  }
}
export const displayTokenSummary = ({
  files,
  intentTokens,
  fileTokens,
  systemTokens,
  totalTokens,
}: TokenTelemetry): void => {
  const telemetryLines = [
    `${chalk.gray('Total')}: ${chalk.white(formatTokenCount(totalTokens))}`,
    `${chalk.gray('Intent')}: ${chalk.white(formatTokenCount(intentTokens))}`,
    `${chalk.gray('Files')}: ${chalk.white(formatTokenCount(fileTokens))}`,
    `${chalk.gray('System')}: ${chalk.white(formatTokenCount(systemTokens))}`,
  ].join('\n')
  console.log('')
  console.log(
    boxen(telemetryLines, {
      padding: { left: 1, right: 1, top: 0, bottom: 0 },
      borderColor: 'cyan',
      borderStyle: 'round',
      title: chalk.bold.cyan('Context Telemetry'),
      titleAlignment: 'left',
    }),
  )
  console.log('')
  if (files.length === 0) {
    return
  }
  const terminalWidth = Math.max(60, Math.min(output.columns ?? 100, 110))
  const numberColumnWidth = 4
  const tokensColumnWidth = 14
  const pathColumnWidth = Math.max(24, terminalWidth - numberColumnWidth - tokensColumnWidth)
  const table = new Table({
    head: [chalk.gray('#'), chalk.gray('Path'), chalk.gray('Tokens')],
    style: { head: [], border: [] },
    wordWrap: true,
    colWidths: [numberColumnWidth, pathColumnWidth, tokensColumnWidth],
  })
  files.slice(0, 10).forEach((file, index) => {
    table.push([
      chalk.dim(String(index + 1)),
      chalk.white(file.path),
      chalk.green(formatTokenCount(file.tokens)),
    ])
  })
  console.log(table.toString())
  console.log('')
  if (files.length > 10) {
    console.log(chalk.dim(`…and ${files.length - 10} more context files`))
  }
}
</file>

<file path="src/prompt-generator/model-credentials.ts">
import { loadCliConfig, resolveGeminiCredentials, resolveOpenAiCredentials } from '../config'
import { isGeminiModelId } from '../model-providers'
export const resolveDefaultGenerateModel = async (): Promise<string> => {
  const config = await loadCliConfig()
  return (
    config?.promptGenerator?.defaultModel?.trim() ||
    process.env.PROMPT_MAKER_GENERATE_MODEL?.trim() ||
    'gpt-4o-mini'
  )
}
export const ensureModelCredentials = async (model: string): Promise<void> => {
  if (isGemini(model)) {
    if (!process.env.GEMINI_API_KEY) {
      const credentials = await resolveGeminiCredentials()
      process.env.GEMINI_API_KEY = credentials.apiKey
      if (credentials.baseUrl && !process.env.GEMINI_BASE_URL) {
        process.env.GEMINI_BASE_URL = credentials.baseUrl
      }
    }
    return
  }
  if (!process.env.OPENAI_API_KEY) {
    const credentials = await resolveOpenAiCredentials()
    process.env.OPENAI_API_KEY = credentials.apiKey
    if (credentials.baseUrl && !process.env.OPENAI_BASE_URL) {
      process.env.OPENAI_BASE_URL = credentials.baseUrl
    }
  }
}
export const isGemini = (model: string): boolean => isGeminiModelId(model)
</file>

<file path="src/prompt-generator/parse-llm-json.ts">
export const parseLLMJson = <T>(text: string): T => {
  const cleaned = text
    .trim()
    .replace(/^```json\s*/i, '')
    .replace(/^```\s*/i, '')
    .replace(/```$/i, '')
    .trim()
  try {
    return JSON.parse(cleaned) as T
  } catch {
    console.warn('Failed to parse LLM JSON response. Falling back to raw text.')
    throw new Error('LLM did not return valid JSON.')
  }
}
</file>

<file path="src/prompt-generator/series-repair.ts">
import type { SeriesResponse } from './types'
export const isRepairableSeriesValidationError = (error: unknown): boolean => {
  const message = error instanceof Error ? error.message : String(error)
  return (
    message.startsWith('Atomic prompt ') &&
    (message.includes('missing required section(s)') ||
      message.includes('contains forbidden cross-reference phrase') ||
      message.includes('is missing a title') ||
      message.includes('is missing content'))
  )
}
export const buildSeriesRepairUserMessage = (options: {
  intent: string
  validationError: string
  previousSeries: SeriesResponse
}): string => {
  return [
    `User Intent:\n${options.intent.trim()}`,
    '',
    `Validation Error:\n${options.validationError.trim()}`,
    '',
    'Previous SeriesResponse JSON:',
    JSON.stringify(options.previousSeries, null, 2),
    '',
    'Return a corrected SeriesResponse JSON payload now.',
  ].join('\n')
}
</file>

<file path="src/prompt-generator/series-validation.ts">
import type { SeriesResponse } from './types'
const REQUIRED_ATOMIC_PROMPT_SECTIONS: ReadonlyArray<{ label: string; pattern: RegExp }> = [
  { label: '# Title', pattern: /^#\s*Title\b/im },
  { label: 'Role', pattern: /^(?:#{1,6}\s*)?Role\b/im },
  { label: 'Context', pattern: /^(?:#{1,6}\s*)?Context\b/im },
  {
    label: 'Goals & Tasks',
    pattern: /^(?:#{1,6}\s*)?Goals\s*(?:&|and)\s*Tasks\b/im,
  },
  { label: 'Inputs', pattern: /^(?:#{1,6}\s*)?Inputs\b/im },
  { label: 'Constraints', pattern: /^(?:#{1,6}\s*)?Constraints\b/im },
  { label: 'Execution Plan', pattern: /^(?:#{1,6}\s*)?Execution\s+Plan\b/im },
  { label: 'Output Format', pattern: /^(?:#{1,6}\s*)?Output\s+Format\b/im },
  { label: 'Validation', pattern: /^(?:#{1,6}\s*)?Validation\b/im },
]
const FORBIDDEN_CROSS_REFERENCE_PATTERNS: ReadonlyArray<{ label: string; pattern: RegExp }> = [
  { label: '"as above"', pattern: /\bas above\b/i },
  { label: '"as mentioned earlier"', pattern: /\bas mentioned earlier\b/i },
  { label: '"as described earlier"', pattern: /\bas described earlier\b/i },
  {
    label: '"previous step" / "prior step" / "earlier step"',
    pattern: /\b(previous|prior|earlier)\s+step\b/i,
  },
  {
    label: '"previous prompt" / "prior prompt" / "earlier prompt"',
    pattern: /\b(previous|prior|earlier)\s+prompt\b/i,
  },
  { label: '"from step N"', pattern: /\bfrom\s+step\s+\d+\b/i },
  { label: '"in step N"', pattern: /\bin\s+step\s+\d+\b/i },
  { label: '"see step N"', pattern: /\bsee\s+step\s+\d+\b/i },
  { label: '"step N above/below"', pattern: /\bstep\s+\d+\s+(above|below)\b/i },
  { label: '"continue from step N"', pattern: /\bcontinue\s+from\s+step\s+\d+\b/i },
]
const findMissingAtomicPromptSections = (content: string): string[] => {
  return REQUIRED_ATOMIC_PROMPT_SECTIONS.filter((section) => !section.pattern.test(content)).map(
    (section) => section.label,
  )
}
const findForbiddenCrossReference = (content: string): string | null => {
  const hit = FORBIDDEN_CROSS_REFERENCE_PATTERNS.find((entry) => entry.pattern.test(content))
  return hit?.label ?? null
}
export const validateSeriesResponse = (response: SeriesResponse): void => {
  if (!response || typeof response !== 'object') {
    throw new Error('LLM returned SeriesResponse with invalid shape.')
  }
  if (typeof response.reasoning !== 'string' || !response.reasoning.trim()) {
    throw new Error('Series reasoning is required.')
  }
  if (typeof response.overviewPrompt !== 'string' || !response.overviewPrompt.trim()) {
    throw new Error('Series overviewPrompt is required.')
  }
  if (!Array.isArray(response.atomicPrompts) || response.atomicPrompts.length === 0) {
    throw new Error('Series atomicPrompts must include at least one entry.')
  }
  response.atomicPrompts.forEach((entry, index) => {
    const promptNumber = index + 1
    if (!entry || typeof entry !== 'object') {
      throw new Error(`Atomic prompt ${promptNumber} is invalid.`)
    }
    if (typeof entry.title !== 'string' || !entry.title.trim()) {
      throw new Error(`Atomic prompt ${promptNumber} is missing a title.`)
    }
    if (typeof entry.content !== 'string' || !entry.content.trim()) {
      throw new Error(`Atomic prompt ${promptNumber} is missing content.`)
    }
    const missingSections = findMissingAtomicPromptSections(entry.content)
    if (missingSections.length > 0) {
      throw new Error(
        `Atomic prompt ${promptNumber} is missing required section(s): ${missingSections.join(', ')}.`,
      )
    }
    const forbiddenCrossReference = findForbiddenCrossReference(entry.content)
    if (forbiddenCrossReference) {
      throw new Error(
        `Atomic prompt ${promptNumber} contains forbidden cross-reference phrase ${forbiddenCrossReference}. Atomic prompts must be standalone.`,
      )
    }
  })
}
</file>

<file path="src/prompt-generator/target-model-guidance.ts">
const TARGET_RUNTIME_MODEL_PHRASE_REGEX = /target runtime model/i
const escapeRegExp = (value: string): string => value.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')
export const sanitizePromptForTargetModelLeakage = ({
  prompt,
  intent,
  targetModel,
}: {
  prompt: string
  intent: string
  targetModel: string
}): string => {
  const normalizedTargetModel = targetModel.trim()
  if (!normalizedTargetModel) {
    return prompt
  }
  const normalizedIntent = intent.trim().toLowerCase()
  const normalizedTargetLower = normalizedTargetModel.toLowerCase()
  if (normalizedIntent.includes(normalizedTargetLower)) {
    return prompt
  }
  const normalizedPromptLower = prompt.toLowerCase()
  if (
    !TARGET_RUNTIME_MODEL_PHRASE_REGEX.test(prompt) &&
    !normalizedPromptLower.includes(normalizedTargetLower)
  ) {
    return prompt
  }
  const withoutTargetModelLines = prompt
    .split('\n')
    .filter((line) => !TARGET_RUNTIME_MODEL_PHRASE_REGEX.test(line))
    .join('\n')
  const targetRegex = new RegExp(escapeRegExp(normalizedTargetModel), 'gi')
  const withoutTargetMentions = withoutTargetModelLines.replace(targetRegex, '')
  const cleaned = withoutTargetMentions
    .split('\n')
    .map((line) =>
      line
        .replace(/\*\*\s*\*\*/g, '')
        .replace(/[ \t]{2,}/g, ' ')
        .trimEnd(),
    )
    .join('\n')
    .replace(/\n{3,}/g, '\n\n')
  return cleaned.trim()
}
export const buildTargetRuntimeModelGuidance = (targetModel: string): string => {
  const normalized = targetModel.trim()
  if (!normalized) {
    return ''
  }
  return [
    'Internal Optimization Target (do not include in output):',
    `- targetRuntimeModel: ${normalized}`,
    '',
    'Rules (non-negotiable):',
    '- Use the target runtime model only to tune the contract for compliance, clarity, and formatting expectations.',
    '- Do NOT mention or output the target runtime model id/label/name anywhere in the returned prompt text.',
    '- Do NOT include phrases like "Target runtime model" / "Target Runtime Model" in the returned prompt text.',
    '- Only include the target model id/label/name if the user intent explicitly asks to mention it.',
  ].join('\n')
}
</file>

<file path="src/rag/vector-store.ts">
import crypto from 'node:crypto'
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
import { getEmbedding } from '@prompt-maker/core'
const CACHE_DIR = path.join(os.homedir(), '.config', 'prompt-maker-cli')
const CACHE_FILE = path.join(CACHE_DIR, 'embeddings_cache.json')
type CacheEntry = {
  hash: string
  embedding: number[]
}
type EmbeddingCache = Record<string, CacheEntry>
let inMemoryCache: EmbeddingCache | null = null
export const indexFiles = async (filePaths: string[]): Promise<void> => {
  if (filePaths.length === 0) {
    return
  }
  const uniquePaths = [...new Set(filePaths)]
  const cache = await loadCache()
  let changed = false
  for (const filePath of uniquePaths) {
    try {
      const contents = await fs.readFile(filePath, 'utf8')
      const hash = hashContent(contents)
      const existing = cache[filePath]
      if (existing && existing.hash === hash) {
        continue
      }
      const embedding = await getEmbedding(contents)
      cache[filePath] = { hash, embedding }
      changed = true
    } catch (error) {
      const message = error instanceof Error ? error.message : String(error)
      console.warn(`Failed to index ${filePath}: ${message}`)
    }
  }
  if (changed) {
    await saveCache(cache)
  }
}
export const search = async (
  query: string,
  k: number,
  allowedPaths?: Iterable<string>,
): Promise<string[]> => {
  if (!query.trim() || k <= 0) {
    return []
  }
  const cache = await loadCache()
  let entries = Object.entries(cache)
  if (entries.length === 0) {
    return []
  }
  if (allowedPaths) {
    const allowedSet = new Set(Array.from(allowedPaths, (filePath) => path.resolve(filePath)))
    entries = entries.filter(([filePath]) => allowedSet.has(path.resolve(filePath)))
    if (entries.length === 0) {
      return []
    }
  }
  const queryEmbedding = await getEmbedding(query)
  const scored = entries
    .map(([filePath, entry]) => ({
      filePath,
      score: cosineSimilarity(queryEmbedding, entry.embedding),
    }))
    .filter(({ score }) => Number.isFinite(score))
    .sort((a, b) => b.score - a.score)
    .slice(0, Math.min(k, entries.length))
    .map(({ filePath }) => filePath)
  return scored
}
const loadCache = async (): Promise<EmbeddingCache> => {
  if (inMemoryCache) {
    return inMemoryCache
  }
  try {
    const contents = await fs.readFile(CACHE_FILE, 'utf8')
    const parsed = JSON.parse(contents) as unknown
    if (isEmbeddingCache(parsed)) {
      inMemoryCache = parsed
      return parsed
    }
    console.warn('Embeddings cache file is malformed. Resetting cache.')
  } catch (error) {
    if (!isFileMissingError(error)) {
      const message = error instanceof Error ? error.message : String(error)
      console.warn(`Failed to load embeddings cache: ${message}`)
    }
  }
  inMemoryCache = {}
  return inMemoryCache
}
const saveCache = async (cache: EmbeddingCache): Promise<void> => {
  await fs.mkdir(CACHE_DIR, { recursive: true })
  await fs.writeFile(CACHE_FILE, JSON.stringify(cache, null, 2), 'utf8')
  inMemoryCache = cache
}
const hashContent = (contents: string): string => {
  return crypto.createHash('sha256').update(contents).digest('hex')
}
const cosineSimilarity = (left: number[], right: number[]): number => {
  if (left.length === 0 || right.length === 0 || left.length !== right.length) {
    return Number.NEGATIVE_INFINITY
  }
  let dot = 0
  let normLeft = 0
  let normRight = 0
  for (let i = 0; i < left.length; i += 1) {
    const a = left[i]
    const b = right[i]
    if (a === undefined || b === undefined) {
      return Number.NEGATIVE_INFINITY
    }
    dot += a * b
    normLeft += a * a
    normRight += b * b
  }
  if (normLeft === 0 || normRight === 0) {
    return Number.NEGATIVE_INFINITY
  }
  return dot / (Math.sqrt(normLeft) * Math.sqrt(normRight))
}
const isEmbeddingCache = (value: unknown): value is EmbeddingCache => {
  if (!value || typeof value !== 'object') {
    return false
  }
  return Object.values(value as Record<string, unknown>).every(isCacheEntry)
}
const isCacheEntry = (value: unknown): value is CacheEntry => {
  if (!value || typeof value !== 'object') {
    return false
  }
  const record = value as Record<string, unknown>
  return (
    typeof record.hash === 'string' &&
    Array.isArray(record.embedding) &&
    record.embedding.every(isFiniteNumber)
  )
}
const isFiniteNumber = (value: unknown): value is number =>
  typeof value === 'number' && Number.isFinite(value)
const hasErrnoCode = (error: unknown): error is { code: string } =>
  typeof error === 'object' && error !== null && 'code' in error
const isFileMissingError = (error: unknown): boolean =>
  hasErrnoCode(error) && error.code === 'ENOENT'
</file>

<file path="src/testing/evaluator.ts">
import { callLLM } from '@prompt-maker/core'
import { ensureModelCredentials } from '../prompt-generator-service'
const DEFAULT_JUDGE_MODEL = process.env.PROMPT_MAKER_JUDGE_MODEL?.trim() || 'gpt-4o'
const SYSTEM_PROMPT =
  'You are a QA bot. Evaluate if the following prompt meets these criteria. Return strict JSON with keys "pass" (boolean) and "reason" (string).'
type JudgeVerdict = {
  pass: boolean
  reason: string
}
export const evaluatePrompt = async (
  generatedPrompt: string,
  criteria: string[],
): Promise<JudgeVerdict> => {
  if (!criteria || criteria.length === 0) {
    return { pass: true, reason: 'No criteria provided.' }
  }
  const model = DEFAULT_JUDGE_MODEL
  await ensureModelCredentials(model)
  const criteriaList = criteria.map((item, index) => `${index + 1}. ${item}`).join('\n')
  const userMessage = [
    'Prompt to Evaluate:',
    generatedPrompt,
    '',
    'Criteria:',
    criteriaList,
    '',
    'Return JSON like { "pass": boolean, "reason": string } indicating whether the prompt satisfies all criteria.',
  ].join('\n')
  try {
    const response = await callLLM(
      [
        { role: 'system', content: SYSTEM_PROMPT },
        { role: 'user', content: userMessage },
      ],
      model,
    )
    const verdict = parseVerdict(response)
    return verdict
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown evaluation error.'
    return { pass: false, reason: `Evaluation failed: ${message}` }
  }
}
const parseVerdict = (text: string): JudgeVerdict => {
  try {
    const cleaned = text
      .trim()
      .replace(/```json/gi, '')
      .replace(/```/g, '')
      .trim()
    const parsed = JSON.parse(cleaned) as Partial<JudgeVerdict>
    if (typeof parsed.pass === 'boolean' && typeof parsed.reason === 'string') {
      return parsed as JudgeVerdict
    }
    throw new Error('Missing pass/reason fields')
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown parse error'
    return {
      pass: false,
      reason: `Judge returned invalid JSON: ${message}. Raw response: ${truncate(text, 200)}`,
    }
  }
}
const truncate = (value: string, max: number): string => {
  if (value.length <= max) {
    return value
  }
  return `${value.slice(0, max)}…`
}
</file>

<file path="src/testing/test-schema.ts">
import { z } from 'zod'
export const PromptTestSchema = z.object({
  name: z.string().min(1, 'Test name is required'),
  intent: z.string().min(1, 'Intent text is required'),
  context: z.array(z.string().min(1)).default([]),
  image: z.array(z.string()).optional(),
  video: z.array(z.string()).optional(),
  smartContext: z.boolean().optional(),
  smartContextRoot: z.string().min(1).optional(),
  expect: z.array(z.string().min(1)).min(1, 'Each test must include at least one expectation'),
})
export const PromptTestSuiteSchema = z.object({
  tests: z.array(PromptTestSchema).min(1, 'Provide at least one test case'),
})
export type PromptTest = z.infer<typeof PromptTestSchema>
export type PromptTestSuite = z.infer<typeof PromptTestSuiteSchema>
export const parsePromptTestSuite = (data: unknown): PromptTestSuite => {
  return PromptTestSuiteSchema.parse(data)
}
</file>

<file path="src/tui/components/core/BackgroundFill.tsx">
import React, { useMemo } from 'react'
import { Box, Text } from 'ink'
const NBSP = '\u00A0'
import type { InkColorValue } from '../../theme/theme-types'
import { inkBackgroundColorProps } from '../../theme/theme-types'
export type BackgroundFillProps = {
  rows: number
  columns: number
  background: InkColorValue
}
export const BackgroundFill: React.FC<BackgroundFillProps> = ({ rows, columns, background }) => {
  const safeRows = Math.max(0, Math.floor(rows))
  const safeColumns = Math.max(0, Math.floor(columns))
  const line = useMemo(() => {
    if (safeColumns === 0) {
      return ''
    }
    return NBSP.repeat(safeColumns)
  }, [safeColumns])
  if (safeRows === 0 || safeColumns === 0) {
    return null
  }
  return (
    <Box flexDirection="column" width={safeColumns} height={safeRows} overflow="hidden">
      {Array.from({ length: safeRows }).map((_, index) => (
        <Text key={`bg-${index}`} {...inkBackgroundColorProps(background)} wrap="truncate">
          {line}
        </Text>
      ))}
    </Box>
  )
}
</file>

<file path="src/tui/components/core/bracketed-paste.ts">
export const stripBracketedPasteControlSequences = (value: string): string => {
  if (!value) {
    return value
  }
  return value.replace(/(?:\u001b)?\[(?:200|201)~/g, '')
}
export const stripTerminalPasteArtifacts = (value: string): string => {
  if (!value) {
    return value
  }
  const withoutBracketedPaste = stripBracketedPasteControlSequences(value)
  return withoutBracketedPaste.replace(/(?:\u001b)?\[[0-9;]*m/g, '')
}
</file>

<file path="src/tui/components/core/CommandMenu.tsx">
import { memo, useMemo } from 'react'
import { Box, Text, useStdout } from 'ink'
import type { CommandDescriptor } from '../../types'
import { useTheme } from '../../theme/theme-provider'
import {
  inkBackgroundColorProps,
  inkBorderColorProps,
  inkColorProps,
} from '../../theme/theme-types'
const APP_CONTAINER_PADDING_X = 2
const COMMAND_SCREEN_PADDING_X = 1
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type CommandMenuProps = {
  commands: readonly CommandDescriptor[]
  selectedIndex: number
}
export const CommandMenu = memo(({ commands, selectedIndex }: CommandMenuProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const borderColumns = 2
  const paddingColumns = 2
  const boxWidth = Math.max(
    0,
    terminalColumns - 2 * (APP_CONTAINER_PADDING_X + COMMAND_SCREEN_PADDING_X),
  )
  const contentWidth = Math.max(0, boxWidth - borderColumns - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.panelBackground)
  const commandLines = useMemo(
    () =>
      commands.map((command) => {
        const shortcut = `/${command.id}`.padEnd(10)
        return padRight(`${shortcut} ${command.description}`, contentWidth)
      }),
    [commands, contentWidth],
  )
  return (
    <Box
      flexDirection="column"
      borderStyle="round"
      paddingX={1}
      paddingY={0}
      width="100%"
      {...inkBorderColorProps(theme.border)}
      {...backgroundProps}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Commands', contentWidth)}
      </Text>
      {commandLines.length === 0 ? (
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('No commands match.', contentWidth)}
        </Text>
      ) : (
        commandLines.map((line, index) => {
          const isSelected = index === selectedIndex
          const textProps = isSelected
            ? {
                ...inkColorProps(theme.selectionText),
                ...inkBackgroundColorProps(theme.selectionBackground),
              }
            : { ...backgroundProps, ...inkColorProps(theme.text) }
          return (
            <Text key={commands[index]?.id ?? String(index)} {...textProps}>
              {line}
            </Text>
          )
        })
      )}
    </Box>
  )
})
CommandMenu.displayName = 'CommandMenu'
</file>

<file path="src/tui/components/core/help-overlay-scroll.ts">
export const getHelpOverlayContentRows = (height: number): number => {
  return Math.max(1, height - 5)
}
export const getHelpOverlayMaxScroll = (lineCount: number, contentRows: number): number => {
  return Math.max(0, lineCount - contentRows)
}
export const clampHelpOverlayScrollOffset = (
  offset: number,
  lineCount: number,
  contentRows: number,
): number => {
  const maxScroll = getHelpOverlayMaxScroll(lineCount, contentRows)
  return Math.max(0, Math.min(offset, maxScroll))
}
export const scrollHelpOverlayBy = (
  offset: number,
  delta: number,
  lineCount: number,
  contentRows: number,
): number => {
  return clampHelpOverlayScrollOffset(offset + delta, lineCount, contentRows)
}
</file>

<file path="src/tui/components/core/HelpOverlay.tsx">
import React, { useEffect, useMemo, useState } from 'react'
import { Box, Text, useInput, useStdout } from 'ink'
import { COMMAND_DESCRIPTORS } from '../../config'
import { createHelpSections, estimateHelpOverlayHeight } from '../../help-config'
import {
  clampHelpOverlayScrollOffset,
  getHelpOverlayContentRows,
  getHelpOverlayMaxScroll,
  scrollHelpOverlayBy,
} from './help-overlay-scroll'
import { useTheme } from '../../theme/theme-provider'
import {
  inkBackgroundColorProps,
  inkBorderColorProps,
  inkColorProps,
} from '../../theme/theme-types'
const APP_CONTAINER_PADDING_X = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
const padLeft = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padStart(width, ' ')
}
export type HelpOverlayProps = {
  activeView: 'generate' | 'tests'
  maxHeight?: number
}
export const HelpOverlay: React.FC<HelpOverlayProps> = ({ activeView: _activeView, maxHeight }) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const overlayWidth = Math.max(40, terminalColumns - 2 * APP_CONTAINER_PADDING_X)
  const borderColumns = 2
  const paddingColumns = 2
  const contentWidth = Math.max(0, overlayWidth - borderColumns - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const sections = useMemo(
    () => createHelpSections({ commandDescriptors: COMMAND_DESCRIPTORS }),
    [],
  )
  const idealHeight = estimateHelpOverlayHeight(sections)
  const clampedHeight = maxHeight ? Math.min(idealHeight, maxHeight) : idealHeight
  const height = Math.max(10, clampedHeight)
  const contentLines = useMemo(() => {
    const lines: string[] = []
    for (const section of sections) {
      lines.push(section.title)
      lines.push(...section.lines)
      lines.push('')
    }
    while (lines.length > 0 && lines[lines.length - 1] === '') {
      lines.pop()
    }
    return lines
  }, [sections])
  const contentRows = getHelpOverlayContentRows(height)
  const maxScroll = getHelpOverlayMaxScroll(contentLines.length, contentRows)
  const [scrollOffset, setScrollOffset] = useState(0)
  useEffect(() => {
    setScrollOffset((prev) => clampHelpOverlayScrollOffset(prev, contentLines.length, contentRows))
  }, [contentLines.length, contentRows])
  useInput((_, key) => {
    if (key.upArrow) {
      setScrollOffset((prev) => scrollHelpOverlayBy(prev, -1, contentLines.length, contentRows))
      return
    }
    if (key.downArrow) {
      setScrollOffset((prev) => scrollHelpOverlayBy(prev, 1, contentLines.length, contentRows))
      return
    }
    if (key.pageUp) {
      setScrollOffset((prev) =>
        scrollHelpOverlayBy(prev, -contentRows, contentLines.length, contentRows),
      )
      return
    }
    if (key.pageDown) {
      setScrollOffset((prev) =>
        scrollHelpOverlayBy(prev, contentRows, contentLines.length, contentRows),
      )
    }
  })
  const sectionTitles = useMemo(() => new Set(sections.map((section) => section.title)), [sections])
  const clampedOffset = clampHelpOverlayScrollOffset(scrollOffset, contentLines.length, contentRows)
  const visibleLines = useMemo(() => {
    const slice = contentLines.slice(clampedOffset, clampedOffset + contentRows)
    if (slice.length >= contentRows) {
      return slice
    }
    const padded = [...slice]
    while (padded.length < contentRows) {
      padded.push('')
    }
    return padded
  }, [clampedOffset, contentLines, contentRows])
  const showScrollHint = maxScroll > 0
  const scrollLabel = showScrollHint
    ? `↑/↓ scroll (${clampedOffset + 1}-${Math.min(clampedOffset + contentRows, contentLines.length)}/${contentLines.length})`
    : ''
  const headerLeft = 'Help'
  const headerRight = 'Esc / ? to close'
  const headerGap = Math.max(0, contentWidth - headerLeft.length - headerRight.length)
  return (
    <Box
      flexDirection="column"
      borderStyle="round"
      paddingX={1}
      paddingY={0}
      height={height}
      width={overlayWidth}
      overflow="hidden"
      {...inkBorderColorProps(theme.border)}
      {...backgroundProps}
    >
      <Box flexDirection="row">
        <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
          {headerLeft}
        </Text>
        <Text {...backgroundProps}>{' '.repeat(headerGap)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {headerRight}
        </Text>
      </Box>
      <Box flexDirection="column" marginTop={1} height={contentRows} overflow="hidden">
        {visibleLines.map((line, index) => {
          const isSectionTitle = sectionTitles.has(line)
          const color = isSectionTitle ? theme.accent : theme.mutedText
          return (
            <Text key={`${scrollOffset}-${index}`} {...backgroundProps} {...inkColorProps(color)}>
              {padRight(line, contentWidth)}
            </Text>
          )
        })}
      </Box>
      <Box flexDirection="row">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padLeft(scrollLabel, contentWidth)}
        </Text>
      </Box>
    </Box>
  )
}
</file>

<file path="src/tui/components/core/input-bar-layout.ts">
import { getLineCount } from './multiline-text-buffer'
export type InputBarRowEstimateOptions = {
  value: string
  hint?: string | undefined
  debugLine?: string | undefined
}
export const estimateInputBarRows = ({
  value,
  hint,
  debugLine,
}: InputBarRowEstimateOptions): number => {
  const lineCount = getLineCount(value)
  const contentRows = 2 + (hint ? 1 : 0) + (debugLine ? 1 : 0) + lineCount
  // Keep the input bar as compact as possible; callers can pad if desired.
  return contentRows
}
</file>

<file path="src/tui/components/core/input-bar-presentation.ts">
export type InputBarMode = 'intent' | 'refinement'
type InputBarTone = 'default' | 'warning'
type InputBarLabelTone = 'muted' | 'warning'
export type InputBarPresentation = {
  borderTone: InputBarTone
  label: string
  labelTone: InputBarLabelTone
  labelBold: boolean
}
export const resolveInputBarPresentation = (mode: InputBarMode): InputBarPresentation => {
  if (mode === 'refinement') {
    return {
      borderTone: 'warning',
      label: 'Refinement (Enter to submit · empty to finish)',
      labelTone: 'warning',
      labelBold: true,
    }
  }
  return {
    borderTone: 'default',
    label: 'Intent / Command',
    labelTone: 'muted',
    labelBold: false,
  }
}
</file>

<file path="src/tui/components/core/multiline-text-buffer.ts">
import { stripBracketedPasteControlSequences } from './bracketed-paste'
export type MultilineTextBufferState = {
  value: string
  cursor: number
}
export type CursorCoordinates = {
  row: number
  column: number
}
export const clampCursor = (cursor: number, value: string): number =>
  Math.max(0, Math.min(cursor, value.length))
export const getLineCount = (value: string): number => {
  if (!value) {
    return 1
  }
  return value.split('\n').length
}
export const getCursorCoordinates = (value: string, cursor: number): CursorCoordinates => {
  const clampedCursor = clampCursor(cursor, value)
  let row = 0
  let column = 0
  for (let index = 0; index < clampedCursor; index += 1) {
    const character = value.charAt(index)
    if (character === '\n') {
      row += 1
      column = 0
    } else {
      column += 1
    }
  }
  return { row, column }
}
export const insertText = (
  state: MultilineTextBufferState,
  text: string,
): MultilineTextBufferState => {
  const sanitized = stripBracketedPasteControlSequences(text)
  if (!sanitized) {
    return state
  }
  const cursor = clampCursor(state.cursor, state.value)
  const nextValue = state.value.slice(0, cursor) + sanitized + state.value.slice(cursor)
  return {
    value: nextValue,
    cursor: cursor + sanitized.length,
  }
}
export const backspace = (state: MultilineTextBufferState): MultilineTextBufferState => {
  const cursor = clampCursor(state.cursor, state.value)
  if (cursor === 0) {
    return { value: state.value, cursor }
  }
  const nextValue = state.value.slice(0, cursor - 1) + state.value.slice(cursor)
  return {
    value: nextValue,
    cursor: cursor - 1,
  }
}
export const deleteForward = (state: MultilineTextBufferState): MultilineTextBufferState => {
  const cursor = clampCursor(state.cursor, state.value)
  if (cursor >= state.value.length) {
    return { value: state.value, cursor }
  }
  const nextValue = state.value.slice(0, cursor) + state.value.slice(cursor + 1)
  return {
    value: nextValue,
    cursor,
  }
}
export const moveCursorLeft = (state: MultilineTextBufferState): MultilineTextBufferState => ({
  value: state.value,
  cursor: Math.max(0, clampCursor(state.cursor, state.value) - 1),
})
export const moveCursorRight = (state: MultilineTextBufferState): MultilineTextBufferState => ({
  value: state.value,
  cursor: Math.min(state.value.length, clampCursor(state.cursor, state.value) + 1),
})
</file>

<file path="src/tui/components/core/single-line-text-input-keymap.ts">
import type { Key } from 'ink'
import {
  backspace,
  deleteForward,
  insertText,
  moveCursorLeft,
  moveCursorRight,
  type MultilineTextBufferState,
} from './multiline-text-buffer'
import { isBackspaceKey } from './text-input-keys'
export type SingleLineKeyAction =
  | { type: 'none' }
  | { type: 'submit' }
  | { type: 'change'; nextState: MultilineTextBufferState }
export type ResolveSingleLineKeyActionOptions = {
  input: string
  key: Key
  state: MultilineTextBufferState
}
export const resolveSingleLineKeyAction = ({
  input,
  key,
  state,
}: ResolveSingleLineKeyActionOptions): SingleLineKeyAction => {
  if (key.return) {
    return { type: 'submit' }
  }
  if (isBackspaceKey(input, key)) {
    return { type: 'change', nextState: backspace(state) }
  }
  if (key.delete) {
    return { type: 'change', nextState: deleteForward(state) }
  }
  if (key.leftArrow) {
    return { type: 'change', nextState: moveCursorLeft(state) }
  }
  if (key.rightArrow) {
    return { type: 'change', nextState: moveCursorRight(state) }
  }
  if (!input) {
    return { type: 'none' }
  }
  if (key.ctrl || key.meta) {
    return { type: 'none' }
  }
  const sanitized = input.replace(/[\r\n]/g, '')
  if (!sanitized) {
    return { type: 'none' }
  }
  return { type: 'change', nextState: insertText(state, sanitized) }
}
</file>

<file path="src/tui/components/core/SingleLineTextInput.tsx">
import React, { useEffect, useMemo, useRef, useState } from 'react'
import { Box, Text, useInput, type Key } from 'ink'
import { clampCursor, type MultilineTextBufferState } from './multiline-text-buffer'
import { resolveSingleLineKeyAction } from './single-line-text-input-keymap'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps, type InkColorValue } from '../../theme/theme-types'
export type DebugKeyEvent = {
  input: string
  key: Key
}
export type SingleLineTextInputProps = {
  value: string
  onChange: (next: string) => void
  onSubmit: (value: string) => void
  placeholder?: string | undefined
  focus?: boolean
  isDisabled?: boolean
  onDebugKeyEvent?: ((event: DebugKeyEvent) => void) | undefined
  // Optional rendering constraints (useful for overlays that must paint their full width).
  width?: number | undefined
  backgroundColor?: InkColorValue
}
type RenderLine = {
  before: string
  cursorCharacter: string
  after: string
  isPlaceholder: boolean
}
const toRenderLine = (
  value: string,
  placeholder: string | undefined,
  cursor: number,
): RenderLine => {
  if (!value) {
    const placeholderText = placeholder ?? ''
    const safeCursor = Math.min(Math.max(cursor, 0), placeholderText.length)
    const before = placeholderText.slice(0, safeCursor)
    const cursorCharacter =
      safeCursor < placeholderText.length ? placeholderText.charAt(safeCursor) : ' '
    const after = safeCursor < placeholderText.length ? placeholderText.slice(safeCursor + 1) : ''
    return { before, cursorCharacter, after, isPlaceholder: true }
  }
  const safeCursor = Math.min(Math.max(cursor, 0), value.length)
  const before = value.slice(0, safeCursor)
  const cursorCharacter = safeCursor < value.length ? value.charAt(safeCursor) : ' '
  const after = safeCursor < value.length ? value.slice(safeCursor + 1) : ''
  return { before, cursorCharacter, after, isPlaceholder: false }
}
export const SingleLineTextInput: React.FC<SingleLineTextInputProps> = ({
  value,
  onChange,
  onSubmit,
  placeholder,
  focus = false,
  isDisabled = false,
  onDebugKeyEvent,
  width,
  backgroundColor,
}) => {
  const { theme } = useTheme()
  const [cursor, setCursor] = useState<number>(value.length)
  const internalUpdateRef = useRef(false)
  const state: MultilineTextBufferState = useMemo(
    () => ({ value, cursor: clampCursor(cursor, value) }),
    [cursor, value],
  )
  const applyNextState = (nextState: MultilineTextBufferState): void => {
    internalUpdateRef.current = true
    setCursor(nextState.cursor)
    onChange(nextState.value)
  }
  useEffect(() => {
    if (internalUpdateRef.current) {
      internalUpdateRef.current = false
      return
    }
    setCursor(value.length)
  }, [value])
  useInput(
    (input, key) => {
      if (!focus || isDisabled) {
        return
      }
      if (onDebugKeyEvent) {
        onDebugKeyEvent({ input, key })
      }
      const action = resolveSingleLineKeyAction({ input, key, state })
      if (action.type === 'none') {
        return
      }
      if (action.type === 'submit') {
        onSubmit(value)
        return
      }
      applyNextState(action.nextState)
    },
    { isActive: focus && !isDisabled },
  )
  const rendered = useMemo(
    () => toRenderLine(value, placeholder, state.cursor),
    [placeholder, state.cursor, value],
  )
  const backgroundProps = inkBackgroundColorProps(backgroundColor)
  const colorProps = rendered.isPlaceholder ? inkColorProps(theme.mutedText) : {}
  const renderedLength =
    rendered.before.length + rendered.cursorCharacter.length + rendered.after.length
  const filler =
    typeof width === 'number' && width > renderedLength ? ' '.repeat(width - renderedLength) : ''
  return (
    <Box>
      <Text {...backgroundProps} {...colorProps}>
        {rendered.before}
      </Text>
      <Text inverse {...backgroundProps} {...colorProps}>
        {rendered.cursorCharacter}
      </Text>
      <Text {...backgroundProps} {...colorProps}>
        {rendered.after}
      </Text>
      {filler ? <Text {...backgroundProps}>{filler}</Text> : null}
    </Box>
  )
}
</file>

<file path="src/tui/components/core/text-input-keys.ts">
import type { Key } from 'ink'
const parseKittyCsiUCode = (input: string): number | null => {
  const match = /^\u001b\[([0-9]+)(?:;[0-9]+)*u$/.exec(input)
  if (!match) {
    return null
  }
  const raw = match[1]
  if (!raw) {
    return null
  }
  const parsed = Number(raw)
  return Number.isFinite(parsed) ? parsed : null
}
export const isBackspaceKey = (input: string, key: Key): boolean => {
  const kittyCsiUCode = parseKittyCsiUCode(input)
  const isKittyBackspaceSequence =
    (kittyCsiUCode !== null && [8, 51, 127].includes(kittyCsiUCode)) ||
    input === '\u001b[127~' ||
    input === '\u001b[8~' ||
    input === '\u001b[51~'
  const hasDel = input.includes('\u007f')
  const hasBackspace = input.includes('\b')
  const isCtrl = key.ctrl === true
  return (
    key.backspace === true ||
    hasDel ||
    hasBackspace ||
    (isCtrl && input.toLowerCase() === 'h') ||
    (isCtrl && input === '?') ||
    isKittyBackspaceSequence ||
    (key.delete === true && input === '')
  )
}
</file>

<file path="src/tui/components/core/tokenized-text.ts">
export type TokenLabelLookup = (token: string) => string | null
export const expandTokenizedText = (value: string, tokenLabel: TokenLabelLookup): string => {
  let output = ''
  for (const character of value) {
    const label = tokenLabel(character)
    output += label ?? character
  }
  return output
}
export type CursorCoordinates = {
  readonly row: number
  readonly column: number
}
export const getTokenizedCursorCoordinates = (
  value: string,
  cursor: number,
  tokenLabel: TokenLabelLookup,
): CursorCoordinates => {
  let row = 0
  let column = 0
  const safeCursor = Math.max(0, Math.min(cursor, value.length))
  for (let index = 0; index < safeCursor; index += 1) {
    const character = value[index]
    if (!character) {
      break
    }
    if (character === '\n') {
      row += 1
      column = 0
      continue
    }
    const label = tokenLabel(character)
    column += label ? label.length : 1
  }
  return { row, column }
}
export const expandTokenizedLines = (
  value: string,
  tokenLabel: TokenLabelLookup,
): readonly string[] => {
  const lines = value.split('\n')
  return lines.map((line) => expandTokenizedText(line, tokenLabel))
}
</file>

<file path="src/tui/components/popups/InstructionsPopup.tsx">
import { Box, Text, useStdout } from 'ink'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type InstructionsPopupProps = {
  draft: string
  onDraftChange: (value: string) => void
  onSubmitDraft: (value: string) => void
}
export const InstructionsPopup = ({
  draft,
  onDraftChange,
  onSubmitDraft,
}: InstructionsPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const popupHeight = 10
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Meta Instructions', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Add optional guidance (blank clears; Enter applies)', contentWidth)}
        </Text>
        <SingleLineTextInput
          value={draft}
          onChange={onDraftChange}
          onSubmit={() => onSubmitDraft(draft)}
          placeholder="Be concise and focus on security"
          focus
          width={contentWidth}
          backgroundColor={theme.popupBackground}
        />
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Esc closes · Enter saves', contentWidth)}
      </Text>
    </PopupSheet>
  )
}
</file>

<file path="src/tui/components/popups/list-window.ts">
/*
 * List “windowing” primitives.
 *
 * In a TUI, rendering huge lists is both slow and visually noisy.
 * These helpers answer: “Given N items and a selection cursor, which slice
 * should we render, and should we show ‘… earlier …’ / ‘… later …’ indicators?”
 *
 * This module is intentionally:
 * - Pure (no React/Ink imports)
 * - Deterministic (same input => same output)
 * - Unit-testable (used by popups and other scrolling views)
 */
export type WindowedList = {
  start: number
  end: number
  showBefore: boolean
  showAfter: boolean
}
export type WindowRange = {
  startIndex: number
  endIndexExclusive: number
}
type ResolveWindowedListOptions = {
  itemCount: number
  selectedIndex: number
  maxVisibleRows: number
  lead?: number
}
const clamp = (value: number, min: number, max: number): number => {
  if (value < min) {
    return min
  }
  if (value > max) {
    return max
  }
  return value
}
const normalizeSelectedIndex = (itemCount: number, selectedIndex: number): number => {
  if (itemCount <= 0) {
    return 0
  }
  return clamp(selectedIndex, 0, itemCount - 1)
}
const resolveCursorWindowInternal = (
  itemCount: number,
  cursorIndex: number,
  windowSize: number,
  lead: number,
): WindowRange => {
  if (itemCount <= 0 || windowSize <= 0) {
    return { startIndex: 0, endIndexExclusive: 0 }
  }
  const normalizedCursor = normalizeSelectedIndex(itemCount, cursorIndex)
  const safeLead = Math.max(0, lead)
  const upperBound = Math.max(itemCount - windowSize, 0)
  const startIndex = clamp(
    normalizedCursor - Math.min(safeLead, Math.max(windowSize - 1, 0)),
    0,
    upperBound,
  )
  return {
    startIndex,
    endIndexExclusive: Math.min(startIndex + windowSize, itemCount),
  }
}
/**
 * The simplest windowing primitive.
 *
 * Required by the refactor plan: `(itemCount, cursorIndex, windowSize) -> range`.
 * We keep a small “lead” (2 rows) so the cursor stays slightly below the top,
 * matching the current popup UX.
 */
export const resolveCursorWindow = (
  itemCount: number,
  cursorIndex: number,
  windowSize: number,
): WindowRange => resolveCursorWindowInternal(itemCount, cursorIndex, windowSize, 2)
export const resolveWindowedList = ({
  itemCount,
  selectedIndex,
  maxVisibleRows,
  lead = 2,
}: ResolveWindowedListOptions): WindowedList => {
  if (itemCount <= 0 || maxVisibleRows <= 0) {
    return { start: 0, end: 0, showBefore: false, showAfter: false }
  }
  const normalizedSelected = normalizeSelectedIndex(itemCount, selectedIndex)
  const safeLead = Math.max(0, lead)
  let showBefore = true
  let showAfter = true
  let start = 0
  let end = 0
  // We may need a couple iterations because the presence/absence of the
  // “earlier/later” indicator lines reduces the number of actual items that fit.
  for (let iteration = 0; iteration < 4; iteration += 1) {
    const indicatorRows = (showBefore ? 1 : 0) + (showAfter ? 1 : 0)
    const visibleItems = Math.max(1, maxVisibleRows - indicatorRows)
    const range =
      safeLead === 2
        ? resolveCursorWindow(itemCount, normalizedSelected, visibleItems)
        : resolveCursorWindowInternal(itemCount, normalizedSelected, visibleItems, safeLead)
    start = range.startIndex
    end = range.endIndexExclusive
    const nextShowBefore = start > 0
    const nextShowAfter = end < itemCount
    if (nextShowBefore === showBefore && nextShowAfter === showAfter) {
      showBefore = nextShowBefore
      showAfter = nextShowAfter
      break
    }
    showBefore = nextShowBefore
    showAfter = nextShowAfter
  }
  return { start, end, showBefore, showAfter }
}
</file>

<file path="src/tui/components/popups/PopupSheet.tsx">
import React from 'react'
import { Box } from 'ink'
import { BackgroundFill } from '../core/BackgroundFill'
import type { InkColorValue } from '../../theme/theme-types'
export type PopupSheetProps = {
  width: number
  height: number
  paddingX: number
  paddingY: number
  background: InkColorValue
  children: React.ReactNode
}
export const PopupSheet: React.FC<PopupSheetProps> = ({
  width,
  height,
  paddingX,
  paddingY,
  background,
  children,
}) => {
  const safeWidth = Math.max(0, Math.floor(width))
  const safeHeight = Math.max(0, Math.floor(height))
  return (
    <Box width={safeWidth} height={safeHeight} overflow="hidden">
      <Box position="absolute" width={safeWidth} height={safeHeight} overflow="hidden">
        <BackgroundFill rows={safeHeight} columns={safeWidth} background={background} />
      </Box>
      <Box
        flexDirection="column"
        paddingX={paddingX}
        paddingY={paddingY}
        width={safeWidth}
        height={safeHeight}
        overflow="hidden"
      >
        {children}
      </Box>
    </Box>
  )
}
</file>

<file path="src/tui/components/popups/ReasoningPopup.tsx">
import { Box, Text, useStdout } from 'ink'
import { ScrollableOutput } from '../core/ScrollableOutput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import type { HistoryEntry } from '../../types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type ReasoningPopupProps = {
  lines: readonly HistoryEntry[]
  visibleRows: number
  scrollOffset: number
}
export const ReasoningPopup = ({ lines, visibleRows, scrollOffset }: ReasoningPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const popupHeight = lines.length === 0 ? 9 : visibleRows + 8
  if (lines.length === 0) {
    return (
      <PopupSheet
        width={popupWidth}
        height={popupHeight}
        paddingX={POPUP_PADDING_X}
        paddingY={POPUP_PADDING_Y}
        background={theme.popupBackground}
      >
        <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
          {padRight('Model Reasoning', contentWidth)}
        </Text>
        <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('No reasoning recorded yet. Run generation first.', contentWidth)}
        </Text>
        <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Esc to close', contentWidth)}
        </Text>
      </PopupSheet>
    )
  }
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Model Reasoning', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column" height={visibleRows} overflow="hidden">
        <ScrollableOutput
          lines={lines}
          visibleRows={visibleRows}
          scrollOffset={scrollOffset}
          contentWidth={contentWidth}
          backgroundColor={theme.popupBackground}
        />
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('↑/↓ scroll · PgUp/PgDn · Esc to close', contentWidth)}
      </Text>
    </PopupSheet>
  )
}
</file>

<file path="src/tui/components/popups/SeriesIntentPopup.tsx">
import { Box, Text, useStdout } from 'ink'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type SeriesIntentPopupProps = {
  draft: string
  hint?: string | undefined
  isRunning: boolean
  onDraftChange: (value: string) => void
  onSubmitDraft: (value: string) => void
}
export const SeriesIntentPopup = ({
  draft,
  hint,
  isRunning,
  onDraftChange,
  onSubmitDraft,
}: SeriesIntentPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const hintLines = hint
    ? [hint, 'Draft may come from typed text, last run, or the intent file.']
    : ['Draft may come from typed text, last run, or the intent file.']
  const popupHeight = 9 + hintLines.length
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Series Intent', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        {hintLines.map((line) => (
          <Text key={line} {...backgroundProps} {...inkColorProps(theme.mutedText)}>
            {padRight(line, contentWidth)}
          </Text>
        ))}
        <SingleLineTextInput
          value={draft}
          onChange={onDraftChange}
          onSubmit={() => onSubmitDraft(draft)}
          placeholder="Describe the project to plan"
          focus
          width={contentWidth}
          backgroundColor={theme.popupBackground}
        />
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight(
          isRunning ? 'Series run in progress… please wait' : 'Enter runs series · Esc closes',
          contentWidth,
        )}
      </Text>
    </PopupSheet>
  )
}
</file>

<file path="src/tui/components/popups/SettingsPopup.tsx">
import { useMemo } from 'react'
import { Box, Text, useStdout } from 'ink'
import { resolveIndicatorSegments, type IndicatorSegment } from '../core/status-indicators-layout'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import type { InkColorValue } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type SettingsPopupProps = {
  chips: readonly string[]
}
const resolveSegmentLabel = (segment: IndicatorSegment): string => segment.label
export const SettingsPopup = ({ chips }: SettingsPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const segments = useMemo(() => resolveIndicatorSegments(chips), [chips])
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const resolveSegmentColor = (segment: IndicatorSegment): InkColorValue => {
    switch (segment.style) {
      case 'success':
        return theme.success
      case 'warning':
        return theme.warning
      case 'danger':
        return theme.error
      case 'primary':
        return theme.text
      case 'muted':
      default:
        return theme.mutedText
    }
  }
  const popupHeight = 8 + Math.max(1, segments.length)
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Current Settings', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        {segments.length === 0 ? (
          <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
            {padRight('No settings available yet.', contentWidth)}
          </Text>
        ) : (
          segments.map((segment) => {
            const label = `${resolveSegmentLabel(segment)}: `
            const value = segment.value
            const remaining = Math.max(0, contentWidth - label.length - value.length)
            return (
              <Box key={segment.id} flexDirection="row">
                <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
                  {label}
                </Text>
                <Text {...backgroundProps} {...inkColorProps(resolveSegmentColor(segment))}>
                  {value}
                </Text>
                <Text {...backgroundProps}>{' '.repeat(remaining)}</Text>
              </Box>
            )
          })
        )}
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Esc to close', contentWidth)}
      </Text>
    </PopupSheet>
  )
}
</file>

<file path="src/tui/components/popups/TestPopup.tsx">
import { Box, Text, useStdout } from 'ink'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type TestPopupProps = {
  draft: string
  isRunning: boolean
  onDraftChange: (value: string) => void
  onSubmitDraft: (value: string) => void
}
export const TestPopup = ({ draft, isRunning, onDraftChange, onSubmitDraft }: TestPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const popupHeight = 10
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Prompt Tests', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Suite path (Enter to run; blank uses prompt-tests.yaml)', contentWidth)}
        </Text>
        <SingleLineTextInput
          value={draft}
          onChange={onDraftChange}
          onSubmit={() => onSubmitDraft(draft)}
          placeholder="prompt-tests.yaml"
          focus
          width={contentWidth}
          backgroundColor={theme.popupBackground}
        />
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight(
          isRunning ? 'Tests running… please wait' : 'Enter to start tests · Esc to close',
          contentWidth,
        )}
      </Text>
    </PopupSheet>
  )
}
</file>

<file path="src/tui/components/popups/ThemeModePopup.tsx">
import { Box, Text, useStdout } from 'ink'
import type { ThemeMode } from '../../theme/theme-types'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type ThemeModePopupProps = {
  selectionIndex: number
  initialMode: ThemeMode
}
const OPTIONS: readonly ThemeMode[] = ['system', 'dark', 'light']
const formatMode = (mode: ThemeMode): string => {
  if (mode === 'system') {
    return 'System'
  }
  return mode.charAt(0).toUpperCase() + mode.slice(1)
}
export const ThemeModePopup = ({ selectionIndex, initialMode }: ThemeModePopupProps) => {
  const { theme, mode, error } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const selected = Math.min(selectionIndex, OPTIONS.length - 1)
  const popupHeight = 12 + (error ? 1 : 0)
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Theme Mode', contentWidth)}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight(
          `Current: ${formatMode(initialMode)} · Active: ${formatMode(mode)}`,
          contentWidth,
        )}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        {OPTIONS.map((option, index) => {
          const isSelected = index === selected
          const textProps = isSelected
            ? {
                ...inkColorProps(theme.selectionText),
                ...inkBackgroundColorProps(theme.selectionBackground),
              }
            : { ...backgroundProps, ...inkColorProps(theme.text) }
          return (
            <Text key={option} {...textProps}>
              {padRight(formatMode(option), contentWidth)}
            </Text>
          )
        })}
      </Box>
      {error ? (
        <Text {...backgroundProps} {...inkColorProps(theme.error)}>
          {padRight(error.message, contentWidth)}
        </Text>
      ) : null}
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('↑/↓ select · Enter apply · Esc close', contentWidth)}
      </Text>
    </PopupSheet>
  )
}
export const THEME_MODE_OPTIONS = OPTIONS
</file>

<file path="src/tui/components/popups/TogglePopup.tsx">
import { Box, Text, useStdout } from 'ink'
import { TOGGLE_LABELS } from '../../config'
import type { ToggleField } from '../../types'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type TogglePopupProps = {
  field: ToggleField
  selectionIndex: number
}
export const TogglePopup = ({ field, selectionIndex }: TogglePopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const options = ['On', 'Off']
  const popupHeight = 10
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight(`${TOGGLE_LABELS[field]} Setting`, contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        {options.map((label, index) => {
          const isSelected = index === selectionIndex
          const textProps = isSelected
            ? {
                ...inkColorProps(theme.selectionText),
                ...inkBackgroundColorProps(theme.selectionBackground),
              }
            : { ...backgroundProps, ...inkColorProps(theme.text) }
          return (
            <Text key={label} {...textProps}>
              {padRight(label, contentWidth)}
            </Text>
          )
        })}
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Use arrows to select · Enter to confirm · Esc to cancel', contentWidth)}
      </Text>
    </PopupSheet>
  )
}
</file>

<file path="src/tui/components/OpencodeSpinner.tsx">
import React, { useEffect, useMemo, useState } from 'react'
import { Text } from 'ink'
import { inkBackgroundColorProps, type InkColorValue } from '../theme/theme-types'
const DEFAULT_LENGTH = 12
const DEFAULT_INTERVAL_MS = 80
const DEFAULT_TRAIL_LENGTH = 3
export type OpencodeSpinnerProps = {
  length?: number
  intervalMs?: number
  trailLength?: number
  backgroundColor?: InkColorValue
}
export const OpencodeSpinner: React.FC<OpencodeSpinnerProps> = ({
  length = DEFAULT_LENGTH,
  intervalMs = DEFAULT_INTERVAL_MS,
  trailLength = DEFAULT_TRAIL_LENGTH,
  backgroundColor,
}) => {
  const [frame, setFrame] = useState(0)
  useEffect(() => {
    const timer = setInterval(() => {
      setFrame((prev) => prev + 1)
    }, intervalMs)
    return () => {
      clearInterval(timer)
    }
  }, [intervalMs])
  const backgroundProps = inkBackgroundColorProps(backgroundColor)
  const segments = useMemo(() => {
    const safeLength = Math.max(1, Math.floor(length))
    const safeTrail = Math.max(0, Math.floor(trailLength))
    const span = safeLength - 1
    const period = span <= 0 ? 1 : span * 2
    const phase = frame % period
    const headIndex = span <= 0 ? 0 : phase <= span ? phase : period - phase
    const direction = span <= 0 ? 1 : phase < span ? 1 : -1
    return Array.from({ length: safeLength }, (_, index) => {
      const distanceBehind = direction === 1 ? headIndex - index : index - headIndex
      if (distanceBehind === 0) {
        return (
          <Text key={index} {...backgroundProps} color="#A78BFA">
            ▄
          </Text>
        )
      }
      if (distanceBehind === 1 && safeTrail >= 1) {
        return (
          <Text key={index} {...backgroundProps} color="#7C3AED">
            ▄
          </Text>
        )
      }
      if (distanceBehind === 2 && safeTrail >= 2) {
        return (
          <Text key={index} {...backgroundProps} color="#5B21B6">
            ▄
          </Text>
        )
      }
      if (distanceBehind === 3 && safeTrail >= 3) {
        return (
          <Text key={index} {...backgroundProps} color="#3B0764">
            ▄
          </Text>
        )
      }
      return (
        <Text key={index} {...backgroundProps} color="#333333">
          _
        </Text>
      )
    })
  }, [backgroundColor, frame, length, trailLength])
  return <Text {...backgroundProps}>{segments}</Text>
}
</file>

<file path="src/tui/hooks/useDebouncedValue.ts">
import { useEffect, useState } from 'react'
export const useDebouncedValue = <T>(value: T, delayMs: number): T => {
  const [debouncedValue, setDebouncedValue] = useState<T>(value)
  useEffect(() => {
    const timeoutHandle: ReturnType<typeof setTimeout> = setTimeout(() => {
      setDebouncedValue(value)
    }, delayMs)
    return () => {
      clearTimeout(timeoutHandle)
    }
  }, [delayMs, value])
  return debouncedValue
}
</file>

<file path="src/tui/hooks/useLatestRef.ts">
import { useEffect, useRef, type MutableRefObject } from 'react'
/**
 * Returns a ref that always holds the latest value of the passed argument.
 * Useful for accessing the latest props/state inside asynchronous callbacks
 * (like stream handlers or timeouts) without triggering effect re-runs.
 */
export const useLatestRef = <T>(value: T): MutableRefObject<T> => {
  const ref = useRef<T>(value)
  useEffect(() => {
    ref.current = value
  }, [value])
  return ref
}
</file>

<file path="src/tui/hooks/usePersistentCommandHistory.ts">
import { useCallback, useEffect, useRef, useState } from 'react'
import type { CommandHistoryRecord } from '../command-history'
import { readCommandHistory, updateCommandHistory, writeCommandHistory } from '../command-history'
export type UsePersistentCommandHistoryOptions = {
  maxEntries?: number
  onError?: (message: string) => void
}
export const usePersistentCommandHistory = ({
  maxEntries = 200,
  onError,
}: UsePersistentCommandHistoryOptions = {}): {
  entries: CommandHistoryRecord[]
  addEntry: (value: string) => void
  isLoaded: boolean
} => {
  const [entries, setEntries] = useState<CommandHistoryRecord[]>([])
  const [isLoaded, setIsLoaded] = useState(false)
  const writeQueueRef = useRef(Promise.resolve())
  useEffect(() => {
    let cancelled = false
    const load = async (): Promise<void> => {
      try {
        const loaded = await readCommandHistory()
        if (cancelled) {
          return
        }
        setEntries(loaded)
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown history error.'
        onError?.(message)
      } finally {
        if (!cancelled) {
          setIsLoaded(true)
        }
      }
    }
    void load()
    return () => {
      cancelled = true
    }
  }, [onError])
  const persist = useCallback(
    (next: CommandHistoryRecord[]) => {
      writeQueueRef.current = writeQueueRef.current
        .then(async () => {
          await writeCommandHistory(next)
        })
        .catch((error: unknown) => {
          const message = error instanceof Error ? error.message : 'Unknown history write error.'
          onError?.(message)
        })
    },
    [onError],
  )
  const addEntry = useCallback(
    (value: string) => {
      setEntries((prev) => {
        const next = updateCommandHistory({ previous: prev, nextValue: value, maxEntries })
        if (next !== prev) {
          persist(next)
        }
        return next
      })
    },
    [maxEntries, persist],
  )
  return { entries, addEntry, isLoaded }
}
</file>

<file path="src/tui/hooks/useStableCallback.ts">
import { useCallback, useEffect, useLayoutEffect, useRef } from 'react'
const useIsomorphicLayoutEffect = typeof window !== 'undefined' ? useLayoutEffect : useEffect
export const useStableCallback = <Args extends unknown[], R>(
  fn: (...args: Args) => R,
): ((...args: Args) => R) => {
  const fnRef = useRef(fn)
  useIsomorphicLayoutEffect(() => {
    fnRef.current = fn
  }, [fn])
  return useCallback((...args: Args) => fnRef.current(...args), [])
}
</file>

<file path="src/tui/screens/command/components/CommandInput.tsx">
/*
 * CommandInput
 *
 * Presentational wrapper around the core `InputBar`.
 *
 * Why wrap it?
 * - `CommandScreen` has a lot of orchestration logic; extracting this makes the
 *   render tree more readable.
 * - The wrapper also makes it clearer which props are part of the "screen model".
 */
import { InputBar } from '../../../components/core/InputBar'
import type { DebugKeyEvent } from '../../../components/core/MultilineTextInput'
export type CommandInputProps = {
  value: string
  onChange: (next: string) => void
  onSubmit: (value: string) => void
  mode: 'intent' | 'refinement'
  isDisabled: boolean
  isPasteActive: boolean
  isBusy: boolean
  statusChips: string[]
  hint?: string | undefined
  debugLine?: string | undefined
  tokenLabel: (token: string) => string | null
  onDebugKeyEvent?: ((event: DebugKeyEvent) => void) | undefined
  placeholder: string
}
export const CommandInput = ({
  value,
  onChange,
  onSubmit,
  mode,
  isDisabled,
  isPasteActive,
  isBusy,
  statusChips,
  hint,
  debugLine,
  tokenLabel,
  onDebugKeyEvent,
  placeholder,
}: CommandInputProps) => {
  return (
    <InputBar
      value={value}
      onChange={onChange}
      onSubmit={onSubmit}
      mode={mode}
      isDisabled={isDisabled}
      isPasteActive={isPasteActive}
      isBusy={isBusy}
      statusChips={statusChips}
      hint={hint}
      debugLine={debugLine}
      tokenLabel={tokenLabel}
      onDebugKeyEvent={onDebugKeyEvent}
      placeholder={placeholder}
    />
  )
}
</file>

<file path="src/tui/screens/command/components/CommandMenuPane.tsx">
/*
 * CommandMenuPane
 *
 * Presentational wrapper for the command palette list.
 */
import { Box } from 'ink'
import { CommandMenu } from '../../../components/core/CommandMenu'
import type { CommandDescriptor } from '../../../types'
export type CommandMenuPaneProps = {
  isActive: boolean
  height: number
  commands: readonly CommandDescriptor[]
  selectedIndex: number
}
export const CommandMenuPane = ({
  isActive,
  height,
  commands,
  selectedIndex,
}: CommandMenuPaneProps) => {
  if (!isActive) {
    return null
  }
  return (
    <Box marginBottom={1} height={height} flexShrink={0} overflow="hidden">
      <CommandMenu commands={commands} selectedIndex={selectedIndex} />
    </Box>
  )
}
</file>

<file path="src/tui/screens/command/components/HistoryPane.tsx">
/*
 * HistoryPane
 *
 * Presentational component: renders the scrollable history/log output.
 *
 * Keeping this separate from `CommandScreen` makes the screen easier to scan:
 * the screen model decides *what* to show, and this component decides *how* it
 * is laid out.
 */
import { Box, useStdout } from 'ink'
import { ScrollableOutput } from '../../../components/core/ScrollableOutput'
import { useTheme } from '../../../theme/theme-provider'
import { inkBackgroundColorProps } from '../../../theme/theme-types'
const APP_CONTAINER_PADDING_X = 2
const COMMAND_SCREEN_PADDING_X = 1
import type { HistoryEntry } from '../../../types'
export type HistoryPaneProps = {
  lines: HistoryEntry[]
  visibleRows: number
  scrollOffset: number
}
export const HistoryPane = ({ lines, visibleRows, scrollOffset }: HistoryPaneProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const contentWidth = Math.max(
    0,
    terminalColumns - 2 * (APP_CONTAINER_PADDING_X + COMMAND_SCREEN_PADDING_X),
  )
  return (
    <Box
      flexDirection="column"
      height={visibleRows}
      width="100%"
      flexShrink={0}
      overflow="hidden"
      marginBottom={1}
      {...inkBackgroundColorProps(theme.background)}
    >
      <ScrollableOutput
        lines={lines}
        visibleRows={visibleRows}
        scrollOffset={scrollOffset}
        contentWidth={contentWidth}
        backgroundColor={theme.background}
      />
    </Box>
  )
}
</file>

<file path="src/tui/screens/command/hooks/useCommandMenuManager.ts">
import { useEffect, useMemo, useRef } from 'react'
import { useInput, type Key } from 'ink'
import { useStableCallback } from '../../../hooks/useStableCallback'
import { resolveCommandMenuKeyAction } from '../../../components/core/command-menu-keymap'
import { filterCommandDescriptors, resolveCommandMenuSearchState } from '../../../command-filter'
import type { CommandDescriptor, PopupState } from '../../../types'
import { isCommandInput } from '../../../drag-drop-path'
type SetNumber = (next: number | ((prev: number) => number)) => void
type ScrollTo = (row: number) => void
type SetInputValue = (next: string) => void
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
type ExistsSync = (candidate: string) => boolean
export type UseCommandMenuManagerOptions = {
  inputValue: string
  existsSync: ExistsSync
  popupState: PopupState
  helpOpen: boolean
  commandMenuSignal?: number
  commands: readonly CommandDescriptor[]
  commandMenuHeight: number
  commandSelectionIndex: number
  setCommandSelectionIndex: SetNumber
  setInputValue: SetInputValue
  setPopupState: SetPopupState
  scrollTo: ScrollTo
}
export type UseCommandMenuManagerResult = {
  isCommandMode: boolean
  commandMenuFilterQuery: string
  commandMenuArgsRaw: string
  visibleCommands: readonly CommandDescriptor[]
  isCommandMenuActive: boolean
  menuHeight: number
  selectedCommand: CommandDescriptor | undefined
}
export const useCommandMenuManager = ({
  inputValue,
  existsSync,
  popupState,
  helpOpen,
  commandMenuSignal,
  commands,
  commandMenuHeight,
  commandSelectionIndex,
  setCommandSelectionIndex,
  setInputValue,
  setPopupState,
  scrollTo,
}: UseCommandMenuManagerOptions): UseCommandMenuManagerResult => {
  const trimmedInput = inputValue.trimStart()
  const isCommandMode = isCommandInput(inputValue, existsSync)
  const commandQuery = isCommandMode ? trimmedInput.slice(1).trimStart() : ''
  const parsedCommand = useMemo<{ keyword: string; args: string }>(() => {
    if (!commandQuery) {
      return { keyword: '', args: '' }
    }
    const parts = commandQuery.split(/\s+/).filter((part) => part.length > 0)
    if (parts.length === 0) {
      return { keyword: '', args: '' }
    }
    const keyword = parts[0] ?? ''
    const rest = parts.slice(1)
    return { keyword, args: rest.join(' ') }
  }, [commandQuery])
  const commandArgsRaw = parsedCommand.args
  const commandMenuSearchState = useMemo(
    () => resolveCommandMenuSearchState({ commandQuery, commands }),
    [commandQuery, commands],
  )
  const commandMenuFilterQuery = commandMenuSearchState.filterQuery
  const commandMenuArgsRaw = commandMenuSearchState.treatRemainderAsArgs ? commandArgsRaw : ''
  const commandMatches = useMemo(() => {
    if (!isCommandMode) {
      return commands
    }
    return filterCommandDescriptors({
      query: commandMenuFilterQuery,
      commands,
    })
  }, [commandMenuFilterQuery, commands, isCommandMode])
  const visibleCommands = commandMatches
  const isPopupOpen = popupState !== null
  const isCommandMenuActive = isCommandMode && !isPopupOpen && !helpOpen
  const menuHeight = isCommandMenuActive
    ? Math.min(commandMenuHeight, Math.max(visibleCommands.length, 1) + 2)
    : 0
  useEffect(() => {
    setCommandSelectionIndex(0)
  }, [commandMenuFilterQuery, isCommandMode, setCommandSelectionIndex])
  useEffect(() => {
    if (!commandMatches.length) {
      setCommandSelectionIndex(0)
      return
    }
    setCommandSelectionIndex((prev) => Math.min(prev, commandMatches.length - 1))
  }, [commandMatches.length, setCommandSelectionIndex])
  const lastCommandMenuSignalRef = useRef<number>(0)
  useEffect(() => {
    if (!commandMenuSignal || commandMenuSignal === lastCommandMenuSignalRef.current) {
      return
    }
    lastCommandMenuSignalRef.current = commandMenuSignal
    setPopupState(null)
    setInputValue('/')
    setCommandSelectionIndex(0)
    scrollTo(Number.MAX_SAFE_INTEGER)
  }, [commandMenuSignal, scrollTo, setCommandSelectionIndex, setInputValue, setPopupState])
  const handleCommandMenuKey = useStableCallback((_input: string, key: Key) => {
    if (!isCommandMenuActive) {
      return
    }
    const action = resolveCommandMenuKeyAction({
      key,
      selectedIndex: commandSelectionIndex,
      itemCount: visibleCommands.length,
    })
    if (action.type === 'close') {
      setInputValue('')
      setCommandSelectionIndex(0)
      return
    }
    if (action.type === 'change-selection') {
      setCommandSelectionIndex(action.nextIndex)
    }
  })
  useInput(handleCommandMenuKey, { isActive: isCommandMenuActive && !helpOpen })
  const selectedCommand =
    isCommandMenuActive && visibleCommands.length > 0
      ? visibleCommands[Math.min(commandSelectionIndex, visibleCommands.length - 1)]
      : undefined
  return {
    isCommandMode,
    commandMenuFilterQuery,
    commandMenuArgsRaw,
    visibleCommands,
    isCommandMenuActive,
    menuHeight,
    selectedCommand,
  }
}
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenChips.ts">
import path from 'node:path'
import { useMemo } from 'react'
import { formatProviderStatusChip } from '../../../provider-chip'
import type { ModelOption, ProviderStatusMap } from '../../../types'
export type UseCommandScreenChipsOptions = {
  currentModel: ModelOption['id']
  providerStatuses: ProviderStatusMap
  statusChips: string[]
  intentFilePath: string
  metaInstructions: string
}
export type UseCommandScreenChipsResult = {
  providerChip: string
  enhancedStatusChips: string[]
}
export const useCommandScreenChips = ({
  currentModel,
  providerStatuses,
  statusChips,
  intentFilePath,
  metaInstructions,
}: UseCommandScreenChipsOptions): UseCommandScreenChipsResult => {
  const providerChip = useMemo(
    () => formatProviderStatusChip(currentModel, providerStatuses),
    [currentModel, providerStatuses],
  )
  const trimmedIntentFilePath = intentFilePath.trim()
  const trimmedMetaInstructions = metaInstructions.trim()
  const enhancedStatusChips = useMemo(() => {
    const chips = [...statusChips, providerChip]
    if (trimmedIntentFilePath) {
      chips.push('[intent:file]')
      chips.push(`[file:${path.basename(trimmedIntentFilePath)}]`)
    } else {
      chips.push('[intent:text]')
    }
    if (trimmedMetaInstructions) {
      chips.push('[instr:on]')
    }
    return chips
  }, [providerChip, statusChips, trimmedIntentFilePath, trimmedMetaInstructions])
  return {
    providerChip,
    enhancedStatusChips,
  }
}
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenHistoryIntentPopupBindings.ts">
import type { PopupState } from '../../../types'
import { useHistoryPopupGlue } from './useHistoryPopupGlue'
import { useIntentPopupGlue } from './useIntentPopupGlue'
export type UseCommandScreenHistoryIntentPopupBindingsOptions = {
  popupState: PopupState
  setPopupState: import('react').Dispatch<import('react').SetStateAction<PopupState>>
  closePopup: () => void
  setInputValue: (value: string | ((prev: string) => string)) => void
  consumeSuppressedTextInputChange: () => boolean
  suppressNextInput: () => void
  commandHistoryValues: string[]
}
export type UseCommandScreenHistoryIntentPopupBindingsResult = {
  history: ReturnType<typeof useHistoryPopupGlue>
  intent: ReturnType<typeof useIntentPopupGlue>
}
export const useCommandScreenHistoryIntentPopupBindings = ({
  popupState,
  setPopupState,
  closePopup,
  setInputValue,
  consumeSuppressedTextInputChange,
  suppressNextInput,
  commandHistoryValues,
}: UseCommandScreenHistoryIntentPopupBindingsOptions): UseCommandScreenHistoryIntentPopupBindingsResult => {
  const history = useHistoryPopupGlue({
    popupState,
    setPopupState,
    closePopup,
    setInputValue,
    consumeSuppressedTextInputChange,
    suppressNextInput,
    commandHistoryValues,
  })
  const intent = useIntentPopupGlue({ popupState, setPopupState })
  return { history, intent }
}
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenLayout.ts">
import path from 'node:path'
import { useMemo } from 'react'
import { estimateInputBarRows } from '../../../components/core/InputBar'
import type { PopupKind, PopupState } from '../../../types'
import type { InteractiveAwaitingMode } from '../../../generation-pipeline-reducer'
export type UseCommandScreenLayoutOptions = {
  terminalRows: number
  reservedRows: number
  helpOpen: boolean
  isPopupOpen: boolean
  popupState: PopupState
  menuHeight: number
  popupHeights: Record<PopupKind, number>
  inputValue: string
  droppedFilePath: string | null
  debugKeysEnabled: boolean
  debugKeyLine: string | null
  interactiveTransportPath?: string | undefined
  isGenerating: boolean
  awaitingInteractiveMode: InteractiveAwaitingMode | null
  isCommandMenuActive: boolean
  appStaticRows: number
  commandScreenOverheadRows: number
}
export type UseCommandScreenLayoutResult = {
  overlayHeight: number
  inputBarHint: string | undefined
  inputBarDebugLine: string | undefined
  inputBarRows: number
  isAwaitingTransportInput: boolean
  historyRows: number
}
export const useCommandScreenLayout = ({
  terminalRows,
  reservedRows,
  helpOpen,
  isPopupOpen,
  popupState,
  menuHeight,
  popupHeights,
  inputValue,
  droppedFilePath,
  debugKeysEnabled,
  debugKeyLine,
  interactiveTransportPath,
  isGenerating,
  awaitingInteractiveMode,
  isCommandMenuActive,
  appStaticRows,
  commandScreenOverheadRows,
}: UseCommandScreenLayoutOptions): UseCommandScreenLayoutResult => {
  const overlayHeight = useMemo(() => {
    if (helpOpen) {
      return 0
    }
    if (popupState) {
      return popupHeights[popupState.type as PopupKind]
    }
    return menuHeight
  }, [helpOpen, menuHeight, popupHeights, popupState])
  const inputBarHint = useMemo(() => {
    if (isPopupOpen || helpOpen || !droppedFilePath) {
      return undefined
    }
    return `Press Tab to add ${path.basename(droppedFilePath)} to context`
  }, [droppedFilePath, helpOpen, isPopupOpen])
  const inputBarDebugLine = useMemo(() => {
    if (!debugKeysEnabled) {
      return undefined
    }
    return debugKeyLine ?? 'dbg: press Backspace'
  }, [debugKeyLine, debugKeysEnabled])
  const inputBarRows = useMemo(
    () =>
      estimateInputBarRows({
        value: inputValue,
        hint: inputBarHint,
        debugLine: inputBarDebugLine,
      }),
    [inputBarDebugLine, inputBarHint, inputValue],
  )
  const isAwaitingTransportInput =
    isGenerating && Boolean(interactiveTransportPath) && awaitingInteractiveMode === 'transport'
  const historyRows = useMemo(() => {
    // Popups are now rendered as an absolute overlay, so they should NOT consume
    // layout rows. Only the in-flow command menu affects available history space.
    const overlaySpacingRows = !helpOpen && isCommandMenuActive ? 1 : 0
    const baseChromeRows = appStaticRows + commandScreenOverheadRows + inputBarRows
    const transportHeaderRows = interactiveTransportPath ? 1 : 0
    const transportAwaitingRows = isAwaitingTransportInput ? 1 : 0
    const parentRows = baseChromeRows + transportHeaderRows + transportAwaitingRows
    const availableRows = terminalRows - menuHeight - parentRows - overlaySpacingRows - reservedRows
    return Math.max(1, availableRows)
  }, [
    appStaticRows,
    commandScreenOverheadRows,
    helpOpen,
    inputBarRows,
    interactiveTransportPath,
    isAwaitingTransportInput,
    isCommandMenuActive,
    menuHeight,
    reservedRows,
    terminalRows,
  ])
  return {
    overlayHeight,
    inputBarHint,
    inputBarDebugLine,
    inputBarRows,
    isAwaitingTransportInput,
    historyRows,
  }
}
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenPasteBindings.ts">
import type { PopupState } from '../../../types'
import { usePasteManager } from './usePasteManager'
export type UseCommandScreenPasteBindingsOptions = {
  inputValue: string
  popupState: PopupState
  helpOpen: boolean
  setInputValue: (value: string | ((prev: string) => string)) => void
  setPasteActive: (active: boolean) => void
  consumeSuppressedTextInputChange: () => boolean
  suppressNextInput: () => void
  updateLastTypedIntent: (next: string) => void
}
export type UseCommandScreenPasteBindingsResult = {
  tokenLabel: (token: string) => string | null
  handleInputChange: (next: string) => void
  expandInputForSubmit: (value: string) => string
}
export const useCommandScreenPasteBindings = ({
  inputValue,
  popupState,
  helpOpen,
  setInputValue,
  setPasteActive,
  consumeSuppressedTextInputChange,
  suppressNextInput,
  updateLastTypedIntent,
}: UseCommandScreenPasteBindingsOptions): UseCommandScreenPasteBindingsResult => {
  const { tokenLabel, handleInputChange, expandInputForSubmit } = usePasteManager({
    inputValue,
    popupState,
    helpOpen,
    setInputValue,
    setPasteActive,
    consumeSuppressedTextInputChange,
    suppressNextInput,
    updateLastTypedIntent,
  })
  return {
    tokenLabel,
    handleInputChange,
    expandInputForSubmit,
  }
}
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenPopupVisibility.ts">
import { useEffect } from 'react'
export type UseCommandScreenPopupVisibilityOptions = {
  isPopupOpen: boolean
  onPopupVisibilityChange?: ((isOpen: boolean) => void) | undefined
}
export const useCommandScreenPopupVisibility = ({
  isPopupOpen,
  onPopupVisibilityChange,
}: UseCommandScreenPopupVisibilityOptions): void => {
  useEffect(() => {
    if (!onPopupVisibilityChange) {
      return
    }
    onPopupVisibilityChange(isPopupOpen)
  }, [isPopupOpen, onPopupVisibilityChange])
  useEffect(() => {
    if (!onPopupVisibilityChange) {
      return undefined
    }
    return () => {
      onPopupVisibilityChange(false)
    }
  }, [onPopupVisibilityChange])
}
</file>

<file path="src/tui/screens/command/hooks/useHistoryPopupGlue.ts">
import { useCallback, useEffect, useMemo } from 'react'
import type { PopupState } from '../../../types'
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
type ClosePopup = () => void
type SetInputValue = (next: string) => void
type ConsumeSuppressedTextInputChange = () => boolean
type SuppressNextInput = () => void
export type UseHistoryPopupGlueOptions = {
  popupState: PopupState
  setPopupState: SetPopupState
  closePopup: ClosePopup
  setInputValue: SetInputValue
  consumeSuppressedTextInputChange: ConsumeSuppressedTextInputChange
  suppressNextInput: SuppressNextInput
  commandHistoryValues: readonly string[]
}
export type UseHistoryPopupGlueResult = {
  historyPopupItems: string[]
  onHistoryPopupDraftChange: (next: string) => void
  onHistoryPopupSubmit: (value: string) => void
}
export const useHistoryPopupGlue = ({
  popupState,
  setPopupState,
  closePopup,
  setInputValue,
  consumeSuppressedTextInputChange,
  suppressNextInput,
  commandHistoryValues,
}: UseHistoryPopupGlueOptions): UseHistoryPopupGlueResult => {
  const historyPopupDraft = popupState?.type === 'history' ? popupState.draft : ''
  const historyPopupItems = useMemo(() => {
    const trimmed = historyPopupDraft.trim().toLowerCase()
    if (!trimmed) {
      return [...commandHistoryValues]
    }
    return commandHistoryValues.filter((value) => value.toLowerCase().includes(trimmed))
  }, [commandHistoryValues, historyPopupDraft])
  useEffect(() => {
    if (popupState?.type !== 'history') {
      return
    }
    setPopupState((prev) => {
      if (prev?.type !== 'history') {
        return prev
      }
      const maxIndex = Math.max(historyPopupItems.length - 1, 0)
      const nextIndex = Math.min(prev.selectionIndex, maxIndex)
      return prev.selectionIndex === nextIndex ? prev : { ...prev, selectionIndex: nextIndex }
    })
  }, [historyPopupItems.length, popupState?.type, setPopupState])
  const onHistoryPopupDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) =>
        prev?.type === 'history' ? { ...prev, draft: next, selectionIndex: 0 } : prev,
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  const onHistoryPopupSubmit = useCallback(
    (value: string) => {
      if (popupState?.type !== 'history') {
        return
      }
      const trimmed = value.trim()
      const fallback = historyPopupItems[popupState.selectionIndex] ?? ''
      const selection = trimmed || fallback
      if (!selection.trim()) {
        return
      }
      suppressNextInput()
      setInputValue(selection)
      closePopup()
    },
    [closePopup, historyPopupItems, popupState, setInputValue, suppressNextInput],
  )
  return {
    historyPopupItems,
    onHistoryPopupDraftChange,
    onHistoryPopupSubmit,
  }
}
</file>

<file path="src/tui/screens/command/hooks/useHistoryScrollKeys.ts">
import { useInput, type Key } from 'ink'
import { useStableCallback } from '../../../hooks/useStableCallback'
export type UseHistoryScrollKeysOptions = {
  isCommandMenuActive: boolean
  isPopupOpen: boolean
  helpOpen: boolean
  historyRows: number
  scrollBy: (delta: number) => void
}
export const useHistoryScrollKeys = ({
  isCommandMenuActive,
  isPopupOpen,
  helpOpen,
  historyRows,
  scrollBy,
}: UseHistoryScrollKeysOptions): void => {
  const handleInput = useStableCallback((_input: string, key: Key) => {
    if (key.upArrow) {
      scrollBy(-1)
      return
    }
    if (key.downArrow) {
      scrollBy(1)
      return
    }
    if (key.pageUp) {
      scrollBy(-historyRows)
      return
    }
    if (key.pageDown) {
      scrollBy(historyRows)
    }
  })
  useInput(handleInput, { isActive: !isCommandMenuActive && !isPopupOpen && !helpOpen })
}
</file>

<file path="src/tui/screens/command/hooks/useModelPopupData.ts">
import { useMemo } from 'react'
import { useDebouncedValue } from '../../../hooks/useDebouncedValue'
import { resolveModelPopupQuery } from '../../../model-filter'
import { buildModelPopupOptions } from '../../../model-popup-options'
import { getRecentSessionModels } from '../../../model-session'
import type { ModelOption, PopupState } from '../../../types'
export type UseModelPopupDataOptions = {
  popupState: PopupState
  modelOptions: readonly ModelOption[]
}
export type UseModelPopupDataResult = {
  modelPopupOptions: ModelOption[]
  modelPopupRecentCount: number
  modelPopupSelection: number
}
export const useModelPopupData = ({
  popupState,
  modelOptions,
}: UseModelPopupDataOptions): UseModelPopupDataResult => {
  const modelPopupQuery = popupState?.type === 'model' ? popupState.query : ''
  const debouncedModelPopupQuery = useDebouncedValue(modelPopupQuery, 75)
  const effectiveModelPopupQuery = resolveModelPopupQuery(modelPopupQuery, debouncedModelPopupQuery)
  const modelPopupData = useMemo(() => {
    if (popupState?.type !== 'model') {
      return { options: [], recentCount: 0 }
    }
    const recentModelIds = getRecentSessionModels()
    return buildModelPopupOptions({
      query: effectiveModelPopupQuery,
      modelOptions,
      recentModelIds,
    })
  }, [effectiveModelPopupQuery, modelOptions, popupState?.type])
  const modelPopupOptions = modelPopupData.options
  const modelPopupRecentCount = modelPopupData.recentCount
  const modelPopupSelection =
    popupState?.type === 'model'
      ? Math.min(popupState.selectionIndex, Math.max(modelPopupOptions.length - 1, 0))
      : 0
  return {
    modelPopupOptions,
    modelPopupRecentCount,
    modelPopupSelection,
  }
}
</file>

<file path="src/tui/screens/command/hooks/useModelProviderState.ts">
import { useCallback, useEffect, useRef, useState } from 'react'
import {
  DEFAULT_MODEL_ID,
  getBuiltInModelOptions,
  getPreferredModelId,
  loadModelOptions,
} from '../../../model-options'
import { getLastSessionModel, setLastSessionModel } from '../../../model-session'
import { checkProviderStatus } from '../../../provider-status'
import type { HistoryEntry, ModelOption, ProviderStatus, ProviderStatusMap } from '../../../types'
import type { ModelProvider } from '../../../../model-providers'
import { resolveDefaultGenerateModel } from '../../../../prompt-generator-service'
const DEFAULT_PROVIDER_STATUSES: ProviderStatusMap = {
  openai: { provider: 'openai', status: 'error', message: 'Status unavailable' },
  gemini: { provider: 'gemini', status: 'error', message: 'Status unavailable' },
  other: {
    provider: 'other',
    status: 'ok',
    message: 'Custom provider (not validated)',
  },
}
type PushHistory = (content: string, kind?: HistoryEntry['kind']) => void
export type UseModelProviderStateResult = {
  modelOptions: ModelOption[]
  currentModel: ModelOption['id']
  selectModel: (nextId: ModelOption['id']) => void
  providerStatuses: ProviderStatusMap
  updateProviderStatus: (status: ProviderStatus) => void
}
export type UseModelProviderStateOptions = {
  pushHistory: PushHistory
}
export const useModelProviderState = ({
  pushHistory,
}: UseModelProviderStateOptions): UseModelProviderStateResult => {
  const builtInModelOptionsRef = useRef<ModelOption[]>(getBuiltInModelOptions())
  const initialSessionModelRef = useRef<string | null>(getLastSessionModel())
  const userSelectedModelRef = useRef(Boolean(initialSessionModelRef.current))
  const [modelOptions, setModelOptions] = useState<ModelOption[]>(builtInModelOptionsRef.current)
  const [currentModel, setCurrentModelState] = useState<ModelOption['id']>(
    initialSessionModelRef.current ?? builtInModelOptionsRef.current[0]?.id ?? DEFAULT_MODEL_ID,
  )
  const [providerStatuses, setProviderStatuses] =
    useState<ProviderStatusMap>(DEFAULT_PROVIDER_STATUSES)
  const applyCurrentModel = useCallback((nextId: ModelOption['id'], markUserSelection: boolean) => {
    setCurrentModelState((prev: ModelOption['id']) => (prev === nextId ? prev : nextId))
    setLastSessionModel(nextId)
    if (markUserSelection) {
      userSelectedModelRef.current = true
    }
  }, [])
  const selectModel = useCallback(
    (nextId: ModelOption['id']) => {
      applyCurrentModel(nextId, true)
    },
    [applyCurrentModel],
  )
  const updateProviderStatus = useCallback((status: ProviderStatus) => {
    setProviderStatuses((prev: ProviderStatusMap) => {
      const current = prev[status.provider]
      if (current && current.status === status.status && current.message === status.message) {
        return prev
      }
      return { ...prev, [status.provider]: status }
    })
  }, [])
  useEffect(() => {
    let cancelled = false
    const providers: ModelProvider[] = ['openai', 'gemini']
    const refreshStatuses = async (): Promise<void> => {
      for (const provider of providers) {
        try {
          const status = await checkProviderStatus(provider)
          if (cancelled) {
            return
          }
          updateProviderStatus(status)
        } catch (error) {
          if (cancelled) {
            return
          }
          const message = error instanceof Error ? error.message : 'Unknown provider error.'
          updateProviderStatus({ provider, status: 'error', message })
        }
      }
    }
    void refreshStatuses()
    return () => {
      cancelled = true
    }
  }, [updateProviderStatus])
  useEffect(() => {
    let cancelled = false
    const loadOptions = async (): Promise<void> => {
      try {
        const result = await loadModelOptions()
        if (cancelled) {
          return
        }
        setModelOptions(result.options)
        if (result.warning) {
          pushHistory(result.warning, 'system')
        }
        if (userSelectedModelRef.current) {
          return
        }
        const resolvedDefault = await resolveDefaultGenerateModel().catch(() => null)
        if (cancelled || userSelectedModelRef.current) {
          return
        }
        const preferred = getPreferredModelId(result.options, resolvedDefault)
        applyCurrentModel(preferred, false)
      } catch (error) {
        if (cancelled) {
          return
        }
        const message = error instanceof Error ? error.message : 'Unknown model option error.'
        pushHistory(`[model] Failed to load CLI models: ${message}`, 'system')
      }
    }
    void loadOptions()
    return () => {
      cancelled = true
    }
  }, [applyCurrentModel, pushHistory])
  return {
    modelOptions,
    currentModel,
    selectModel,
    providerStatuses,
    updateProviderStatus,
  }
}
</file>

<file path="src/tui/screens/command/hooks/usePasteManager.ts">
import { useCallback, useEffect, useRef } from 'react'
import { useInput, type Key } from 'ink'
import { useStableCallback } from '../../../hooks/useStableCallback'
import { stripBracketedPasteControlSequences } from '../../../components/core/bracketed-paste'
import {
  consumeBracketedPasteChunk,
  createBracketedPasteState,
  createPastedSnippet,
  detectPastedSnippetFromInputChange,
  type BracketedPasteState,
  type PastedSnippet,
} from '../../../paste-snippet'
import type { PopupState } from '../../../types'
import { dropMissingPasteTokens, expandPasteTokens } from '../utils/paste-tokens'
type SetInputValue = (value: string | ((prev: string) => string)) => void
type SuppressNextInput = () => void
type ConsumeSuppressedInputChange = () => boolean
type UpdateLastTypedIntent = (next: string) => void
type TokenLabel = (token: string) => string | null
type SetPasteActive = (active: boolean) => void
export type UsePasteManagerOptions = {
  inputValue: string
  popupState: PopupState
  helpOpen: boolean
  setInputValue: SetInputValue
  setPasteActive: SetPasteActive
  consumeSuppressedTextInputChange: ConsumeSuppressedInputChange
  suppressNextInput: SuppressNextInput
  updateLastTypedIntent: UpdateLastTypedIntent
}
export type UsePasteManagerResult = {
  tokenLabel: TokenLabel
  handleInputChange: (next: string) => void
  expandInputForSubmit: (value: string) => string
}
const PASTE_TOKEN_START = 0xe000
const PASTE_TOKEN_END = 0xf8ff
export const usePasteManager = ({
  inputValue,
  popupState,
  helpOpen,
  setInputValue,
  setPasteActive,
  consumeSuppressedTextInputChange,
  suppressNextInput,
  updateLastTypedIntent,
}: UsePasteManagerOptions): UsePasteManagerResult => {
  const inputValueRef = useRef('')
  inputValueRef.current = inputValue
  const pastedSnippetTokensRef = useRef<Map<string, PastedSnippet>>(new Map())
  const nextPasteTokenRef = useRef(PASTE_TOKEN_START)
  const suppressTextInputDuringPasteRef = useRef(false)
  const bracketedPasteStateRef = useRef<BracketedPasteState>(createBracketedPasteState())
  const resetPasteTokens = useCallback((): void => {
    pastedSnippetTokensRef.current.clear()
    nextPasteTokenRef.current = PASTE_TOKEN_START
  }, [])
  useEffect(() => {
    if (inputValue.length > 0) {
      return
    }
    resetPasteTokens()
  }, [inputValue.length, resetPasteTokens])
  const tokenLabel = useCallback<TokenLabel>((token: string) => {
    return pastedSnippetTokensRef.current.get(token)?.label ?? null
  }, [])
  const allocatePasteToken = useCallback((): string => {
    const map = pastedSnippetTokensRef.current
    let nextCodePoint = nextPasteTokenRef.current
    while (nextCodePoint <= PASTE_TOKEN_END) {
      const token = String.fromCharCode(nextCodePoint)
      nextCodePoint += 1
      if (!map.has(token)) {
        nextPasteTokenRef.current = nextCodePoint
        return token
      }
    }
    resetPasteTokens()
    const fallback = String.fromCharCode(PASTE_TOKEN_START)
    nextPasteTokenRef.current = PASTE_TOKEN_START + 1
    return fallback
  }, [resetPasteTokens])
  const appendInlinePaste = useCallback(
    (raw: string): void => {
      const normalized = stripBracketedPasteControlSequences(
        raw
          .replace(/\r\n/g, '\n')
          .replace(/\r/g, '\n')
          .replace(/\u0000/g, ''),
      )
      const snippet = createPastedSnippet(normalized)
      suppressNextInput()
      if (snippet) {
        const token = allocatePasteToken()
        pastedSnippetTokensRef.current.set(token, snippet)
        setInputValue((prev) => {
          const next = prev + token
          updateLastTypedIntent(next)
          return next
        })
        return
      }
      setInputValue((prev) => {
        const next = prev + normalized
        updateLastTypedIntent(next)
        return next
      })
    },
    [allocatePasteToken, setInputValue, suppressNextInput, updateLastTypedIntent],
  )
  const handlePasteInput = useStableCallback((input: string, _key: Key) => {
    if (popupState || helpOpen) {
      return
    }
    const result = consumeBracketedPasteChunk(bracketedPasteStateRef.current, input)
    bracketedPasteStateRef.current = result.state
    const nextPasteActive = result.state.isActive
    const prevPasteActive = suppressTextInputDuringPasteRef.current
    suppressTextInputDuringPasteRef.current = nextPasteActive
    if (prevPasteActive !== nextPasteActive) {
      setPasteActive(nextPasteActive)
    }
    if (result.completed.length === 0) {
      return
    }
    const latestPaste = result.completed[result.completed.length - 1] ?? ''
    appendInlinePaste(latestPaste)
  })
  useInput(handlePasteInput, { isActive: !helpOpen })
  const handleInputChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      if (popupState) {
        return
      }
      if (suppressTextInputDuringPasteRef.current) {
        return
      }
      const previous = inputValueRef.current
      const normalizedNext = stripBracketedPasteControlSequences(
        next
          .replace(/\r\n/g, '\n')
          .replace(/\r/g, '\n')
          .replace(/\u0000/g, ''),
      )
      const detection = detectPastedSnippetFromInputChange(previous, normalizedNext)
      if (detection) {
        const token = allocatePasteToken()
        pastedSnippetTokensRef.current.set(token, detection.snippet)
        const replaced =
          detection.normalizedNextValue.slice(0, detection.range.start) +
          token +
          detection.normalizedNextValue.slice(detection.range.end)
        dropMissingPasteTokens(previous, replaced, pastedSnippetTokensRef.current)
        setInputValue(replaced)
        updateLastTypedIntent(replaced)
        return
      }
      dropMissingPasteTokens(previous, normalizedNext, pastedSnippetTokensRef.current)
      setInputValue(normalizedNext)
      updateLastTypedIntent(normalizedNext)
    },
    [
      allocatePasteToken,
      consumeSuppressedTextInputChange,
      popupState,
      setInputValue,
      updateLastTypedIntent,
    ],
  )
  const expandInputForSubmit = useCallback((value: string) => {
    return expandPasteTokens(value, pastedSnippetTokensRef.current)
  }, [])
  return {
    tokenLabel,
    handleInputChange,
    expandInputForSubmit,
  }
}
</file>

<file path="src/tui/screens/command/hooks/useReasoningPopup.ts">
import { useMemo } from 'react'
import wrapAnsi from 'wrap-ansi'
import type { HistoryEntry } from '../../../types'
export type UseReasoningPopupOptions = {
  lastReasoning: string | null
  terminalColumns: number
  popupHeight: number
}
export type UseReasoningPopupResult = {
  reasoningPopupVisibleRows: number
  reasoningPopupLines: HistoryEntry[]
}
export const useReasoningPopup = ({
  lastReasoning,
  terminalColumns,
  popupHeight,
}: UseReasoningPopupOptions): UseReasoningPopupResult => {
  const reasoningPopupVisibleRows = Math.max(1, popupHeight - 5)
  const reasoningPopupLines = useMemo(() => {
    const reasoning = lastReasoning?.trim() ?? ''
    if (!reasoning) {
      return []
    }
    const entries: HistoryEntry[] = []
    const wrapWidth = Math.max(40, terminalColumns - 6)
    let entryIndex = 0
    reasoning.split('\n').forEach((line) => {
      const wrapped = wrapAnsi(line, wrapWidth, { trim: false, hard: true })
      wrapped.split('\n').forEach((wrappedLine) => {
        entries.push({
          id: `reasoning-${entryIndex}`,
          content: wrappedLine,
          kind: 'system',
        })
        entryIndex += 1
      })
    })
    return entries
  }, [lastReasoning, terminalColumns])
  return { reasoningPopupVisibleRows, reasoningPopupLines }
}
</file>

<file path="src/tui/screens/command/hooks/useSessionCommands.ts">
import type { MutableRefObject } from 'react'
import { useCallback } from 'react'
import { planSessionCommand } from '../../../new-command'
import type { HistoryEntry, PopupState } from '../../../types'
type ResetContext = () => void
type ResetHistory = () => void
type ScrollTo = (row: number) => void
type SetInputValue = (next: string) => void
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
type SetMetaInstructions = (next: string) => void
type PushHistory = (content: string, kind?: HistoryEntry['kind']) => void
export type UseSessionCommandsOptions = {
  isGenerating: boolean
  lastGeneratedPrompt: string | null
  resetContext: ResetContext
  resetHistory: ResetHistory
  scrollTo: ScrollTo
  setInputValue: SetInputValue
  setPopupState: SetPopupState
  setIntentFilePath: (value: string) => void
  setMetaInstructions: SetMetaInstructions
  lastUserIntentRef: MutableRefObject<string | null>
  lastTypedIntentRef: MutableRefObject<string>
  pushHistory: PushHistory
}
export type UseSessionCommandsResult = {
  resetSessionState: () => void
  handleNewCommand: (argsRaw: string) => void
  handleReuseCommand: () => void
}
export const useSessionCommands = ({
  isGenerating,
  lastGeneratedPrompt,
  resetContext,
  resetHistory,
  scrollTo,
  setInputValue,
  setPopupState,
  setIntentFilePath,
  setMetaInstructions,
  lastUserIntentRef,
  lastTypedIntentRef,
  pushHistory,
}: UseSessionCommandsOptions): UseSessionCommandsResult => {
  const resetSessionState = useCallback(() => {
    resetContext()
    setIntentFilePath('')
    lastUserIntentRef.current = null
    lastTypedIntentRef.current = ''
    setInputValue('')
    setPopupState(null)
    resetHistory()
    scrollTo(Number.MAX_SAFE_INTEGER)
  }, [
    lastTypedIntentRef,
    lastUserIntentRef,
    resetContext,
    resetHistory,
    scrollTo,
    setInputValue,
    setIntentFilePath,
    setPopupState,
  ])
  const handleNewCommand = useCallback(
    (argsRaw: string) => {
      if (isGenerating) {
        pushHistory('[new] Cannot reset while generation is running.', 'system')
        return
      }
      resetSessionState()
      const plan = planSessionCommand({ commandId: 'new', lastGeneratedPrompt: null })
      pushHistory(plan.message, 'system')
      if (argsRaw.includes('--reuse')) {
        pushHistory('[new] Tip: use /reuse to reuse the last prompt.', 'system')
      }
    },
    [isGenerating, pushHistory, resetSessionState],
  )
  const handleReuseCommand = useCallback(() => {
    if (isGenerating) {
      pushHistory('[reuse] Cannot reset while generation is running.', 'system')
      return
    }
    const previousPrompt = lastGeneratedPrompt
    resetSessionState()
    const plan = planSessionCommand({
      commandId: 'reuse',
      lastGeneratedPrompt: previousPrompt,
    })
    if (plan.type === 'reset-and-load-meta') {
      setMetaInstructions(plan.metaInstructions)
    }
    pushHistory(plan.message, 'system')
  }, [isGenerating, lastGeneratedPrompt, pushHistory, resetSessionState, setMetaInstructions])
  return {
    resetSessionState,
    handleNewCommand,
    handleReuseCommand,
  }
}
</file>

<file path="src/tui/screens/command/hooks/useThemeModePopupGlue.ts">
import { useCallback, useEffect, useMemo } from 'react'
import type { Dispatch, SetStateAction } from 'react'
import type { PopupState } from '../../../types'
import { useTheme } from '../../../theme/theme-provider'
import { THEME_MODE_OPTIONS } from '../../../components/popups/ThemeModePopup'
export type UseThemeModePopupGlueOptions = {
  popupState: PopupState
  setPopupState: Dispatch<SetStateAction<PopupState>>
  closePopup: () => void
}
export type UseThemeModePopupGlueResult = {
  optionCount: number
  onConfirm: () => void
  onCancel: () => void
}
export const useThemeModePopupGlue = ({
  popupState,
  setPopupState,
  closePopup,
}: UseThemeModePopupGlueOptions): UseThemeModePopupGlueResult => {
  const { setMode } = useTheme()
  const optionCount = THEME_MODE_OPTIONS.length
  useEffect(() => {
    if (popupState?.type !== 'themeMode') {
      return
    }
    setPopupState((prev) => {
      if (prev?.type !== 'themeMode') {
        return prev
      }
      const clamped = Math.min(prev.selectionIndex, optionCount - 1)
      if (clamped === prev.selectionIndex) {
        return prev
      }
      return { ...prev, selectionIndex: clamped }
    })
  }, [optionCount, popupState?.type, setPopupState])
  const onCancel = useCallback(() => {
    closePopup()
  }, [closePopup])
  const onConfirm = useCallback(() => {
    if (popupState?.type !== 'themeMode') {
      return
    }
    const selected = THEME_MODE_OPTIONS[popupState.selectionIndex]
    if (!selected) {
      return
    }
    const run = async (): Promise<void> => {
      const ok = await setMode(selected)
      if (ok) {
        closePopup()
      }
    }
    void run()
  }, [closePopup, popupState, setMode])
  return useMemo(() => ({ optionCount, onConfirm, onCancel }), [onCancel, onConfirm, optionCount])
}
</file>

<file path="src/tui/screens/command/hooks/useThemePopupGlue.ts">
import { useCallback, useEffect, useMemo, useRef } from 'react'
import type { Dispatch, SetStateAction } from 'react'
import type { PopupState } from '../../../types'
import { useTheme } from '../../../theme/theme-provider'
export type UseThemePopupGlueOptions = {
  popupState: PopupState
  setPopupState: Dispatch<SetStateAction<PopupState>>
  closePopup: () => void
}
export type UseThemePopupGlueResult = {
  themeCount: number
  onThemeConfirm: () => void
  onThemeCancel: () => void
}
export const useThemePopupGlue = ({
  popupState,
  setPopupState,
  closePopup,
}: UseThemePopupGlueOptions): UseThemePopupGlueResult => {
  const { themes, previewTheme, setTheme } = useTheme()
  const themeNames = useMemo(() => themes.map((descriptor) => descriptor.name), [themes])
  const themeCount = themeNames.length
  useEffect(() => {
    if (popupState?.type !== 'theme' || themeCount === 0) {
      return
    }
    setPopupState((prev) => {
      if (prev?.type !== 'theme') {
        return prev
      }
      const clamped = Math.min(prev.selectionIndex, Math.max(themeCount - 1, 0))
      if (clamped === prev.selectionIndex) {
        return prev
      }
      return { ...prev, selectionIndex: clamped }
    })
  }, [popupState?.type, setPopupState, themeCount])
  const lastPreviewNameRef = useRef<string | null>(null)
  useEffect(() => {
    if (popupState?.type !== 'theme') {
      lastPreviewNameRef.current = null
      return
    }
    const selectedName = themeNames[popupState.selectionIndex]
    if (!selectedName) {
      return
    }
    if (selectedName === lastPreviewNameRef.current) {
      return
    }
    lastPreviewNameRef.current = selectedName
    previewTheme(selectedName)
  }, [popupState, previewTheme, themeNames])
  const onThemeCancel = useCallback(() => {
    if (popupState?.type !== 'theme') {
      closePopup()
      return
    }
    previewTheme(popupState.initialThemeName)
    closePopup()
  }, [closePopup, popupState, previewTheme])
  const onThemeConfirm = useCallback(() => {
    if (popupState?.type !== 'theme') {
      closePopup()
      return
    }
    const selectedName = themeNames[popupState.selectionIndex]
    if (!selectedName) {
      return
    }
    const commit = async (): Promise<void> => {
      const ok = await setTheme(selectedName)
      if (ok) {
        closePopup()
      }
    }
    void commit()
  }, [closePopup, popupState, setTheme, themeNames])
  return {
    themeCount,
    onThemeConfirm,
    onThemeCancel,
  }
}
</file>

<file path="src/tui/screens/command/utils/control-key.ts">
export type ControlKey = {
  ctrl?: boolean | undefined
}
export const isControlKey = (input: string, key: ControlKey, target: string): boolean => {
  if (!target) {
    return false
  }
  const normalized = target.toLowerCase()
  const code = normalized.charCodeAt(0)
  const controlChar = code >= 97 && code <= 122 ? String.fromCharCode(code - 96) : null
  if (key.ctrl === true && input.toLowerCase() === normalized) {
    return true
  }
  return controlChar ? input === controlChar : false
}
</file>

<file path="src/tui/screens/command/utils/debug-keys.ts">
import type { DebugKeyEvent } from '../../../components/core/MultilineTextInput'
export const formatDebugKeyEvent = (event: DebugKeyEvent): string => {
  const codes = Array.from(event.input)
    .map((character) => character.codePointAt(0) ?? 0)
    .map((code) => `0x${code.toString(16).padStart(2, '0')}`)
    .join(' ')
  const activeFlags = (Object.entries(event.key) as Array<[string, unknown]>)
    .filter(([, value]) => value === true)
    .map(([name]) => name)
    .join(',')
  const safeInput = JSON.stringify(event.input)
  return `dbg input=${safeInput} codes=[${codes}] key=[${activeFlags}]`
}
</file>

<file path="src/tui/screens/command/utils/paste-tokens.ts">
import type { PastedSnippet } from '../../../paste-snippet'
type PasteTokenMap = ReadonlyMap<string, PastedSnippet>
type MutablePasteTokenMap = Map<string, PastedSnippet>
const tokensForValue = (value: string, tokens: PasteTokenMap): Set<string> => {
  return new Set(Array.from(value).filter((token) => tokens.has(token)))
}
export const dropMissingPasteTokens = (
  previous: string,
  next: string,
  tokens: MutablePasteTokenMap,
): void => {
  if (tokens.size === 0) {
    return
  }
  const previousTokens = tokensForValue(previous, tokens)
  if (previousTokens.size === 0) {
    return
  }
  const nextTokens = tokensForValue(next, tokens)
  for (const token of previousTokens) {
    if (!nextTokens.has(token)) {
      tokens.delete(token)
    }
  }
}
export const expandPasteTokens = (value: string, tokens: PasteTokenMap): string => {
  if (tokens.size === 0) {
    return value
  }
  let expanded = ''
  for (const character of value) {
    const snippet = tokens.get(character)
    expanded += snippet ? snippet.text : character
  }
  return expanded
}
</file>

<file path="src/tui/screens/command/CommandScreen.tsx">
export { CommandScreen } from './CommandScreenImpl'
export type { CommandScreenHandle } from './CommandScreenImpl'
</file>

<file path="src/tui/screens/test-runner/components/TestList.tsx">
/*
 * TestList
 *
 * Presentational component for rendering the loaded test list.
 *
 * Note: we intentionally only show the first 15 tests, matching the existing UX.
 */
import { Box, Text } from 'ink'
import { useTheme } from '../../../theme/theme-provider'
import { inkColorProps } from '../../../theme/theme-types'
import type { TestDisplayState, TestStatus } from '../test-runner-reducer'
const STATUS_LABEL: Record<TestStatus, string> = {
  pending: 'PENDING',
  running: 'RUNNING',
  pass: 'PASS',
  fail: 'FAIL',
}
export type TestListProps = {
  tests: readonly TestDisplayState[]
}
export const TestList = ({ tests }: TestListProps) => {
  const { theme } = useTheme()
  const resolveStatusColor = (status: TestStatus) => {
    switch (status) {
      case 'pending':
        return theme.mutedText
      case 'running':
        return theme.accent
      case 'pass':
        return theme.success
      case 'fail':
        return theme.error
      default: {
        const exhaustive: never = status
        return exhaustive
      }
    }
  }
  if (tests.length === 0) {
    return <Text {...inkColorProps(theme.mutedText)}>No test suite loaded yet.</Text>
  }
  const displayed = tests.slice(0, 15).map((testState, index) => (
    <Box key={`${testState.name}-${index}`} flexDirection="column">
      <Text {...inkColorProps(resolveStatusColor(testState.status))}>
        {STATUS_LABEL[testState.status].padEnd(7)} {testState.name}
      </Text>
      {testState.reason && testState.status === 'fail' ? (
        <Text {...inkColorProps(theme.mutedText)}>↳ {testState.reason}</Text>
      ) : null}
    </Box>
  ))
  return (
    <>
      {displayed}
      {tests.length > 15 ? (
        <Text {...inkColorProps(theme.mutedText)}>…and {tests.length - 15} more test(s)</Text>
      ) : null}
    </>
  )
}
</file>

<file path="src/tui/screens/test-runner/components/TestRunnerActions.tsx">
/*
 * TestRunnerActions
 *
 * Presentational component for the "actions" section.
 */
import { Box, Text } from 'ink'
import { useTheme } from '../../../theme/theme-provider'
import { inkColorProps } from '../../../theme/theme-types'
export type TestRunnerActionsProps = {
  isFocused: boolean
  status: 'idle' | 'running'
  lastRunFile: string | null
}
export const TestRunnerActions = ({ isFocused, status, lastRunFile }: TestRunnerActionsProps) => {
  const { theme } = useTheme()
  return (
    <Box marginTop={1} flexDirection="column">
      {isFocused ? <Text {...inkColorProps(theme.accent)}>Actions</Text> : <Text>Actions</Text>}
      <Text>Press Enter to run tests</Text>
      <Text {...inkColorProps(theme.mutedText)}>
        Status: {status === 'running' ? 'Running tests…' : 'Idle'}
      </Text>
      {lastRunFile ? (
        <Text {...inkColorProps(theme.mutedText)}>Last suite: {lastRunFile}</Text>
      ) : (
        <Text {...inkColorProps(theme.mutedText)}>No runs yet</Text>
      )}
    </Box>
  )
}
</file>

<file path="src/tui/screens/test-runner/components/TestRunnerError.tsx">
/*
 * TestRunnerError
 *
 * Presentational component for showing an error message.
 */
import { Text } from 'ink'
import { useTheme } from '../../../theme/theme-provider'
import { inkColorProps } from '../../../theme/theme-types'
export type TestRunnerErrorProps = {
  message: string | null
}
export const TestRunnerError = ({ message }: TestRunnerErrorProps) => {
  const { theme } = useTheme()
  if (!message) {
    return null
  }
  return <Text {...inkColorProps(theme.error)}>{message}</Text>
}
</file>

<file path="src/tui/screens/test-runner/components/TestRunnerFileInput.tsx">
/*
 * TestRunnerFileInput
 *
 * Presentational component for the test file input section.
 */
import { Box, Text, useStdout } from 'ink'
import { SingleLineTextInput } from '../../../components/core/SingleLineTextInput'
import { useTheme } from '../../../theme/theme-provider'
import {
  inkBackgroundColorProps,
  inkBorderColorProps,
  inkColorProps,
} from '../../../theme/theme-types'
const APP_CONTAINER_PADDING_X = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type TestRunnerFileInputProps = {
  filePath: string
  isFocused: boolean
  helpOpen: boolean
  onChange: (next: string) => void
  onSubmit: () => void
}
export const TestRunnerFileInput = ({
  filePath,
  isFocused,
  helpOpen,
  onChange,
  onSubmit,
}: TestRunnerFileInputProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const boxWidth = Math.max(0, terminalColumns - 2 * APP_CONTAINER_PADDING_X)
  const borderColumns = 2
  const paddingColumns = 2
  const contentWidth = Math.max(0, boxWidth - borderColumns - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.panelBackground)
  const borderColor = isFocused ? theme.accent : theme.border
  return (
    <>
      <Text {...backgroundProps} {...inkColorProps(isFocused ? theme.accent : theme.text)}>
        {padRight('Test File', contentWidth)}
      </Text>
      <Box
        borderStyle="round"
        paddingX={1}
        width={boxWidth}
        {...inkBorderColorProps(borderColor)}
        {...backgroundProps}
      >
        <SingleLineTextInput
          value={filePath}
          onChange={onChange}
          placeholder="prompt-tests.yaml"
          focus={isFocused && !helpOpen}
          onSubmit={onSubmit}
          width={contentWidth}
          backgroundColor={theme.panelBackground}
        />
      </Box>
    </>
  )
}
</file>

<file path="src/tui/screens/test-runner/components/TestRunnerLogs.tsx">
/*
 * TestRunnerLogs
 *
 * Presentational component for displaying recent test logs.
 */
import { Box, Text } from 'ink'
import type { LogEntry } from '../../../useLogBuffer'
import { useTheme } from '../../../theme/theme-provider'
import { inkColorProps } from '../../../theme/theme-types'
export type TestRunnerLogsProps = {
  logs: readonly LogEntry[]
}
export const TestRunnerLogs = ({ logs }: TestRunnerLogsProps) => {
  const { theme } = useTheme()
  if (logs.length === 0) {
    return null
  }
  return (
    <Box marginTop={1} flexDirection="column">
      <Text {...inkColorProps(theme.accent)}>Recent Logs</Text>
      {logs.map((entry) => {
        const color =
          entry.level === 'error'
            ? theme.error
            : entry.level === 'warn'
              ? theme.warning
              : theme.mutedText
        return (
          <Text key={entry.id} {...inkColorProps(color)}>
            {entry.level.toUpperCase()}: {entry.message}
          </Text>
        )
      })}
    </Box>
  )
}
</file>

<file path="src/tui/screens/test-runner/components/TestRunnerSummary.tsx">
/*
 * TestRunnerSummary
 *
 * Presentational component for displaying test run summary.
 */
import { Box, Text } from 'ink'
import { useTheme } from '../../../theme/theme-provider'
import { inkColorProps } from '../../../theme/theme-types'
import type { TestRunSummary } from '../test-runner-reducer'
export type TestRunnerSummaryProps = {
  summary: TestRunSummary | null
}
export const TestRunnerSummary = ({ summary }: TestRunnerSummaryProps) => {
  const { theme } = useTheme()
  if (!summary) {
    return null
  }
  return (
    <Box marginTop={1} flexDirection="column">
      <Text {...inkColorProps(theme.accent)}>Summary</Text>
      <Text {...inkColorProps(theme.success)}>Passed: {summary.passed}</Text>
      <Text {...inkColorProps(summary.failed > 0 ? theme.error : theme.success)}>
        Failed: {summary.failed}
      </Text>
    </Box>
  )
}
</file>

<file path="src/tui/screens/test-runner/test-runner-reducer.ts">
/*
 * Test runner reducer (pure state transitions).
 *
 * This reducer manages UI state for the TestRunnerScreen:
 * - current test file path
 * - focus (file input vs actions)
 * - loaded tests + per-test status
 * - last run info + summary + error
 *
 * Reducers in plain terms:
 * - We send "actions" describing what happened.
 * - The reducer is a pure function that returns the next state.
 *
 * Keeping this logic pure makes it easy to unit test and helps ensure the
 * screen refactor does not change behavior.
 */
export type SetStateAction<State> = State | ((prev: State) => State)
export type TestStatus = 'pending' | 'running' | 'pass' | 'fail'
export type TestDisplayState = {
  name: string
  status: TestStatus
  reason: string | null
}
export type FocusField = 'file' | 'actions'
export type RunStatus = 'idle' | 'running'
export type TestRunSummary = { passed: number; failed: number }
export type TestRunnerState = {
  filePath: string
  tests: TestDisplayState[]
  status: RunStatus
  error: string | null
  summary: TestRunSummary | null
  lastRunFile: string | null
  focus: FocusField
}
export type TestRunnerAction =
  | { type: 'set-file-path'; next: SetStateAction<string> }
  | { type: 'set-focus'; focus: FocusField }
  | { type: 'run-start' }
  | { type: 'suite-loaded'; loadedPath: string; testNames: string[] }
  | { type: 'test-start'; ordinal: number; name: string }
  | { type: 'test-complete'; ordinal: number; name: string; pass: boolean; reason: string | null }
  | { type: 'run-complete'; passed: number; failed: number }
  | { type: 'run-error'; message: string }
export const INITIAL_TEST_RUNNER_STATE: TestRunnerState = {
  filePath: 'prompt-tests.yaml',
  tests: [],
  status: 'idle',
  error: null,
  summary: null,
  lastRunFile: null,
  focus: 'file',
}
const updateTestAtOrdinal = (
  tests: TestDisplayState[],
  ordinal: number,
  updater: (prev: TestDisplayState) => TestDisplayState,
): TestDisplayState[] => {
  // Ordinals are 1-based in the test runner callbacks.
  const index = ordinal - 1
  if (index < 0 || index >= tests.length) {
    return tests
  }
  const next = [...tests]
  next[index] = updater(next[index] ?? { name: 'unknown', status: 'pending', reason: null })
  return next
}
export const testRunnerReducer = (
  state: TestRunnerState,
  action: TestRunnerAction,
): TestRunnerState => {
  switch (action.type) {
    case 'set-file-path': {
      const next = typeof action.next === 'function' ? action.next(state.filePath) : action.next
      return next === state.filePath ? state : { ...state, filePath: next }
    }
    case 'set-focus':
      return action.focus === state.focus ? state : { ...state, focus: action.focus }
    case 'run-start':
      return {
        ...state,
        status: 'running',
        error: null,
        summary: null,
        tests: [],
      }
    case 'suite-loaded':
      return {
        ...state,
        lastRunFile: action.loadedPath,
        tests: action.testNames.map((name) => ({ name, status: 'pending', reason: null })),
      }
    case 'test-start':
      return {
        ...state,
        tests: updateTestAtOrdinal(state.tests, action.ordinal, (prev) => ({
          ...prev,
          name: action.name,
          status: 'running',
        })),
      }
    case 'test-complete':
      return {
        ...state,
        tests: updateTestAtOrdinal(state.tests, action.ordinal, () => ({
          name: action.name,
          status: action.pass ? 'pass' : 'fail',
          reason: action.reason,
        })),
      }
    case 'run-complete':
      return {
        ...state,
        status: 'idle',
        summary: { passed: action.passed, failed: action.failed },
      }
    case 'run-error':
      return {
        ...state,
        status: 'idle',
        error: action.message,
      }
    default:
      return state
  }
}
</file>

<file path="src/tui/screens/test-runner/TestRunnerScreen.tsx">
/*
 * TestRunnerScreen
 *
 * This screen runs `prompt-tests.yaml` (or another file) and shows progress.
 *
 * Architecture note:
 * - Ink screens tend to accumulate lots of tiny state updates.
 * - We keep state transitions explicit via a reducer (`test-runner-reducer.ts`).
 * - This screen component stays focused on orchestration:
 *   input handling + calling `runPromptTestSuite`.
 */
import { forwardRef, useCallback, useImperativeHandle, useMemo, useRef } from 'react'
import path from 'node:path'
import { Box, Text, useInput } from 'ink'
import { runPromptTestSuite, type PromptTestRunReporter } from '../../../test-command'
import { useLogBuffer } from '../../useLogBuffer'
import { useTheme } from '../../theme/theme-provider'
import { inkColorProps } from '../../theme/theme-types'
import { useTestRunnerScreen } from './useTestRunnerScreen'
import { TestRunnerActions } from './components/TestRunnerActions'
import { TestRunnerError } from './components/TestRunnerError'
import { TestRunnerFileInput } from './components/TestRunnerFileInput'
import { TestRunnerLogs } from './components/TestRunnerLogs'
import { TestList } from './components/TestList'
import { TestRunnerSummary } from './components/TestRunnerSummary'
export type TestRunnerScreenHandle = {
  suppressNextInput: () => void
}
export type TestRunnerScreenProps = {
  helpOpen?: boolean
}
export const TestRunnerScreen = forwardRef<TestRunnerScreenHandle, TestRunnerScreenProps>(
  ({ helpOpen = false }, ref) => {
    const { theme } = useTheme()
    const {
      state,
      canRun,
      setFilePath,
      setFocus,
      focusNext,
      focusPrevious,
      startRun,
      suiteLoaded,
      testStarted,
      testCompleted,
      runCompleted,
      runErrored,
    } = useTestRunnerScreen()
    const { logs, log, clearLogs } = useLogBuffer()
    const suppressNextInputRef = useRef(false)
    useImperativeHandle(
      ref,
      () => ({
        suppressNextInput: () => {
          suppressNextInputRef.current = true
        },
      }),
      [],
    )
    const consumeSuppressedTextInputChange = useCallback((): boolean => {
      if (!suppressNextInputRef.current) {
        return false
      }
      suppressNextInputRef.current = false
      return true
    }, [])
    const handleFilePathChange = useCallback(
      (next: string) => {
        if (consumeSuppressedTextInputChange()) {
          return
        }
        setFilePath(next)
      },
      [consumeSuppressedTextInputChange, setFilePath],
    )
    const reporter = useMemo<PromptTestRunReporter>(
      () => ({
        onSuiteLoaded: (suite, loadedPath) => {
          suiteLoaded(
            loadedPath,
            suite.tests.map((test) => test.name),
          )
          clearLogs()
          log.info(`Loaded ${suite.tests.length} test(s) from ${loadedPath}`)
        },
        onTestStart: (ordinal, test) => {
          testStarted(ordinal, test.name)
        },
        onTestComplete: (ordinal, result) => {
          testCompleted(ordinal, result.name, result.pass, result.reason ?? null)
          if (!result.pass) {
            log.warn(`[${result.name}] ${result.reason}`)
          }
        },
        onComplete: (results) => {
          const passed = results.filter((result) => result.pass).length
          const failed = results.length - passed
          runCompleted(passed, failed)
          if (failed === 0) {
            log.info('All tests passed')
          } else {
            log.error(`${failed} test(s) failed`)
          }
        },
      }),
      [clearLogs, log, runCompleted, suiteLoaded, testCompleted, testStarted],
    )
    const handleRun = useCallback(async () => {
      if (!canRun) {
        return
      }
      startRun()
      const resolvedPath = path.resolve(process.cwd(), state.filePath.trim())
      try {
        await runPromptTestSuite(resolvedPath, { reporter })
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown test execution error.'
        runErrored(message)
        log.error(message)
      }
    }, [canRun, log, reporter, runErrored, startRun, state.filePath])
    useInput(
      (_, key) => {
        if (state.status === 'running') {
          return
        }
        if (key.tab && key.shift) {
          focusPrevious()
          return
        }
        if (key.tab) {
          focusNext()
          return
        }
        if (state.focus === 'actions' && key.return && canRun) {
          void handleRun()
        }
      },
      { isActive: !helpOpen },
    )
    return (
      <Box flexDirection="column" marginTop={1}>
        <TestRunnerFileInput
          filePath={state.filePath}
          isFocused={state.focus === 'file'}
          helpOpen={helpOpen}
          onChange={handleFilePathChange}
          onSubmit={() => setFocus('actions')}
        />
        <TestRunnerActions
          isFocused={state.focus === 'actions'}
          status={state.status}
          lastRunFile={state.lastRunFile}
        />
        <Box marginTop={1} flexDirection="column">
          <Text {...inkColorProps(theme.accent)}>Tests</Text>
          <TestList tests={state.tests} />
        </Box>
        <TestRunnerSummary summary={state.summary} />
        <TestRunnerLogs logs={logs} />
        <TestRunnerError message={state.error} />
      </Box>
    )
  },
)
TestRunnerScreen.displayName = 'TestRunnerScreen'
</file>

<file path="src/tui/screens/test-runner/useTestRunnerScreen.ts">
/*
 * useTestRunnerScreen
 *
 * Hook wrapper around the pure test runner reducer.
 *
 * This keeps `TestRunnerScreen` focused on orchestration (Ink input, async test
 * execution) and keeps state transitions explicit and testable.
 */
import { useCallback, useMemo, useReducer } from 'react'
import {
  INITIAL_TEST_RUNNER_STATE,
  testRunnerReducer,
  type FocusField,
  type SetStateAction,
  type TestRunnerState,
} from './test-runner-reducer'
const focusOrder: FocusField[] = ['file', 'actions']
const nextFocus = (current: FocusField): FocusField => {
  const index = focusOrder.indexOf(current)
  return focusOrder[(index + 1) % focusOrder.length] ?? 'file'
}
const previousFocus = (current: FocusField): FocusField => {
  const index = focusOrder.indexOf(current)
  return focusOrder[(index - 1 + focusOrder.length) % focusOrder.length] ?? 'file'
}
export type UseTestRunnerScreenResult = {
  state: TestRunnerState
  canRun: boolean
  setFilePath: (next: SetStateAction<string>) => void
  focusNext: () => void
  focusPrevious: () => void
  setFocus: (focus: FocusField) => void
  startRun: () => void
  suiteLoaded: (loadedPath: string, testNames: string[]) => void
  testStarted: (ordinal: number, name: string) => void
  testCompleted: (ordinal: number, name: string, pass: boolean, reason: string | null) => void
  runCompleted: (passed: number, failed: number) => void
  runErrored: (message: string) => void
}
export const useTestRunnerScreen = (): UseTestRunnerScreenResult => {
  const [state, dispatch] = useReducer(testRunnerReducer, INITIAL_TEST_RUNNER_STATE)
  const canRun = useMemo(
    () => state.status !== 'running' && state.filePath.trim().length > 0,
    [state.filePath, state.status],
  )
  const setFilePath = useCallback((next: SetStateAction<string>) => {
    dispatch({ type: 'set-file-path', next })
  }, [])
  const setFocus = useCallback((focus: FocusField) => {
    dispatch({ type: 'set-focus', focus })
  }, [])
  const focusNext = useCallback(() => {
    setFocus(nextFocus(state.focus))
  }, [setFocus, state.focus])
  const focusPrevious = useCallback(() => {
    setFocus(previousFocus(state.focus))
  }, [setFocus, state.focus])
  const startRun = useCallback(() => {
    dispatch({ type: 'run-start' })
  }, [])
  const suiteLoaded = useCallback((loadedPath: string, testNames: string[]) => {
    dispatch({ type: 'suite-loaded', loadedPath, testNames })
  }, [])
  const testStarted = useCallback((ordinal: number, name: string) => {
    dispatch({ type: 'test-start', ordinal, name })
  }, [])
  const testCompleted = useCallback(
    (ordinal: number, name: string, pass: boolean, reason: string | null) => {
      dispatch({ type: 'test-complete', ordinal, name, pass, reason })
    },
    [],
  )
  const runCompleted = useCallback((passed: number, failed: number) => {
    dispatch({ type: 'run-complete', passed, failed })
  }, [])
  const runErrored = useCallback((message: string) => {
    dispatch({ type: 'run-error', message })
  }, [])
  return {
    state,
    canRun,
    setFilePath,
    focusNext,
    focusPrevious,
    setFocus,
    startRun,
    suiteLoaded,
    testStarted,
    testCompleted,
    runCompleted,
    runErrored,
  }
}
</file>

<file path="src/tui/theme/builtins/pm-dark.ts">
import type { ThemeJson } from '../theme-types'
export const PM_DARK_THEME = {
  defs: {
    bg: '#0b0f14',
    panelBg: '#111820',
    text: '#e6edf3',
    muted: '#8b949e',
    border: '#30363d',
    accent: '#58a6ff',
    warning: '#d29922',
    error: '#f85149',
    success: '#3fb950',
    selectionBg: '#1f6feb',
  },
  theme: {
    background: 'bg',
    text: 'text',
    mutedText: 'muted',
    border: 'border',
    accent: 'accent',
    accentText: 'bg',
    warning: 'warning',
    error: 'error',
    success: 'success',
    panelBackground: 'panelBg',
    popupBackground: 'panelBackground',
    selectionBackground: 'selectionBg',
    selectionText: 'bg',
    chipBackground: 'panelBackground',
    chipText: 'text',
    chipMutedText: 'mutedText',
  },
} as const satisfies ThemeJson
</file>

<file path="src/tui/theme/builtins/pm-light.ts">
import type { ThemeJson } from '../theme-types'
export const PM_LIGHT_THEME = {
  defs: {
    bg: '#ffffff',
    panelBg: '#f6f8fa',
    text: '#24292f',
    muted: '#57606a',
    border: '#d0d7de',
    accent: '#0969da',
    warning: '#9a6700',
    error: '#cf222e',
    success: '#1a7f37',
    selectionBg: '#ddf4ff',
  },
  theme: {
    background: 'bg',
    text: 'text',
    mutedText: 'muted',
    border: 'border',
    accent: 'accent',
    accentText: 'bg',
    warning: 'warning',
    error: 'error',
    success: 'success',
    panelBackground: 'panelBg',
    popupBackground: 'panelBackground',
    selectionBackground: 'selectionBg',
    selectionText: 'text',
    chipBackground: 'panelBackground',
    chipText: 'text',
    chipMutedText: 'mutedText',
  },
} as const satisfies ThemeJson
</file>

<file path="src/tui/theme/examples/ocean-example.json">
{
  "defs": {
    "bg": { "dark": "#10141a", "light": "#fafcff" },
    "panel": { "dark": "#1a2029", "light": "#eef3f8" },
    "text": { "dark": "#e6edf3", "light": "#1f2328" },
    "muted": { "dark": "#9aa4b2", "light": "#66707f" },
    "border": { "dark": "#30363d", "light": "#d0d7de" },
    "accent": { "dark": "#6cb6ff", "light": "#0969da" },
    "warning": "#f2cc60",
    "error": "#ff6b6b",
    "success": "#3fb950"
  },
  "theme": {
    "background": "bg",
    "text": "text",
    "mutedText": "muted",
    "border": "border",

    "accent": "accent",
    "accentText": "background",

    "warning": "warning",
    "error": "error",
    "success": "success",

    "panelBackground": "panel",
    "popupBackground": "panelBackground",

    "selectionBackground": { "dark": 60, "light": 153 },
    "selectionText": "text",

    "chipBackground": "panelBackground",
    "chipText": "text",
    "chipMutedText": "muted"
  }
}
</file>

<file path="src/tui/theme/terminal-appearance.ts">
import type { ThemeAppearanceMode } from './theme-types'
const parseAnsiIndex = (value: string): number | null => {
  const trimmed = value.trim()
  if (!trimmed) {
    return null
  }
  const parsed = Number.parseInt(trimmed, 10)
  return Number.isFinite(parsed) ? parsed : null
}
const detectFromColorFgBg = (envValue: string): ThemeAppearanceMode | null => {
  // `COLORFGBG` commonly looks like "15;0" (foreground;background)
  // but some terminals include extra segments. We only care about the last.
  const parts = envValue
    .split(';')
    .map((part) => part.trim())
    .filter((part) => part.length > 0)
  const last = parts[parts.length - 1]
  if (!last) {
    return null
  }
  const background = parseAnsiIndex(last)
  if (background === null) {
    return null
  }
  if (background >= 0 && background <= 6) {
    return 'dark'
  }
  if (background >= 7 && background <= 15) {
    return 'light'
  }
  return null
}
export const detectTerminalAppearanceMode = (
  env: NodeJS.ProcessEnv,
): ThemeAppearanceMode | null => {
  const explicit = env.TERM_BACKGROUND?.trim().toLowerCase()
  if (explicit === 'light' || explicit === 'dark') {
    return explicit
  }
  const colorFgBg = env.COLORFGBG?.trim()
  if (colorFgBg) {
    return detectFromColorFgBg(colorFgBg)
  }
  return null
}
</file>

<file path="src/tui/theme/theme-provider.tsx">
import React, { createContext, useCallback, useContext, useEffect, useMemo, useState } from 'react'
import { detectTerminalAppearanceMode } from './terminal-appearance'
import { DEFAULT_THEME_NAME, getThemeJson } from './theme-registry'
import type { ThemeDescriptor } from './theme-loader'
import { resolveTheme } from './theme-resolver'
import {
  loadThemeSelection,
  saveThemeSelection,
  type ThemeSelectionWarning,
} from './theme-settings-service'
import {
  REQUIRED_THEME_SLOTS,
  type ResolvedTheme,
  type ThemeAppearanceMode,
  type ThemeJson,
  type ThemeMode,
} from './theme-types'
export type ThemeProviderError = {
  kind: 'load-failed' | 'resolve-failed' | 'save-failed'
  message: string
}
export type ThemeContextValue = {
  theme: ResolvedTheme
  mode: ThemeMode
  setMode: (mode: ThemeMode) => Promise<boolean>
  activeThemeName: string
  setTheme: (name: string) => Promise<boolean>
  previewTheme: (name: string) => boolean
  themes: readonly ThemeDescriptor[]
  warnings: readonly ThemeSelectionWarning[]
  error: ThemeProviderError | null
}
const ThemeContext = createContext<ThemeContextValue | null>(null)
export const useTheme = (): ThemeContextValue => {
  const context = useContext(ThemeContext)
  if (!context) {
    throw new Error('useTheme must be used within ThemeProvider')
  }
  return context
}
const FALLBACK_THEME: ResolvedTheme = Object.fromEntries(
  REQUIRED_THEME_SLOTS.map((slot) => [slot, undefined]),
) as ResolvedTheme
const resolveThemeOrThrow = (themeJson: ThemeJson, mode: ThemeAppearanceMode): ResolvedTheme =>
  resolveTheme(themeJson, mode)
const resolveAppearanceMode = (mode: ThemeMode): ThemeAppearanceMode => {
  if (mode === 'light') {
    return 'light'
  }
  if (mode === 'dark') {
    return 'dark'
  }
  // Pragmatic "system" mode: if we can't reliably detect the terminal background,
  // we deterministically fall back to `dark`.
  return detectTerminalAppearanceMode(process.env) ?? 'dark'
}
const resolveThemeFromName = (params: {
  themes: readonly ThemeDescriptor[]
  name: string
  appearanceMode: ThemeAppearanceMode
}): { theme: ResolvedTheme; themeName: string } => {
  const descriptor = params.themes.find((theme) => theme.name === params.name)
  if (descriptor) {
    return {
      theme: resolveThemeOrThrow(descriptor.theme, params.appearanceMode),
      themeName: params.name,
    }
  }
  const fallbackJson = getThemeJson(DEFAULT_THEME_NAME)
  if (!fallbackJson) {
    return { theme: FALLBACK_THEME, themeName: DEFAULT_THEME_NAME }
  }
  return {
    theme: resolveThemeOrThrow(fallbackJson, params.appearanceMode),
    themeName: DEFAULT_THEME_NAME,
  }
}
const resolveThemeFromNameStrict = (params: {
  themes: readonly ThemeDescriptor[]
  name: string
  appearanceMode: ThemeAppearanceMode
}): { theme: ResolvedTheme; themeName: string } => {
  const descriptor = params.themes.find((theme) => theme.name === params.name)
  if (!descriptor) {
    throw new Error(`Unknown theme '${params.name}'.`)
  }
  return {
    theme: resolveThemeOrThrow(descriptor.theme, params.appearanceMode),
    themeName: descriptor.name,
  }
}
export const ThemeProvider: React.FC<{ children: React.ReactNode }> = ({ children }) => {
  const [themes, setThemes] = useState<readonly ThemeDescriptor[]>([])
  const [activeThemeName, setActiveThemeName] = useState(DEFAULT_THEME_NAME)
  const [mode, setModeState] = useState<ThemeMode>('dark')
  const [warnings, setWarnings] = useState<readonly ThemeSelectionWarning[]>([])
  const [error, setError] = useState<ThemeProviderError | null>(null)
  const [theme, setThemeState] = useState<ResolvedTheme>(() => {
    const base = getThemeJson(DEFAULT_THEME_NAME)
    if (!base) {
      return FALLBACK_THEME
    }
    try {
      return resolveThemeOrThrow(base, 'dark')
    } catch {
      return FALLBACK_THEME
    }
  })
  useEffect(() => {
    let cancelled = false
    const run = async (): Promise<void> => {
      try {
        const loaded = await loadThemeSelection()
        if (cancelled) {
          return
        }
        setThemes(loaded.themes)
        setWarnings(loaded.warnings)
        setModeState(loaded.selection.themeMode)
        const appearanceMode = resolveAppearanceMode(loaded.selection.themeMode)
        try {
          const resolved = resolveThemeFromName({
            themes: loaded.themes,
            name: loaded.selection.themeName,
            appearanceMode,
          })
          setActiveThemeName(resolved.themeName)
          setThemeState(resolved.theme)
          if (resolved.themeName !== loaded.selection.themeName) {
            void saveThemeSelection({ themeName: resolved.themeName })
          }
          setError(null)
        } catch (resolveError) {
          const message =
            resolveError instanceof Error ? resolveError.message : 'Unknown theme resolution error.'
          setError({ kind: 'resolve-failed', message })
          const fallbackJson = getThemeJson(DEFAULT_THEME_NAME)
          if (fallbackJson) {
            setActiveThemeName(DEFAULT_THEME_NAME)
            setThemeState(resolveThemeOrThrow(fallbackJson, appearanceMode))
            void saveThemeSelection({ themeName: DEFAULT_THEME_NAME })
          }
        }
      } catch (loadError) {
        const message = loadError instanceof Error ? loadError.message : 'Unknown theme load error.'
        setError({ kind: 'load-failed', message })
      }
    }
    void run()
    return () => {
      cancelled = true
    }
  }, [])
  const previewTheme = useCallback(
    (name: string): boolean => {
      if (themes.length === 0) {
        setError({ kind: 'resolve-failed', message: 'No themes loaded.' })
        return false
      }
      const appearanceMode = resolveAppearanceMode(mode)
      try {
        const resolved = resolveThemeFromNameStrict({ themes, name, appearanceMode })
        setActiveThemeName(resolved.themeName)
        setThemeState(resolved.theme)
        setError(null)
        return true
      } catch (resolveError) {
        const message =
          resolveError instanceof Error ? resolveError.message : 'Unknown theme resolution error.'
        setError({ kind: 'resolve-failed', message })
        return false
      }
    },
    [mode, themes],
  )
  const setTheme = useCallback(
    async (name: string): Promise<boolean> => {
      if (themes.length === 0) {
        setError({ kind: 'resolve-failed', message: 'No themes loaded.' })
        return false
      }
      const appearanceMode = resolveAppearanceMode(mode)
      let resolved: { theme: ResolvedTheme; themeName: string }
      try {
        resolved = resolveThemeFromNameStrict({ themes, name, appearanceMode })
      } catch (resolveError) {
        const message =
          resolveError instanceof Error ? resolveError.message : 'Unknown theme resolution error.'
        setError({ kind: 'resolve-failed', message })
        return false
      }
      try {
        await saveThemeSelection({ themeName: resolved.themeName })
      } catch (saveError) {
        const message = saveError instanceof Error ? saveError.message : 'Unknown theme save error.'
        setError({ kind: 'save-failed', message })
        return false
      }
      setActiveThemeName(resolved.themeName)
      setThemeState(resolved.theme)
      setError(null)
      return true
    },
    [mode, themes],
  )
  const setMode = useCallback(
    async (nextMode: ThemeMode): Promise<boolean> => {
      if (themes.length === 0) {
        setError({ kind: 'resolve-failed', message: 'No themes loaded.' })
        return false
      }
      const appearanceMode = resolveAppearanceMode(nextMode)
      let resolved: { theme: ResolvedTheme; themeName: string }
      try {
        resolved = resolveThemeFromNameStrict({ themes, name: activeThemeName, appearanceMode })
      } catch (resolveError) {
        const message =
          resolveError instanceof Error ? resolveError.message : 'Unknown theme resolution error.'
        setError({ kind: 'resolve-failed', message })
        return false
      }
      try {
        await saveThemeSelection({ themeMode: nextMode })
      } catch (saveError) {
        const message = saveError instanceof Error ? saveError.message : 'Unknown theme save error.'
        setError({ kind: 'save-failed', message })
        return false
      }
      setModeState(nextMode)
      setThemeState(resolved.theme)
      setError(null)
      return true
    },
    [activeThemeName, themes],
  )
  const value = useMemo<ThemeContextValue>(
    () => ({
      theme,
      mode,
      setMode,
      activeThemeName,
      setTheme,
      previewTheme,
      themes,
      warnings,
      error,
    }),
    [activeThemeName, error, mode, previewTheme, setMode, setTheme, theme, themes, warnings],
  )
  return <ThemeContext.Provider value={value}>{children}</ThemeContext.Provider>
}
</file>

<file path="src/tui/theme/theme-registry.ts">
import type { ThemeJson } from './theme-types'
import { PM_DARK_THEME } from './builtins/pm-dark'
import { PM_LIGHT_THEME } from './builtins/pm-light'
export type ThemeRegistryEntry = {
  name: string
  label: string
  theme: ThemeJson
}
export const DEFAULT_THEME_NAME = 'pm-dark'
const BUILTIN_THEMES: readonly ThemeRegistryEntry[] = [
  {
    name: 'pm-dark',
    label: 'Prompt Maker Dark',
    theme: PM_DARK_THEME,
  },
  {
    name: 'pm-light',
    label: 'Prompt Maker Light',
    theme: PM_LIGHT_THEME,
  },
]
export const listThemes = (): readonly ThemeRegistryEntry[] => BUILTIN_THEMES
export const getTheme = (name: string): ThemeRegistryEntry | undefined =>
  BUILTIN_THEMES.find((theme) => theme.name === name)
export const getThemeJson = (name: string): ThemeJson | undefined => getTheme(name)?.theme
</file>

<file path="src/tui/theme/theme-settings-service.ts">
import { loadCliConfig, updateCliThemeSettings, type PromptMakerCliConfig } from '../../config'
import { DEFAULT_THEME_NAME } from './theme-registry'
import {
  loadThemes,
  type LoadThemesOptions,
  type ThemeDescriptor,
  type ThemeLoadError,
} from './theme-loader'
import type { ThemeMode } from './theme-types'
export const DEFAULT_THEME_MODE: ThemeMode = 'dark'
export type ThemeSelection = {
  themeName: string
  themeMode: ThemeMode
}
export type ThemeSelectionWarning = {
  kind: 'unknown-theme'
  requested: string
  fallback: string
  message: string
}
const resolveThemeMode = (config: PromptMakerCliConfig | null): ThemeMode =>
  config?.themeMode ?? DEFAULT_THEME_MODE
const resolveThemeName = (
  config: PromptMakerCliConfig | null,
  themes: readonly ThemeDescriptor[],
): { themeName: string; warnings: ThemeSelectionWarning[] } => {
  const requested = config?.theme?.trim()
  if (!requested) {
    return { themeName: DEFAULT_THEME_NAME, warnings: [] }
  }
  const exists = themes.some((theme) => theme.name === requested)
  if (exists) {
    return { themeName: requested, warnings: [] }
  }
  return {
    themeName: DEFAULT_THEME_NAME,
    warnings: [
      {
        kind: 'unknown-theme',
        requested,
        fallback: DEFAULT_THEME_NAME,
        message: `Unknown theme '${requested}', falling back to '${DEFAULT_THEME_NAME}'.`,
      },
    ],
  }
}
export const resolveThemeSelectionFromConfig = (params: {
  config: PromptMakerCliConfig | null
  themes: readonly ThemeDescriptor[]
}): { selection: ThemeSelection; warnings: ThemeSelectionWarning[] } => {
  const themeMode = resolveThemeMode(params.config)
  const resolved = resolveThemeName(params.config, params.themes)
  return {
    selection: {
      themeName: resolved.themeName,
      themeMode,
    },
    warnings: resolved.warnings,
  }
}
export const loadThemeSelection = async (params?: {
  themeLoadOptions?: LoadThemesOptions
}): Promise<{
  themes: ThemeDescriptor[]
  loadErrors: ThemeLoadError[]
  selection: ThemeSelection
  warnings: ThemeSelectionWarning[]
}> => {
  const [config, loaded] = await Promise.all([
    loadCliConfig(),
    loadThemes(params?.themeLoadOptions ?? {}),
  ])
  const resolved = resolveThemeSelectionFromConfig({ config, themes: loaded.themes })
  return {
    themes: loaded.themes,
    loadErrors: loaded.errors,
    selection: resolved.selection,
    warnings: resolved.warnings,
  }
}
export const saveThemeSelection = async (selection: Partial<ThemeSelection>): Promise<void> => {
  await updateCliThemeSettings({
    ...(selection.themeName !== undefined ? { theme: selection.themeName } : {}),
    ...(selection.themeMode !== undefined ? { themeMode: selection.themeMode } : {}),
  })
}
</file>

<file path="src/tui/command-filter.ts">
/*
 * Command palette filtering.
 *
 * The CommandScreen has a command palette ("/command") with fuzzy-ish matching.
 * This file is intentionally pure so we can unit test and refactor the UI safely.
 *
 * Key UX rules (preserved by tests):
 * - Empty query returns all commands in their configured order.
 * - Matches are case-insensitive.
 * - Prefix matches (id/alias/label) are ranked above substring matches.
 * - When scores tie, original order is preserved (stable ordering).
 */
import type { CommandDescriptor } from './types'
type ScoredCommand = {
  command: CommandDescriptor
  score: number
  index: number
}
const normalizeQueryTokens = (query: string): string[] =>
  query
    .trim()
    .toLowerCase()
    .split(/\s+/)
    .filter((token) => token.length > 0)
const getCommandAliases = (command: CommandDescriptor): readonly string[] => {
  if (!('aliases' in command)) {
    return []
  }
  const aliases = command.aliases
  return Array.isArray(aliases) ? aliases : []
}
const scoreCommandToken = (token: string, command: CommandDescriptor): number | null => {
  const id = command.id.toLowerCase()
  const label = command.label.toLowerCase()
  const description = command.description.toLowerCase()
  const aliases = getCommandAliases(command).map((alias) => alias.toLowerCase())
  const combinedHaystack = `${id} ${label} ${aliases.join(' ')} ${description}`
  if (!combinedHaystack.includes(token)) {
    return null
  }
  if (id.startsWith(token)) {
    return 100
  }
  if (aliases.some((alias) => alias.startsWith(token))) {
    return 95
  }
  if (label.startsWith(token)) {
    return 90
  }
  if (id.includes(token)) {
    return 80
  }
  if (aliases.some((alias) => alias.includes(token))) {
    return 75
  }
  if (label.includes(token)) {
    return 70
  }
  return 60
}
const scoreCommand = (tokens: readonly string[], command: CommandDescriptor): number | null => {
  if (tokens.length === 0) {
    return 0
  }
  let score = 0
  for (const token of tokens) {
    const tokenScore = scoreCommandToken(token, command)
    if (tokenScore === null) {
      return null
    }
    score += tokenScore
  }
  return score
}
export type FilterCommandDescriptorsOptions = {
  query: string
  commands: readonly CommandDescriptor[]
}
export const filterCommandDescriptors = ({
  query,
  commands,
}: FilterCommandDescriptorsOptions): CommandDescriptor[] => {
  const tokens = normalizeQueryTokens(query)
  if (tokens.length === 0) {
    return [...commands]
  }
  const scored: ScoredCommand[] = []
  commands.forEach((command, index) => {
    const score = scoreCommand(tokens, command)
    if (score === null) {
      return
    }
    scored.push({ command, score, index })
  })
  return scored
    .sort((a, b) => {
      if (b.score !== a.score) {
        return b.score - a.score
      }
      return a.index - b.index
    })
    .map((entry) => entry.command)
}
const matchesCommandPrefixToken = (token: string, command: CommandDescriptor): boolean => {
  if (!token) {
    return false
  }
  const normalized = token.toLowerCase()
  if (command.id.toLowerCase().startsWith(normalized)) {
    return true
  }
  if (command.label.toLowerCase().startsWith(normalized)) {
    return true
  }
  return getCommandAliases(command).some((alias) => alias.toLowerCase().startsWith(normalized))
}
export type HasCommandDescriptorPrefixMatchOptions = {
  token: string
  commands: readonly CommandDescriptor[]
}
export const hasCommandDescriptorPrefixMatch = ({
  token,
  commands,
}: HasCommandDescriptorPrefixMatchOptions): boolean => {
  const normalized = token.trim().toLowerCase()
  if (!normalized) {
    return false
  }
  return commands.some((command) => matchesCommandPrefixToken(normalized, command))
}
export type CommandMenuSearchState = {
  filterQuery: string
  treatRemainderAsArgs: boolean
}
export type ResolveCommandMenuSearchStateOptions = {
  commandQuery: string
  commands: readonly CommandDescriptor[]
}
export const resolveCommandMenuSearchState = ({
  commandQuery,
  commands,
}: ResolveCommandMenuSearchStateOptions): CommandMenuSearchState => {
  const normalized = commandQuery.trim().toLowerCase()
  if (!normalized) {
    return { filterQuery: '', treatRemainderAsArgs: false }
  }
  const parts = normalized.split(/\s+/).filter((part) => part.length > 0)
  const firstToken = parts[0] ?? ''
  const treatRemainderAsArgs = hasCommandDescriptorPrefixMatch({ token: firstToken, commands })
  return {
    filterQuery: treatRemainderAsArgs ? firstToken : normalized,
    treatRemainderAsArgs,
  }
}
</file>

<file path="src/tui/command-history.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
export type CommandHistoryRecord = {
  value: string
  timestamp: string
}
const HISTORY_FILE = path.join(os.homedir(), '.config', 'prompt-maker-cli', 'tui-history.json')
const getErrorCode = (error: unknown): string | null => {
  if (!error || typeof error !== 'object') {
    return null
  }
  if (!('code' in error)) {
    return null
  }
  const code = (error as { code?: unknown }).code
  return typeof code === 'string' ? code : null
}
const isFileMissingError = (error: unknown): boolean => getErrorCode(error) === 'ENOENT'
const isRecoverableHistoryError = (error: unknown): boolean => {
  if (error instanceof SyntaxError) {
    return true
  }
  if (error instanceof Error && error.message === 'History file must contain a JSON array.') {
    return true
  }
  return false
}
const sanitizeTimestamp = (timestamp: string): string => timestamp.replace(/[:.]/g, '-')
const repairCorruptHistoryFile = async (): Promise<void> => {
  const directory = path.dirname(HISTORY_FILE)
  const backupPath = path.join(
    directory,
    `tui-history.corrupt-${sanitizeTimestamp(new Date().toISOString())}.json`,
  )
  try {
    await fs.mkdir(directory, { recursive: true })
  } catch {
    return
  }
  try {
    await fs.rename(HISTORY_FILE, backupPath)
  } catch {
    // Best effort: ignore backup failures.
  }
  try {
    await fs.writeFile(HISTORY_FILE, '[]\n', 'utf8')
  } catch {
    // Best effort: ignore repair failures.
  }
}
const parseHistoryRecords = (raw: unknown): CommandHistoryRecord[] => {
  if (!Array.isArray(raw)) {
    throw new Error('History file must contain a JSON array.')
  }
  return raw
    .map((entry) => {
      if (!entry || typeof entry !== 'object') {
        return null
      }
      if (!('value' in entry) || typeof entry.value !== 'string') {
        return null
      }
      const timestamp =
        'timestamp' in entry && typeof entry.timestamp === 'string' ? entry.timestamp : null
      return { value: entry.value, timestamp: timestamp ?? new Date().toISOString() }
    })
    .filter((entry): entry is CommandHistoryRecord => Boolean(entry))
}
export const readCommandHistory = async (): Promise<CommandHistoryRecord[]> => {
  try {
    const contents = await fs.readFile(HISTORY_FILE, 'utf8')
    if (!contents.trim()) {
      return []
    }
    try {
      const parsed = JSON.parse(contents) as unknown
      return parseHistoryRecords(parsed)
    } catch (parseError) {
      if (isRecoverableHistoryError(parseError)) {
        await repairCorruptHistoryFile()
        return []
      }
      throw parseError
    }
  } catch (error) {
    if (isFileMissingError(error)) {
      return []
    }
    if (isRecoverableHistoryError(error)) {
      await repairCorruptHistoryFile()
      return []
    }
    const message = error instanceof Error ? error.message : 'Unknown history error.'
    throw new Error(`Failed to load history at ${HISTORY_FILE}: ${message}`)
  }
}
export const writeCommandHistory = async (entries: CommandHistoryRecord[]): Promise<void> => {
  const directory = path.dirname(HISTORY_FILE)
  await fs.mkdir(directory, { recursive: true })
  const contents = JSON.stringify(entries, null, 2)
  const tempFile = `${HISTORY_FILE}.${process.pid}.tmp`
  await fs.writeFile(tempFile, contents, 'utf8')
  try {
    await fs.rename(tempFile, HISTORY_FILE)
  } catch {
    await fs.writeFile(HISTORY_FILE, contents, 'utf8')
  }
}
export const updateCommandHistory = (params: {
  previous: CommandHistoryRecord[]
  nextValue: string
  timestamp?: string
  maxEntries: number
}): CommandHistoryRecord[] => {
  const normalized = params.nextValue.trim()
  if (!normalized) {
    return params.previous
  }
  const lastEntry = params.previous[0]
  if (lastEntry && lastEntry.value === normalized) {
    return params.previous
  }
  const next: CommandHistoryRecord[] = [
    { value: normalized, timestamp: params.timestamp ?? new Date().toISOString() },
    ...params.previous,
  ]
  return next.slice(0, Math.max(1, params.maxEntries))
}
</file>

<file path="src/tui/CommandScreen.tsx">
/*
 * CommandScreen entry point.
 *
 * This file intentionally stays small.
 * The real implementation lives under `src/tui/screens/command/*` so we can
 * split the screen into a reducer-driven model + smaller components over time.
 */
export { CommandScreen } from './screens/command/CommandScreen'
export type { CommandScreenHandle } from './screens/command/CommandScreen'
</file>

<file path="src/tui/drag-drop-path.ts">
import { stripTerminalPasteArtifacts } from './components/core/bracketed-paste'
export const parseAbsolutePathFromInput = (input: string): string | null => {
  const sanitizedInput = stripTerminalPasteArtifacts(input)
  const trimmed = sanitizedInput.trim()
  if (!trimmed) {
    return null
  }
  const { unquoted, wasQuoted } = stripMatchingQuotes(trimmed)
  const normalized = unquoted.trim()
  if (!normalized) {
    return null
  }
  if (isWindowsAbsolute(normalized)) {
    if (!wasQuoted && /\s/.test(normalized)) {
      return null
    }
    return normalized
  }
  if (!wasQuoted && containsUnescapedWhitespace(normalized)) {
    return null
  }
  const unescaped = unescapeBackslashes(normalized).trim()
  if (!unescaped) {
    return null
  }
  return isPosixAbsolute(unescaped) ? unescaped : null
}
export const isCommandInput = (
  input: string,
  existsSync: (candidate: string) => boolean,
): boolean => {
  const sanitizedInput = stripTerminalPasteArtifacts(input)
  const trimmedStart = sanitizedInput.trimStart()
  if (!trimmedStart.startsWith('/')) {
    return false
  }
  if (trimmedStart === '/') {
    return true
  }
  const absoluteCandidate = parseAbsolutePathFromInput(trimmedStart)
  if (!absoluteCandidate) {
    return true
  }
  const hasNestedSegment = absoluteCandidate.length > 1 && absoluteCandidate.slice(1).includes('/')
  if (hasNestedSegment) {
    return false
  }
  return !existsSync(absoluteCandidate)
}
const stripMatchingQuotes = (value: string): { unquoted: string; wasQuoted: boolean } => {
  if (value.length < 2) {
    return { unquoted: value, wasQuoted: false }
  }
  const first = value[0]
  const last = value[value.length - 1]
  const isQuote = first === '"' || first === "'"
  if (!isQuote || first !== last) {
    return { unquoted: value, wasQuoted: false }
  }
  return { unquoted: value.slice(1, -1), wasQuoted: true }
}
const containsUnescapedWhitespace = (value: string): boolean => {
  let escaping = false
  for (const ch of value) {
    if (escaping) {
      escaping = false
      continue
    }
    if (ch === '\\') {
      escaping = true
      continue
    }
    if (ch === ' ' || ch === '\t' || ch === '\n' || ch === '\r') {
      return true
    }
  }
  return false
}
const unescapeBackslashes = (value: string): string => {
  let result = ''
  let escaping = false
  for (const ch of value) {
    if (escaping) {
      result += ch
      escaping = false
      continue
    }
    if (ch === '\\') {
      escaping = true
      continue
    }
    result += ch
  }
  if (escaping) {
    result += '\\'
  }
  return result
}
const isPosixAbsolute = (value: string): boolean => value.startsWith('/')
const isWindowsAbsolute = (value: string): boolean => /^[a-zA-Z]:[\\/]/.test(value)
</file>

<file path="src/tui/intent-source.ts">
export type IntentSourceSelection =
  | { kind: 'text'; intent: string }
  | { kind: 'file'; intentFile: string }
  | { kind: 'empty' }
export const resolveIntentSource = (
  intentValue: string,
  intentFileValue: string,
): IntentSourceSelection => {
  const trimmedFile = intentFileValue.trim()
  if (trimmedFile.length > 0) {
    return { kind: 'file', intentFile: trimmedFile }
  }
  const trimmedIntent = intentValue.trim()
  if (trimmedIntent.length > 0) {
    return { kind: 'text', intent: trimmedIntent }
  }
  return { kind: 'empty' }
}
</file>

<file path="src/tui/model-filter.ts">
import type { ModelOption } from './types'
const DEFAULT_FILTER_LIMIT = 200
export const resolveModelPopupQuery = (query: string, debouncedQuery: string): string => {
  return query.trim() ? debouncedQuery : ''
}
const normalizeQueryTokens = (query: string): string[] =>
  query
    .trim()
    .toLowerCase()
    .split(/\s+/)
    .filter((token) => token.length > 0)
type SubsequenceMatch = {
  start: number
  end: number
  gaps: number
}
const matchSubsequence = (needle: string, haystack: string): SubsequenceMatch | null => {
  if (!needle) {
    return { start: 0, end: 0, gaps: 0 }
  }
  let needleIndex = 0
  let start = -1
  let lastMatch = -1
  let gaps = 0
  for (let haystackIndex = 0; haystackIndex < haystack.length; haystackIndex += 1) {
    if (haystack.charAt(haystackIndex) !== needle.charAt(needleIndex)) {
      continue
    }
    if (needleIndex === 0) {
      start = haystackIndex
    } else {
      gaps += Math.max(0, haystackIndex - lastMatch - 1)
    }
    lastMatch = haystackIndex
    needleIndex += 1
    if (needleIndex >= needle.length) {
      return { start, end: haystackIndex, gaps }
    }
  }
  return null
}
const scoreSubsequenceMatch = (needle: string, match: SubsequenceMatch): number => {
  const span = match.end - match.start + 1
  const density = needle.length / Math.max(span, 1)
  const densityBoost = Math.min(30, Math.round(density * 30))
  const startBoost = Math.max(0, 10 - match.start)
  const gapPenalty = Math.min(20, match.gaps)
  return 40 + densityBoost + startBoost - gapPenalty
}
const scoreModelToken = (token: string, option: ModelOption): number | null => {
  const normalized = token.trim().toLowerCase()
  if (!normalized) {
    return 0
  }
  const id = option.id.toLowerCase()
  const label = option.label.toLowerCase()
  const provider = option.provider.toLowerCase()
  const description = option.description.toLowerCase()
  const capabilities = option.capabilities.join(' ').toLowerCase()
  const notes = option.notes?.toLowerCase() ?? ''
  if (id === normalized) {
    return 200
  }
  if (id.startsWith(normalized)) {
    return 165
  }
  if (label.startsWith(normalized)) {
    return 155
  }
  if (provider.startsWith(normalized)) {
    return 145
  }
  if (id.includes(normalized)) {
    return 125
  }
  if (label.includes(normalized)) {
    return 115
  }
  if (description.includes(normalized)) {
    return 85
  }
  if (capabilities.includes(normalized)) {
    return 75
  }
  if (notes.includes(normalized)) {
    return 65
  }
  const idMatch = matchSubsequence(normalized, id)
  if (idMatch) {
    return scoreSubsequenceMatch(normalized, idMatch) + 55
  }
  const labelMatch = matchSubsequence(normalized, label)
  if (labelMatch) {
    return scoreSubsequenceMatch(normalized, labelMatch) + 45
  }
  return null
}
export const filterModelOptions = (
  query: string,
  options: readonly ModelOption[],
  limit: number = DEFAULT_FILTER_LIMIT,
): ModelOption[] => {
  const tokens = normalizeQueryTokens(query)
  if (tokens.length === 0) {
    return [...options]
  }
  const scored: Array<{ option: ModelOption; score: number; index: number }> = []
  options.forEach((option, index) => {
    let score = 0
    for (const token of tokens) {
      const tokenScore = scoreModelToken(token, option)
      if (tokenScore === null) {
        return
      }
      score += tokenScore
    }
    scored.push({ option, score, index })
  })
  scored.sort((a, b) => {
    if (b.score !== a.score) {
      return b.score - a.score
    }
    return a.index - b.index
  })
  return scored.slice(0, limit).map((entry) => entry.option)
}
</file>

<file path="src/tui/model-popup-options.ts">
import type { ModelProvider } from '../model-providers'
import { filterModelOptions } from './model-filter'
import type { ModelOption } from './types'
export type ModelPopupOptionsResult = {
  options: ModelOption[]
  recentCount: number
}
type BuildModelPopupOptionsParams = {
  query: string
  modelOptions: readonly ModelOption[]
  recentModelIds?: readonly string[]
}
const PROVIDER_ORDER: readonly ModelProvider[] = ['openai', 'gemini', 'other']
const resolveRecentOptions = (
  modelOptions: readonly ModelOption[],
  recentModelIds: readonly string[],
): ModelOption[] => {
  if (recentModelIds.length === 0) {
    return []
  }
  const byId = new Map(modelOptions.map((option) => [option.id, option]))
  const resolved: ModelOption[] = []
  for (const modelId of recentModelIds) {
    const option = byId.get(modelId)
    if (option) {
      resolved.push(option)
    }
  }
  return resolved
}
const groupByProvider = (options: readonly ModelOption[]): ModelOption[] => {
  const grouped: ModelOption[] = []
  for (const provider of PROVIDER_ORDER) {
    for (const option of options) {
      if (option.provider === provider) {
        grouped.push(option)
      }
    }
  }
  const groupedIds = new Set(grouped.map((option) => option.id))
  for (const option of options) {
    if (!groupedIds.has(option.id)) {
      grouped.push(option)
    }
  }
  return grouped
}
export const buildModelPopupOptions = ({
  query,
  modelOptions,
  recentModelIds = [],
}: BuildModelPopupOptionsParams): ModelPopupOptionsResult => {
  const trimmedQuery = query.trim()
  if (!trimmedQuery) {
    const recentOptions = resolveRecentOptions(modelOptions, recentModelIds)
    const recentIds = new Set(recentOptions.map((option) => option.id))
    const remaining = modelOptions.filter((option) => !recentIds.has(option.id))
    const groupedRemaining = groupByProvider(remaining)
    return { options: [...recentOptions, ...groupedRemaining], recentCount: recentOptions.length }
  }
  const filtered = filterModelOptions(trimmedQuery, modelOptions)
  return { options: groupByProvider(filtered), recentCount: 0 }
}
</file>

<file path="src/tui/model-session.ts">
let lastSessionModelId: string | null = null
let recentSessionModelIds: string[] = []
const RECENT_MODEL_LIMIT = 5
export const getLastSessionModel = (): string | null => lastSessionModelId
export const getRecentSessionModels = (): string[] => [...recentSessionModelIds]
export const recordRecentSessionModel = (modelId: string): void => {
  const normalized = modelId.trim()
  if (!normalized) {
    return
  }
  recentSessionModelIds = [
    normalized,
    ...recentSessionModelIds.filter((existing) => existing !== normalized),
  ].slice(0, RECENT_MODEL_LIMIT)
}
export const setLastSessionModel = (modelId: string): void => {
  const normalized = modelId.trim()
  lastSessionModelId = normalized || null
}
export const resetLastSessionModelForTests = (): void => {
  lastSessionModelId = null
  recentSessionModelIds = []
}
export const resetRecentSessionModelsForTests = (): void => {
  recentSessionModelIds = []
}
</file>

<file path="src/tui/new-command.ts">
export type SessionCommandId = 'new' | 'reuse'
export type SessionCommandPlan =
  | { type: 'reset-only'; message: string }
  | { type: 'reset-and-load-meta'; message: string; metaInstructions: string }
export type PlanSessionCommandOptions = {
  commandId: SessionCommandId
  lastGeneratedPrompt: string | null
}
export const planSessionCommand = ({
  commandId,
  lastGeneratedPrompt,
}: PlanSessionCommandOptions): SessionCommandPlan => {
  if (commandId === 'new') {
    return { type: 'reset-only', message: '[new] Session reset.' }
  }
  const prompt = lastGeneratedPrompt?.trim() ?? ''
  if (!prompt) {
    return { type: 'reset-only', message: '[reuse] Session reset · no previous prompt to reuse.' }
  }
  return {
    type: 'reset-and-load-meta',
    message: '[reuse] Session reset · loaded last prompt into meta instructions.',
    metaInstructions: prompt,
  }
}
</file>

<file path="src/tui/paste-snippet.ts">
import { stripBracketedPasteControlSequences } from './components/core/bracketed-paste'
export const BRACKETED_PASTE_START = '[200~'
export const BRACKETED_PASTE_END = '[201~'
export type BracketedPasteState = {
  readonly isActive: boolean
  readonly buffer: string
}
export type ConsumeBracketedPasteResult = {
  readonly state: BracketedPasteState
  readonly completed: readonly string[]
  readonly didSeeBracketedPaste: boolean
}
export const createBracketedPasteState = (): BracketedPasteState => ({
  isActive: false,
  buffer: '',
})
export const consumeBracketedPasteChunk = (
  state: BracketedPasteState,
  chunk: string,
): ConsumeBracketedPasteResult => {
  let remaining = chunk
  let isActive = state.isActive
  let buffer = state.buffer
  let didSeeBracketedPaste = state.isActive
  const completed: string[] = []
  while (remaining.length > 0) {
    if (!isActive) {
      const startIndex = remaining.indexOf(BRACKETED_PASTE_START)
      if (startIndex === -1) {
        break
      }
      didSeeBracketedPaste = true
      isActive = true
      buffer = ''
      remaining = remaining.slice(startIndex + BRACKETED_PASTE_START.length)
      continue
    }
    const endIndex = remaining.indexOf(BRACKETED_PASTE_END)
    if (endIndex === -1) {
      buffer += remaining
      remaining = ''
      break
    }
    didSeeBracketedPaste = true
    buffer += remaining.slice(0, endIndex)
    completed.push(buffer)
    buffer = ''
    isActive = false
    remaining = remaining.slice(endIndex + BRACKETED_PASTE_END.length)
  }
  return {
    state: {
      isActive,
      buffer,
    },
    completed,
    didSeeBracketedPaste,
  }
}
export const MIN_PASTE_CHARS = 80
const PREVIEW_LINE_LIMIT = 3
export type PastedSnippet = {
  readonly text: string
  readonly lineCount: number
  readonly charCount: number
  readonly label: string
  readonly previewLines: readonly string[]
}
const normalizeLineEndings = (value: string): string => {
  const normalized = value
    .replace(/\r\n/g, '\n')
    .replace(/\r/g, '\n')
    .replace(/\u0000/g, '')
  return stripBracketedPasteControlSequences(normalized)
}
const countLines = (value: string): number => {
  const trimmed = value.trimEnd()
  if (!trimmed) {
    return 0
  }
  return trimmed.split('\n').length
}
export const formatPastedSnippetLabel = (lineCount: number): string =>
  `[Pasted ~${lineCount} ${lineCount === 1 ? 'line' : 'lines'}]`
export const createPastedSnippet = (raw: string): PastedSnippet | null => {
  const normalized = normalizeLineEndings(raw)
  const text = normalized.trimEnd()
  const lineCount = countLines(text)
  const charCount = text.length
  if (charCount < MIN_PASTE_CHARS) {
    return null
  }
  const previewLines = text
    .split('\n')
    .slice(0, PREVIEW_LINE_LIMIT)
    .map((line) => line.trimEnd())
  return {
    text,
    lineCount,
    charCount,
    label: formatPastedSnippetLabel(lineCount),
    previewLines,
  }
}
export type PastedSnippetDetection = {
  readonly snippet: PastedSnippet
  readonly range: {
    readonly start: number
    readonly end: number
  }
  readonly normalizedNextValue: string
}
const findInsertedRange = (
  previousValue: string,
  nextValue: string,
): { start: number; end: number } => {
  const maxPrefix = Math.min(previousValue.length, nextValue.length)
  let start = 0
  while (start < maxPrefix && previousValue[start] === nextValue[start]) {
    start += 1
  }
  let previousEnd = previousValue.length
  let nextEnd = nextValue.length
  while (
    previousEnd > start &&
    nextEnd > start &&
    previousValue[previousEnd - 1] === nextValue[nextEnd - 1]
  ) {
    previousEnd -= 1
    nextEnd -= 1
  }
  return { start, end: nextEnd }
}
export const detectPastedSnippetFromInputChange = (
  previousValue: string,
  nextValue: string,
): PastedSnippetDetection | null => {
  const previousNormalized = normalizeLineEndings(previousValue)
  const nextNormalized = normalizeLineEndings(nextValue)
  const range = findInsertedRange(previousNormalized, nextNormalized)
  const inserted = nextNormalized.slice(range.start, range.end)
  const snippet = createPastedSnippet(inserted)
  if (!snippet) {
    return null
  }
  return { snippet, range, normalizedNextValue: nextNormalized }
}
</file>

<file path="src/tui/provider-chip.ts">
import { inferProviderFromModelId } from '../model-providers'
import type { ProviderStatusMap } from './types'
const resolveProviderStatusSuffix = (
  providerStatus: ProviderStatusMap[keyof ProviderStatusMap] | undefined,
): string => {
  if (!providerStatus) {
    return 'unknown'
  }
  return providerStatus.status === 'ok'
    ? 'ok'
    : providerStatus.status === 'missing'
      ? 'missing-key'
      : 'error'
}
export const formatProviderStatusChip = (
  modelId: string,
  providerStatuses: ProviderStatusMap,
): string => {
  const provider = inferProviderFromModelId(modelId)
  const suffix = resolveProviderStatusSuffix(providerStatuses[provider])
  return `[${provider}:${suffix}]`
}
</file>

<file path="src/tui/string-filter.ts">
/*
 * String filtering primitives for the Ink TUI.
 *
 * Why this exists:
 * - We do a lot of “suggestion list” filtering (files, directories, etc.).
 * - Keeping the filtering algorithm in one place makes later refactors safer.
 *
 * Design goals:
 * - Pure: no filesystem/Ink/React dependencies.
 * - Stable ordering: results preserve original ordering within each match group.
 * - Behavior-preserving: matches the existing UX used by `filterFileSuggestions`.
 */
const normalize = (value: string): string => value.trim().toLowerCase()
const isPrefixMatch = (candidate: string, query: string): boolean =>
  query.length > 0 && candidate.startsWith(query)
const isSubstringMatch = (candidate: string, query: string): boolean =>
  query.length > 0 && candidate.includes(query)
export const filterStringsByQuery = (items: readonly string[], query: string): string[] => {
  const normalizedQuery = normalize(query)
  // When the query is blank, return items unchanged.
  // This is important because other code relies on this being stable.
  if (!normalizedQuery) {
    return [...items]
  }
  // UX detail: prefix matches feel “more correct” than substring matches.
  // We keep the original ordering inside each group to avoid jitter.
  const prefixMatches: string[] = []
  const substringMatches: string[] = []
  for (const item of items) {
    const candidate = item.toLowerCase()
    if (isPrefixMatch(candidate, normalizedQuery)) {
      prefixMatches.push(item)
      continue
    }
    if (isSubstringMatch(candidate, normalizedQuery)) {
      substringMatches.push(item)
    }
  }
  return [...prefixMatches, ...substringMatches]
}
</file>

<file path="src/tui/TestRunnerScreen.tsx">
/*
 * TestRunnerScreen entry point.
 *
 * This file intentionally stays small.
 * The implementation lives under `src/tui/screens/test-runner/*` so it can share
 * the same reducer-driven architecture as the command screen.
 */
export { TestRunnerScreen } from './screens/test-runner/TestRunnerScreen'
export type {
  TestRunnerScreenHandle,
  TestRunnerScreenProps,
} from './screens/test-runner/TestRunnerScreen'
</file>

<file path="src/tui/token-usage-store.ts">
import type { TokenTelemetry } from '../generate-command'
export type TokenUsageIteration = {
  iteration: number
  promptTokens: number
  reasoningTokens: number
}
export type TokenUsageRun = {
  id: string
  startedAt: string
  model: string
  telemetry: TokenTelemetry | null
  iterations: TokenUsageIteration[]
}
export type TokenUsageBreakdown = {
  input: {
    intent: number
    files: number
    system: number
    total: number
  }
  output: {
    reasoning: number
    prompt: number
    total: number
  }
  totals: {
    tokens: number
    estimatedCostUsd: number | null
  }
}
type ModelPricing = {
  inputUsdPer1K: number
  outputUsdPer1K: number
}
const MODEL_PRICING_USD_PER_1K: Record<string, ModelPricing> = {
  // OpenAI published list prices (approx): https://openai.com/pricing
  'gpt-4o-mini': { inputUsdPer1K: 0.00015, outputUsdPer1K: 0.0006 },
  'gpt-4o': { inputUsdPer1K: 0.005, outputUsdPer1K: 0.015 },
}
const roundUsd = (value: number): number => Math.round(value * 1_000_000) / 1_000_000
const estimateCostUsd = (
  model: string,
  inputTokens: number,
  outputTokens: number,
): number | null => {
  const pricing = MODEL_PRICING_USD_PER_1K[model]
  if (!pricing) {
    return null
  }
  const inputCost = (inputTokens / 1000) * pricing.inputUsdPer1K
  const outputCost = (outputTokens / 1000) * pricing.outputUsdPer1K
  return roundUsd(inputCost + outputCost)
}
const sumBy = <T>(items: readonly T[], selector: (item: T) => number): number =>
  items.reduce((total, item) => total + selector(item), 0)
export type TokenUsageStore = {
  startRun: (model: string) => string
  recordTelemetry: (runId: string, telemetry: TokenTelemetry) => void
  recordIteration: (runId: string, iteration: TokenUsageIteration) => void
  getRuns: () => readonly TokenUsageRun[]
  getLatestRun: () => TokenUsageRun | null
  getLatestBreakdown: () => TokenUsageBreakdown | null
  clear: () => void
}
export const createTokenUsageStore = (): TokenUsageStore => {
  let runs: TokenUsageRun[] = []
  let counter = 0
  const startRun = (model: string): string => {
    counter += 1
    const runId = `run-${counter}`
    const startedAt = new Date().toISOString()
    const run: TokenUsageRun = {
      id: runId,
      startedAt,
      model,
      telemetry: null,
      iterations: [],
    }
    runs = [run, ...runs].slice(0, 50)
    return runId
  }
  const updateRun = (runId: string, updater: (run: TokenUsageRun) => TokenUsageRun): void => {
    runs = runs.map((run) => (run.id === runId ? updater(run) : run))
  }
  const recordTelemetry = (runId: string, telemetry: TokenTelemetry): void => {
    updateRun(runId, (run) => ({ ...run, telemetry }))
  }
  const recordIteration = (runId: string, iteration: TokenUsageIteration): void => {
    updateRun(runId, (run) => {
      const withoutDuplicate = run.iterations.filter(
        (value) => value.iteration !== iteration.iteration,
      )
      const nextIterations = [...withoutDuplicate, iteration].sort(
        (a, b) => a.iteration - b.iteration,
      )
      return { ...run, iterations: nextIterations }
    })
  }
  const getRuns = (): readonly TokenUsageRun[] => runs
  const getLatestRun = (): TokenUsageRun | null => runs[0] ?? null
  const getLatestBreakdown = (): TokenUsageBreakdown | null => {
    const run = getLatestRun()
    if (!run?.telemetry) {
      return null
    }
    const intent = run.telemetry.intentTokens
    const files = run.telemetry.fileTokens
    const system = run.telemetry.systemTokens
    const inputTotal = intent + files + system
    const reasoning = sumBy(run.iterations, (entry) => entry.reasoningTokens)
    const prompt = sumBy(run.iterations, (entry) => entry.promptTokens)
    const outputTotal = reasoning + prompt
    const estimatedCostUsd = estimateCostUsd(run.model, inputTotal, outputTotal)
    return {
      input: { intent, files, system, total: inputTotal },
      output: { reasoning, prompt, total: outputTotal },
      totals: { tokens: inputTotal + outputTotal, estimatedCostUsd },
    }
  }
  const clear = (): void => {
    runs = []
    counter = 0
  }
  return {
    startRun,
    recordTelemetry,
    recordIteration,
    getRuns,
    getLatestRun,
    getLatestBreakdown,
    clear,
  }
}
</file>

<file path="src/tui/useLogBuffer.ts">
import { useCallback, useMemo, useRef, useState } from 'react'
export type LogEntry = {
  id: string
  level: 'info' | 'warn' | 'error'
  message: string
}
export const useLogBuffer = (initialCapacity = 20) => {
  const capacityRef = useRef(initialCapacity)
  const [logs, setLogs] = useState<LogEntry[]>([])
  const pushLog = useCallback((entry: LogEntry) => {
    setLogs((prev) => {
      const next = [...prev, entry]
      if (next.length > capacityRef.current) {
        return next.slice(next.length - capacityRef.current)
      }
      return next
    })
  }, [])
  const clearLogs = useCallback(() => {
    setLogs([])
  }, [])
  const logger = useMemo(
    () => ({
      info: (message: string) =>
        pushLog({ id: `${Date.now()}-${Math.random()}`, level: 'info', message }),
      warn: (message: string) =>
        pushLog({ id: `${Date.now()}-${Math.random()}`, level: 'warn', message }),
      error: (message: string) =>
        pushLog({ id: `${Date.now()}-${Math.random()}`, level: 'error', message }),
    }),
    [pushLog],
  )
  return { logs, log: logger, clearLogs }
}
</file>

<file path="src/types/jsdom.d.ts">
declare module 'jsdom' {
  export class JSDOM {
    constructor(html?: string, options?: Record<string, unknown>)
    window: Window & typeof globalThis
  }
}
</file>

<file path="src/utils/series-path.ts">
import { createHash } from 'node:crypto'
const padTwoDigits = (value: number): string => value.toString().padStart(2, '0')
export const formatSeriesTimestamp = (date: Date = new Date()): string => {
  const year = date.getFullYear().toString()
  const month = padTwoDigits(date.getMonth() + 1)
  const day = padTwoDigits(date.getDate())
  const hours = padTwoDigits(date.getHours())
  const minutes = padTwoDigits(date.getMinutes())
  const seconds = padTwoDigits(date.getSeconds())
  return `${year}${month}${day}-${hours}${minutes}${seconds}`
}
export const sanitizeForPathSegment = (
  value: string,
  fallback: string,
  maxLength?: number,
): string => {
  const normalized = value.trim().toLowerCase()
  const slug = normalized
    .replace(/[^a-z0-9]+/g, '-')
    .replace(/-+/g, '-')
    .replace(/^-+|-+$/g, '')
  const candidate = slug || fallback
  if (!maxLength || candidate.length <= maxLength) {
    return candidate
  }
  const truncated = candidate.slice(0, maxLength).replace(/-+$/g, '')
  return truncated || fallback
}
export const sanitizeForPathSegmentWithHash = (
  value: string,
  fallback: string,
  maxLength: number,
): string => {
  const candidate = sanitizeForPathSegment(value, fallback)
  if (candidate.length <= maxLength) {
    return candidate
  }
  const digest = createHash('sha1').update(value).digest('hex').slice(0, 8)
  const suffix = `-${digest}`
  const available = Math.max(0, maxLength - suffix.length)
  const prefix = candidate.slice(0, available).replace(/-+$/g, '')
  if (prefix.length === 0) {
    return fallback.slice(0, maxLength)
  }
  return `${prefix}${suffix}`
}
export const buildSeriesOutputDirName = (intent: string, date: Date = new Date()): string => {
  const timestamp = formatSeriesTimestamp(date)
  const intentSlug = sanitizeForPathSegmentWithHash(intent, 'intent', 80)
  return `${timestamp}-${intentSlug}`
}
</file>

<file path="src/file-context.ts">
import fs from 'node:fs/promises'
import fg from 'fast-glob'
export type FileContext = {
  path: string
  content: string
}
export const resolveFileContext = async (patterns: string[]): Promise<FileContext[]> => {
  if (patterns.length === 0) {
    return []
  }
  const entries = await fg(patterns, { dot: true })
  if (entries.length === 0) {
    console.warn(`Warning: No files matched the context patterns: ${patterns.join(', ')}`)
    return []
  }
  const results: FileContext[] = []
  for (const filePath of entries) {
    try {
      const content = await fs.readFile(filePath, 'utf8')
      results.push({ path: filePath, content })
    } catch {
      console.warn(`Warning: Failed to read context file ${filePath}`)
    }
  }
  return results
}
export const formatContextForPrompt = (files: FileContext[]): string => {
  if (files.length === 0) return ''
  return files.map((file) => `<file path="${file.path}">\n${file.content}\n</file>`).join('\n\n')
}
</file>

<file path="src/generate-command.ts">
export { runGenerateCommand } from './generate/command'
export { runGeneratePipeline } from './generate/pipeline'
export { maybeCopyToClipboard, maybeOpenChatGpt } from './generate/actions'
export { InteractiveTransport } from './generate/interactive-transport'
export type {
  ContextPathMetadata,
  GenerateArgs,
  GenerateJsonPayload,
  GeneratePipelineOptions,
  GeneratePipelineResult,
  InteractiveDelegate,
  StreamEventInput,
  StreamMode,
  TokenTelemetry,
} from './generate/types'
</file>

<file path="src/github-context.ts">
import type { FileContext } from './file-context'
export const MAX_GITHUB_FILE_BYTES = 64 * 1024
export const MAX_GITHUB_FILES = 60
const GITHUB_FETCH_TIMEOUT_MS = 20_000
const GITHUB_API_BASE = 'https://api.github.com'
const GITHUB_RAW_BASE = 'https://raw.githubusercontent.com'
const IGNORE_PATTERNS = [
  /^node_modules\//i,
  /^dist\//i,
  /^coverage\//i,
  /^\.git\//i,
  /^\.nx\//i,
  /package-lock\.json$/i,
  /pnpm-lock\.yaml$/i,
  /yarn\.lock$/i,
  /\.zip$/i,
  /\.tgz$/i,
]
export type ResolveGithubOptions = {
  onProgress?: (message: string) => void
}
type GithubBlobTarget = {
  kind: 'blob'
  owner: string
  repo: string
  ref: string
  path: string
}
type GithubTreeTarget = {
  kind: 'tree'
  owner: string
  repo: string
  ref: string
  path?: string
}
type GithubTarget = GithubBlobTarget | GithubTreeTarget
type GithubTreeResponse = {
  tree?: { path: string; type: string; size?: number }[]
}
type GithubContentResponse = {
  encoding?: string
  content?: string
}
export const resolveGithubUrl = async (
  url: URL,
  options?: ResolveGithubOptions,
): Promise<FileContext[]> => {
  const target = parseGithubUrl(url)
  if (!target) {
    console.warn(`Warning: Unsupported GitHub URL ${url.href}.`)
    return []
  }
  if (target.kind === 'blob') {
    options?.onProgress?.(`Downloading ${target.owner}/${target.repo}/${target.path}`)
    return fetchGithubBlob(target)
  }
  options?.onProgress?.(`Scanning ${target.owner}/${target.repo}`)
  return fetchGithubTree(target, options)
}
const fetchGithubBlob = async (target: GithubBlobTarget): Promise<FileContext[]> => {
  const rawPath = buildRawPath(target.path)
  const rawUrl = `${GITHUB_RAW_BASE}/${target.owner}/${target.repo}/${encodeURIComponent(target.ref)}/${rawPath}`
  let response: Response
  try {
    response = await fetchWithTimeout(rawUrl)
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown fetch error.'
    console.warn(`Warning: Failed to download ${target.path}: ${message}`)
    return []
  }
  if (!response.ok) {
    console.warn(`Warning: Failed to download ${target.path} (HTTP ${response.status}).`)
    return []
  }
  const buffer = Buffer.from(await response.arrayBuffer())
  if (!isWithinSize(buffer)) {
    console.warn(
      `Warning: ${target.path} is ${Math.round(buffer.byteLength / 1024)} KB (> ${
        MAX_GITHUB_FILE_BYTES / 1024
      } KB limit). Skipping.`,
    )
    return []
  }
  if (isBinary(buffer)) {
    console.warn(`Warning: ${target.path} appears to be binary. Skipping.`)
    return []
  }
  return [
    {
      path: `github:${target.owner}/${target.repo}/${target.path}`,
      content: buffer.toString('utf8'),
    },
  ]
}
const fetchGithubTree = async (
  target: GithubTreeTarget,
  options?: ResolveGithubOptions,
): Promise<FileContext[]> => {
  const treeUrl = `${GITHUB_API_BASE}/repos/${target.owner}/${target.repo}/git/trees/${encodeURIComponent(
    target.ref,
  )}?recursive=1`
  let response: Response
  try {
    response = await fetchWithTimeout(treeUrl, { headers: apiHeaders(true) })
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown GitHub error.'
    console.warn(`Warning: Failed to list ${target.owner}/${target.repo}: ${message}`)
    return []
  }
  if (!response.ok) {
    console.warn(`Warning: GitHub tree request failed with ${response.status}.`)
    return []
  }
  const payload = (await response.json()) as GithubTreeResponse
  const treeEntries = payload.tree ?? []
  const targetPath = target.path
  const pathPrefix =
    typeof targetPath === 'string' && targetPath.length > 0
      ? withTrailingSlash(targetPath)
      : undefined
  const filtered = treeEntries
    .filter((entry) => entry.type === 'blob')
    .filter((entry) => !pathPrefix || entry.path.startsWith(pathPrefix))
    .filter((entry) => !matchesIgnore(entry.path))
    .filter((entry) => typeof entry.size !== 'number' || entry.size <= MAX_GITHUB_FILE_BYTES)
    .slice(0, MAX_GITHUB_FILES)
  if (filtered.length === 0) {
    console.warn(`Warning: No eligible files found in ${target.owner}/${target.repo}.`)
    return []
  }
  const files: FileContext[] = []
  for (const entry of filtered) {
    options?.onProgress?.(`Fetching ${entry.path}`)
    const content = await fetchFileContent(target, entry.path, target.ref)
    if (!content) {
      continue
    }
    files.push({ path: `github:${target.owner}/${target.repo}/${entry.path}`, content })
    if (files.length >= MAX_GITHUB_FILES) {
      break
    }
  }
  return files
}
const fetchFileContent = async (
  target: GithubTreeTarget,
  path: string,
  ref: string,
): Promise<string | null> => {
  const encodedPath = encodeGithubPath(path)
  const contentsUrl = `${GITHUB_API_BASE}/repos/${target.owner}/${target.repo}/contents/${encodedPath}?ref=${encodeURIComponent(
    ref,
  )}`
  let response: Response
  try {
    response = await fetchWithTimeout(contentsUrl, { headers: apiHeaders() })
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown download error.'
    console.warn(`Warning: Failed to download ${path}: ${message}`)
    return null
  }
  if (!response.ok) {
    console.warn(`Warning: Failed to download ${path} (HTTP ${response.status}).`)
    return null
  }
  const json = (await response.json()) as GithubContentResponse
  if (json.encoding !== 'base64' || !json.content) {
    console.warn(`Warning: Unexpected content response for ${path}.`)
    return null
  }
  const buffer = Buffer.from(json.content, 'base64')
  if (!isWithinSize(buffer) || isBinary(buffer)) {
    return null
  }
  return buffer.toString('utf8')
}
const fetchWithTimeout = async (url: string, init?: RequestInit): Promise<Response> => {
  const controller = new AbortController()
  const timeout = setTimeout(() => controller.abort(), GITHUB_FETCH_TIMEOUT_MS)
  try {
    return await fetch(url, {
      ...init,
      signal: controller.signal,
      headers: {
        'User-Agent': 'prompt-maker-cli',
        ...(init?.headers ?? {}),
      },
    })
  } finally {
    clearTimeout(timeout)
  }
}
const parseGithubUrl = (url: URL): GithubTarget | null => {
  const segments = url.pathname.split('/').filter(Boolean)
  if (segments.length < 2) {
    return null
  }
  const owner = segments[0]
  const rawRepo = segments[1]
  if (!owner || !rawRepo) {
    return null
  }
  const type = segments[2]
  const ref = segments[3]
  const rest = segments.slice(4)
  const repo = sanitizeRepo(rawRepo)
  if (!type) {
    return { kind: 'tree', owner, repo, ref: 'HEAD' }
  }
  if (type === 'blob') {
    if (!ref || rest.length === 0) {
      return null
    }
    return { kind: 'blob', owner, repo, ref, path: rest.join('/') }
  }
  if (type === 'tree') {
    if (!ref) {
      return null
    }
    const joined = rest.join('/')
    if (joined) {
      return { kind: 'tree', owner, repo, ref, path: joined }
    }
    return { kind: 'tree', owner, repo, ref }
  }
  if (!ref) {
    return { kind: 'tree', owner, repo, ref: 'HEAD' }
  }
  return null
}
const matchesIgnore = (path: string): boolean =>
  IGNORE_PATTERNS.some((pattern) => pattern.test(path))
const withTrailingSlash = (value: string): string => (value.endsWith('/') ? value : `${value}/`)
const isWithinSize = (buffer: Buffer): boolean => buffer.byteLength <= MAX_GITHUB_FILE_BYTES
const isBinary = (buffer: Buffer): boolean => {
  if (buffer.includes(0)) {
    return true
  }
  let printable = 0
  for (const byte of buffer) {
    if (byte === 9 || byte === 10 || byte === 13 || (byte >= 32 && byte <= 126)) {
      printable += 1
    }
  }
  return printable / buffer.length < 0.8
}
const buildRawPath = (path: string): string => path.split('/').map(encodeURIComponent).join('/')
const encodeGithubPath = (path: string): string => path.split('/').map(encodeURIComponent).join('/')
const apiHeaders = (json?: boolean): HeadersInit => {
  const headers: Record<string, string> = {
    'User-Agent': 'prompt-maker-cli',
  }
  if (json) {
    headers.accept = 'application/vnd.github+json'
  }
  const token = process.env.GITHUB_TOKEN?.trim()
  if (token) {
    headers.authorization = `Bearer ${token}`
  }
  return headers
}
const sanitizeRepo = (repo: string): string => (repo.endsWith('.git') ? repo.slice(0, -4) : repo)
</file>

<file path="src/image-loader.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import { type ImagePart } from '@prompt-maker/core'
const MAX_IMAGE_SIZE_MB = 20
const SUPPORTED_EXTS = ['.png', '.jpg', '.jpeg', '.webp', '.gif']
export const resolveImageParts = async (
  filePaths: string[],
  onUploadStateChange?: (
    state: 'start' | 'finish',
    detail: { kind: 'image'; filePath: string },
  ) => void,
): Promise<ImagePart[]> => {
  const parts: ImagePart[] = []
  for (const filePath of filePaths) {
    const ext = path.extname(filePath).toLowerCase()
    if (!SUPPORTED_EXTS.includes(ext)) {
      console.warn(`Skipping unsupported image type: ${filePath}`)
      continue
    }
    onUploadStateChange?.('start', { kind: 'image', filePath })
    try {
      const buffer = await fs.readFile(filePath)
      const sizeMb = buffer.length / (1024 * 1024)
      if (sizeMb > MAX_IMAGE_SIZE_MB) {
        console.warn(`Skipping image too large (${sizeMb.toFixed(1)}MB): ${filePath}`)
        continue
      }
      const mimeType =
        ext === '.png'
          ? 'image/png'
          : ext === '.webp'
            ? 'image/webp'
            : ext === '.gif'
              ? 'image/gif'
              : 'image/jpeg'
      parts.push({
        type: 'image',
        mimeType,
        data: buffer.toString('base64'),
      })
    } catch (error) {
      console.warn(`Failed to read image ${filePath}:`, error)
    } finally {
      onUploadStateChange?.('finish', { kind: 'image', filePath })
    }
  }
  return parts
}
</file>

<file path="src/io.ts">
export const readFromStdin = async (): Promise<string | null> => {
  if (process.stdin.isTTY) {
    return null
  }
  const chunks: Buffer[] = []
  return await new Promise<string>((resolve, reject) => {
    process.stdin.on('data', (chunk: Buffer | string) => {
      chunks.push(typeof chunk === 'string' ? Buffer.from(chunk, 'utf8') : chunk)
    })
    process.stdin.on('error', reject)
    process.stdin.on('end', () => {
      resolve(Buffer.concat(chunks).toString('utf8'))
    })
  })
}
</file>

<file path="src/model-providers.ts">
export type ModelProvider = 'openai' | 'gemini' | 'other'
export type ModelDefinition = {
  id: string
  label?: string
  provider?: ModelProvider
  description?: string
  capabilities?: string[]
  notes?: string
  default?: boolean
}
export const MODEL_PROVIDER_LABELS: Record<ModelProvider, string> = {
  openai: 'OpenAI',
  gemini: 'Gemini',
  other: 'Custom',
}
const GEMINI_MODEL_PREFIXES = ['gemini', 'gemma']
const OPENAI_MODEL_PREFIXES = ['gpt', 'o', 'chatgpt', 'text-davinci']
const normalizeModelId = (modelId: string): string => modelId.trim().toLowerCase()
export const isGeminiModelId = (modelId: string): boolean => {
  const normalized = normalizeModelId(modelId)
  return GEMINI_MODEL_PREFIXES.some((prefix) => normalized.startsWith(prefix))
}
const isOpenAiModelId = (modelId: string): boolean => {
  const normalized = normalizeModelId(modelId)
  return OPENAI_MODEL_PREFIXES.some((prefix) => normalized.startsWith(prefix))
}
export const inferProviderFromModelId = (modelId: string): ModelProvider => {
  if (!modelId.trim()) {
    return 'other'
  }
  if (isGeminiModelId(modelId)) {
    return 'gemini'
  }
  if (isOpenAiModelId(modelId)) {
    return 'openai'
  }
  return 'other'
}
</file>

<file path="src/smart-context-service.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import fg from 'fast-glob'
import type { FileContext } from './file-context'
import * as vectorStore from './rag/vector-store'
type ProgressCallback = (message: string) => void
const SMART_CONTEXT_PATTERNS = ['**/*.{ts,tsx,js,jsx,py,md,json}']
const SMART_CONTEXT_IGNORE_PATTERNS = [
  '**/node_modules/**',
  '**/dist/**',
  '**/coverage/**',
  '**/.git/**',
  '**/.nx/**',
  '**/.next/**',
  '**/package-lock.json',
  '**/pnpm-lock.yaml',
  '**/yarn.lock',
]
const MAX_EMBEDDING_FILE_SIZE = 25 * 1024
export const resolveSmartContextFiles = async (
  intent: string,
  currentContext: FileContext[],
  onProgress?: ProgressCallback,
  rootDirectory?: string,
): Promise<FileContext[]> => {
  const baseDir = rootDirectory ? path.resolve(rootDirectory) : process.cwd()
  onProgress?.('Scanning workspace for smart context files')
  let filesToIndex: string[] = []
  try {
    filesToIndex = await fg(SMART_CONTEXT_PATTERNS, {
      dot: true,
      absolute: true,
      cwd: baseDir,
      ignore: SMART_CONTEXT_IGNORE_PATTERNS,
    })
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown smart context glob error.'
    console.warn(`Smart context scan failed: ${message}`)
    onProgress?.('Smart context scan failed')
    return []
  }
  if (filesToIndex.length === 0) {
    onProgress?.('No smart context files found')
    return []
  }
  const uniqueFiles = [...new Set(filesToIndex.map((filePath) => path.resolve(filePath)))]
  const validFiles: string[] = []
  for (const file of uniqueFiles) {
    try {
      const stats = await fs.stat(file)
      if (stats.size < MAX_EMBEDDING_FILE_SIZE) {
        validFiles.push(file)
      }
    } catch {
      // Ignore files that cannot be read
    }
  }
  if (validFiles.length === 0) {
    onProgress?.('No smart context files within size limit')
    return []
  }
  onProgress?.('Indexing smart context')
  try {
    await vectorStore.indexFiles(validFiles)
    onProgress?.('Indexed smart context ✓')
  } catch (error) {
    onProgress?.('Failed to index smart context')
    const message = error instanceof Error ? error.message : 'Unknown smart context error.'
    console.warn(`Smart context indexing failed: ${message}`)
    return []
  }
  onProgress?.('Searching smart context')
  let relatedPaths: string[] = []
  try {
    relatedPaths = await vectorStore.search(intent, 5, validFiles)
  } catch (error) {
    onProgress?.('Failed to search smart context')
    const message = error instanceof Error ? error.message : 'Unknown smart context search error.'
    console.warn(`Smart context search failed: ${message}`)
    return []
  }
  const availableSet = new Set(validFiles.map((filePath) => normalizePath(filePath)))
  const filtered = relatedPaths
    .map((filePath) => normalizePath(filePath))
    .filter((filePath) => availableSet.has(filePath))
  if (filtered.length === 0) {
    onProgress?.('No related smart context files found')
    return []
  }
  const results = await readSmartContextFiles(filtered, currentContext)
  if (results.length > 0) {
    onProgress?.('Smart context ready')
  } else {
    onProgress?.('No smart context files added')
  }
  return results
}
const readSmartContextFiles = async (
  candidatePaths: string[],
  currentContext: FileContext[],
): Promise<FileContext[]> => {
  const existingPaths = new Set(currentContext.map((file) => normalizePath(file.path)))
  const results: FileContext[] = []
  for (const filePath of candidatePaths) {
    if (existingPaths.has(filePath)) {
      continue
    }
    try {
      const content = await fs.readFile(filePath, 'utf8')
      results.push({ path: toDisplayPath(filePath), content })
      existingPaths.add(filePath)
    } catch (error) {
      const message = error instanceof Error ? error.message : 'Unknown file read error.'
      console.warn(`Warning: Failed to read smart context file ${filePath}: ${message}`)
    }
  }
  return results
}
const normalizePath = (filePath: string): string => path.resolve(filePath)
const toDisplayPath = (absolutePath: string): string => {
  const cwd = process.cwd()
  const relative = path.relative(cwd, absolutePath)
  if (!relative || relative.startsWith('..')) {
    return absolutePath
  }
  return relative
}
</file>

<file path="src/test-command.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import readline from 'node:readline'
import yaml from 'js-yaml'
import yargs from 'yargs'
import type { ArgumentsCamelCase } from 'yargs'
import { resolveFileContext, type FileContext } from './file-context'
import {
  createPromptGeneratorService,
  resolveDefaultGenerateModel,
  type PromptGenerationRequest,
} from './prompt-generator-service'
import { resolveSmartContextFiles } from './smart-context-service'
import { parsePromptTestSuite, type PromptTestSuite, type PromptTest } from './testing/test-schema'
import { evaluatePrompt } from './testing/evaluator'
const DEFAULT_TEST_FILE = 'prompt-tests.yaml'
const PROGRESS_BAR_WIDTH = 24
type TestArgs = {
  file: string
}
export type TestResult = {
  name: string
  pass: boolean
  reason: string
}
type TestProgressReporter = {
  startTest: (ordinal: number, testName: string) => void
  completeTest: () => void
  completeAll: () => void
}
export type PromptTestRunReporter = {
  onSuiteLoaded?: (suite: PromptTestSuite, filePath: string) => void
  onTestStart?: (ordinal: number, test: PromptTest) => void
  onTestComplete?: (ordinal: number, result: TestResult) => void
  onComplete?: (results: TestResult[]) => void
}
export type PromptTestRunOptions = {
  reporter?: PromptTestRunReporter
}
export const runPromptTestSuite = async (
  filePath: string,
  options: PromptTestRunOptions = {},
): Promise<TestResult[]> => {
  const absolutePath = path.resolve(process.cwd(), filePath)
  const suite = await loadTestSuite(absolutePath)
  options.reporter?.onSuiteLoaded?.(suite, absolutePath)
  const results = await executePromptTests(suite, options.reporter)
  options.reporter?.onComplete?.(results)
  return results
}
export const runTestCommand = async (argv: string[]): Promise<void> => {
  const { file } = parseTestArgs(argv)
  const filePath = path.resolve(process.cwd(), file)
  let progressReporter: TestProgressReporter | null = null
  const reporter: PromptTestRunReporter = {
    onSuiteLoaded: (suite, loadedPath) => {
      console.log(`Loaded ${suite.tests.length} test(s) from ${formatDisplayPath(loadedPath)}.`)
      progressReporter = createTestProgressReporter(suite.tests.length)
    },
    onTestStart: (ordinal, test) => {
      progressReporter?.startTest(ordinal, test.name)
    },
    onTestComplete: () => {
      progressReporter?.completeTest()
    },
    onComplete: () => {
      progressReporter?.completeAll()
    },
  }
  const results = await runPromptTestSuite(filePath, { reporter })
  console.log('\nTest Results')
  console.log('────────────')
  for (const result of results) {
    const status = result.pass ? 'PASS' : 'FAIL'
    console.log(`${status.padEnd(4)}  ${result.name} - ${result.reason}`)
  }
  const failures = results.filter((result) => !result.pass)
  if (failures.length > 0) {
    console.log(`\n${failures.length} test(s) failed.`)
    process.exitCode = 1
  } else {
    console.log('\nAll tests passed!')
  }
}
const parseTestArgs = (argv: string[]): TestArgs => {
  const parser = yargs(argv)
    .scriptName('prompt-maker-cli test')
    .usage('prompt-maker-cli test [file]')
    .command('$0 [file]', 'Run prompt quality tests', (cmd) =>
      cmd.positional('file', {
        type: 'string',
        describe: 'Path to a prompt test definition file (YAML)',
        default: DEFAULT_TEST_FILE,
      }),
    )
    .help('help')
    .alias('help', 'h')
    .exitProcess(false)
    .showHelpOnFail(false)
    .parserConfiguration({ 'halt-at-non-option': true })
    .strict(false)
  const parsed = parser.parseSync() as ArgumentsCamelCase<{ file?: string }>
  const file =
    typeof parsed.file === 'string' && parsed.file.trim().length > 0
      ? parsed.file
      : DEFAULT_TEST_FILE
  return { file }
}
const executePromptTests = async (
  suite: PromptTestSuite,
  reporter?: PromptTestRunReporter,
): Promise<TestResult[]> => {
  const service = await createPromptGeneratorService()
  const defaultModel = await resolveDefaultGenerateModel()
  const results: TestResult[] = []
  for (const [index, test] of suite.tests.entries()) {
    const ordinal = index + 1
    reporter?.onTestStart?.(ordinal, test)
    const result = await runSingleTest({ test, service, model: defaultModel })
    results.push(result)
    reporter?.onTestComplete?.(ordinal, result)
  }
  return results
}
const runSingleTest = async ({
  test,
  service,
  model,
}: {
  test: PromptTest
  service: Awaited<ReturnType<typeof createPromptGeneratorService>>
  model: string
}): Promise<TestResult> => {
  try {
    let fileContext = await resolveContextFiles(test.context)
    if (test.smartContext) {
      const smartFiles = await resolveSmartContextFiles(
        test.intent,
        fileContext,
        () => undefined,
        test.smartContextRoot,
      )
      if (smartFiles.length > 0) {
        fileContext = [...fileContext, ...smartFiles]
      }
    }
    const promptRequest: PromptGenerationRequest = {
      intent: test.intent,
      model,
      targetModel: model,
      fileContext,
      images: test.image ?? [],
      videos: test.video ?? [],
    }
    const generatedPrompt = await service.generatePrompt(promptRequest)
    const verdict = await evaluatePrompt(generatedPrompt, test.expect)
    return {
      name: test.name,
      pass: verdict.pass,
      reason: verdict.reason,
    }
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown test error.'
    return {
      name: test.name,
      pass: false,
      reason: message,
    }
  }
}
const resolveContextFiles = async (patterns: string[]): Promise<FileContext[]> => {
  if (!patterns || patterns.length === 0) {
    return []
  }
  return await resolveFileContext(patterns)
}
const loadTestSuite = async (filePath: string): Promise<PromptTestSuite> => {
  let raw: string
  try {
    raw = await fs.readFile(filePath, 'utf8')
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown file error.'
    throw new Error(`Failed to read test file ${formatDisplayPath(filePath)}: ${message}`)
  }
  let parsedYaml: unknown
  try {
    parsedYaml = yaml.load(raw) ?? {}
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown YAML error.'
    throw new Error(`Failed to parse YAML in ${formatDisplayPath(filePath)}: ${message}`)
  }
  try {
    return parsePromptTestSuite(parsedYaml)
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown schema error.'
    throw new Error(`Test file ${formatDisplayPath(filePath)} is invalid: ${message}`)
  }
}
const createTestProgressReporter = (total: number): TestProgressReporter => {
  if (total <= 0) {
    return {
      startTest: () => undefined,
      completeTest: () => undefined,
      completeAll: () => undefined,
    }
  }
  if (!process.stdout.isTTY) {
    let completed = 0
    return {
      startTest(ordinal, testName) {
        console.log(`Running test ${ordinal}/${total}: ${testName}`)
      },
      completeTest() {
        completed = Math.min(completed + 1, total)
        console.log(`Progress ${completed}/${total}`)
      },
      completeAll() {
        console.log('All tests complete.')
      },
    }
  }
  return createTtyProgressReporter(total)
}
const createTtyProgressReporter = (total: number): TestProgressReporter => {
  let completed = 0
  let currentLabel = ''
  let hasRendered = false
  const render = (): void => {
    if (!hasRendered) {
      process.stdout.write('\n')
      hasRendered = true
    }
    const ratio = total === 0 ? 1 : completed / total
    const filledUnits = Math.min(PROGRESS_BAR_WIDTH, Math.round(ratio * PROGRESS_BAR_WIDTH))
    const emptyUnits = PROGRESS_BAR_WIDTH - filledUnits
    const bar = `${'█'.repeat(filledUnits)}${'░'.repeat(emptyUnits)}`
    const line = `[${bar}] ${completed}/${total} ${currentLabel}`
    readline.clearLine(process.stdout, 0)
    readline.cursorTo(process.stdout, 0)
    process.stdout.write(line)
  }
  return {
    startTest(ordinal, testName) {
      currentLabel = `Running test ${ordinal}/${total}: ${testName}`
      render()
    },
    completeTest() {
      completed = Math.min(completed + 1, total)
      render()
    },
    completeAll() {
      currentLabel = 'All tests complete'
      render()
      process.stdout.write('\n')
    },
  }
}
const formatDisplayPath = (absolutePath: string): string => {
  const relative = path.relative(process.cwd(), absolutePath)
  return relative && !relative.startsWith('..') ? relative : absolutePath
}
</file>

<file path="src/token-counter.ts">
import { getEncoding, type Tiktoken } from 'js-tiktoken'
let encoder: Tiktoken | null = null
const getEncoder = (): Tiktoken => {
  if (!encoder) {
    encoder = getEncoding('cl100k_base')
  }
  return encoder
}
export const countTokens = (text: string): number => {
  if (!text) return 0
  try {
    return getEncoder().encode(text).length
  } catch {
    console.warn('Token counting failed, defaulting to character heuristic.')
    return Math.ceil(text.length / 4)
  }
}
export const formatTokenCount = (count: number): string => {
  const formatted = new Intl.NumberFormat().format(count)
  if (count > 100000) return `⚠️ ${formatted} tokens (High)`
  if (count > 30000) return `${formatted} tokens (Medium)`
  return `${formatted} tokens`
}
</file>

<file path="src/url-context.ts">
import { htmlToText } from 'html-to-text'
import type { FileContext } from './file-context'
import { resolveGithubUrl } from './github-context'
export const MAX_HTML_BYTES = 1 * 1024 * 1024
const URL_FETCH_TIMEOUT_MS = 15_000
export type ResolveUrlContextOptions = {
  onProgress?: (message: string) => void
}
export const resolveUrlContext = async (
  urls: string[],
  options?: ResolveUrlContextOptions,
): Promise<FileContext[]> => {
  if (urls.length === 0) {
    return []
  }
  const entries: FileContext[] = []
  const seen = new Set<string>()
  for (const raw of urls) {
    const trimmed = raw?.trim()
    if (!trimmed || seen.has(trimmed)) {
      continue
    }
    seen.add(trimmed)
    let parsed: URL
    try {
      parsed = new URL(trimmed)
    } catch {
      console.warn(`Warning: Skipping invalid URL "${trimmed}".`)
      continue
    }
    if (!/^https?:$/.test(parsed.protocol)) {
      console.warn(
        `Warning: Unsupported protocol for ${parsed.href}. Only HTTP(S) URLs are allowed.`,
      )
      continue
    }
    if (isGithubHost(parsed.host)) {
      try {
        const githubFiles = await resolveGithubUrl(parsed, options)
        if (githubFiles.length === 0) {
          continue
        }
        entries.push(...githubFiles)
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown GitHub error.'
        console.warn(`Warning: Failed to fetch GitHub URL ${parsed.href}: ${message}`)
      }
      continue
    }
    options?.onProgress?.(`Downloading ${parsed.href}`)
    const file = await fetchGenericUrl(parsed)
    if (file) {
      entries.push(file)
    }
  }
  return entries
}
const fetchGenericUrl = async (url: URL): Promise<FileContext | null> => {
  try {
    const response = await fetchWithTimeout(url.href)
    if (!response.ok) {
      console.warn(`Warning: ${url.href} responded with ${response.status}. Skipping.`)
      return null
    }
    const contentLengthHeader = response.headers.get('content-length')
    if (contentLengthHeader) {
      const declaredLength = Number(contentLengthHeader)
      if (!Number.isNaN(declaredLength) && declaredLength > MAX_HTML_BYTES) {
        console.warn(
          `Warning: ${url.href} is ${Math.round(declaredLength / 1024)} KB which exceeds the ${
            MAX_HTML_BYTES / 1024
          } KB limit. Skipping.`,
        )
        return null
      }
    }
    const buffer = Buffer.from(await response.arrayBuffer())
    if (buffer.byteLength > MAX_HTML_BYTES) {
      console.warn(
        `Warning: ${url.href} exceeded the ${MAX_HTML_BYTES / 1024} KB limit while downloading. Skipping.`,
      )
      return null
    }
    const html = buffer.toString('utf8')
    const text = extractText(html)
    if (!text) {
      console.warn(`Warning: ${url.href} did not contain readable text.`)
      return null
    }
    return {
      path: `url:${url.href}`,
      content: text,
    }
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown fetch error.'
    console.warn(`Warning: Failed to download ${url.href}: ${message}`)
    return null
  }
}
const extractText = (html: string): string => {
  const text = htmlToText(html, {
    wordwrap: false,
    preserveNewlines: true,
    selectors: [
      { selector: 'script', format: 'skip' },
      { selector: 'style', format: 'skip' },
      { selector: 'noscript', format: 'inline' },
    ],
  })
    .replace(/\r/g, '')
    .replace(/\n{3,}/g, '\n\n')
    .trim()
  return text
}
const fetchWithTimeout = async (url: string): Promise<Response> => {
  const controller = new AbortController()
  const timeout = setTimeout(() => controller.abort(), URL_FETCH_TIMEOUT_MS)
  try {
    return await fetch(url, {
      signal: controller.signal,
      headers: {
        'User-Agent': 'prompt-maker-cli',
      },
    })
  } finally {
    clearTimeout(timeout)
  }
}
const isGithubHost = (host: string): boolean => {
  const normalized = host.toLowerCase()
  return normalized === 'github.com' || normalized === 'www.github.com'
}
</file>

<file path="tests/mocks/boxen.ts">
type BoxenOptions = {
  borderColor?: string
  borderStyle?: string
  padding?: number | { top?: number; bottom?: number; left?: number; right?: number }
  title?: string
  titleAlignment?: 'left' | 'center' | 'right'
}
const boxen = (content: string, _options?: BoxenOptions): string => content
export default boxen
</file>

<file path="tests/mocks/chalk.ts">
interface ChalkPalette {
  [key: string]: ChalkFn
}
type ChalkFn = ((value?: unknown) => string) & ChalkPalette
const toStringValue = (value?: unknown): string => {
  if (typeof value === 'string') {
    return value
  }
  if (value === undefined || value === null) {
    return ''
  }
  return String(value)
}
const createStyle = (): ChalkFn => {
  const style = ((value?: unknown) => toStringValue(value)) as ChalkFn
  return style
}
const attach = (target: ChalkFn, name: string): ChalkFn => {
  const style = createStyle()
  target[name] = style
  return style
}
const chalkMock = createStyle()
const rootStyles = ['dim', 'gray', 'white', 'green', 'magenta', 'cyan', 'yellow']
rootStyles.forEach((style) => {
  attach(chalkMock, style)
})
const bold = attach(chalkMock, 'bold')
;['green', 'magenta', 'cyan'].forEach((style) => {
  attach(bold, style)
})
export default chalkMock
</file>

<file path="tests/mocks/ora.ts">
type Spinner = {
  text: string
  succeed: (message?: string) => void
  fail: (message?: string) => void
  stop: () => void
}
type OraOptions = {
  text?: string
  color?: string
  spinner?: string
}
const createSpinner = (text: string): Spinner => {
  let currentText = text
  return {
    get text() {
      return currentText
    },
    set text(value: string) {
      currentText = value
    },
    succeed: () => undefined,
    fail: () => undefined,
    stop: () => undefined,
  }
}
const ora = (options?: OraOptions) => {
  const spinner = createSpinner(options?.text ?? '')
  return {
    start: () => spinner,
  }
}
export default ora
</file>

<file path=".eslintrc.json">
{
  "root": true,
  "ignorePatterns": ["dist/**"],
  "env": {
    "es2022": true,
    "node": true
  },
  "parser": "@typescript-eslint/parser",
  "parserOptions": {
    "ecmaVersion": "latest",
    "sourceType": "module"
  },
  "plugins": ["@typescript-eslint"],
  "extends": ["eslint:recommended", "plugin:@typescript-eslint/recommended"]
}
</file>

<file path=".gitignore">
# Dependencies
node_modules/
.pnp*

# Build output
.next/
out/
dist/
build/

# Caches
.cache/
.turbo/

# Testing
coverage/

# Environment variables
.env
.env.*

# Logs
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
*.log

# Editors
.vscode/
.idea/

# Misc
.DS_Store
</file>

<file path=".nvmrc">
22.15
</file>

<file path=".prettierignore">
# Intentionally-invalid fixtures
src/__tests__/__fixtures__/themes/global/invalid.json
</file>

<file path=".prettierrc">
{
  "printWidth": 100,
  "singleQuote": true,
  "trailingComma": "all",
  "semi": false
}
</file>

<file path="AGENTS.md">
# AGENTS.md — prompt-maker

Instructions for agentic coding tools (opencode/Codex, Cursor, Copilot Chat, etc.).
Goal: small, focused changes that match existing patterns and keep tests green.

## Quick Facts

- Package manager: `npm` (`package-lock.json`)
- Node: `>=18` (see `.nvmrc` — `22.15` works)
- Modules: ESM (`"type": "module"`); build output is ESM in `dist/`
- TypeScript: `strict` + `noUncheckedIndexedAccess` + `exactOptionalPropertyTypes`
- UI: Ink + React under `src/tui/`
- Tests: Jest + ts-jest (`jest.config.cjs`, `tsconfig.jest.json`)

## Commands

- Install deps: `npm ci` (preferred)
- Update deps/lockfile: `npm install`
- Build: `npm run build` (esbuild via `scripts/build.mjs`)
  - Output: `dist/index.js` + `dist/meta.json`
  - Note: build aliases `@prompt-maker/core/*` to `packages/core/src/*`
- Build (watch): `node scripts/build.mjs --watch`
- Build (minify): `node scripts/build.mjs --minify`
- Run (compiled): `npm start`
- Run (dev, watch + restart): `npm run dev -- <cli args>` (ex: `npm run dev -- --help`)
- Typecheck: `npm run typecheck`
- Format (write): `npm run format`
- Format (check, optional): `npx prettier -c .`

### Lint

- No `lint` script is configured.
- `.eslintrc.json` extends `next/core-web-vitals` but `eslint/next` deps are not present; treat as inactive.

### Tests (Jest)

- All tests: `npm test`
- Watch: `npm test -- --watch`
- Single file: `npm test -- src/__tests__/config.test.ts`
- File outside `testMatch`: `npm test -- --runTestsByPath packages/core/src/__tests__/llm.test.ts`
- Single test by name: `npm test -- -t "ThemeResolver"`
- Debug: `npm test -- --runInBand`, `npm test -- --detectOpenHandles`, `npm test -- --listTests`

## Repo Structure

- CLI entry: `src/index.ts` (routes to `generate`, `test`, or TUI)
- Core package: `packages/core/src/` (imported via `@prompt-maker/core`)
- TUI: `src/tui/` (Ink + React)
  - Reducers: `*-reducer.ts` (pure, no React/Ink imports)
  - Hooks: `src/tui/hooks/` (effects, timers, async, IO)
  - Components: `src/tui/components/` (mostly presentational)
- Architecture note: `src/tui/DEVELOPER_NOTE.md` (some paths are historical)

### TUI input invariants

Input routing is easy to regress; keep this priority order:

1. Help overlay (when open, suppress screen input)
2. Popup input (popups “own” the keyboard)
3. Screen input
4. AppContainer global keys (exit, etc.)
   Avoid “fallthrough” where a single key is handled by both popup and screen.

### TUI change checklist

- Keep reducers pure (`*-reducer.ts`) and add/adjust reducer tests in `src/__tests__/tui/`.
- If adding a popup: update popup types/state machine and ensure popups “own” input.
- If adding async suggestion scans: guard against stale updates (scan id + popup type checks).

### Strict TS gotchas

- Don’t assume `array[index]` or `map.get(key)` is defined; handle `undefined`.
- With `exactOptionalPropertyTypes`, prefer omitting optional fields (vs setting them to `undefined`).
- Prefer `satisfies`/`as const` when you need literal unions.
- When narrowing in `catch`, always handle non-`Error` values.

## Coding Conventions

### TypeScript / Types

- Do not use `any`.
- Prefer `unknown` + type guards (or Zod) for external data.
- Prefer functional composition over OO; keep functions small and pure.
- Prefer `type` aliases; use `interface` only for declaration merging/extensibility.
- With `noUncheckedIndexedAccess`, assume indexing can yield `undefined`.
- With `exactOptionalPropertyTypes`, distinguish “missing” vs “present but undefined”.
- For recoverable failures, prefer typed results: `{ ok: true, value } | { ok: false, error }`.

### Imports

- Order: Node built-ins (`node:`) → blank line → third-party → blank line → local.
- Use `import type { ... }` for type-only imports.
- Prefer named exports/imports; avoid introducing new default exports.

### Naming

- Files: `kebab-case.ts` / `kebab-case.tsx`
- Values/functions: `camelCase`
- Types/components: `PascalCase`
- Constants: `SCREAMING_SNAKE_CASE`
- Booleans: `isX` / `hasX` / `shouldX`

### Formatting

- Prettier is the source of truth (`npm run format`).
- Keep changes consistent with surrounding code; prefer early returns.

### Error Handling

- Programmer/invariant errors: `throw new Error('...')`.
- User/config/IO errors: return a typed error OR throw with actionable context.
- When wrapping errors, preserve `error.message` and include paths/flags.
- In `catch (error)`, narrow with `error instanceof Error`.

### Validation

- Use runtime validation for JSON/YAML/API responses.
- Zod is already used (see `src/testing/test-schema.ts`); small type guards are also common.

### React/Ink Patterns

- Keep reducers pure and unit-testable; put effects in hooks.
- Prefer stable callbacks; avoid re-render churn from new objects/arrays.
- For stale closures, prefer refs + stable callbacks.

## Testing Conventions

- Jest `testMatch`: `src/**/__tests__/**/*.test.ts?(x)`.
- Prefer deterministic tests (fake timers/mocks) where possible.
- When mocking modules, follow `jest.config.cjs` `moduleNameMapper` patterns (see `tests/mocks/`).

## Environment Variables

- `OPENAI_API_KEY`
- `GEMINI_API_KEY`

## Build Artifacts

- Do not edit `dist/` by hand; source of truth is `src/` and `packages/`.

## Cursor / Copilot Rules

- No `.cursor/rules/`, `.cursorrules`, or `.github/copilot-instructions.md` found in this repo.

## Hygiene (Agents)

- Keep PRs focused; avoid drive-by refactors.
- Add/adjust tests only when behavior changes.
- Before finalizing: run `npm run typecheck` and `npm test`.
</file>

<file path="jest.config.cjs">
/** @type {import('jest').Config} */
module.exports = {
  testMatch: ['<rootDir>/src/**/__tests__/**/*.test.ts?(x)'],
  testEnvironment: 'node',
  transform: {
    '^.+\\.(ts|tsx)$': [
      'ts-jest',
      {
        tsconfig: '<rootDir>/tsconfig.jest.json',
      },
    ],
  },
  moduleNameMapper: {
    '^@prompt-maker/core$': '<rootDir>/packages/core/src/index.ts',
    '^@prompt-maker/core/(.*)$': '<rootDir>/packages/core/src/$1',
    '^boxen$': '<rootDir>/tests/mocks/boxen.ts',
    '^chalk$': '<rootDir>/tests/mocks/chalk.ts',
    '^ora$': '<rootDir>/tests/mocks/ora.ts',
    '^yargs$': '<rootDir>/tests/mocks/yargs.ts',
  },
}
</file>

<file path="prompt-tests.yaml">
tests:
  - name: 'RAG Feature Check'
    intent: 'Explain the vector store implementation'
    # We force the context here to ensure the GENERATION has the right info
    context: ['apps/prompt-maker-cli/src/rag/vector-store.ts']
    expect:
      - 'Mention cosine similarity'
      - 'Mention file hashing'
      - 'Explain how indexFiles works'
  - name: 'Simple Greeting'
    intent: 'Write a friendly hello world python script'
    expect:
      - "Must contain print('Hello, World!')"
      - 'Must be in Python'
  - name: 'URL Context Summary'
    intent: 'Summarize the Example Domain landing page'
    url:
      - 'https://example.com'
    expect:
      - 'Must mention Example Domain'
      - 'Must note the illustrative examples link section'
  - name: 'Video Bug Analysis'
    intent: 'Analyze this video to find the UI glitch'
    video: ['test-assets/demo.mp4']
    expect:
      - 'Must mention the flickering button'
      - 'Must suggest a CSS fix'
  - name: 'Smart Context Refactor'
    intent: 'Refactor the vector store class to use a singleton pattern'
    smartContext: true
    expect:
      - 'Must find vector-store.ts automatically'
      - 'Must provide valid TypeScript code'
</file>

<file path="repomix-output.xml">
This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.
<file_summary>
This section contains a summary of this file.
<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>
<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>
<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>
<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>
</file_summary>
<directory_structure>
docs/
  context-templates.md
  cookbook.md
  fzf-commands.txt
  neovim-plugin-integration.md
  pmc-tutorial.md
  prompt-maker-cli-tui-encyclopedia.md
  sidekick.nvim-prompt-maker-jsonl-fixtures.md
  sidekick.nvim-prompt-maker-tui-integration.md
  tui-design.md
  tui-styling-guide.md
  tui-updates.md
exports/
  cli.json
  feeset-changes-prompt-export.yaml
  gemini-pdf-prompt-export.yaml
  loantek-publication-downloader-prompt-export.yaml
  pdf-2-prompt-export.yaml
  pdf-3-prompt-export.yaml
  pdf-4-prompt-export.yaml
  pdf-5-prompt-export.yaml
  pdf-6-prompt-export.yaml
  pdf-7-prompt-export.yaml
  pdf-document-prompt-export.yaml
  pdf-issue.yaml
  prompt-export.json
packages/
  core/
    src/
      __tests__/
        llm.test.ts
      lib/
        llm.ts
      index.ts
runs/
  prompt-export.yaml
scripts/
  build.mjs
  dev.mjs
src/
  __tests__/
    __fixtures__/
      themes/
        adapt/
          opencode.json
        global/
          invalid.json
          ocean.json
          pm-dark.json
        project/
          .prompt-maker-cli/
            themes/
              broken-theme.json
              forest.json
          a/
            .prompt-maker-cli/
              themes/
                pm-dark.json
    generate/
      interactive.test.ts
      payload-io.test.ts
      resume-from-file.test.ts
      resume-loaded-stream-event.test.ts
    history/
      generate-history.test.ts
    tui/
      budgets-persistence.test.ts
      budgets-settings.test.ts
      buffered-history-writer.test.ts
      command-screen-popup-bindings-contract.test.ts
      context-popup-glue-contract.test.ts
      context-popup-state-mutations.test.ts
      generation-history-formatters.test.ts
      generation-pipeline-reducer.test.ts
      help-command.test.ts
      list-popup-model.test.ts
      list-windowing.test.ts
      markdown-highlight.test.ts
      model-popup-model.test.ts
      notifier-dedupe.test.ts
      popup-manager-command-mapping.test.ts
      popup-reducer.test.ts
      popup-scan-orchestrator.test.ts
      popup-shortcuts.test.ts
      theme-adapter.test.ts
      theme-loader.test.ts
      theme-persistence.test.ts
      theme-provider.test.ts
      theme-registry.test.ts
      theme-resolver.test.ts
      theme-validate.test.ts
      tui-exit-clear-screen.test.ts
      url-args.test.ts
      useDroppedFileDetection.test.ts
    app-container-keymap.test.ts
    command-filter.test.ts
    command-menu-keymap.test.ts
    command-screen-reducer.test.ts
    compose-command.test.ts
    config.test.ts
    context-budget.test.ts
    context-smart-root.test.ts
    drag-drop-path.test.ts
    export-command.test.ts
    file-context.test.ts
    file-suggestions.test.ts
    generate-args.test.ts
    generate-command.test.ts
    generate-stream-dispatcher.test.ts
    generation-pipeline-reducer.test.ts
    github-context.test.ts
    help-config.test.ts
    help-overlay-scroll.test.ts
    history-logger.test.ts
    image-loader.test.ts
    index.test.ts
    input-bar.test.ts
    intent-source.test.ts
    io.test.ts
    list-window.test.ts
    media-loader.test.ts
    message-builders.test.ts
    model-filter.test.ts
    model-manager.test.ts
    model-options.test.ts
    model-popup-options.test.ts
    model-session.test.ts
    multiline-text-buffer.test.ts
    new-command.test.ts
    notifier.test.ts
    paste-snippet.test.ts
    pdf-parts.test.ts
    popup-reducer.test.ts
    prompt-generator-service.test.ts
    provider-chip.test.ts
    provider-status.test.ts
    series-path.test.ts
    smart-context-service.test.ts
    soft-wrap.test.ts
    status-indicators-layout.test.ts
    string-filter.test.ts
    test-command.test.ts
    test-runner-reducer.test.ts
    text-input-keys.test.ts
    token-counter.test.ts
    token-usage-store.test.ts
    tui-command-descriptors.test.ts
    tui-command-history.test.ts
    url-context.test.ts
    useCommandHistory.test.ts
    useGenerationPipeline.test.ts
    usePopupManager.test.ts
  export/
    export-generate-payload.ts
  generate/
    actions.ts
    args.ts
    command.ts
    context-budget.ts
    context-output.ts
    context-templates.ts
    debug.ts
    display.ts
    fs-utils.ts
    intent.ts
    interactive-transport.ts
    interactive.ts
    models.ts
    payload-io.ts
    pipeline.ts
    polish.ts
    progress.ts
    stream.ts
    token-telemetry.ts
    types.ts
  history/
    generate-history.ts
  prompt-generator/
    message-builders.ts
    model-credentials.ts
    parse-llm-json.ts
    pdf-parts.ts
    prompts.ts
    series-repair.ts
    series-validation.ts
    target-model-guidance.ts
    types.ts
    video-parts.ts
  rag/
    vector-store.ts
  testing/
    evaluator.ts
    test-schema.ts
  tui/
    components/
      core/
        BackgroundFill.tsx
        bracketed-paste.ts
        command-menu-keymap.ts
        CommandMenu.tsx
        help-overlay-scroll.ts
        HelpOverlay.tsx
        input-bar-layout.ts
        input-bar-presentation.ts
        InputBar.tsx
        multiline-text-buffer.ts
        MultilineTextInput.tsx
        ScrollableOutput.tsx
        single-line-text-input-keymap.ts
        SingleLineTextInput.tsx
        soft-wrap.ts
        status-indicators-layout.ts
        text-input-keys.ts
        Toast.tsx
        ToastOverlay.tsx
        tokenized-text.ts
      popups/
        BudgetsPopup.tsx
        ExportPopup.tsx
        InstructionsPopup.tsx
        IntentFilePopup.tsx
        list-popup-layout.ts
        list-popup-model.ts
        list-window.ts
        list-windowing.ts
        ListPopup.tsx
        model-popup-model.ts
        ModelPopup.tsx
        PopupSheet.tsx
        ReasoningPopup.tsx
        ResumePopup.tsx
        SeriesIntentPopup.tsx
        SettingsPopup.tsx
        SmartPopup.tsx
        TestPopup.tsx
        ThemeModePopup.tsx
        ThemePickerPopup.tsx
        TogglePopup.tsx
        TokenUsagePopup.tsx
      OpencodeSpinner.tsx
    hooks/
      popup-manager/
        command-mapping.ts
        scan-orchestrator.ts
      buffered-history-writer.ts
      generation-history-formatters.ts
      popup-scans.ts
      series-artifacts-io.ts
      useCommandHistory.ts
      useDebouncedValue.ts
      useGenerationPipeline.ts
      useLatestRef.ts
      usePersistentCommandHistory.ts
      usePopupManager.ts
      useStableCallback.ts
    markdown/
      markdown-highlight.ts
    screens/
      command/
        components/
          CommandInput.tsx
          CommandMenuPane.tsx
          HistoryPane.tsx
          PopupArea.tsx
        hooks/
          context-popup-glue/
            popup-state-mutations.ts
            useFilePopupGlue.ts
            useImagePopupGlue.ts
            usePdfPopupGlue.ts
            useSmartPopupGlue.ts
            useUrlPopupGlue.ts
            useVideoPopupGlue.ts
          popup-shortcuts/
            budgets-popup-shortcuts.ts
            escape-only-popup-shortcuts.ts
            export-popup-shortcuts.ts
            history-popup-shortcuts.ts
            intent-popup-shortcuts.ts
            model-popup-shortcuts.ts
            reasoning-popup-shortcuts.ts
            resume-popup-shortcuts.ts
            selection.ts
            smart-popup-shortcuts.ts
            suggested-input.ts
            suggested-selected-list-popup-shortcuts.ts
            suggested-selected-list.ts
            theme-popup-shortcuts.ts
            toggle-popup-shortcuts.ts
            types.ts
            url-popup-shortcuts.ts
          useCommandGenerationPipeline.ts
          useCommandMenuManager.ts
          useCommandScreenBindings.ts
          useCommandScreenChips.ts
          useCommandScreenContextPopupBindings.ts
          useCommandScreenController.ts
          useCommandScreenHistoryAndTests.ts
          useCommandScreenHistoryIntentPopupBindings.ts
          useCommandScreenInputState.ts
          useCommandScreenLayout.ts
          useCommandScreenModelGeneration.ts
          useCommandScreenPasteBindings.ts
          useCommandScreenPopupAndView.ts
          useCommandScreenPopupAndView.types.ts
          useCommandScreenPopupBindings.ts
          useCommandScreenPopupManager.ts
          useCommandScreenPopupVisibility.ts
          useCommandScreenShell.ts
          useCommandScreenSubmitBindings.ts
          useCommandScreenViewModel.ts
          useContextPopupGlue.ts
          useDroppedFileDetection.ts
          useDroppedFilePath.ts
          useHistoryPopupGlue.ts
          useHistoryScrollKeys.ts
          useIntentPopupGlue.ts
          useIntentSubmitHandler.ts
          useMiscPopupDraftHandlers.ts
          useModelPopupData.ts
          useModelProviderState.ts
          usePasteManager.ts
          usePopupKeyboardShortcuts.ts
          usePopupSelectionClamp.ts
          usePromptTestRunner.ts
          useReasoningPopup.ts
          useSessionCommands.ts
          useTerminalEffects.ts
          useThemeModePopupGlue.ts
          useThemePopupGlue.ts
        utils/
          control-key.ts
          debug-keys.ts
          paste-tokens.ts
          submit-plan.ts
          url-args.ts
        command-screen-reducer.ts
        CommandScreen.tsx
        CommandScreenImpl.tsx
        useCommandScreen.ts
      test-runner/
        components/
          TestList.tsx
          TestRunnerActions.tsx
          TestRunnerError.tsx
          TestRunnerFileInput.tsx
          TestRunnerLogs.tsx
          TestRunnerSummary.tsx
        test-runner-reducer.ts
        TestRunnerScreen.tsx
        useTestRunnerScreen.ts
    theme/
      builtins/
        pm-dark.ts
        pm-light.ts
      examples/
        ocean-example.json
      terminal-appearance.ts
      theme-adapter.ts
      theme-discovery.ts
      theme-loader.ts
      theme-parse.ts
      theme-provider.tsx
      theme-registry.ts
      theme-resolver.ts
      theme-settings-service.ts
      theme-types.ts
      theme-validate.ts
    app-container-keymap.ts
    AppContainer.tsx
    budget-settings.ts
    command-filter.ts
    command-history.ts
    CommandScreen.tsx
    config.ts
    context-store.ts
    context.tsx
    DEVELOPER_NOTE.md
    drag-drop-path.ts
    file-suggestions.ts
    generation-pipeline-reducer.ts
    help-config.ts
    index.tsx
    intent-source.ts
    model-filter.ts
    model-options.ts
    model-popup-options.ts
    model-session.ts
    new-command.ts
    notifier.ts
    paste-snippet.ts
    popup-reducer.ts
    provider-chip.ts
    provider-status.ts
    resume-history.ts
    string-filter.ts
    TestRunnerScreen.tsx
    toast-constants.ts
    toast-state.ts
    token-usage-store.ts
    types.ts
    useLogBuffer.ts
  types/
    jsdom.d.ts
  utils/
    model-manager.ts
    series-path.ts
  compose-command.ts
  config.ts
  export-command.ts
  file-context.ts
  generate-command.ts
  github-context.ts
  history-logger.ts
  image-loader.ts
  index.ts
  io.ts
  media-loader.ts
  model-providers.ts
  prompt-generator-service.ts
  smart-context-service.ts
  test-command.ts
  token-counter.ts
  url-context.ts
tests/
  mocks/
    boxen.ts
    chalk.ts
    ora.ts
    yargs.ts
.eslintrc.json
.gitignore
.nvmrc
.prettierignore
.prettierrc
AGENTS.md
jest.config.cjs
package.json
prompt-tests.yaml
README.md
session-ses_4602.md
tsconfig.jest.json
tsconfig.json
</directory_structure>
<files>
This section contains the contents of the repository's files.
<file path="scripts/build.mjs">
import path from 'node:path'
import fs from 'node:fs/promises'
import { fileURLToPath } from 'node:url'
import { build, context } from 'esbuild'
const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)
const repoRoot = path.resolve(__dirname, '..')
const cliArgs = process.argv.slice(2)
const watch = cliArgs.includes('--watch')
const minify = cliArgs.includes('--minify')
const srcEntry = path.join(repoRoot, 'src/index.ts')
const outFile = path.join(repoRoot, 'dist/index.js')
const metaFile = path.join(repoRoot, 'dist/meta.json')
const coreSrcRoot = path.join(repoRoot, 'packages/core/src')
const promptMakerCoreAliasPlugin = {
  name: 'prompt-maker-core-alias',
  setup(pluginBuild) {
    pluginBuild.onResolve({ filter: /^@prompt-maker\/core$/ }, () => ({
      path: path.join(coreSrcRoot, 'index.ts'),
    }))
    pluginBuild.onResolve({ filter: /^@prompt-maker\/core\/.+$/ }, (args) => {
      const subPath = args.path.slice('@prompt-maker/core/'.length)
      return {
        path: path.join(coreSrcRoot, subPath),
      }
    })
  },
}
const buildOptions = {
  entryPoints: [srcEntry],
  outfile: outFile,
  bundle: true,
  packages: 'external',
  platform: 'node',
  target: 'node18',
  format: 'esm',
  sourcemap: true,
  minify,
  plugins: [promptMakerCoreAliasPlugin],
  metafile: true,
  logLevel: 'info',
}
await fs.mkdir(path.dirname(outFile), { recursive: true })
if (watch) {
  const ctx = await context(buildOptions)
  await ctx.watch()
  console.log('esbuild is watching for changes...')
} else {
  const result = await build(buildOptions)
  await fs.writeFile(metaFile, JSON.stringify(result.metafile, null, 2), 'utf8')
}
</file>
<file path="scripts/dev.mjs">
import path from 'node:path'
import { fileURLToPath } from 'node:url'
import { spawn } from 'node:child_process'
import { context } from 'esbuild'
const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)
const repoRoot = path.resolve(__dirname, '..')
const srcEntry = path.join(repoRoot, 'src/index.ts')
const outFile = path.join(repoRoot, 'dist/index.js')
const coreSrcRoot = path.join(repoRoot, 'packages/core/src')
const cliArgs = process.argv.slice(2)
let child = null
const killChild = () => {
  if (!child) return
  child.kill('SIGTERM')
  child = null
}
const startChild = () => {
  killChild()
  child = spawn('node', [outFile, ...cliArgs], {
    stdio: 'inherit',
  })
}
const promptMakerCoreAliasPlugin = {
  name: 'prompt-maker-core-alias',
  setup(pluginBuild) {
    pluginBuild.onResolve({ filter: /^@prompt-maker\/core$/ }, () => ({
      path: path.join(coreSrcRoot, 'index.ts'),
    }))
    pluginBuild.onResolve({ filter: /^@prompt-maker\/core\/.+$/ }, (args) => {
      const subPath = args.path.slice('@prompt-maker/core/'.length)
      return {
        path: path.join(coreSrcRoot, subPath),
      }
    })
  },
}
const restartPlugin = {
  name: 'restart-on-rebuild',
  setup(pluginBuild) {
    pluginBuild.onEnd((result) => {
      if (result.errors.length > 0) {
        return
      }
      startChild()
    })
  },
}
const ctx = await context({
  entryPoints: [srcEntry],
  outfile: outFile,
  bundle: true,
  packages: 'external',
  platform: 'node',
  target: 'node18',
  format: 'esm',
  sourcemap: true,
  plugins: [promptMakerCoreAliasPlugin, restartPlugin],
  logLevel: 'info',
})
const shutdown = async () => {
  killChild()
  await ctx.dispose()
}
process.on('SIGINT', () => {
  void shutdown().finally(() => process.exit(0))
})
process.on('SIGTERM', () => {
  void shutdown().finally(() => process.exit(0))
})
await ctx.watch()
await ctx.rebuild()
</file>
<file path="src/__tests__/__fixtures__/themes/global/invalid.json">
{ this is not valid json }
</file>
<file path="src/__tests__/__fixtures__/themes/global/ocean.json">
{
  "theme": {
    "background": "#001219",
    "text": "#e9d8a6",
    "mutedText": "#94d2bd",
    "border": "#005f73",
    "accent": "#0a9396",
    "accentText": "#001219",
    "warning": "#ee9b00",
    "error": "#ae2012",
    "success": "#2a9d8f",
    "panelBackground": "#0b2a32",
    "popupBackground": "panelBackground",
    "selectionBackground": "#005f73",
    "selectionText": "#e9d8a6",
    "chipBackground": "panelBackground",
    "chipText": "text",
    "chipMutedText": "mutedText"
  }
}
</file>
<file path="src/__tests__/__fixtures__/themes/global/pm-dark.json">
{
  "defs": {
    "bg": "#111111",
    "text": "#eeeeee",
    "muted": "#777777",
    "border": "#444444",
    "accent": "#ff00ff",
    "warning": "#ffff00",
    "error": "#ff0000",
    "success": "#00ff00",
    "panelBg": "#1a1a1a",
    "selectionBg": "#333333"
  },
  "theme": {
    "background": "bg",
    "text": "text",
    "mutedText": "muted",
    "border": "border",
    "accent": "accent",
    "accentText": "bg",
    "warning": "warning",
    "error": "error",
    "success": "success",
    "panelBackground": "panelBg",
    "popupBackground": "panelBackground",
    "selectionBackground": "selectionBg",
    "selectionText": "text",
    "chipBackground": "panelBackground",
    "chipText": "text",
    "chipMutedText": "mutedText"
  }
}
</file>
<file path="src/__tests__/__fixtures__/themes/project/.prompt-maker-cli/themes/broken-theme.json">
{
  "theme": {
    "background": "#000000"
  }
}
</file>
<file path="src/__tests__/__fixtures__/themes/project/.prompt-maker-cli/themes/forest.json">
{
  "theme": {
    "background": "#0b1f0f",
    "text": "#e7f6e7",
    "mutedText": "#96b69a",
    "border": "#2f4f38",
    "accent": "#3fb950",
    "accentText": "#0b1f0f",
    "warning": "#d29922",
    "error": "#f85149",
    "success": "#3fb950",
    "panelBackground": "#0f2a14",
    "popupBackground": "panelBackground",
    "selectionBackground": "#2f4f38",
    "selectionText": "#e7f6e7",
    "chipBackground": "panelBackground",
    "chipText": "text",
    "chipMutedText": "mutedText"
  }
}
</file>
<file path="src/__tests__/__fixtures__/themes/project/a/.prompt-maker-cli/themes/pm-dark.json">
{
  "defs": {
    "bg": "#000000",
    "text": "#ffffff",
    "accent": "#00ffff",
    "panelBg": "#101010"
  },
  "theme": {
    "background": "bg",
    "text": "text",
    "mutedText": "#888888",
    "border": 240,
    "accent": "accent",
    "accentText": "bg",
    "warning": 220,
    "error": 196,
    "success": 46,
    "panelBackground": "panelBg",
    "popupBackground": { "dark": "panelBackground", "light": "#f6f8fa" },
    "selectionBackground": "accent",
    "selectionText": "bg",
    "chipBackground": "panelBackground",
    "chipText": "text",
    "chipMutedText": "mutedText"
  }
}
</file>
<file path="src/__tests__/tui/theme-loader.test.ts">
import path from 'node:path'
import { loadThemes } from '../../tui/theme/theme-loader'
const FIXTURES_ROOT = path.join(__dirname, '..', '__fixtures__', 'themes')
const GLOBAL_DIR = path.join(FIXTURES_ROOT, 'global')
const PROJECT_ROOT = path.join(FIXTURES_ROOT, 'project')
const PROJECT_CWD = path.join(PROJECT_ROOT, 'a')
describe('theme loader', () => {
  test('merges built-in + global + project with correct precedence', async () => {
    const { themes, errors } = await loadThemes({
      cwd: PROJECT_CWD,
      stopAt: PROJECT_ROOT,
      globalThemesDir: GLOBAL_DIR,
    })
    expect(errors).not.toHaveLength(0)
    const names = themes.map((theme) => theme.name)
    expect(names).toEqual(['pm-dark', 'pm-light', 'forest', 'ocean'])
    const pmDark = themes.find((theme) => theme.name === 'pm-dark')
    expect(pmDark?.source).toBe('project')
    expect(pmDark?.filePath).toContain(
      path.join('a', '.prompt-maker-cli', 'themes', 'pm-dark.json'),
    )
    const ocean = themes.find((theme) => theme.name === 'ocean')
    expect(ocean?.source).toBe('global')
    // Project-local `pm-dark` fixture uses ANSI numbers.
    expect(pmDark?.theme.theme.border).toBe(240)
  })
  test('invalid themes surface errors but do not prevent other themes from loading', async () => {
    const { themes, errors } = await loadThemes({
      cwd: PROJECT_CWD,
      stopAt: PROJECT_ROOT,
      globalThemesDir: GLOBAL_DIR,
    })
    expect(themes.map((theme) => theme.name)).toContain('ocean')
    expect(themes.map((theme) => theme.name)).toContain('forest')
    const errorFiles = errors.map((error) => path.basename(error.filePath))
    expect(errorFiles).toContain('invalid.json')
    expect(errorFiles).toContain('broken-theme.json')
  })
  test('listing is deterministic across calls', async () => {
    const first = await loadThemes({
      cwd: PROJECT_CWD,
      stopAt: PROJECT_ROOT,
      globalThemesDir: GLOBAL_DIR,
    })
    const second = await loadThemes({
      cwd: PROJECT_CWD,
      stopAt: PROJECT_ROOT,
      globalThemesDir: GLOBAL_DIR,
    })
    expect(first.themes.map((theme) => theme.name)).toEqual(
      second.themes.map((theme) => theme.name),
    )
    expect(first.themes.map((theme) => theme.source)).toEqual(
      second.themes.map((theme) => theme.source),
    )
  })
})
</file>
<file path="src/__tests__/tui/theme-persistence.test.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
const readJson = async (filePath: string): Promise<unknown> => {
  const contents = await fs.readFile(filePath, 'utf8')
  return JSON.parse(contents) as unknown
}
const writeJson = async (filePath: string, value: unknown): Promise<void> => {
  await fs.mkdir(path.dirname(filePath), { recursive: true })
  await fs.writeFile(filePath, `${JSON.stringify(value, null, 2)}\n`, 'utf8')
}
describe('theme persistence', () => {
  const envBefore = { ...process.env }
  afterEach(() => {
    process.env = { ...envBefore }
  })
  test('defaults theme + mode when config lacks fields', async () => {
    const tempRoot = await fs.mkdtemp(path.join(os.tmpdir(), 'pm-theme-persist-'))
    const configPath = path.join(tempRoot, 'config.json')
    const globalThemesDir = path.join(tempRoot, 'global-themes')
    await writeJson(configPath, {})
    process.env.PROMPT_MAKER_CLI_CONFIG = configPath
    jest.resetModules()
    const { loadThemeSelection } = await import('../../tui/theme/theme-settings-service')
    const { DEFAULT_THEME_NAME } = await import('../../tui/theme/theme-registry')
    const result = await loadThemeSelection({
      themeLoadOptions: {
        cwd: tempRoot,
        stopAt: tempRoot,
        globalThemesDir,
      },
    })
    expect(result.selection.themeName).toBe(DEFAULT_THEME_NAME)
    expect(result.selection.themeMode).toBe('dark')
    expect(result.warnings).toEqual([])
  })
  test('invalid theme name falls back deterministically with warning', async () => {
    const tempRoot = await fs.mkdtemp(path.join(os.tmpdir(), 'pm-theme-persist-'))
    const configPath = path.join(tempRoot, 'config.json')
    const globalThemesDir = path.join(tempRoot, 'global-themes')
    await writeJson(configPath, { theme: 'not-a-theme' })
    process.env.PROMPT_MAKER_CLI_CONFIG = configPath
    jest.resetModules()
    const { loadThemeSelection } = await import('../../tui/theme/theme-settings-service')
    const { DEFAULT_THEME_NAME } = await import('../../tui/theme/theme-registry')
    const result = await loadThemeSelection({
      themeLoadOptions: {
        cwd: tempRoot,
        stopAt: tempRoot,
        globalThemesDir,
      },
    })
    expect(result.selection.themeName).toBe(DEFAULT_THEME_NAME)
    expect(result.warnings).toHaveLength(1)
    expect(result.warnings[0]?.kind).toBe('unknown-theme')
    expect(result.warnings[0]?.requested).toBe('not-a-theme')
    expect(result.warnings[0]?.fallback).toBe(DEFAULT_THEME_NAME)
  })
  test('auto themeMode is treated as system', async () => {
    const tempRoot = await fs.mkdtemp(path.join(os.tmpdir(), 'pm-theme-persist-'))
    const configPath = path.join(tempRoot, 'config.json')
    const globalThemesDir = path.join(tempRoot, 'global-themes')
    await writeJson(configPath, { themeMode: 'auto' })
    process.env.PROMPT_MAKER_CLI_CONFIG = configPath
    jest.resetModules()
    const { loadThemeSelection } = await import('../../tui/theme/theme-settings-service')
    const result = await loadThemeSelection({
      themeLoadOptions: {
        cwd: tempRoot,
        stopAt: tempRoot,
        globalThemesDir,
      },
    })
    expect(result.selection.themeMode).toBe('system')
  })
  test('saving updates config without rewriting other fields', async () => {
    const tempRoot = await fs.mkdtemp(path.join(os.tmpdir(), 'pm-theme-persist-'))
    const configPath = path.join(tempRoot, 'config.json')
    await writeJson(configPath, {
      openaiApiKey: 'keep-me',
      theme: 'pm-dark',
      themeMode: 'dark',
      promptGenerator: { defaultModel: 'gpt-4o' },
    })
    process.env.PROMPT_MAKER_CLI_CONFIG = configPath
    jest.resetModules()
    const { saveThemeSelection } = await import('../../tui/theme/theme-settings-service')
    await saveThemeSelection({ themeName: 'pm-light', themeMode: 'light' })
    const updated = await readJson(configPath)
    expect(updated).toMatchObject({
      openaiApiKey: 'keep-me',
      theme: 'pm-light',
      themeMode: 'light',
      promptGenerator: { defaultModel: 'gpt-4o' },
    })
  })
})
</file>
<file path="src/__tests__/tui/theme-provider.test.ts">
import React from 'react'
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import type { ThemeDescriptor } from '../../tui/theme/theme-loader'
import { ThemeProvider, useTheme } from '../../tui/theme/theme-provider'
import { loadThemeSelection, saveThemeSelection } from '../../tui/theme/theme-settings-service'
const pmDarkTheme: ThemeDescriptor = {
  name: 'pm-dark',
  label: 'Prompt Maker Dark',
  source: 'builtin',
  theme: {
    theme: {
      background: { dark: '#000000', light: '#ffffff' },
      text: '#ffffff',
      mutedText: '#888888',
      border: '#444444',
      accent: '#00ffff',
      accentText: '#000000',
      warning: '#ffff00',
      error: '#ff0000',
      success: '#00ff00',
      panelBackground: '#111111',
      popupBackground: 'panelBackground',
      selectionBackground: '#333333',
      selectionText: '#ffffff',
      chipBackground: '#222222',
      chipText: '#ffffff',
      chipMutedText: '#aaaaaa',
    },
  },
}
const pmLightTheme: ThemeDescriptor = {
  name: 'pm-light',
  label: 'Prompt Maker Light',
  source: 'builtin',
  theme: {
    theme: {
      background: '#ffffff',
      text: '#000000',
      mutedText: '#555555',
      border: '#dddddd',
      accent: '#0000ff',
      accentText: '#ffffff',
      warning: '#ff8800',
      error: '#ff0000',
      success: '#00aa00',
      panelBackground: '#f6f8fa',
      popupBackground: 'panelBackground',
      selectionBackground: '#ddddff',
      selectionText: '#000000',
      chipBackground: '#f6f8fa',
      chipText: '#000000',
      chipMutedText: '#555555',
    },
  },
}
jest.mock('../../tui/theme/theme-settings-service')
const dom = new JSDOM('<!doctype html><html><body></body></html>')
type GlobalDom = { window: Window; document: Document }
beforeAll(() => {
  const target = globalThis as unknown as GlobalDom
  target.window = dom.window as unknown as Window
  target.document = dom.window.document
})
afterAll(() => {
  const target = globalThis as unknown as Partial<GlobalDom>
  delete target.window
  delete target.document
})
describe('ThemeProvider', () => {
  beforeEach(() => {
    const mockedLoad = jest.mocked(loadThemeSelection)
    mockedLoad.mockResolvedValue({
      themes: [pmDarkTheme, pmLightTheme],
      loadErrors: [],
      selection: { themeName: 'pm-dark', themeMode: 'dark' },
      warnings: [],
    })
    const mockedSave = jest.mocked(saveThemeSelection)
    mockedSave.mockResolvedValue()
  })
  test('previewTheme swaps without persisting', async () => {
    const wrapper: React.FC<{ children: React.ReactNode }> = ({ children }) =>
      React.createElement(ThemeProvider, null, children)
    const { result } = renderHook(() => useTheme(), { wrapper })
    await act(async () => {
      await Promise.resolve()
    })
    expect(result.current.activeThemeName).toBe('pm-dark')
    act(() => {
      const ok = result.current.previewTheme('pm-light')
      expect(ok).toBe(true)
    })
    expect(result.current.activeThemeName).toBe('pm-light')
    expect(jest.mocked(saveThemeSelection)).not.toHaveBeenCalled()
    act(() => {
      const ok = result.current.previewTheme('pm-dark')
      expect(ok).toBe(true)
    })
    expect(result.current.activeThemeName).toBe('pm-dark')
    expect(jest.mocked(saveThemeSelection)).not.toHaveBeenCalled()
  })
  test('setTheme re-resolves and persists', async () => {
    const wrapper: React.FC<{ children: React.ReactNode }> = ({ children }) =>
      React.createElement(ThemeProvider, null, children)
    const { result } = renderHook(() => useTheme(), { wrapper })
    await act(async () => {
      await Promise.resolve()
    })
    await act(async () => {
      const ok = await result.current.setTheme('pm-light')
      expect(ok).toBe(true)
    })
    expect(result.current.activeThemeName).toBe('pm-light')
    expect(result.current.theme.background).toBe('#ffffff')
    expect(jest.mocked(saveThemeSelection)).toHaveBeenCalledWith({ themeName: 'pm-light' })
  })
  test('setMode re-resolves and persists', async () => {
    const wrapper: React.FC<{ children: React.ReactNode }> = ({ children }) =>
      React.createElement(ThemeProvider, null, children)
    const { result } = renderHook(() => useTheme(), { wrapper })
    await act(async () => {
      await Promise.resolve()
    })
    expect(result.current.mode).toBe('dark')
    await act(async () => {
      const ok = await result.current.setMode('light')
      expect(ok).toBe(true)
    })
    expect(result.current.mode).toBe('light')
    expect(result.current.theme.background).toBe('#ffffff')
    expect(jest.mocked(saveThemeSelection)).toHaveBeenCalledWith({ themeMode: 'light' })
  })
  test('system mode uses env detection with dark fallback', async () => {
    const envBefore = { ...process.env }
    process.env = { ...envBefore, COLORFGBG: '15;7' }
    const wrapper: React.FC<{ children: React.ReactNode }> = ({ children }) =>
      React.createElement(ThemeProvider, null, children)
    const { result } = renderHook(() => useTheme(), { wrapper })
    await act(async () => {
      await Promise.resolve()
    })
    await act(async () => {
      const ok = await result.current.setMode('system')
      expect(ok).toBe(true)
    })
    expect(result.current.mode).toBe('system')
    expect(result.current.theme.background).toBe('#ffffff')
    expect(jest.mocked(saveThemeSelection)).toHaveBeenCalledWith({ themeMode: 'system' })
    process.env = envBefore
  })
})
</file>
<file path="src/__tests__/tui/theme-registry.test.ts">
import { DEFAULT_THEME_NAME, getTheme, listThemes } from '../../tui/theme/theme-registry'
describe('theme registry', () => {
  test('returns built-in themes deterministically', () => {
    const first = listThemes().map((theme) => theme.name)
    const second = listThemes().map((theme) => theme.name)
    expect(first).toEqual(second)
    expect(first).toEqual(['pm-dark', 'pm-light'])
  })
  test('default theme exists in registry', () => {
    const names = listThemes().map((theme) => theme.name)
    expect(names).toContain(DEFAULT_THEME_NAME)
    expect(getTheme(DEFAULT_THEME_NAME)).toBeDefined()
  })
})
</file>
<file path="src/__tests__/command-filter.test.ts">
import { filterCommandDescriptors, resolveCommandMenuSearchState } from '../tui/command-filter'
import { COMMAND_DESCRIPTORS } from '../tui/config'
describe('command filtering', () => {
  it('returns all commands for an empty query', () => {
    const result = filterCommandDescriptors({ query: '   ', commands: COMMAND_DESCRIPTORS })
    expect(result.map((command) => command.id)).toEqual(COMMAND_DESCRIPTORS.map((c) => c.id))
  })
  it('matches aliases case-insensitively', () => {
    const result = filterCommandDescriptors({ query: 'MeTa', commands: COMMAND_DESCRIPTORS })
    expect(result.map((command) => command.id)).toEqual(['instructions'])
  })
  it('matches by description tokens when not a command prefix', () => {
    const state = resolveCommandMenuSearchState({
      commandQuery: 'prompt tests',
      commands: COMMAND_DESCRIPTORS,
    })
    expect(state.treatRemainderAsArgs).toBe(false)
    const result = filterCommandDescriptors({
      query: state.filterQuery,
      commands: COMMAND_DESCRIPTORS,
    })
    expect(result.map((command) => command.id)).toContain('test')
  })
  it('treats the remainder as args when there is a prefix match', () => {
    const state = resolveCommandMenuSearchState({
      commandQuery: 'test prompt-tests.yaml',
      commands: COMMAND_DESCRIPTORS,
    })
    expect(state).toEqual({ filterQuery: 'test', treatRemainderAsArgs: true })
    const result = filterCommandDescriptors({
      query: state.filterQuery,
      commands: COMMAND_DESCRIPTORS,
    })
    expect(result.map((command) => command.id)).toContain('test')
  })
  it('prefers id/alias/label prefix matches', () => {
    const result = filterCommandDescriptors({ query: 'why', commands: COMMAND_DESCRIPTORS })
    expect(result.map((command) => command.id)[0]).toBe('reasoning')
  })
  it('matches substrings in label and description', () => {
    const result = filterCommandDescriptors({ query: 'switch', commands: COMMAND_DESCRIPTORS })
    expect(result.map((command) => command.id)).toContain('model')
  })
})
</file>
<file path="src/__tests__/command-screen-reducer.test.ts">
import {
  commandScreenReducer,
  createInitialCommandScreenState,
} from '../tui/screens/command/command-screen-reducer'
describe('commandScreenReducer', () => {
  it('sets terminal size and avoids needless changes', () => {
    const initial = createInitialCommandScreenState({ terminalRows: 24, terminalColumns: 80 })
    const same = commandScreenReducer(initial, {
      type: 'set-terminal-size',
      rows: 24,
      columns: 80,
    })
    expect(same).toBe(initial)
    const next = commandScreenReducer(initial, {
      type: 'set-terminal-size',
      rows: 40,
      columns: 120,
    })
    expect(next).toEqual({ ...initial, terminalRows: 40, terminalColumns: 120 })
  })
  it('supports functional input updates', () => {
    const initial = createInitialCommandScreenState({ terminalRows: 24, terminalColumns: 80 })
    const next = commandScreenReducer(initial, {
      type: 'set-input',
      next: (prev) => `${prev}hello`,
    })
    expect(next.inputValue).toBe('hello')
  })
  it('supports functional command selection updates', () => {
    const initial = createInitialCommandScreenState({ terminalRows: 24, terminalColumns: 80 })
    const next = commandScreenReducer(initial, {
      type: 'set-command-selection',
      next: (prev) => prev + 2,
    })
    expect(next.commandSelectionIndex).toBe(2)
  })
})
</file>
<file path="src/__tests__/context-smart-root.test.ts">
import React from 'react'
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import { ContextProvider } from '../tui/context'
import { useContextDispatch, useContextState } from '../tui/context-store'
const dom = new JSDOM('<!doctype html><html><body></body></html>')
const globalScope = globalThis as typeof globalThis & {
  window: Window & typeof globalThis
  document: Document
  navigator: Navigator
}
globalScope.window = dom.window
globalScope.document = dom.window.document
globalScope.navigator = dom.window.navigator
const wrapper: React.FC<{ children: React.ReactNode }> = ({ children }) =>
  React.createElement(ContextProvider, null, children)
describe('smart context root', () => {
  it('overwrites instead of accumulating roots', () => {
    const { result } = renderHook(
      () => ({ state: useContextState(), dispatch: useContextDispatch() }),
      { wrapper },
    )
    act(() => {
      result.current.dispatch.setSmartRoot('src')
    })
    expect(result.current.state.smartContextRoot).toBe('src')
    act(() => {
      result.current.dispatch.setSmartRoot('apps')
    })
    expect(result.current.state.smartContextRoot).toBe('apps')
  })
  it('clears the root when passed empty input', () => {
    const { result } = renderHook(
      () => ({ state: useContextState(), dispatch: useContextDispatch() }),
      { wrapper },
    )
    act(() => {
      result.current.dispatch.setSmartRoot('src')
    })
    expect(result.current.state.smartContextRoot).toBe('src')
    act(() => {
      result.current.dispatch.setSmartRoot('')
    })
    expect(result.current.state.smartContextRoot).toBeNull()
  })
})
</file>
<file path="src/__tests__/drag-drop-path.test.ts">
import { isCommandInput, parseAbsolutePathFromInput } from '../tui/drag-drop-path'
describe('drag-drop-path', () => {
  describe('parseAbsolutePathFromInput', () => {
    it('parses a plain posix absolute path', () => {
      expect(parseAbsolutePathFromInput('/Users/alice/file.txt')).toBe('/Users/alice/file.txt')
    })
    it('parses a quoted absolute path with spaces', () => {
      expect(parseAbsolutePathFromInput('"/Users/alice/My File.md"')).toBe(
        '/Users/alice/My File.md',
      )
    })
    it('parses a backslash-escaped absolute path with spaces', () => {
      expect(parseAbsolutePathFromInput('/Users/alice/My\\ File.md')).toBe(
        '/Users/alice/My File.md',
      )
    })
    it('returns null when additional tokens exist', () => {
      expect(parseAbsolutePathFromInput('/file arg')).toBeNull()
    })
    it('returns null for relative paths', () => {
      expect(parseAbsolutePathFromInput('src/index.ts')).toBeNull()
    })
    it('preserves Windows-style absolute paths', () => {
      expect(parseAbsolutePathFromInput('C:\\Users\\alice\\file.txt')).toBe(
        'C:\\Users\\alice\\file.txt',
      )
    })
    it('parses pasted paths with bracketed paste markers', () => {
      expect(parseAbsolutePathFromInput("[200~'/Users/alice/My File.md'[201~")).toBe(
        '/Users/alice/My File.md',
      )
    })
    it('parses pasted paths with stray sgr fragments', () => {
      expect(parseAbsolutePathFromInput("[200~'/Users/alice/My File.md'[7m")).toBe(
        '/Users/alice/My File.md',
      )
    })
  })
  describe('isCommandInput', () => {
    const existsSync = (candidate: string): boolean => candidate === '/Users'
    it('treats / (slash alone) as a command opener', () => {
      expect(isCommandInput('/', existsSync)).toBe(true)
    })
    it('treats command keywords as command input', () => {
      expect(isCommandInput('/files', existsSync)).toBe(true)
    })
    it('treats nested absolute paths as non-command input', () => {
      expect(isCommandInput('/tmp/foo', existsSync)).toBe(false)
    })
    it('treats existing root-level paths as non-command input', () => {
      expect(isCommandInput('/Users', existsSync)).toBe(false)
    })
    it('treats /keyword with args as command input', () => {
      expect(isCommandInput('/file src/index.ts', existsSync)).toBe(true)
    })
  })
})
</file>
<file path="src/__tests__/file-context.test.ts">
import { resolveFileContext, formatContextForPrompt } from '../file-context'
import fg from 'fast-glob'
jest.mock('fast-glob')
const globMock = fg as unknown as jest.MockedFunction<typeof fg>
jest.mock('node:fs/promises', () => ({ readFile: jest.fn() }))
const fs = jest.requireMock('node:fs/promises') as { readFile: jest.Mock }
describe('file-context', () => {
  beforeEach(() => {
    jest.clearAllMocks()
  })
  it('returns empty array when patterns list is empty', async () => {
    const result = await resolveFileContext([])
    expect(result).toEqual([])
    expect(fg).not.toHaveBeenCalled()
  })
  it('resolves matching files into FileContext entries', async () => {
    globMock.mockResolvedValue(['a.txt', 'b.txt'])
    fs.readFile.mockResolvedValueOnce('alpha').mockResolvedValueOnce('beta')
    const result = await resolveFileContext(['*.txt'])
    expect(result).toEqual([
      { path: 'a.txt', content: 'alpha' },
      { path: 'b.txt', content: 'beta' },
    ])
  })
  it('warns and returns [] when no files matched', async () => {
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    globMock.mockResolvedValue([])
    const result = await resolveFileContext(['none.txt'])
    expect(result).toEqual([])
    expect(warn).toHaveBeenCalledWith(
      expect.stringContaining('No files matched the context patterns'),
    )
    warn.mockRestore()
  })
  it('skips files that fail to read', async () => {
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    globMock.mockResolvedValue(['ok.txt', 'bad.txt'])
    fs.readFile.mockResolvedValueOnce('good').mockRejectedValueOnce(new Error('boom'))
    const result = await resolveFileContext(['*.txt'])
    expect(result).toEqual([{ path: 'ok.txt', content: 'good' }])
    expect(warn).toHaveBeenCalledWith('Warning: Failed to read context file bad.txt')
    warn.mockRestore()
  })
  it('formats context with XML-like wrappers', () => {
    const text = formatContextForPrompt([
      { path: 'src/a.ts', content: 'const a = 1' },
      { path: 'src/b.ts', content: 'const b = 2' },
    ])
    expect(text).toContain('<file path="src/a.ts">')
    expect(text).toContain('</file>\n\n<file path="src/b.ts">')
  })
  it('returns empty string when no files provided to formatter', () => {
    expect(formatContextForPrompt([])).toBe('')
  })
})
</file>
<file path="src/__tests__/generation-pipeline-reducer.test.ts">
import {
  generationPipelineReducer,
  INITIAL_GENERATION_PIPELINE_STATE,
  type GenerationPipelineState,
} from '../tui/generation-pipeline-reducer'
describe('generationPipelineReducer', () => {
  const reduce = (
    state: GenerationPipelineState,
    action: Parameters<typeof generationPipelineReducer>[1],
  ): GenerationPipelineState => generationPipelineReducer(state, action)
  it('starts generation and clears interactive state', () => {
    const next = reduce(
      { ...INITIAL_GENERATION_PIPELINE_STATE, awaitingInteractiveMode: 'transport' },
      { type: 'generation-start', statusMessage: 'PreparingâŚ' },
    )
    expect(next.isGenerating).toBe(true)
    expect(next.statusMessage).toBe('PreparingâŚ')
    expect(next.awaitingInteractiveMode).toBeNull()
    expect(next.isAwaitingRefinement).toBe(false)
    expect(next.latestTelemetry).toBeNull()
  })
  it('stops generation and clears awaiting flags', () => {
    const next = reduce(
      {
        ...INITIAL_GENERATION_PIPELINE_STATE,
        isGenerating: true,
        statusMessage: 'WorkingâŚ',
        awaitingInteractiveMode: 'tty',
        isAwaitingRefinement: true,
      },
      { type: 'generation-stop', statusMessage: 'Complete' },
    )
    expect(next.isGenerating).toBe(false)
    expect(next.statusMessage).toBe('Complete')
    expect(next.awaitingInteractiveMode).toBeNull()
    expect(next.isAwaitingRefinement).toBe(false)
  })
  it('updates interactive awaiting mode and status together', () => {
    const next = reduce(INITIAL_GENERATION_PIPELINE_STATE, {
      type: 'set-awaiting-interactive',
      awaitingInteractiveMode: 'transport',
      statusMessage: 'WaitingâŚ',
    })
    expect(next.awaitingInteractiveMode).toBe('transport')
    expect(next.statusMessage).toBe('WaitingâŚ')
  })
  it('does not overwrite status when updating awaiting mode without a message', () => {
    const next = reduce(
      { ...INITIAL_GENERATION_PIPELINE_STATE, statusMessage: 'Keep me' },
      { type: 'set-awaiting-interactive', awaitingInteractiveMode: null },
    )
    expect(next.awaitingInteractiveMode).toBeNull()
    expect(next.statusMessage).toBe('Keep me')
  })
})
</file>
<file path="src/__tests__/github-context.test.ts">
import { resolveGithubUrl } from '../github-context'
describe('resolveGithubUrl', () => {
  const originalFetch = global.fetch
  const mockFetch = jest.fn() as jest.MockedFunction<typeof fetch>
  beforeEach(() => {
    mockFetch.mockReset()
    global.fetch = mockFetch
  })
  afterAll(() => {
    global.fetch = originalFetch
  })
  it('fetches blob URLs via raw content', async () => {
    mockFetch.mockResolvedValueOnce(createBufferResponse(Buffer.from('console.log("hi")')))
    const files = await resolveGithubUrl(
      new URL('https://github.com/org/repo/blob/main/src/index.ts'),
    )
    expect(mockFetch).toHaveBeenCalledWith(
      expect.stringContaining('raw.githubusercontent.com'),
      expect.any(Object),
    )
    expect(files).toEqual([
      {
        path: 'github:org/repo/src/index.ts',
        content: 'console.log("hi")',
      },
    ])
  })
  it('fetches tree URLs and filters ignored files', async () => {
    mockFetch.mockImplementation(async (input) => {
      const url = typeof input === 'string' ? input : input.toString()
      if (url.includes('/git/trees/HEAD')) {
        return createJsonResponse({
          tree: [
            { path: 'README.md', type: 'blob', size: 1000 },
            { path: 'node_modules/pkg/index.js', type: 'blob', size: 10 },
            { path: 'src/app.ts', type: 'blob', size: 500 },
          ],
        })
      }
      if (url.includes('/contents/README.md')) {
        return createJsonResponse({
          content: Buffer.from('# Readme').toString('base64'),
          encoding: 'base64',
        })
      }
      if (url.includes('/contents/src/app.ts')) {
        return createJsonResponse({
          content: Buffer.from('export const hi = true').toString('base64'),
          encoding: 'base64',
        })
      }
      throw new Error(`Unexpected fetch ${url}`)
    })
    const files = await resolveGithubUrl(new URL('https://github.com/org/repo'))
    expect(files).toEqual([
      { path: 'github:org/repo/README.md', content: '# Readme' },
      { path: 'github:org/repo/src/app.ts', content: 'export const hi = true' },
    ])
  })
})
const createBufferResponse = (buffer: Buffer, status = 200): Response =>
  ({
    ok: status >= 200 && status < 300,
    status,
    headers: new Headers(),
    arrayBuffer: async () => buffer,
  }) as unknown as Response
const createJsonResponse = (body: unknown, status = 200): Response =>
  ({
    ok: status >= 200 && status < 300,
    status,
    headers: new Headers({ 'content-type': 'application/json' }),
    json: async () => body,
  }) as unknown as Response
</file>
<file path="src/__tests__/help-overlay-scroll.test.ts">
import {
  clampHelpOverlayScrollOffset,
  getHelpOverlayContentRows,
  getHelpOverlayMaxScroll,
  scrollHelpOverlayBy,
} from '../tui/components/core/help-overlay-scroll'
describe('help-overlay-scroll', () => {
  it('computes content rows accounting for header/footer spacing', () => {
    expect(getHelpOverlayContentRows(10)).toBe(5)
    expect(getHelpOverlayContentRows(5)).toBe(1)
  })
  it('clamps scroll offset to valid range', () => {
    const contentRows = 5
    expect(getHelpOverlayMaxScroll(5, contentRows)).toBe(0)
    expect(getHelpOverlayMaxScroll(8, contentRows)).toBe(3)
    expect(clampHelpOverlayScrollOffset(-1, 8, contentRows)).toBe(0)
    expect(clampHelpOverlayScrollOffset(0, 8, contentRows)).toBe(0)
    expect(clampHelpOverlayScrollOffset(3, 8, contentRows)).toBe(3)
    expect(clampHelpOverlayScrollOffset(4, 8, contentRows)).toBe(3)
  })
  it('scrolls by delta and clamps at edges', () => {
    const contentRows = 5
    const lineCount = 8
    expect(scrollHelpOverlayBy(0, 1, lineCount, contentRows)).toBe(1)
    expect(scrollHelpOverlayBy(0, -1, lineCount, contentRows)).toBe(0)
    expect(scrollHelpOverlayBy(3, 1, lineCount, contentRows)).toBe(3)
    expect(scrollHelpOverlayBy(0, contentRows, lineCount, contentRows)).toBe(3)
  })
})
</file>
<file path="src/__tests__/history-logger.test.ts">
import { appendToHistory } from '../history-logger'
jest.mock('node:fs/promises', () => ({
  mkdir: jest.fn(),
  appendFile: jest.fn(),
}))
const fs = jest.requireMock('node:fs/promises') as { mkdir: jest.Mock; appendFile: jest.Mock }
describe('history-logger', () => {
  beforeEach(() => {
    jest.clearAllMocks()
  })
  it('creates the history directory and appends a line', async () => {
    fs.mkdir.mockResolvedValue(undefined)
    fs.appendFile.mockResolvedValue(undefined)
    await appendToHistory({ intent: 'demo' })
    expect(fs.mkdir).toHaveBeenCalledWith(expect.stringContaining('.config'), { recursive: true })
    expect(fs.appendFile).toHaveBeenCalledWith(
      expect.stringContaining('history.jsonl'),
      expect.stringContaining('"intent":"demo"'),
      'utf8',
    )
  })
  it('logs a warning when append fails but does not throw', async () => {
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    fs.mkdir.mockResolvedValue(undefined)
    fs.appendFile.mockRejectedValue(new Error('disk full'))
    await appendToHistory({ intent: 'demo' })
    expect(warn).toHaveBeenCalledWith('Failed to write history entry:', expect.any(Error))
    warn.mockRestore()
  })
})
</file>
<file path="src/__tests__/image-loader.test.ts">
import { resolveImageParts } from '../image-loader'
jest.mock('node:fs/promises', () => ({
  readFile: jest.fn(),
}))
const fs = jest.requireMock('node:fs/promises') as { readFile: jest.Mock }
describe('image-loader', () => {
  beforeEach(() => {
    jest.clearAllMocks()
  })
  it('skips unsupported extensions with a warning', async () => {
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    const parts = await resolveImageParts(['doc.txt'])
    expect(parts).toEqual([])
    expect(warn).toHaveBeenCalledWith('Skipping unsupported image type: doc.txt')
    warn.mockRestore()
  })
  it('returns base64 encoded part for supported image', async () => {
    const buffer = Buffer.from('image-bytes')
    fs.readFile.mockResolvedValue(buffer)
    const onUpload = jest.fn()
    const parts = await resolveImageParts(['photo.png'], onUpload)
    expect(onUpload).toHaveBeenNthCalledWith(1, 'start', { kind: 'image', filePath: 'photo.png' })
    expect(onUpload).toHaveBeenLastCalledWith('finish', { kind: 'image', filePath: 'photo.png' })
    expect(parts).toEqual([
      {
        type: 'image',
        mimeType: 'image/png',
        data: buffer.toString('base64'),
      },
    ])
  })
  it('skips images larger than limit', async () => {
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    const buffer = Buffer.alloc(25 * 1024 * 1024)
    fs.readFile.mockResolvedValue(buffer)
    const parts = await resolveImageParts(['large.jpg'])
    expect(parts).toEqual([])
    expect(warn).toHaveBeenCalledWith(expect.stringContaining('Skipping image too large'))
    warn.mockRestore()
  })
  it('logs and skips unreadable files', async () => {
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    fs.readFile.mockRejectedValue(new Error('nope'))
    const parts = await resolveImageParts(['broken.png'])
    expect(parts).toEqual([])
    expect(warn).toHaveBeenCalledWith('Failed to read image broken.png:', expect.any(Error))
    warn.mockRestore()
  })
})
</file>
<file path="src/__tests__/input-bar.test.ts">
import { estimateInputBarRows } from '../tui/components/core/input-bar-layout'
import { resolveInputBarPresentation } from '../tui/components/core/input-bar-presentation'
describe('InputBar presentation', () => {
  it('renders intent mode styling by default', () => {
    expect(resolveInputBarPresentation('intent')).toEqual({
      borderTone: 'default',
      label: 'Intent / Command',
      labelTone: 'muted',
      labelBold: false,
    })
  })
  it('renders refinement mode with prominent styling', () => {
    expect(resolveInputBarPresentation('refinement')).toEqual({
      borderTone: 'warning',
      label: 'Refinement (Enter to submit Âˇ empty to finish)',
      labelTone: 'warning',
      labelBold: true,
    })
  })
  it('estimates rows without border padding', () => {
    expect(
      estimateInputBarRows({
        value: 'one\ntwo\nthree',
      }),
    ).toBe(5)
    expect(
      estimateInputBarRows({
        value: Array.from({ length: 10 }, (_, index) => `line-${index}`).join('\n'),
      }),
    ).toBe(12)
  })
})
</file>
<file path="src/__tests__/intent-source.test.ts">
import { resolveIntentSource } from '../tui/intent-source'
describe('resolveIntentSource', () => {
  it('returns text intent when no file path is provided', () => {
    const result = resolveIntentSource('  draft prompt ', '   ')
    expect(result).toEqual({ kind: 'text', intent: 'draft prompt' })
  })
  it('prefers the file path when both are provided', () => {
    const result = resolveIntentSource('typed intent', ' ./intent.md ')
    expect(result).toEqual({ kind: 'file', intentFile: './intent.md' })
  })
  it('returns empty when both sources are blank', () => {
    const result = resolveIntentSource('   ', '')
    expect(result).toEqual({ kind: 'empty' })
  })
})
</file>
<file path="src/__tests__/io.test.ts">
import { EventEmitter } from 'node:events'
import { readFromStdin } from '../io'
describe('readFromStdin', () => {
  const originalStdin = process.stdin
  afterEach(() => {
    Object.defineProperty(process, 'stdin', { value: originalStdin, configurable: true })
  })
  it('returns null when stdin is a TTY', async () => {
    Object.defineProperty(process, 'stdin', {
      value: { isTTY: true } as NodeJS.ReadStream,
      configurable: true,
    })
    await expect(readFromStdin()).resolves.toBeNull()
  })
  it('collects data chunks when stdin is piped', async () => {
    class MockStdin extends EventEmitter {
      isTTY = false
      on(event: string, listener: (...args: unknown[]) => void): this {
        super.on(event, listener)
        return this
      }
    }
    const mock = new MockStdin()
    Object.defineProperty(process, 'stdin', { value: mock, configurable: true })
    const promise = readFromStdin()
    mock.emit('data', 'hello ')
    mock.emit('data', Buffer.from('world'))
    mock.emit('end')
    await expect(promise).resolves.toBe('hello world')
  })
  it('rejects when stdin emits an error', async () => {
    class MockStdin extends EventEmitter {
      isTTY = false
      on(event: string, listener: (...args: unknown[]) => void): this {
        super.on(event, listener)
        return this
      }
    }
    const mock = new MockStdin()
    Object.defineProperty(process, 'stdin', { value: mock, configurable: true })
    const promise = readFromStdin()
    mock.emit('error', new Error('boom'))
    await expect(promise).rejects.toThrow('boom')
  })
})
</file>
<file path="src/__tests__/model-filter.test.ts">
import { filterModelOptions, resolveModelPopupQuery } from '../tui/model-filter'
import type { ModelOption } from '../tui/types'
const createOption = (id: string, label: string): ModelOption => ({
  id,
  label,
  provider: 'openai',
  description: `${label} description`,
  capabilities: [],
  source: 'builtin',
})
describe('model-filter helpers', () => {
  it('resets to empty query when input is blank', () => {
    expect(resolveModelPopupQuery('', 'stale')).toBe('')
    expect(resolveModelPopupQuery('   ', 'stale')).toBe('')
  })
  it('keeps debounced query when input is non-empty', () => {
    expect(resolveModelPopupQuery('g', 'gpt')).toBe('gpt')
  })
  it('returns all models when query is blank', () => {
    const options = [createOption('gpt-4o-mini', 'GPT-4o Mini'), createOption('gpt-4o', 'GPT-4o')]
    expect(filterModelOptions('', options).map((option) => option.id)).toEqual([
      'gpt-4o-mini',
      'gpt-4o',
    ])
  })
  it('filters models by substring match', () => {
    const options = [
      createOption('gpt-4o-mini', 'GPT-4o Mini'),
      createOption('gemini-1.5-pro', 'Gemini 1.5 Pro'),
    ]
    expect(filterModelOptions('gem', options).map((option) => option.id)).toEqual([
      'gemini-1.5-pro',
    ])
  })
  it('caps results when a limit is provided', () => {
    const options = [
      createOption('model-a', 'Model A'),
      createOption('model-b', 'Model B'),
      createOption('model-c', 'Model C'),
    ]
    expect(filterModelOptions('model', options, 2).map((option) => option.id)).toEqual([
      'model-a',
      'model-b',
    ])
  })
  it('supports fuzzy subsequence matching', () => {
    const options = [
      createOption('gpt-4o-mini', 'GPT-4o Mini'),
      createOption('gemini-1.5-pro', 'Gemini 1.5 Pro'),
    ]
    expect(filterModelOptions('gmp', options).map((option) => option.id)).toEqual([
      'gemini-1.5-pro',
    ])
  })
  it('supports multi-token matching', () => {
    const options = [
      createOption('gpt-4o-mini', 'GPT-4o Mini'),
      createOption('gemini-1.5-pro', 'Gemini 1.5 Pro'),
    ]
    expect(filterModelOptions('gem pro', options).map((option) => option.id)).toEqual([
      'gemini-1.5-pro',
    ])
  })
})
</file>
<file path="src/__tests__/model-options.test.ts">
const mockLoadCliConfig = jest.fn()
jest.mock('../config', () => ({
  loadCliConfig: mockLoadCliConfig,
}))
describe('model-options helper', () => {
  beforeEach(() => {
    jest.resetModules()
    mockLoadCliConfig.mockReset()
  })
  it('merges config-defined models with built-ins', async () => {
    mockLoadCliConfig.mockResolvedValue({
      promptGenerator: {
        models: [
          { id: 'custom-model', label: 'Custom', provider: 'openai', capabilities: ['fast'] },
          { id: 'gpt-4o-mini', notes: 'override builtin description' },
        ],
      },
    })
    const module = await import('../tui/model-options')
    const result = await module.loadModelOptions()
    const custom = result.options.find((option) => option.id === 'custom-model')
    expect(custom).toBeDefined()
    expect(custom?.source).toBe('config')
    expect(custom?.capabilities).toContain('fast')
    const overridden = result.options.find((option) => option.id === 'gpt-4o-mini')
    expect(overridden?.notes).toBe('override builtin description')
  })
  it('falls back to built-ins when config load fails', async () => {
    mockLoadCliConfig.mockRejectedValue(new Error('bad config'))
    const module = await import('../tui/model-options')
    const result = await module.loadModelOptions()
    expect(result.warning).toMatch(/Failed to load CLI model entries/)
    expect(result.options.length).toBeGreaterThan(0)
  })
  it('selects preferred model id with fallback logic', async () => {
    mockLoadCliConfig.mockResolvedValue({ promptGenerator: { models: [] } })
    const module = await import('../tui/model-options')
    const builtIns = module.getBuiltInModelOptions()
    const requested = module.getPreferredModelId(builtIns, 'does-not-exist')
    expect(builtIns.map((option) => option.id)).toContain(requested)
  })
})
</file>
<file path="src/__tests__/model-popup-options.test.ts">
import { buildModelPopupOptions } from '../tui/model-popup-options'
import type { ModelOption } from '../tui/types'
const createOption = (id: string, provider: ModelOption['provider']): ModelOption => ({
  id,
  label: id,
  provider,
  description: id,
  capabilities: [],
  source: 'builtin',
})
describe('buildModelPopupOptions', () => {
  it('prepends recent models when query is empty', () => {
    const modelOptions: ModelOption[] = [
      createOption('gpt-4o-mini', 'openai'),
      createOption('gemini-1.5-pro', 'gemini'),
      createOption('other-model', 'other'),
    ]
    const result = buildModelPopupOptions({
      query: '',
      modelOptions,
      recentModelIds: ['gemini-1.5-pro', 'missing-model', 'gpt-4o-mini'],
    })
    expect(result.recentCount).toBe(2)
    expect(result.options.map((option) => option.id)).toEqual([
      'gemini-1.5-pro',
      'gpt-4o-mini',
      'other-model',
    ])
  })
  it('groups filtered options by provider while preserving intra-provider score order', () => {
    const modelOptions: ModelOption[] = [
      createOption('gpt-4o-mini', 'openai'),
      createOption('gpt-4o', 'openai'),
      createOption('gemini-1.5-pro', 'gemini'),
      createOption('gemini-1.5-flash', 'gemini'),
    ]
    const result = buildModelPopupOptions({ query: 'gpt mini', modelOptions })
    expect(result.recentCount).toBe(0)
    expect(result.options.map((option) => option.id)).toEqual(['gpt-4o-mini'])
  })
})
</file>
<file path="src/__tests__/model-session.test.ts">
import {
  getLastSessionModel,
  getRecentSessionModels,
  recordRecentSessionModel,
  resetLastSessionModelForTests,
  resetRecentSessionModelsForTests,
  setLastSessionModel,
} from '../tui/model-session'
describe('model-session helpers', () => {
  afterEach(() => {
    resetLastSessionModelForTests()
  })
  it('stores and retrieves the last session model id', () => {
    resetLastSessionModelForTests()
    expect(getLastSessionModel()).toBeNull()
    setLastSessionModel('gpt-4o-mini')
    expect(getLastSessionModel()).toBe('gpt-4o-mini')
  })
  it('clears the session value when set to empty', () => {
    setLastSessionModel('   ')
    expect(getLastSessionModel()).toBeNull()
  })
})
describe('model-session recent models', () => {
  beforeEach(() => {
    resetRecentSessionModelsForTests()
  })
  it('records recent models most-recent first', () => {
    recordRecentSessionModel('gpt-4o-mini')
    recordRecentSessionModel('gemini-1.5-pro')
    expect(getRecentSessionModels()).toEqual(['gemini-1.5-pro', 'gpt-4o-mini'])
  })
  it('deduplicates when recording the same model', () => {
    recordRecentSessionModel('gpt-4o-mini')
    recordRecentSessionModel('gemini-1.5-pro')
    recordRecentSessionModel('gpt-4o-mini')
    expect(getRecentSessionModels()).toEqual(['gpt-4o-mini', 'gemini-1.5-pro'])
  })
  it('caps the list length', () => {
    recordRecentSessionModel('a')
    recordRecentSessionModel('b')
    recordRecentSessionModel('c')
    recordRecentSessionModel('d')
    recordRecentSessionModel('e')
    recordRecentSessionModel('f')
    expect(getRecentSessionModels()).toEqual(['f', 'e', 'd', 'c', 'b'])
  })
})
</file>
<file path="src/__tests__/multiline-text-buffer.test.ts">
import {
  backspace,
  deleteForward,
  getCursorCoordinates,
  getLineCount,
  insertText,
  moveCursorLeft,
  moveCursorRight,
  type MultilineTextBufferState,
} from '../tui/components/core/multiline-text-buffer'
describe('multiline-text-buffer', () => {
  it('counts lines based on newlines', () => {
    expect(getLineCount('')).toBe(1)
    expect(getLineCount('hello')).toBe(1)
    expect(getLineCount('a\nb')).toBe(2)
    expect(getLineCount('a\nb\n')).toBe(3)
  })
  it('inserts text at cursor and advances cursor', () => {
    const initial: MultilineTextBufferState = { value: 'ace', cursor: 1 }
    expect(insertText(initial, 'b')).toEqual({ value: 'abce', cursor: 2 })
  })
  it('supports inserting newlines', () => {
    const initial: MultilineTextBufferState = { value: 'hello', cursor: 5 }
    expect(insertText(initial, '\nworld')).toEqual({ value: 'hello\nworld', cursor: 11 })
  })
  it('strips bracketed paste markers during insert', () => {
    const initial: MultilineTextBufferState = { value: '', cursor: 0 }
    expect(insertText(initial, '\u001b[200~hello\u001b[201~')).toEqual({
      value: 'hello',
      cursor: 5,
    })
    expect(insertText(initial, '[200~hello[201~')).toEqual({ value: 'hello', cursor: 5 })
  })
  it('backspace removes the previous character', () => {
    const initial: MultilineTextBufferState = { value: 'abc', cursor: 2 }
    expect(backspace(initial)).toEqual({ value: 'ac', cursor: 1 })
  })
  it('deleteForward removes the next character', () => {
    const initial: MultilineTextBufferState = { value: 'abc', cursor: 1 }
    expect(deleteForward(initial)).toEqual({ value: 'ac', cursor: 1 })
  })
  it('moves cursor left and right with clamping', () => {
    const start: MultilineTextBufferState = { value: 'abc', cursor: 0 }
    expect(moveCursorLeft(start).cursor).toBe(0)
    expect(moveCursorRight(start).cursor).toBe(1)
    expect(moveCursorRight({ value: 'abc', cursor: 3 }).cursor).toBe(3)
  })
  it('reports cursor coordinates across lines', () => {
    expect(getCursorCoordinates('a\nbc\n', 0)).toEqual({ row: 0, column: 0 })
    expect(getCursorCoordinates('a\nbc\n', 1)).toEqual({ row: 0, column: 1 })
    expect(getCursorCoordinates('a\nbc\n', 2)).toEqual({ row: 1, column: 0 })
    expect(getCursorCoordinates('a\nbc\n', 4)).toEqual({ row: 1, column: 2 })
    expect(getCursorCoordinates('a\nbc\n', 5)).toEqual({ row: 2, column: 0 })
  })
})
</file>
<file path="src/__tests__/new-command.test.ts">
import { planSessionCommand } from '../tui/new-command'
describe('session command planner', () => {
  it('plans /new as reset-only', () => {
    expect(planSessionCommand({ commandId: 'new', lastGeneratedPrompt: 'ignored' })).toEqual({
      type: 'reset-only',
      message: '[new] Session reset.',
    })
  })
  it('plans /reuse with no prompt as reset-only', () => {
    expect(planSessionCommand({ commandId: 'reuse', lastGeneratedPrompt: null })).toEqual({
      type: 'reset-only',
      message: '[reuse] Session reset Âˇ no previous prompt to reuse.',
    })
    expect(planSessionCommand({ commandId: 'reuse', lastGeneratedPrompt: '   ' })).toEqual({
      type: 'reset-only',
      message: '[reuse] Session reset Âˇ no previous prompt to reuse.',
    })
  })
  it('plans /reuse with prompt as reset-and-load-meta', () => {
    expect(planSessionCommand({ commandId: 'reuse', lastGeneratedPrompt: ' hello ' })).toEqual({
      type: 'reset-and-load-meta',
      message: '[reuse] Session reset Âˇ loaded last prompt into meta instructions.',
      metaInstructions: 'hello',
    })
  })
})
</file>
<file path="src/__tests__/notifier.test.ts">
import React from 'react'
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import { ToastProvider, useNotifier } from '../tui/notifier'
const dom = new JSDOM('<!doctype html><html><body></body></html>')
const globalEnv = globalThis as typeof globalThis & {
  window: Window & typeof globalThis
  document: Document
  navigator: Navigator
}
globalEnv.window = dom.window as typeof globalEnv.window
globalEnv.document = dom.window.document as Document
globalEnv.navigator = dom.window.navigator
const createWrapper = (options: Omit<React.ComponentProps<typeof ToastProvider>, 'children'>) => {
  const Wrapper: React.FC<{ children: React.ReactNode }> = ({ children }) =>
    React.createElement(ToastProvider, { ...options, children })
  return Wrapper
}
describe('toast provider notifier', () => {
  beforeEach(() => {
    jest.useFakeTimers()
  })
  afterEach(() => {
    jest.useRealTimers()
  })
  it('auto-dismisses after timeout, then removes after exit animation', () => {
    const wrapper = createWrapper({ exitAnimationMs: 40 })
    const { result } = renderHook(() => useNotifier({ autoDismissMs: 50 }), { wrapper })
    act(() => {
      result.current.notify('Hello')
    })
    expect(result.current.toasts).toHaveLength(1)
    expect(result.current.toasts[0]?.message).toBe('Hello')
    expect(result.current.toasts[0]?.isExiting).toBe(false)
    act(() => {
      jest.advanceTimersByTime(50)
    })
    expect(result.current.toasts).toHaveLength(1)
    expect(result.current.toasts[0]?.isExiting).toBe(true)
    act(() => {
      jest.advanceTimersByTime(40)
    })
    expect(result.current.toasts).toHaveLength(0)
  })
  it('does not let an old timer dismiss a newer toast', () => {
    const wrapper = createWrapper({ exitAnimationMs: 10 })
    const { result } = renderHook(() => useNotifier({ autoDismissMs: 100 }), { wrapper })
    act(() => {
      result.current.notify('First')
    })
    act(() => {
      jest.advanceTimersByTime(60)
    })
    act(() => {
      result.current.notify('Second')
    })
    act(() => {
      jest.advanceTimersByTime(49)
    })
    expect(result.current.toasts.map((toast) => toast.message)).toEqual(['First', 'Second'])
    expect(result.current.toasts[0]?.isExiting).toBe(true)
    expect(result.current.toasts[1]?.isExiting).toBe(false)
    act(() => {
      jest.advanceTimersByTime(1)
    })
    expect(result.current.toasts.map((toast) => toast.message)).toEqual(['Second'])
    act(() => {
      jest.advanceTimersByTime(50)
    })
    expect(result.current.toasts).toHaveLength(1)
    expect(result.current.toasts[0]?.message).toBe('Second')
    expect(result.current.toasts[0]?.isExiting).toBe(true)
    act(() => {
      jest.advanceTimersByTime(10)
    })
    expect(result.current.toasts).toHaveLength(0)
  })
  it('enforces max toast cap by exiting the oldest active toast', () => {
    const wrapper = createWrapper({ maxToasts: 2, exitAnimationMs: 20 })
    const { result } = renderHook(() => useNotifier({ autoDismissMs: 100_000 }), { wrapper })
    act(() => {
      result.current.notify('One')
      result.current.notify('Two')
      result.current.notify('Three')
    })
    expect(result.current.toasts.map((toast) => toast.message)).toEqual(['One', 'Two', 'Three'])
    expect(result.current.toasts[0]?.isExiting).toBe(true)
    expect(result.current.toasts[1]?.isExiting).toBe(false)
    expect(result.current.toasts[2]?.isExiting).toBe(false)
    act(() => {
      jest.advanceTimersByTime(20)
    })
    expect(result.current.toasts.map((toast) => toast.message)).toEqual(['Two', 'Three'])
  })
  it('dismiss-by-id only affects the intended toast', () => {
    const wrapper = createWrapper({ exitAnimationMs: 15 })
    const { result } = renderHook(() => useNotifier({ autoDismissMs: 100_000 }), { wrapper })
    let firstToastId: number | null = null
    act(() => {
      firstToastId = result.current.showToast('First')
      result.current.notify('Second')
    })
    if (firstToastId === null) {
      throw new Error('Expected first toast id')
    }
    const firstId = firstToastId
    act(() => {
      result.current.dismissToast(firstId)
    })
    expect(result.current.toasts.map((toast) => toast.message)).toEqual(['First', 'Second'])
    expect(result.current.toasts[0]?.isExiting).toBe(true)
    expect(result.current.toasts[1]?.isExiting).toBe(false)
    act(() => {
      jest.advanceTimersByTime(15)
    })
    expect(result.current.toasts.map((toast) => toast.message)).toEqual(['Second'])
  })
})
</file>
<file path="src/__tests__/paste-snippet.test.ts">
import {
  BRACKETED_PASTE_END,
  BRACKETED_PASTE_START,
  consumeBracketedPasteChunk,
  createBracketedPasteState,
  createPastedSnippet,
  detectPastedSnippetFromInputChange,
} from '../tui/paste-snippet'
describe('paste-snippet', () => {
  describe('consumeBracketedPasteChunk', () => {
    it('returns no completions for normal typing', () => {
      const state = createBracketedPasteState()
      const result = consumeBracketedPasteChunk(state, 'hello')
      expect(result.completed).toEqual([])
      expect(result.state).toEqual(state)
      expect(result.didSeeBracketedPaste).toBe(false)
    })
    it('captures a complete bracketed paste in a single chunk', () => {
      const state = createBracketedPasteState()
      const chunk = `${BRACKETED_PASTE_START}hello\nworld${BRACKETED_PASTE_END}`
      const result = consumeBracketedPasteChunk(state, chunk)
      expect(result.state.isActive).toBe(false)
      expect(result.completed).toEqual(['hello\nworld'])
      expect(result.didSeeBracketedPaste).toBe(true)
    })
    it('captures a bracketed paste across multiple chunks', () => {
      const state = createBracketedPasteState()
      const first = consumeBracketedPasteChunk(state, `${BRACKETED_PASTE_START}hello`)
      expect(first.state.isActive).toBe(true)
      expect(first.state.buffer).toBe('hello')
      expect(first.completed).toEqual([])
      const second = consumeBracketedPasteChunk(first.state, `\nworld${BRACKETED_PASTE_END}`)
      expect(second.state.isActive).toBe(false)
      expect(second.state.buffer).toBe('')
      expect(second.completed).toEqual(['hello\nworld'])
    })
    it('captures multiple pastes from a single chunk', () => {
      const state = createBracketedPasteState()
      const chunk = `${BRACKETED_PASTE_START}one${BRACKETED_PASTE_END}${BRACKETED_PASTE_START}two${BRACKETED_PASTE_END}`
      const result = consumeBracketedPasteChunk(state, chunk)
      expect(result.completed).toEqual(['one', 'two'])
      expect(result.state.isActive).toBe(false)
    })
  })
  describe('createPastedSnippet', () => {
    it('returns null for text below the threshold', () => {
      expect(createPastedSnippet('hello world')).toBeNull()
      expect(createPastedSnippet('line1\nline2\nline3')).toBeNull()
    })
    it('returns a snippet for text above the threshold', () => {
      const raw = `${'x'.repeat(80)}\nline2\nline3`
      const snippet = createPastedSnippet(raw)
      expect(snippet).not.toBeNull()
      expect(snippet?.lineCount).toBe(3)
      expect(snippet?.label).toBe('[Pasted ~3 lines]')
    })
    it('strips bracketed paste markers from snippet text', () => {
      const rawWithEsc = `\u001b[200~${'x'.repeat(80)}\u001b[201~`
      const snippetWithEsc = createPastedSnippet(rawWithEsc)
      expect(snippetWithEsc).not.toBeNull()
      expect(snippetWithEsc?.text).toBe('x'.repeat(80))
      const rawNoEsc = `[200~${'x'.repeat(80)}[201~`
      const snippetNoEsc = createPastedSnippet(rawNoEsc)
      expect(snippetNoEsc).not.toBeNull()
      expect(snippetNoEsc?.text).toBe('x'.repeat(80))
    })
  })
  describe('detectPastedSnippetFromInputChange', () => {
    it('ignores small multi-line edits', () => {
      expect(detectPastedSnippetFromInputChange('a', 'a\nb')).toBeNull()
    })
    it('ignores small single-line edits', () => {
      expect(detectPastedSnippetFromInputChange('hello', 'hello there')).toBeNull()
    })
    it('detects a large multi-line paste', () => {
      const large = `${'x'.repeat(90)}\n${'y'.repeat(10)}`
      const detection = detectPastedSnippetFromInputChange('', large)
      expect(detection).not.toBeNull()
      expect(detection?.snippet.lineCount).toBe(2)
      expect(detection?.snippet.charCount).toBe(101)
      expect(detection?.range).toEqual({ start: 0, end: large.length })
    })
    it('detects a large single-line paste', () => {
      const large = 'x'.repeat(500)
      const detection = detectPastedSnippetFromInputChange('', large)
      expect(detection).not.toBeNull()
      expect(detection?.snippet.charCount).toBe(500)
      expect(detection?.snippet.lineCount).toBe(1)
      expect(detection?.range).toEqual({ start: 0, end: large.length })
    })
  })
})
</file>
<file path="src/__tests__/provider-chip.test.ts">
import { formatProviderStatusChip } from '../tui/provider-chip'
import type { ProviderStatusMap } from '../tui/types'
describe('formatProviderStatusChip', () => {
  it('formats the selected provider chip for OpenAI models', () => {
    const statuses: ProviderStatusMap = {
      openai: { provider: 'openai', status: 'ok', message: 'ready' },
      gemini: { provider: 'gemini', status: 'ok', message: 'ready' },
      other: { provider: 'other', status: 'ok', message: 'ready' },
    }
    expect(formatProviderStatusChip('gpt-4o-mini', statuses)).toBe('[openai:ok]')
  })
  it('formats missing credentials as missing-key', () => {
    const statuses: ProviderStatusMap = {
      openai: { provider: 'openai', status: 'ok', message: 'ready' },
      gemini: { provider: 'gemini', status: 'missing', message: 'missing' },
      other: { provider: 'other', status: 'ok', message: 'ready' },
    }
    expect(formatProviderStatusChip('gemini-1.5-pro', statuses)).toBe('[gemini:missing-key]')
  })
  it('formats errors as error', () => {
    const statuses: ProviderStatusMap = {
      openai: { provider: 'openai', status: 'error', message: 'boom' },
      gemini: { provider: 'gemini', status: 'ok', message: 'ready' },
      other: { provider: 'other', status: 'ok', message: 'ready' },
    }
    expect(formatProviderStatusChip('gpt-4o-mini', statuses)).toBe('[openai:error]')
  })
  it('formats unknown providers as other', () => {
    const statuses: ProviderStatusMap = {
      openai: { provider: 'openai', status: 'ok', message: 'ready' },
      gemini: { provider: 'gemini', status: 'ok', message: 'ready' },
      other: { provider: 'other', status: 'ok', message: 'Custom provider (not validated)' },
    }
    expect(formatProviderStatusChip('my-local-model', statuses)).toBe('[other:ok]')
  })
})
</file>
<file path="src/__tests__/provider-status.test.ts">
const mockResolveOpenAiCredentials = jest.fn()
const mockResolveGeminiCredentials = jest.fn()
jest.mock('../config', () => ({
  resolveOpenAiCredentials: mockResolveOpenAiCredentials,
  resolveGeminiCredentials: mockResolveGeminiCredentials,
}))
describe('provider-status utility', () => {
  beforeEach(() => {
    jest.resetModules()
    mockResolveOpenAiCredentials.mockReset()
    mockResolveGeminiCredentials.mockReset()
  })
  it('returns ok status when credentials resolve', async () => {
    mockResolveOpenAiCredentials.mockResolvedValue({ apiKey: 'ok' })
    const { checkProviderStatus, invalidateProviderStatus } = await import('../tui/provider-status')
    invalidateProviderStatus()
    const status = await checkProviderStatus('openai')
    expect(status).toEqual({ provider: 'openai', status: 'ok', message: 'Credentials available' })
  })
  it('returns missing status when credentials are absent', async () => {
    mockResolveGeminiCredentials.mockRejectedValue(new Error('Missing GEMINI_API_KEY'))
    const { checkProviderStatus, invalidateProviderStatus } = await import('../tui/provider-status')
    invalidateProviderStatus('gemini')
    const status = await checkProviderStatus('gemini')
    expect(status.status).toBe('missing')
    expect(status.message).toContain('Missing GEMINI_API_KEY')
  })
  it('caches provider lookups', async () => {
    mockResolveOpenAiCredentials.mockResolvedValue({ apiKey: 'cached' })
    const { checkProviderStatus, invalidateProviderStatus } = await import('../tui/provider-status')
    invalidateProviderStatus('openai')
    await checkProviderStatus('openai')
    await checkProviderStatus('openai')
    expect(mockResolveOpenAiCredentials).toHaveBeenCalledTimes(1)
  })
})
</file>
<file path="src/__tests__/series-path.test.ts">
import {
  buildSeriesOutputDirName,
  sanitizeForPathSegment,
  sanitizeForPathSegmentWithHash,
} from '../utils/series-path'
describe('series-path', () => {
  test('sanitizeForPathSegment slugifies and truncates', () => {
    expect(sanitizeForPathSegment('Hello, World!', 'fallback')).toBe('hello-world')
    expect(sanitizeForPathSegment('---', 'fallback')).toBe('fallback')
    expect(sanitizeForPathSegment('Hello, World!', 'fallback', 5)).toBe('hello')
  })
  test('sanitizeForPathSegmentWithHash keeps within maxLength', () => {
    const longValue = 'a'.repeat(500)
    const result = sanitizeForPathSegmentWithHash(longValue, 'fallback', 40)
    expect(result.length).toBeLessThanOrEqual(40)
    expect(result).toMatch(/-[0-9a-f]{8}$/)
  })
  test('buildSeriesOutputDirName caps intent slug length', () => {
    const intent =
      'We need to replace the DataTable component in the Pricing Rules section with the same library component in the Fee Manager'
    const date = new Date(2025, 11, 27, 11, 46, 55)
    const dirName = buildSeriesOutputDirName(intent, date)
    expect(dirName.startsWith('20251227-114655-')).toBe(true)
    const intentSlug = dirName.replace(/^\d{8}-\d{6}-/, '')
    expect(intentSlug.length).toBeLessThanOrEqual(80)
    expect(intentSlug).not.toMatch(/\s/)
  })
})
</file>
<file path="src/__tests__/smart-context-service.test.ts">
import path from 'node:path'
import fg from 'fast-glob'
import { resolveSmartContextFiles } from '../smart-context-service'
jest.mock('fast-glob')
jest.mock('node:fs/promises', () => ({
  stat: jest.fn(),
  readFile: jest.fn(),
}))
jest.mock('../rag/vector-store', () => ({
  indexFiles: jest.fn(),
  search: jest.fn(),
}))
const globMock = fg as unknown as jest.MockedFunction<typeof fg>
const fs = jest.requireMock('node:fs/promises') as { stat: jest.Mock; readFile: jest.Mock }
const vectorStore = jest.requireMock('../rag/vector-store') as {
  indexFiles: jest.Mock
  search: jest.Mock
}
describe('smart-context-service', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    globMock.mockResolvedValue([])
    fs.stat.mockResolvedValue({ size: 1000 })
    fs.readFile.mockResolvedValue('contents')
    vectorStore.search.mockResolvedValue([])
  })
  it('returns empty array when no files match glob patterns', async () => {
    const onProgress = jest.fn()
    const result = await resolveSmartContextFiles('intent', [], onProgress)
    expect(result).toEqual([])
    expect(onProgress).toHaveBeenCalledWith('No smart context files found')
  })
  it('filters by size, indexes, searches, and returns new files', async () => {
    globMock.mockResolvedValue(['/repo/a.md', '/repo/b.md'])
    vectorStore.search.mockResolvedValue(['/repo/a.md', '/repo/b.md'])
    const result = await resolveSmartContextFiles(
      'intent',
      [{ path: '/repo/a.md', content: '' }],
      undefined,
    )
    expect(vectorStore.indexFiles).toHaveBeenCalledWith(['/repo/a.md', '/repo/b.md'])
    expect(result).toEqual([{ path: '/repo/b.md', content: 'contents' }])
  })
  it('scans the provided smart context root when supplied', async () => {
    globMock.mockResolvedValue([])
    await resolveSmartContextFiles('intent', [], undefined, './packages')
    expect(globMock).toHaveBeenCalledWith(
      expect.any(Array),
      expect.objectContaining({ cwd: path.resolve('./packages') }),
    )
  })
  it('logs warning and returns [] when indexing fails', async () => {
    globMock.mockResolvedValue(['/repo/a.md'])
    vectorStore.indexFiles.mockRejectedValue(new Error('fail'))
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    const result = await resolveSmartContextFiles('intent', [], undefined)
    expect(result).toEqual([])
    expect(warn).toHaveBeenCalledWith('Smart context indexing failed: fail')
    warn.mockRestore()
  })
  it('skips files that fail to read while still returning others', async () => {
    globMock.mockResolvedValue(['/repo/a.md', '/repo/b.md'])
    vectorStore.indexFiles.mockResolvedValue(undefined)
    vectorStore.search.mockResolvedValue(['/repo/a.md', '/repo/b.md'])
    fs.readFile.mockResolvedValueOnce('content-a').mockRejectedValueOnce(new Error('nope'))
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    const result = await resolveSmartContextFiles('intent', [], undefined)
    expect(result).toEqual([{ path: '/repo/a.md', content: 'content-a' }])
    expect(warn).toHaveBeenCalledWith('Warning: Failed to read smart context file /repo/b.md: nope')
    warn.mockRestore()
  })
})
</file>
<file path="src/__tests__/string-filter.test.ts">
import { filterStringsByQuery } from '../tui/string-filter'
describe('filterStringsByQuery', () => {
  it('returns all items unchanged for an empty query', () => {
    expect(filterStringsByQuery(['b', 'a', 'c'], '   ')).toEqual(['b', 'a', 'c'])
  })
  it('matches case-insensitively and preserves input order', () => {
    expect(filterStringsByQuery(['Readme.md', 'src/app.ts', 'docs/guide.md'], 'MD')).toEqual([
      'Readme.md',
      'docs/guide.md',
    ])
  })
  it('prefers prefix matches over substring matches', () => {
    expect(
      filterStringsByQuery(['packages/readme.md', 'docs/readme.md', 'src/readme.ts'], 'src'),
    ).toEqual(['src/readme.ts'])
  })
  it('returns prefix matches first, then substring matches', () => {
    expect(filterStringsByQuery(['abc', 'xabc', 'ab', 'zab'], 'ab')).toEqual([
      'abc',
      'ab',
      'xabc',
      'zab',
    ])
  })
})
</file>
<file path="src/__tests__/test-command.test.ts">
import { runTestCommand } from '../test-command'
jest.mock('node:fs/promises', () => ({ readFile: jest.fn() }))
jest.mock('js-yaml', () => ({ load: jest.fn() }))
jest.mock('../testing/test-schema', () => ({ parsePromptTestSuite: jest.fn() }))
jest.mock('../testing/evaluator', () => ({ evaluatePrompt: jest.fn() }))
jest.mock('../prompt-generator-service', () => ({
  createPromptGeneratorService: jest.fn(),
  resolveDefaultGenerateModel: jest.fn().mockResolvedValue('gpt-4o-mini'),
}))
jest.mock('../file-context', () => ({ resolveFileContext: jest.fn().mockResolvedValue([]) }))
jest.mock('../smart-context-service', () => ({
  resolveSmartContextFiles: jest.fn().mockResolvedValue([]),
}))
const fs = jest.requireMock('node:fs/promises') as { readFile: jest.Mock }
const yaml = jest.requireMock('js-yaml') as { load: jest.Mock }
const schema = jest.requireMock('../testing/test-schema') as { parsePromptTestSuite: jest.Mock }
const evaluator = jest.requireMock('../testing/evaluator') as { evaluatePrompt: jest.Mock }
const { createPromptGeneratorService, resolveDefaultGenerateModel } = jest.requireMock(
  '../prompt-generator-service',
) as {
  createPromptGeneratorService: jest.Mock
  resolveDefaultGenerateModel: jest.Mock
}
const smartContext = jest.requireMock('../smart-context-service') as {
  resolveSmartContextFiles: jest.Mock
}
const service = {
  generatePrompt: jest.fn(),
}
createPromptGeneratorService.mockResolvedValue(service)
describe('runTestCommand', () => {
  const originalStdoutTty = process.stdout.isTTY
  const originalExitCode = process.exitCode
  beforeEach(() => {
    jest.clearAllMocks()
    createPromptGeneratorService.mockResolvedValue(service)
    resolveDefaultGenerateModel.mockResolvedValue('gpt-4o-mini')
    service.generatePrompt.mockResolvedValue('generated prompt')
    evaluator.evaluatePrompt.mockResolvedValue({ pass: true, reason: 'ok' })
    process.exitCode = undefined
    Object.defineProperty(process.stdout, 'isTTY', { value: false, configurable: true })
  })
  afterAll(() => {
    Object.defineProperty(process.stdout, 'isTTY', { value: originalStdoutTty })
    process.exitCode = originalExitCode
  })
  it('loads default prompt-tests.yaml when no file provided', async () => {
    fs.readFile.mockResolvedValue('suite')
    yaml.load.mockReturnValue({ tests: [] })
    schema.parsePromptTestSuite.mockReturnValue({ tests: [] })
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runTestCommand([])
    expect(fs.readFile).toHaveBeenCalledWith(expect.stringContaining('prompt-tests.yaml'), 'utf8')
    log.mockRestore()
  })
  it('executes tests and evaluates prompts', async () => {
    fs.readFile.mockResolvedValue('suite')
    const tests = [{ name: 'Case 1', intent: 'intent', expect: { contains: 'result' } }]
    yaml.load.mockReturnValue({ tests })
    schema.parsePromptTestSuite.mockReturnValue({ tests })
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runTestCommand(['custom.yaml'])
    expect(service.generatePrompt).toHaveBeenCalledWith(
      expect.objectContaining({ intent: 'intent', model: 'gpt-4o-mini' }),
    )
    expect(evaluator.evaluatePrompt).toHaveBeenCalledWith('generated prompt', {
      contains: 'result',
    })
    log.mockRestore()
  })
  it('invokes smart context resolution when enabled for a test', async () => {
    fs.readFile.mockResolvedValue('suite')
    const tests = [{ name: 'Smart', intent: 'intent', expect: {}, smartContext: true }]
    yaml.load.mockReturnValue({ tests })
    schema.parsePromptTestSuite.mockReturnValue({ tests })
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    smartContext.resolveSmartContextFiles.mockResolvedValue([
      { path: 'smart.md', content: 'smart' },
    ])
    await runTestCommand([])
    expect(smartContext.resolveSmartContextFiles).toHaveBeenCalledWith(
      'intent',
      [],
      expect.any(Function),
      undefined,
    )
    log.mockRestore()
  })
  it('passes smartContextRoot from test definitions when provided', async () => {
    fs.readFile.mockResolvedValue('suite')
    const tests = [
      {
        name: 'Smart Root',
        intent: 'intent',
        expect: {},
        smartContext: true,
        smartContextRoot: './src',
      },
    ]
    yaml.load.mockReturnValue({ tests })
    schema.parsePromptTestSuite.mockReturnValue({ tests })
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    smartContext.resolveSmartContextFiles.mockResolvedValue([])
    await runTestCommand([])
    expect(smartContext.resolveSmartContextFiles).toHaveBeenCalledWith(
      'intent',
      [],
      expect.any(Function),
      './src',
    )
    log.mockRestore()
  })
  it('sets process.exitCode when failures occur', async () => {
    fs.readFile.mockResolvedValue('suite')
    const tests = [{ name: 'Case', intent: 'intent', expect: {} }]
    yaml.load.mockReturnValue({ tests })
    schema.parsePromptTestSuite.mockReturnValue({ tests })
    evaluator.evaluatePrompt.mockResolvedValue({ pass: false, reason: 'bad' })
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runTestCommand([])
    expect(process.exitCode).toBe(1)
    log.mockRestore()
  })
})
</file>
<file path="src/__tests__/test-runner-reducer.test.ts">
import {
  INITIAL_TEST_RUNNER_STATE,
  testRunnerReducer,
} from '../tui/screens/test-runner/test-runner-reducer'
describe('testRunnerReducer', () => {
  it('updates the file path', () => {
    const next = testRunnerReducer(INITIAL_TEST_RUNNER_STATE, {
      type: 'set-file-path',
      next: 'other.yaml',
    })
    expect(next.filePath).toBe('other.yaml')
  })
  it('starts a run and clears transient state', () => {
    const seeded = {
      ...INITIAL_TEST_RUNNER_STATE,
      tests: [{ name: 'a', status: 'fail' as const, reason: 'nope' }],
      error: 'boom',
      summary: { passed: 1, failed: 2 },
    }
    const next = testRunnerReducer(seeded, { type: 'run-start' })
    expect(next.status).toBe('running')
    expect(next.tests).toEqual([])
    expect(next.error).toBeNull()
    expect(next.summary).toBeNull()
  })
  it('loads suite tests as pending', () => {
    const afterLoad = testRunnerReducer(INITIAL_TEST_RUNNER_STATE, {
      type: 'suite-loaded',
      loadedPath: '/tmp/prompt-tests.yaml',
      testNames: ['t1', 't2'],
    })
    expect(afterLoad.lastRunFile).toBe('/tmp/prompt-tests.yaml')
    expect(afterLoad.tests).toEqual([
      { name: 't1', status: 'pending', reason: null },
      { name: 't2', status: 'pending', reason: null },
    ])
  })
  it('updates a test status by ordinal', () => {
    const withSuite = testRunnerReducer(INITIAL_TEST_RUNNER_STATE, {
      type: 'suite-loaded',
      loadedPath: '/tmp/prompt-tests.yaml',
      testNames: ['t1', 't2'],
    })
    const running = testRunnerReducer(withSuite, { type: 'test-start', ordinal: 2, name: 't2' })
    expect(running.tests[1]).toEqual({ name: 't2', status: 'running', reason: null })
    const failed = testRunnerReducer(running, {
      type: 'test-complete',
      ordinal: 2,
      name: 't2',
      pass: false,
      reason: 'bad',
    })
    expect(failed.tests[1]).toEqual({ name: 't2', status: 'fail', reason: 'bad' })
  })
  it('completes run with summary and returns to idle', () => {
    const running = testRunnerReducer(INITIAL_TEST_RUNNER_STATE, { type: 'run-start' })
    const next = testRunnerReducer(running, { type: 'run-complete', passed: 3, failed: 1 })
    expect(next.status).toBe('idle')
    expect(next.summary).toEqual({ passed: 3, failed: 1 })
  })
  it('stores errors and returns to idle', () => {
    const running = testRunnerReducer(INITIAL_TEST_RUNNER_STATE, { type: 'run-start' })
    const next = testRunnerReducer(running, { type: 'run-error', message: 'boom' })
    expect(next.status).toBe('idle')
    expect(next.error).toBe('boom')
  })
})
</file>
<file path="src/__tests__/text-input-keys.test.ts">
import type { Key } from 'ink'
import { resolveSingleLineKeyAction } from '../tui/components/core/single-line-text-input-keymap'
import { isBackspaceKey } from '../tui/components/core/text-input-keys'
const createKey = (overrides: Partial<Key> = {}): Key => overrides as Key
describe('isBackspaceKey', () => {
  it('recognizes ink backspace flag', () => {
    expect(isBackspaceKey('', createKey({ backspace: true }))).toBe(true)
  })
  it('recognizes DEL and BS characters', () => {
    expect(isBackspaceKey('\u007f', createKey())).toBe(true)
    expect(isBackspaceKey('\b', createKey())).toBe(true)
  })
  it('recognizes Ctrl+H and Ctrl+?', () => {
    expect(isBackspaceKey('h', createKey({ ctrl: true }))).toBe(true)
    expect(isBackspaceKey('?', createKey({ ctrl: true }))).toBe(true)
  })
  it('recognizes Kitty CSI-u style sequences', () => {
    expect(isBackspaceKey('\u001b[127u', createKey())).toBe(true)
    expect(isBackspaceKey('\u001b[8u', createKey())).toBe(true)
    expect(isBackspaceKey('\u001b[51u', createKey())).toBe(true)
  })
  it('recognizes Kitty tilde backspace sequences', () => {
    expect(isBackspaceKey('\u001b[127~', createKey())).toBe(true)
    expect(isBackspaceKey('\u001b[8~', createKey())).toBe(true)
    expect(isBackspaceKey('\u001b[51~', createKey())).toBe(true)
  })
  it('treats delete-with-empty-input as backspace', () => {
    expect(isBackspaceKey('', createKey({ delete: true }))).toBe(true)
  })
  it('does not match regular input', () => {
    expect(isBackspaceKey('a', createKey())).toBe(false)
  })
})
describe('resolveSingleLineKeyAction', () => {
  it('applies Kitty CSI-u backspace to the buffer', () => {
    const action = resolveSingleLineKeyAction({
      input: '\u001b[127u',
      key: createKey({}),
      state: { value: 'abc', cursor: 3 },
    })
    expect(action).toEqual({ type: 'change', nextState: { value: 'ab', cursor: 2 } })
  })
  it('treats delete-with-empty-input as a backspace edit', () => {
    const action = resolveSingleLineKeyAction({
      input: '',
      key: createKey({ delete: true }),
      state: { value: 'abc', cursor: 3 },
    })
    expect(action).toEqual({ type: 'change', nextState: { value: 'ab', cursor: 2 } })
  })
})
</file>
<file path="src/__tests__/token-counter.test.ts">
describe('token-counter', () => {
  afterEach(() => {
    jest.resetModules()
  })
  it('counts tokens using the encoder path when available', async () => {
    const module = await import('../token-counter')
    const tokens = module.countTokens('abcd')
    expect(tokens).toBeGreaterThan(0)
  })
  it('falls back to heuristic when encoder fails', async () => {
    jest.doMock('js-tiktoken', () => ({
      getEncoding: () => ({
        encode: () => {
          throw new Error('boom')
        },
      }),
    }))
    const module = await import('../token-counter')
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    expect(module.countTokens('abcd')).toBe(Math.ceil('abcd'.length / 4))
    expect(warn).toHaveBeenCalledWith('Token counting failed, defaulting to character heuristic.')
    warn.mockRestore()
  })
  it('formats token counts with severity tiers', async () => {
    const module = await import('../token-counter')
    expect(module.formatTokenCount(120000)).toBe('â ď¸ 120,000 tokens (High)')
    expect(module.formatTokenCount(40000)).toBe('40,000 tokens (Medium)')
    expect(module.formatTokenCount(100)).toBe('100 tokens')
  })
})
</file>
<file path="src/__tests__/token-usage-store.test.ts">
import { createTokenUsageStore } from '../tui/token-usage-store'
describe('token-usage-store', () => {
  it('computes totals and estimates cost when supported', () => {
    const store = createTokenUsageStore()
    const runId = store.startRun('gpt-4o-mini')
    store.recordTelemetry(runId, {
      files: [],
      intentTokens: 100,
      fileTokens: 200,
      systemTokens: 50,
      totalTokens: 350,
    })
    store.recordIteration(runId, { iteration: 1, promptTokens: 75, reasoningTokens: 25 })
    const breakdown = store.getLatestBreakdown()
    expect(breakdown).not.toBeNull()
    expect(breakdown?.input.total).toBe(350)
    expect(breakdown?.output.total).toBe(100)
    expect(breakdown?.totals.tokens).toBe(450)
    expect(breakdown?.totals.estimatedCostUsd).toBe(0.000113)
  })
  it('deduplicates iterations by iteration number', () => {
    const store = createTokenUsageStore()
    const runId = store.startRun('gpt-4o-mini')
    store.recordTelemetry(runId, {
      files: [],
      intentTokens: 1,
      fileTokens: 1,
      systemTokens: 1,
      totalTokens: 3,
    })
    store.recordIteration(runId, { iteration: 1, promptTokens: 5, reasoningTokens: 0 })
    store.recordIteration(runId, { iteration: 1, promptTokens: 6, reasoningTokens: 2 })
    const run = store.getLatestRun()
    expect(run?.iterations).toEqual([{ iteration: 1, promptTokens: 6, reasoningTokens: 2 }])
  })
})
</file>
<file path="src/__tests__/tui-command-history.test.ts">
import fs from 'node:fs/promises'
import {
  readCommandHistory,
  updateCommandHistory,
  writeCommandHistory,
  type CommandHistoryRecord,
} from '../tui/command-history'
jest.mock('node:os', () => ({ homedir: jest.fn(() => '/home/tester') }))
jest.mock('node:fs/promises', () => ({
  readFile: jest.fn(),
  mkdir: jest.fn(),
  rename: jest.fn(),
  writeFile: jest.fn(),
}))
const fsMock = fs as unknown as {
  readFile: jest.Mock
  mkdir: jest.Mock
  rename: jest.Mock
  writeFile: jest.Mock
}
describe('tui command history', () => {
  beforeEach(() => {
    jest.clearAllMocks()
  })
  it('adds new entries to the front and dedupes consecutive repeats', () => {
    const start: CommandHistoryRecord[] = []
    const first = updateCommandHistory({ previous: start, nextValue: ' hello ', maxEntries: 3 })
    expect(first[0]?.value).toBe('hello')
    const repeated = updateCommandHistory({ previous: first, nextValue: 'hello', maxEntries: 3 })
    expect(repeated).toBe(first)
    const second = updateCommandHistory({ previous: first, nextValue: 'world', maxEntries: 3 })
    expect(second.map((entry) => entry.value)).toEqual(['world', 'hello'])
  })
  it('limits stored entries to maxEntries', () => {
    const first = updateCommandHistory({ previous: [], nextValue: 'one', maxEntries: 2 })
    const second = updateCommandHistory({ previous: first, nextValue: 'two', maxEntries: 2 })
    const third = updateCommandHistory({ previous: second, nextValue: 'three', maxEntries: 2 })
    expect(third.map((entry) => entry.value)).toEqual(['three', 'two'])
  })
  it('returns an empty list when the history file is missing', async () => {
    const enoent = Object.assign(new Error('missing'), { code: 'ENOENT' })
    fsMock.readFile.mockRejectedValueOnce(enoent)
    await expect(readCommandHistory()).resolves.toEqual([])
  })
  it('returns an empty list when the history file is empty', async () => {
    fsMock.readFile.mockResolvedValueOnce('   \n')
    await expect(readCommandHistory()).resolves.toEqual([])
  })
  it('repairs corrupt JSON history files', async () => {
    fsMock.readFile.mockResolvedValueOnce('[')
    fsMock.mkdir.mockResolvedValueOnce(undefined)
    fsMock.rename.mockResolvedValueOnce(undefined)
    fsMock.writeFile.mockResolvedValueOnce(undefined)
    await expect(readCommandHistory()).resolves.toEqual([])
    expect(fsMock.rename).toHaveBeenCalledWith(
      '/home/tester/.config/prompt-maker-cli/tui-history.json',
      expect.stringContaining('/home/tester/.config/prompt-maker-cli/tui-history.corrupt-'),
    )
    expect(fsMock.writeFile).toHaveBeenCalledWith(
      '/home/tester/.config/prompt-maker-cli/tui-history.json',
      '[]\n',
      'utf8',
    )
  })
  it('writes history entries to the config directory', async () => {
    fsMock.mkdir.mockResolvedValueOnce(undefined)
    fsMock.writeFile.mockResolvedValueOnce(undefined)
    fsMock.rename.mockResolvedValueOnce(undefined)
    await writeCommandHistory([{ value: 'cmd', timestamp: 'now' }])
    expect(fsMock.mkdir).toHaveBeenCalledWith('/home/tester/.config/prompt-maker-cli', {
      recursive: true,
    })
    const tempFile = `/home/tester/.config/prompt-maker-cli/tui-history.json.${process.pid}.tmp`
    expect(fsMock.writeFile).toHaveBeenCalledWith(
      tempFile,
      expect.stringContaining('"value": "cmd"'),
      'utf8',
    )
    expect(fsMock.rename).toHaveBeenCalledWith(
      tempFile,
      '/home/tester/.config/prompt-maker-cli/tui-history.json',
    )
  })
})
</file>
<file path="src/__tests__/url-context.test.ts">
import { resolveUrlContext } from '../url-context'
import { resolveGithubUrl } from '../github-context'
jest.mock('../github-context', () => ({
  resolveGithubUrl: jest.fn().mockResolvedValue([]),
}))
describe('resolveUrlContext', () => {
  const originalFetch = global.fetch
  const mockFetch = jest.fn() as jest.MockedFunction<typeof fetch>
  beforeEach(() => {
    mockFetch.mockReset()
    global.fetch = mockFetch
  })
  afterAll(() => {
    global.fetch = originalFetch
  })
  it('returns text content for generic HTML pages', async () => {
    const html = '<html><body><h1>Title</h1><p>Paragraph</p></body></html>'
    mockFetch.mockResolvedValueOnce(createBufferResponse(Buffer.from(html)))
    const results = await resolveUrlContext(['https://example.com/page'])
    expect(mockFetch).toHaveBeenCalledWith('https://example.com/page', expect.any(Object))
    expect(results).toHaveLength(1)
    const file = results[0]
    if (!file) {
      throw new Error('Expected resolveUrlContext to return one file')
    }
    expect(file.path).toBe('url:https://example.com/page')
    expect(file.content.toLowerCase()).toContain('title')
    expect(file.content).toContain('Paragraph')
  })
  it('deduplicates URLs and warns on invalid entries', async () => {
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    const html = '<html><body>Doc</body></html>'
    mockFetch.mockResolvedValue(createBufferResponse(Buffer.from(html)))
    const results = await resolveUrlContext([
      '',
      'notaurl',
      'https://example.com',
      'https://example.com',
    ])
    expect(results).toHaveLength(1)
    expect(mockFetch).toHaveBeenCalledTimes(1)
    expect(warn).toHaveBeenCalled()
    warn.mockRestore()
  })
  it('delegates GitHub URLs to the GitHub resolver', async () => {
    const githubResult = [{ path: 'github:owner/repo/file.ts', content: 'code' }]
    ;(resolveGithubUrl as jest.Mock).mockResolvedValueOnce(githubResult)
    const results = await resolveUrlContext(['https://github.com/owner/repo'])
    expect(resolveGithubUrl).toHaveBeenCalledWith(
      expect.objectContaining({ href: 'https://github.com/owner/repo' }),
      undefined,
    )
    expect(results).toEqual(githubResult)
  })
})
const createBufferResponse = (buffer: Buffer, status = 200): Response =>
  ({
    ok: status >= 200 && status < 300,
    status,
    headers: new Headers({ 'content-length': buffer.byteLength.toString() }),
    arrayBuffer: async () => buffer,
  }) as unknown as Response
</file>
<file path="src/__tests__/useCommandHistory.test.ts">
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import { useCommandHistory } from '../tui/hooks/useCommandHistory'
import type { HistoryEntry } from '../tui/types'
const dom = new JSDOM('<!doctype html><html><body></body></html>')
const globalEnv = globalThis as typeof globalThis & {
  window: Window & typeof globalThis
  document: Document
  navigator: Navigator
}
globalEnv.window = dom.window as typeof globalEnv.window
globalEnv.document = dom.window.document as Document
globalEnv.navigator = dom.window.navigator
describe('useCommandHistory', () => {
  it('resets history back to initial entries', () => {
    const initialEntries: HistoryEntry[] = [
      { id: 'seed-0', content: 'Welcome', kind: 'system' },
      { id: 'seed-1', content: 'Tip', kind: 'system' },
    ]
    const { result } = renderHook(() =>
      useCommandHistory({
        initialEntries,
        visibleRows: 10,
      }),
    )
    act(() => {
      result.current.pushHistory('User message', 'user')
    })
    expect(result.current.history).toHaveLength(3)
    act(() => {
      result.current.resetHistory()
    })
    expect(result.current.history).toEqual(initialEntries)
  })
  it('clears history to an empty buffer', () => {
    const initialEntries: HistoryEntry[] = [
      { id: 'seed-0', content: 'Welcome', kind: 'system' },
      { id: 'seed-1', content: 'Tip', kind: 'system' },
    ]
    const { result } = renderHook(() =>
      useCommandHistory({
        initialEntries,
        visibleRows: 10,
      }),
    )
    act(() => {
      result.current.pushHistory('User message', 'user')
    })
    act(() => {
      result.current.clearHistory()
    })
    expect(result.current.history).toEqual([])
  })
})
</file>
<file path="src/generate/actions.ts">
import chalk from 'chalk'
import clipboard from 'clipboardy'
import open from 'open'
import { shouldTraceCopy } from './debug'
export const maybeCopyToClipboard = async (
  shouldCopy: boolean,
  prompt: string,
  showFeedback: boolean,
): Promise<void> => {
  const traceEnabled = shouldTraceCopy()
  const trace = (message: string): void => {
    if (traceEnabled) {
      console.error(chalk.dim(`[pmc:copy] ${message}`))
    }
  }
  if (!shouldCopy) {
    trace('Skipping clipboard write (flag not provided).')
    return
  }
  trace(`Attempting clipboard write (${prompt.length.toLocaleString()} chars).`)
  try {
    await clipboard.write(prompt)
    if (showFeedback) {
      console.log(chalk.green('â Copied prompt to clipboard.'))
    } else {
      trace('Copied prompt to clipboard (feedback suppressed).')
    }
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown clipboard error.'
    trace(`Clipboard write failed: ${message}`)
    console.warn(chalk.yellow(`Failed to copy prompt to clipboard: ${message}`))
  }
}
export const maybeOpenChatGpt = async (
  shouldOpen: boolean,
  prompt: string,
  showFeedback: boolean,
): Promise<void> => {
  if (!shouldOpen) {
    return
  }
  const url = `https://chatgpt.com/?q=${encodeURIComponent(prompt)}`
  try {
    await open(url)
    if (showFeedback) {
      console.log(chalk.green('â Opened ChatGPT with the generated prompt.'))
    }
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown browser error.'
    console.warn(chalk.yellow(`Failed to open ChatGPT: ${message}`))
  }
}
</file>
<file path="src/generate/command.ts">
import { parseGenerateArgs } from './args'
import { runGeneratePipeline } from './pipeline'
export const runGenerateCommand = async (argv: string[]): Promise<void> => {
  const { args, showHelp } = parseGenerateArgs(argv)
  if (args.help) {
    showHelp()
    return
  }
  await runGeneratePipeline(args)
}
</file>
<file path="src/generate/context-output.ts">
import fs from 'node:fs/promises'
import chalk from 'chalk'
import type { FileContext } from '../file-context'
export const displayContextFiles = (
  files: FileContext[],
  format: 'text' | 'json',
  writeLine: (line: string) => void,
): void => {
  if (format === 'json') {
    writeLine(serializeContextAsJson(files))
    return
  }
  writeLine(`\n${chalk.bold.cyan('Context Files')}`)
  writeLine(chalk.dim('ââââââââââââââ'))
  if (files.length === 0) {
    writeLine(chalk.dim('(none)'))
    return
  }
  files.forEach((file, index) => {
    writeLine(`<file path="${file.path}">`)
    writeLine(file.content)
    writeLine('</file>')
    if (index < files.length - 1) {
      writeLine('')
    }
  })
}
export const writeContextFile = async (
  filePath: string,
  format: 'text' | 'json',
  files: FileContext[],
): Promise<void> => {
  const payload = format === 'json' ? serializeContextAsJson(files) : serializeContextAsText(files)
  await fs.writeFile(filePath, payload, 'utf8')
}
export const serializeContextAsJson = (files: FileContext[]): string =>
  JSON.stringify(
    files.map(({ path, content }) => ({ path, content })),
    null,
    2,
  )
export const serializeContextAsText = (files: FileContext[]): string => {
  if (files.length === 0) {
    return '(none)'
  }
  return files
    .map((file) => [`<file path="${file.path}">`, file.content, '</file>'].join('\n'))
    .join('\n\n')
}
</file>
<file path="src/generate/context-templates.ts">
import { loadCliConfig } from '../config'
const CONTEXT_TEMPLATE_PLACEHOLDER = '{{prompt}}'
const BUILT_IN_CONTEXT_TEMPLATES: Record<string, string> = {
  nvim: [
    '## NeoVim Prompt Buffer',
    'Paste this block into a scratch buffer (e.g., :enew) so you can keep prompts beside your work.',
    CONTEXT_TEMPLATE_PLACEHOLDER,
  ].join('\n\n'),
}
export const renderContextTemplate = (template: string, prompt: string): string => {
  if (template.includes(CONTEXT_TEMPLATE_PLACEHOLDER)) {
    return template.split(CONTEXT_TEMPLATE_PLACEHOLDER).join(prompt)
  }
  const trimmedTemplate = template.trimEnd()
  if (!trimmedTemplate) {
    return prompt
  }
  return `${trimmedTemplate}\n\n${prompt}`
}
export const resolveContextTemplate = async (name: string): Promise<string> => {
  const builtIn = BUILT_IN_CONTEXT_TEMPLATES[name]
  if (builtIn) {
    return builtIn
  }
  const config = await loadCliConfig()
  const fromConfig = config?.contextTemplates?.[name]
  if (fromConfig) {
    return fromConfig
  }
  const available = [
    ...Object.keys(BUILT_IN_CONTEXT_TEMPLATES),
    ...(config?.contextTemplates ? Object.keys(config.contextTemplates) : []),
  ]
  const availableList = available.length > 0 ? available.join(', ') : 'none'
  throw new Error(`Unknown context template "${name}". Available templates: ${availableList}.`)
}
</file>
<file path="src/generate/debug.ts">
const envFlagEnabled = (value: string | undefined): boolean => {
  if (!value) {
    return false
  }
  const normalized = value.trim().toLowerCase()
  return normalized !== '0' && normalized !== 'false'
}
export const shouldTraceFlags = (): boolean => envFlagEnabled(process.env.PROMPT_MAKER_DEBUG_FLAGS)
export const shouldTraceCopy = (): boolean =>
  envFlagEnabled(process.env.PROMPT_MAKER_COPY_TRACE) || shouldTraceFlags()
</file>
<file path="src/generate/display.ts">
import boxen from 'boxen'
import chalk from 'chalk'
import { formatTokenCount } from '../token-counter'
export const displayPrompt = (prompt: string, iteration: number, tokenCount?: number): void => {
  const label = iteration === 1 ? 'Generated Prompt' : `Iteration ${iteration}`
  const meta = typeof tokenCount === 'number' ? chalk.dim(` Âˇ ${formatTokenCount(tokenCount)}`) : ''
  const title = chalk.bold.green(`${label}${meta}`)
  const boxed = boxen(prompt, {
    padding: { left: 1, right: 1, top: 0, bottom: 0 },
    borderColor: 'green',
    borderStyle: 'round',
    title,
    titleAlignment: 'left',
  })
  console.log(`\n${boxed}`)
}
export const displayPolishedPrompt = (prompt: string, model: string): void => {
  const title = chalk.bold.magenta(`Polished Prompt Âˇ ${model}`)
  const boxed = boxen(prompt, {
    padding: { left: 1, right: 1, top: 0, bottom: 0 },
    borderColor: 'magenta',
    borderStyle: 'round',
    title,
    titleAlignment: 'left',
  })
  console.log(`\n${boxed}`)
}
export const displayContextTemplatePrompt = (prompt: string, templateName: string): void => {
  const title = chalk.bold.blue(`Context Template Âˇ ${templateName}`)
  const boxed = boxen(prompt, {
    padding: { left: 1, right: 1, top: 0, bottom: 0 },
    borderColor: 'blue',
    borderStyle: 'round',
    title,
    titleAlignment: 'left',
  })
  console.log(`\n${boxed}`)
}
</file>
<file path="src/generate/fs-utils.ts">
export const isFsNotFoundError = (error: unknown): boolean =>
  Boolean(
    error &&
    typeof error === 'object' &&
    'code' in error &&
    typeof (error as { code?: unknown }).code === 'string' &&
    (error as { code: string }).code === 'ENOENT',
  )
</file>
<file path="src/generate/intent.ts">
import fs from 'node:fs/promises'
import chalk from 'chalk'
import type { GenerateArgs } from './types'
import { readFromStdin } from '../io'
import { isFsNotFoundError } from './fs-utils'
const MAX_INTENT_FILE_BYTES = 512 * 1024
export const resolveIntent = async (args: GenerateArgs): Promise<string> => {
  if (args.intent && args.intentFile) {
    throw new Error('Provide either an inline intent argument or --intent-file, not both.')
  }
  if (args.intentFile) {
    return await readIntentFile(args.intentFile)
  }
  const inlineIntentFromInteractiveFlag = await maybeResolveInlineIntentFile(args)
  if (inlineIntentFromInteractiveFlag) {
    return inlineIntentFromInteractiveFlag
  }
  if (args.intent?.trim()) {
    return args.intent.trim()
  }
  const piped = await readFromStdin()
  if (piped?.trim()) {
    return piped.trim()
  }
  throw new Error(
    'Intent text is required. Provide a quoted argument, use --intent-file, or pipe text via stdin.',
  )
}
const maybeResolveInlineIntentFile = async (args: GenerateArgs): Promise<string | null> => {
  if (!args.inlineIntentAfterInteractive || !args.intent) {
    return null
  }
  const candidatePath = args.intent.trim()
  if (!candidatePath) {
    return null
  }
  try {
    const content = await readIntentFile(candidatePath)
    console.warn(
      chalk.yellow(
        [
          `Detected "${candidatePath}" immediately after -i/--interactive.`,
          'Treating it as an intent file. Use -f/--intent-file (optionally alongside --interactive) for clearer commands and restored progress feedback.',
        ].join(' '),
      ),
    )
    return content
  } catch (error) {
    if (isFsNotFoundError(error)) {
      return null
    }
    throw error
  }
}
const readIntentFile = async (filePath: string): Promise<string> => {
  const stats = await fs.stat(filePath)
  if (stats.size > MAX_INTENT_FILE_BYTES) {
    const sizeKb = (stats.size / 1024).toFixed(1)
    throw new Error(`Intent file ${filePath} is too large (${sizeKb} KB).`)
  }
  const buffer = await fs.readFile(filePath)
  if (buffer.includes(0)) {
    throw new Error(`Intent file ${filePath} appears to be binary. Provide a UTF-8 text file.`)
  }
  const trimmed = buffer.toString('utf8').trim()
  if (!trimmed) {
    throw new Error(`Intent file ${filePath} is empty.`)
  }
  return trimmed
}
</file>
<file path="src/generate/interactive-transport.ts">
import fs from 'node:fs/promises'
import net from 'node:net'
import { isFsNotFoundError } from './fs-utils'
import type { TransportLifecycleEventInput } from './types'
import type { StreamWriter } from './stream'
type InteractiveCommand = { type: 'refine'; instruction: string } | { type: 'finish' }
const isWindowsNamedPipePath = (target: string): boolean => target.startsWith('\\\\.\\pipe\\')
export class InteractiveTransport {
  private server: net.Server | null = null
  private client: net.Socket | null = null
  private buffer = ''
  private commandQueue: InteractiveCommand[] = []
  private pendingResolvers: Array<(command: InteractiveCommand | null) => void> = []
  private stopped = false
  private lifecycleEmitter?: (event: TransportLifecycleEventInput) => void
  constructor(private readonly socketPath: string) {}
  setEventEmitter(callback: (event: TransportLifecycleEventInput) => void): void {
    this.lifecycleEmitter = callback
  }
  async start(): Promise<void> {
    if (!isWindowsNamedPipePath(this.socketPath)) {
      try {
        await fs.unlink(this.socketPath)
      } catch (error) {
        if (!isFsNotFoundError(error)) {
          throw error
        }
      }
    }
    await new Promise<void>((resolve, reject) => {
      const server = net.createServer((socket) => this.handleConnection(socket))
      const onError = (error: Error): void => {
        server.close()
        reject(error)
      }
      server.once('error', onError)
      server.listen(this.socketPath, () => {
        server.off('error', onError)
        this.server = server
        this.emitLifecycle({ event: 'transport.listening', path: this.socketPath })
        resolve()
      })
    })
  }
  async stop(): Promise<void> {
    if (this.stopped) {
      return
    }
    this.stopped = true
    if (this.client) {
      this.client.removeAllListeners()
      this.client.destroy()
      this.client = null
    }
    if (this.server) {
      await new Promise<void>((resolve) => {
        this.server?.close(() => resolve())
      })
      this.server = null
    }
    if (!isWindowsNamedPipePath(this.socketPath)) {
      try {
        await fs.unlink(this.socketPath)
      } catch (error) {
        if (!isFsNotFoundError(error)) {
          throw error
        }
      }
    }
    this.flushPending()
  }
  getEventWriter(): StreamWriter {
    return (chunk) => {
      if (this.client && !this.client.destroyed) {
        this.client.write(chunk)
      }
    }
  }
  async nextCommand(): Promise<InteractiveCommand | null> {
    if (this.commandQueue.length > 0) {
      return this.commandQueue.shift() ?? null
    }
    if (this.stopped) {
      return null
    }
    return await new Promise<InteractiveCommand | null>((resolve) => {
      this.pendingResolvers.push(resolve)
    })
  }
  private handleConnection(socket: net.Socket): void {
    if (this.client && !this.client.destroyed) {
      this.client.destroy()
    }
    this.client = socket
    this.buffer = ''
    this.emitLifecycle({ event: 'transport.client.connected', status: 'connected' })
    socket.setEncoding('utf8')
    socket.on('data', (data: string) => {
      this.handleData(data)
    })
    socket.on('close', () => {
      if (this.client === socket) {
        this.client = null
      }
      this.emitLifecycle({ event: 'transport.client.disconnected', status: 'disconnected' })
      this.flushPending()
    })
    socket.on('error', () => {
      if (this.client === socket) {
        this.client = null
        this.emitLifecycle({ event: 'transport.client.disconnected', status: 'disconnected' })
      }
    })
  }
  private handleData(data: string): void {
    this.buffer += data
    let newlineIndex = this.buffer.indexOf('\n')
    while (newlineIndex >= 0) {
      const raw = this.buffer.slice(0, newlineIndex).trim()
      this.buffer = this.buffer.slice(newlineIndex + 1)
      if (raw) {
        this.processRawCommand(raw)
      }
      newlineIndex = this.buffer.indexOf('\n')
    }
  }
  private processRawCommand(raw: string): void {
    try {
      const parsed = JSON.parse(raw) as { type?: string; instruction?: unknown }
      if (parsed.type === 'refine' && typeof parsed.instruction === 'string') {
        const instruction = parsed.instruction.trim()
        if (!instruction) {
          this.sendTransportError('Refinement instruction must be non-empty.')
          return
        }
        this.enqueueCommand({ type: 'refine', instruction })
        return
      }
      if (parsed.type === 'finish') {
        this.enqueueCommand({ type: 'finish' })
        return
      }
      this.sendTransportError('Unknown interactive command.')
    } catch {
      this.sendTransportError('Invalid command payload; expected JSON.')
    }
  }
  private enqueueCommand(command: InteractiveCommand): void {
    if (this.pendingResolvers.length > 0) {
      const resolve = this.pendingResolvers.shift()
      resolve?.(command)
      return
    }
    this.commandQueue.push(command)
  }
  private flushPending(): void {
    while (this.pendingResolvers.length > 0) {
      const resolve = this.pendingResolvers.shift()
      resolve?.(null)
    }
    this.commandQueue = []
  }
  private sendTransportError(message: string): void {
    if (this.client && !this.client.destroyed) {
      this.client.write(`${JSON.stringify({ event: 'transport.error', message })}\n`)
    }
  }
  private emitLifecycle(event: TransportLifecycleEventInput): void {
    this.lifecycleEmitter?.(event)
  }
}
</file>
<file path="src/generate/polish.ts">
import { callLLM } from '@prompt-maker/core'
import {
  ensureModelCredentials,
  sanitizePromptForTargetModelLeakage,
} from '../prompt-generator-service'
const POLISH_SYSTEM_PROMPT =
  'You refine prompt contracts for language models. Preserve headings, bullet ordering, and constraints. Only tighten wording and fix inconsistencies.'
export const polishPrompt = async (
  originalIntent: string,
  prompt: string,
  model: string,
  targetModel?: string,
): Promise<string> => {
  await ensureModelCredentials(model)
  const normalizedTargetModel = targetModel?.trim() ?? ''
  const targetGuidance = normalizedTargetModel
    ? [
        'Internal Optimization Target (do not include in output):',
        `- targetRuntimeModel: ${normalizedTargetModel}`,
        '',
        'Rules (non-negotiable):',
        '- Use the target runtime model only to tune compliance, clarity, and formatting expectations.',
        '- Do NOT mention or output the target runtime model id/label/name anywhere in the polished prompt text.',
        '- Do NOT include phrases like "Target runtime model" / "Target Runtime Model" in the polished prompt text.',
        '- Only include the target model id/label/name if the user intent explicitly asks to mention it.',
      ].join('\n')
    : ''
  const messages = [
    { role: 'system' as const, content: POLISH_SYSTEM_PROMPT },
    ...(targetGuidance ? [{ role: 'system' as const, content: targetGuidance }] : []),
    {
      role: 'user' as const,
      content: [
        'Intent:',
        originalIntent,
        '---',
        'Generated prompt candidate:',
        prompt,
        '---',
        'Return the polished prompt text, preserving exact sections.',
      ].join('\n'),
    },
  ]
  const raw = await callLLM(messages, model)
  return sanitizePromptForTargetModelLeakage({
    prompt: raw,
    intent: originalIntent,
    targetModel: normalizedTargetModel,
  })
}
</file>
<file path="src/generate/progress.ts">
import chalk from 'chalk'
import ora from 'ora'
import type { UploadStateChange } from '../prompt-generator-service'
import type { StreamDispatcher } from './stream'
export type ProgressHandle = {
  stop: (finalMessage?: string) => void
  setLabel: (label: string) => void
}
export const startProgress = (label: string, options: { showSpinner: boolean }): ProgressHandle => {
  const spinner = options.showSpinner
    ? ora({
        text: chalk.dim(label),
        color: 'cyan',
        spinner: 'dots',
      }).start()
    : null
  let stopped = false
  const stop = (finalMessage?: string): void => {
    if (stopped) {
      return
    }
    stopped = true
    if (spinner) {
      if (finalMessage) {
        spinner.succeed(finalMessage)
      } else {
        spinner.succeed(chalk.green(`${label} â`))
      }
    }
  }
  const setLabel = (nextLabel: string): void => {
    if (stopped) {
      return
    }
    if (spinner) {
      spinner.text = chalk.dim(nextLabel)
    }
  }
  return { stop, setLabel }
}
export const createUploadStateTracker = (
  progress: ProgressHandle | null,
  defaultLabel: string,
  stream?: StreamDispatcher,
): UploadStateChange => {
  let uploadsInFlight = 0
  const uploadLabel = 'Uploading...'
  return (state, detail) => {
    if (state === 'start') {
      uploadsInFlight += 1
      if (uploadsInFlight === 1) {
        progress?.setLabel(uploadLabel)
      }
    } else {
      uploadsInFlight = Math.max(0, uploadsInFlight - 1)
      if (uploadsInFlight === 0) {
        progress?.setLabel(defaultLabel)
      }
    }
    if (stream) {
      stream.emit({ event: 'upload.state', state, detail })
    }
  }
}
</file>
<file path="src/generate/stream.ts">
import { stdout as output } from 'node:process'
import type { StreamEventInput, StreamMode } from './types'
export type StreamWriter = (chunk: string) => void
export type StreamDispatcher = {
  mode: StreamMode
  emit: (event: StreamEventInput) => void
}
type StreamDispatcherOptions = {
  writer?: StreamWriter
  taps?: StreamWriter[]
}
export const createStreamDispatcher = (
  mode: StreamMode,
  options: StreamDispatcherOptions = {},
): StreamDispatcher => {
  const writer =
    options.writer ??
    ((chunk: string): void => {
      output.write(chunk)
    })
  const taps = options.taps ?? []
  const emitToTaps = (serialized: string): void => {
    taps.forEach((tap) => {
      tap(serialized)
    })
  }
  if (mode !== 'jsonl') {
    return {
      mode,
      emit: (event) => {
        if (taps.length === 0) {
          return
        }
        const payload = { ...event, timestamp: new Date().toISOString() }
        const serialized = `${JSON.stringify(payload)}\n`
        emitToTaps(serialized)
      },
    }
  }
  return {
    mode,
    emit: (event) => {
      const payload = { ...event, timestamp: new Date().toISOString() }
      const serialized = `${JSON.stringify(payload)}\n`
      writer(serialized)
      emitToTaps(serialized)
    },
  }
}
</file>
<file path="src/generate/token-telemetry.ts">
import { stdout as output } from 'node:process'
import boxen from 'boxen'
import chalk from 'chalk'
import Table from 'cli-table3'
import type { FileContext } from '../file-context'
import { GEN_SYSTEM_PROMPT } from '../prompt-generator-service'
import { countTokens, formatTokenCount } from '../token-counter'
import type { TokenTelemetry } from './types'
export const buildTokenTelemetry = (
  intentText: string,
  files: FileContext[],
  metaInstructions?: string,
): TokenTelemetry => {
  const fileSummaries = files.map((file) => ({
    path: file.path,
    tokens: countTokens(file.content),
  }))
  const fileTokens = fileSummaries.reduce((acc, file) => acc + file.tokens, 0)
  const intentTokens = countTokens(intentText)
  const metaTokens = metaInstructions?.trim() ? countTokens(metaInstructions) : 0
  const systemTokens = countTokens(GEN_SYSTEM_PROMPT) + metaTokens
  return {
    files: fileSummaries,
    intentTokens,
    fileTokens,
    systemTokens,
    totalTokens: intentTokens + fileTokens + systemTokens,
  }
}
export const displayTokenSummary = ({
  files,
  intentTokens,
  fileTokens,
  systemTokens,
  totalTokens,
}: TokenTelemetry): void => {
  const telemetryLines = [
    `${chalk.gray('Total')}: ${chalk.white(formatTokenCount(totalTokens))}`,
    `${chalk.gray('Intent')}: ${chalk.white(formatTokenCount(intentTokens))}`,
    `${chalk.gray('Files')}: ${chalk.white(formatTokenCount(fileTokens))}`,
    `${chalk.gray('System')}: ${chalk.white(formatTokenCount(systemTokens))}`,
  ].join('\n')
  console.log('')
  console.log(
    boxen(telemetryLines, {
      padding: { left: 1, right: 1, top: 0, bottom: 0 },
      borderColor: 'cyan',
      borderStyle: 'round',
      title: chalk.bold.cyan('Context Telemetry'),
      titleAlignment: 'left',
    }),
  )
  console.log('')
  if (files.length === 0) {
    return
  }
  const terminalWidth = Math.max(60, Math.min(output.columns ?? 100, 110))
  const numberColumnWidth = 4
  const tokensColumnWidth = 14
  const pathColumnWidth = Math.max(24, terminalWidth - numberColumnWidth - tokensColumnWidth)
  const table = new Table({
    head: [chalk.gray('#'), chalk.gray('Path'), chalk.gray('Tokens')],
    style: { head: [], border: [] },
    wordWrap: true,
    colWidths: [numberColumnWidth, pathColumnWidth, tokensColumnWidth],
  })
  files.slice(0, 10).forEach((file, index) => {
    table.push([
      chalk.dim(String(index + 1)),
      chalk.white(file.path),
      chalk.green(formatTokenCount(file.tokens)),
    ])
  })
  console.log(table.toString())
  console.log('')
  if (files.length > 10) {
    console.log(chalk.dim(`âŚand ${files.length - 10} more context files`))
  }
}
</file>
<file path="src/prompt-generator/model-credentials.ts">
import { loadCliConfig, resolveGeminiCredentials, resolveOpenAiCredentials } from '../config'
import { isGeminiModelId } from '../model-providers'
export const resolveDefaultGenerateModel = async (): Promise<string> => {
  const config = await loadCliConfig()
  return (
    config?.promptGenerator?.defaultModel?.trim() ||
    process.env.PROMPT_MAKER_GENERATE_MODEL?.trim() ||
    'gpt-4o-mini'
  )
}
export const ensureModelCredentials = async (model: string): Promise<void> => {
  if (isGemini(model)) {
    if (!process.env.GEMINI_API_KEY) {
      const credentials = await resolveGeminiCredentials()
      process.env.GEMINI_API_KEY = credentials.apiKey
      if (credentials.baseUrl && !process.env.GEMINI_BASE_URL) {
        process.env.GEMINI_BASE_URL = credentials.baseUrl
      }
    }
    return
  }
  if (!process.env.OPENAI_API_KEY) {
    const credentials = await resolveOpenAiCredentials()
    process.env.OPENAI_API_KEY = credentials.apiKey
    if (credentials.baseUrl && !process.env.OPENAI_BASE_URL) {
      process.env.OPENAI_BASE_URL = credentials.baseUrl
    }
  }
}
export const isGemini = (model: string): boolean => isGeminiModelId(model)
</file>
<file path="src/prompt-generator/parse-llm-json.ts">
export const parseLLMJson = <T>(text: string): T => {
  const cleaned = text
    .trim()
    .replace(/^```json\s*/i, '')
    .replace(/^```\s*/i, '')
    .replace(/```$/i, '')
    .trim()
  try {
    return JSON.parse(cleaned) as T
  } catch {
    console.warn('Failed to parse LLM JSON response. Falling back to raw text.')
    throw new Error('LLM did not return valid JSON.')
  }
}
</file>
<file path="src/prompt-generator/series-repair.ts">
import type { SeriesResponse } from './types'
export const isRepairableSeriesValidationError = (error: unknown): boolean => {
  const message = error instanceof Error ? error.message : String(error)
  return (
    message.startsWith('Atomic prompt ') &&
    (message.includes('missing required section(s)') ||
      message.includes('contains forbidden cross-reference phrase') ||
      message.includes('is missing a title') ||
      message.includes('is missing content'))
  )
}
export const buildSeriesRepairUserMessage = (options: {
  intent: string
  validationError: string
  previousSeries: SeriesResponse
}): string => {
  return [
    `User Intent:\n${options.intent.trim()}`,
    '',
    `Validation Error:\n${options.validationError.trim()}`,
    '',
    'Previous SeriesResponse JSON:',
    JSON.stringify(options.previousSeries, null, 2),
    '',
    'Return a corrected SeriesResponse JSON payload now.',
  ].join('\n')
}
</file>
<file path="src/prompt-generator/series-validation.ts">
import type { SeriesResponse } from './types'
const REQUIRED_ATOMIC_PROMPT_SECTIONS: ReadonlyArray<{ label: string; pattern: RegExp }> = [
  { label: '# Title', pattern: /^#\s*Title\b/im },
  { label: 'Role', pattern: /^(?:#{1,6}\s*)?Role\b/im },
  { label: 'Context', pattern: /^(?:#{1,6}\s*)?Context\b/im },
  {
    label: 'Goals & Tasks',
    pattern: /^(?:#{1,6}\s*)?Goals\s*(?:&|and)\s*Tasks\b/im,
  },
  { label: 'Inputs', pattern: /^(?:#{1,6}\s*)?Inputs\b/im },
  { label: 'Constraints', pattern: /^(?:#{1,6}\s*)?Constraints\b/im },
  { label: 'Execution Plan', pattern: /^(?:#{1,6}\s*)?Execution\s+Plan\b/im },
  { label: 'Output Format', pattern: /^(?:#{1,6}\s*)?Output\s+Format\b/im },
  { label: 'Validation', pattern: /^(?:#{1,6}\s*)?Validation\b/im },
]
const FORBIDDEN_CROSS_REFERENCE_PATTERNS: ReadonlyArray<{ label: string; pattern: RegExp }> = [
  { label: '"as above"', pattern: /\bas above\b/i },
  { label: '"as mentioned earlier"', pattern: /\bas mentioned earlier\b/i },
  { label: '"as described earlier"', pattern: /\bas described earlier\b/i },
  {
    label: '"previous step" / "prior step" / "earlier step"',
    pattern: /\b(previous|prior|earlier)\s+step\b/i,
  },
  {
    label: '"previous prompt" / "prior prompt" / "earlier prompt"',
    pattern: /\b(previous|prior|earlier)\s+prompt\b/i,
  },
  { label: '"from step N"', pattern: /\bfrom\s+step\s+\d+\b/i },
  { label: '"in step N"', pattern: /\bin\s+step\s+\d+\b/i },
  { label: '"see step N"', pattern: /\bsee\s+step\s+\d+\b/i },
  { label: '"step N above/below"', pattern: /\bstep\s+\d+\s+(above|below)\b/i },
  { label: '"continue from step N"', pattern: /\bcontinue\s+from\s+step\s+\d+\b/i },
]
const findMissingAtomicPromptSections = (content: string): string[] => {
  return REQUIRED_ATOMIC_PROMPT_SECTIONS.filter((section) => !section.pattern.test(content)).map(
    (section) => section.label,
  )
}
const findForbiddenCrossReference = (content: string): string | null => {
  const hit = FORBIDDEN_CROSS_REFERENCE_PATTERNS.find((entry) => entry.pattern.test(content))
  return hit?.label ?? null
}
export const validateSeriesResponse = (response: SeriesResponse): void => {
  if (!response || typeof response !== 'object') {
    throw new Error('LLM returned SeriesResponse with invalid shape.')
  }
  if (typeof response.reasoning !== 'string' || !response.reasoning.trim()) {
    throw new Error('Series reasoning is required.')
  }
  if (typeof response.overviewPrompt !== 'string' || !response.overviewPrompt.trim()) {
    throw new Error('Series overviewPrompt is required.')
  }
  if (!Array.isArray(response.atomicPrompts) || response.atomicPrompts.length === 0) {
    throw new Error('Series atomicPrompts must include at least one entry.')
  }
  response.atomicPrompts.forEach((entry, index) => {
    const promptNumber = index + 1
    if (!entry || typeof entry !== 'object') {
      throw new Error(`Atomic prompt ${promptNumber} is invalid.`)
    }
    if (typeof entry.title !== 'string' || !entry.title.trim()) {
      throw new Error(`Atomic prompt ${promptNumber} is missing a title.`)
    }
    if (typeof entry.content !== 'string' || !entry.content.trim()) {
      throw new Error(`Atomic prompt ${promptNumber} is missing content.`)
    }
    const missingSections = findMissingAtomicPromptSections(entry.content)
    if (missingSections.length > 0) {
      throw new Error(
        `Atomic prompt ${promptNumber} is missing required section(s): ${missingSections.join(', ')}.`,
      )
    }
    const forbiddenCrossReference = findForbiddenCrossReference(entry.content)
    if (forbiddenCrossReference) {
      throw new Error(
        `Atomic prompt ${promptNumber} contains forbidden cross-reference phrase ${forbiddenCrossReference}. Atomic prompts must be standalone.`,
      )
    }
  })
}
</file>
<file path="src/prompt-generator/target-model-guidance.ts">
const TARGET_RUNTIME_MODEL_PHRASE_REGEX = /target runtime model/i
const escapeRegExp = (value: string): string => value.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')
export const sanitizePromptForTargetModelLeakage = ({
  prompt,
  intent,
  targetModel,
}: {
  prompt: string
  intent: string
  targetModel: string
}): string => {
  const normalizedTargetModel = targetModel.trim()
  if (!normalizedTargetModel) {
    return prompt
  }
  const normalizedIntent = intent.trim().toLowerCase()
  const normalizedTargetLower = normalizedTargetModel.toLowerCase()
  if (normalizedIntent.includes(normalizedTargetLower)) {
    return prompt
  }
  const normalizedPromptLower = prompt.toLowerCase()
  if (
    !TARGET_RUNTIME_MODEL_PHRASE_REGEX.test(prompt) &&
    !normalizedPromptLower.includes(normalizedTargetLower)
  ) {
    return prompt
  }
  const withoutTargetModelLines = prompt
    .split('\n')
    .filter((line) => !TARGET_RUNTIME_MODEL_PHRASE_REGEX.test(line))
    .join('\n')
  const targetRegex = new RegExp(escapeRegExp(normalizedTargetModel), 'gi')
  const withoutTargetMentions = withoutTargetModelLines.replace(targetRegex, '')
  const cleaned = withoutTargetMentions
    .split('\n')
    .map((line) =>
      line
        .replace(/\*\*\s*\*\*/g, '')
        .replace(/[ \t]{2,}/g, ' ')
        .trimEnd(),
    )
    .join('\n')
    .replace(/\n{3,}/g, '\n\n')
  return cleaned.trim()
}
export const buildTargetRuntimeModelGuidance = (targetModel: string): string => {
  const normalized = targetModel.trim()
  if (!normalized) {
    return ''
  }
  return [
    'Internal Optimization Target (do not include in output):',
    `- targetRuntimeModel: ${normalized}`,
    '',
    'Rules (non-negotiable):',
    '- Use the target runtime model only to tune the contract for compliance, clarity, and formatting expectations.',
    '- Do NOT mention or output the target runtime model id/label/name anywhere in the returned prompt text.',
    '- Do NOT include phrases like "Target runtime model" / "Target Runtime Model" in the returned prompt text.',
    '- Only include the target model id/label/name if the user intent explicitly asks to mention it.',
  ].join('\n')
}
</file>
<file path="src/rag/vector-store.ts">
import crypto from 'node:crypto'
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
import { getEmbedding } from '@prompt-maker/core'
const CACHE_DIR = path.join(os.homedir(), '.config', 'prompt-maker-cli')
const CACHE_FILE = path.join(CACHE_DIR, 'embeddings_cache.json')
type CacheEntry = {
  hash: string
  embedding: number[]
}
type EmbeddingCache = Record<string, CacheEntry>
let inMemoryCache: EmbeddingCache | null = null
export const indexFiles = async (filePaths: string[]): Promise<void> => {
  if (filePaths.length === 0) {
    return
  }
  const uniquePaths = [...new Set(filePaths)]
  const cache = await loadCache()
  let changed = false
  for (const filePath of uniquePaths) {
    try {
      const contents = await fs.readFile(filePath, 'utf8')
      const hash = hashContent(contents)
      const existing = cache[filePath]
      if (existing && existing.hash === hash) {
        continue
      }
      const embedding = await getEmbedding(contents)
      cache[filePath] = { hash, embedding }
      changed = true
    } catch (error) {
      const message = error instanceof Error ? error.message : String(error)
      console.warn(`Failed to index ${filePath}: ${message}`)
    }
  }
  if (changed) {
    await saveCache(cache)
  }
}
export const search = async (
  query: string,
  k: number,
  allowedPaths?: Iterable<string>,
): Promise<string[]> => {
  if (!query.trim() || k <= 0) {
    return []
  }
  const cache = await loadCache()
  let entries = Object.entries(cache)
  if (entries.length === 0) {
    return []
  }
  if (allowedPaths) {
    const allowedSet = new Set(Array.from(allowedPaths, (filePath) => path.resolve(filePath)))
    entries = entries.filter(([filePath]) => allowedSet.has(path.resolve(filePath)))
    if (entries.length === 0) {
      return []
    }
  }
  const queryEmbedding = await getEmbedding(query)
  const scored = entries
    .map(([filePath, entry]) => ({
      filePath,
      score: cosineSimilarity(queryEmbedding, entry.embedding),
    }))
    .filter(({ score }) => Number.isFinite(score))
    .sort((a, b) => b.score - a.score)
    .slice(0, Math.min(k, entries.length))
    .map(({ filePath }) => filePath)
  return scored
}
const loadCache = async (): Promise<EmbeddingCache> => {
  if (inMemoryCache) {
    return inMemoryCache
  }
  try {
    const contents = await fs.readFile(CACHE_FILE, 'utf8')
    const parsed = JSON.parse(contents) as unknown
    if (isEmbeddingCache(parsed)) {
      inMemoryCache = parsed
      return parsed
    }
    console.warn('Embeddings cache file is malformed. Resetting cache.')
  } catch (error) {
    if (!isFileMissingError(error)) {
      const message = error instanceof Error ? error.message : String(error)
      console.warn(`Failed to load embeddings cache: ${message}`)
    }
  }
  inMemoryCache = {}
  return inMemoryCache
}
const saveCache = async (cache: EmbeddingCache): Promise<void> => {
  await fs.mkdir(CACHE_DIR, { recursive: true })
  await fs.writeFile(CACHE_FILE, JSON.stringify(cache, null, 2), 'utf8')
  inMemoryCache = cache
}
const hashContent = (contents: string): string => {
  return crypto.createHash('sha256').update(contents).digest('hex')
}
const cosineSimilarity = (left: number[], right: number[]): number => {
  if (left.length === 0 || right.length === 0 || left.length !== right.length) {
    return Number.NEGATIVE_INFINITY
  }
  let dot = 0
  let normLeft = 0
  let normRight = 0
  for (let i = 0; i < left.length; i += 1) {
    const a = left[i]
    const b = right[i]
    if (a === undefined || b === undefined) {
      return Number.NEGATIVE_INFINITY
    }
    dot += a * b
    normLeft += a * a
    normRight += b * b
  }
  if (normLeft === 0 || normRight === 0) {
    return Number.NEGATIVE_INFINITY
  }
  return dot / (Math.sqrt(normLeft) * Math.sqrt(normRight))
}
const isEmbeddingCache = (value: unknown): value is EmbeddingCache => {
  if (!value || typeof value !== 'object') {
    return false
  }
  return Object.values(value as Record<string, unknown>).every(isCacheEntry)
}
const isCacheEntry = (value: unknown): value is CacheEntry => {
  if (!value || typeof value !== 'object') {
    return false
  }
  const record = value as Record<string, unknown>
  return (
    typeof record.hash === 'string' &&
    Array.isArray(record.embedding) &&
    record.embedding.every(isFiniteNumber)
  )
}
const isFiniteNumber = (value: unknown): value is number =>
  typeof value === 'number' && Number.isFinite(value)
const hasErrnoCode = (error: unknown): error is { code: string } =>
  typeof error === 'object' && error !== null && 'code' in error
const isFileMissingError = (error: unknown): boolean =>
  hasErrnoCode(error) && error.code === 'ENOENT'
</file>
<file path="src/testing/evaluator.ts">
import { callLLM } from '@prompt-maker/core'
import { ensureModelCredentials } from '../prompt-generator-service'
const DEFAULT_JUDGE_MODEL = process.env.PROMPT_MAKER_JUDGE_MODEL?.trim() || 'gpt-4o'
const SYSTEM_PROMPT =
  'You are a QA bot. Evaluate if the following prompt meets these criteria. Return strict JSON with keys "pass" (boolean) and "reason" (string).'
type JudgeVerdict = {
  pass: boolean
  reason: string
}
export const evaluatePrompt = async (
  generatedPrompt: string,
  criteria: string[],
): Promise<JudgeVerdict> => {
  if (!criteria || criteria.length === 0) {
    return { pass: true, reason: 'No criteria provided.' }
  }
  const model = DEFAULT_JUDGE_MODEL
  await ensureModelCredentials(model)
  const criteriaList = criteria.map((item, index) => `${index + 1}. ${item}`).join('\n')
  const userMessage = [
    'Prompt to Evaluate:',
    generatedPrompt,
    '',
    'Criteria:',
    criteriaList,
    '',
    'Return JSON like { "pass": boolean, "reason": string } indicating whether the prompt satisfies all criteria.',
  ].join('\n')
  try {
    const response = await callLLM(
      [
        { role: 'system', content: SYSTEM_PROMPT },
        { role: 'user', content: userMessage },
      ],
      model,
    )
    const verdict = parseVerdict(response)
    return verdict
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown evaluation error.'
    return { pass: false, reason: `Evaluation failed: ${message}` }
  }
}
const parseVerdict = (text: string): JudgeVerdict => {
  try {
    const cleaned = text
      .trim()
      .replace(/```json/gi, '')
      .replace(/```/g, '')
      .trim()
    const parsed = JSON.parse(cleaned) as Partial<JudgeVerdict>
    if (typeof parsed.pass === 'boolean' && typeof parsed.reason === 'string') {
      return parsed as JudgeVerdict
    }
    throw new Error('Missing pass/reason fields')
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown parse error'
    return {
      pass: false,
      reason: `Judge returned invalid JSON: ${message}. Raw response: ${truncate(text, 200)}`,
    }
  }
}
const truncate = (value: string, max: number): string => {
  if (value.length <= max) {
    return value
  }
  return `${value.slice(0, max)}âŚ`
}
</file>
<file path="src/testing/test-schema.ts">
import { z } from 'zod'
export const PromptTestSchema = z.object({
  name: z.string().min(1, 'Test name is required'),
  intent: z.string().min(1, 'Intent text is required'),
  context: z.array(z.string().min(1)).default([]),
  image: z.array(z.string()).optional(),
  video: z.array(z.string()).optional(),
  smartContext: z.boolean().optional(),
  smartContextRoot: z.string().min(1).optional(),
  expect: z.array(z.string().min(1)).min(1, 'Each test must include at least one expectation'),
})
export const PromptTestSuiteSchema = z.object({
  tests: z.array(PromptTestSchema).min(1, 'Provide at least one test case'),
})
export type PromptTest = z.infer<typeof PromptTestSchema>
export type PromptTestSuite = z.infer<typeof PromptTestSuiteSchema>
export const parsePromptTestSuite = (data: unknown): PromptTestSuite => {
  return PromptTestSuiteSchema.parse(data)
}
</file>
<file path="src/tui/components/core/BackgroundFill.tsx">
import React, { useMemo } from 'react'
import { Box, Text } from 'ink'
const NBSP = '\u00A0'
import type { InkColorValue } from '../../theme/theme-types'
import { inkBackgroundColorProps } from '../../theme/theme-types'
export type BackgroundFillProps = {
  rows: number
  columns: number
  background: InkColorValue
}
export const BackgroundFill: React.FC<BackgroundFillProps> = ({ rows, columns, background }) => {
  const safeRows = Math.max(0, Math.floor(rows))
  const safeColumns = Math.max(0, Math.floor(columns))
  const line = useMemo(() => {
    if (safeColumns === 0) {
      return ''
    }
    return NBSP.repeat(safeColumns)
  }, [safeColumns])
  if (safeRows === 0 || safeColumns === 0) {
    return null
  }
  return (
    <Box flexDirection="column" width={safeColumns} height={safeRows} overflow="hidden">
      {Array.from({ length: safeRows }).map((_, index) => (
        <Text key={`bg-${index}`} {...inkBackgroundColorProps(background)} wrap="truncate">
          {line}
        </Text>
      ))}
    </Box>
  )
}
</file>
<file path="src/tui/components/core/bracketed-paste.ts">
export const stripBracketedPasteControlSequences = (value: string): string => {
  if (!value) {
    return value
  }
  return value.replace(/(?:\u001b)?\[(?:200|201)~/g, '')
}
export const stripTerminalPasteArtifacts = (value: string): string => {
  if (!value) {
    return value
  }
  const withoutBracketedPaste = stripBracketedPasteControlSequences(value)
  return withoutBracketedPaste.replace(/(?:\u001b)?\[[0-9;]*m/g, '')
}
</file>
<file path="src/tui/components/core/CommandMenu.tsx">
import { memo, useMemo } from 'react'
import { Box, Text, useStdout } from 'ink'
import type { CommandDescriptor } from '../../types'
import { useTheme } from '../../theme/theme-provider'
import {
  inkBackgroundColorProps,
  inkBorderColorProps,
  inkColorProps,
} from '../../theme/theme-types'
const APP_CONTAINER_PADDING_X = 2
const COMMAND_SCREEN_PADDING_X = 1
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type CommandMenuProps = {
  commands: readonly CommandDescriptor[]
  selectedIndex: number
}
export const CommandMenu = memo(({ commands, selectedIndex }: CommandMenuProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const borderColumns = 2
  const paddingColumns = 2
  const boxWidth = Math.max(
    0,
    terminalColumns - 2 * (APP_CONTAINER_PADDING_X + COMMAND_SCREEN_PADDING_X),
  )
  const contentWidth = Math.max(0, boxWidth - borderColumns - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.panelBackground)
  const commandLines = useMemo(
    () =>
      commands.map((command) => {
        const shortcut = `/${command.id}`.padEnd(10)
        return padRight(`${shortcut} ${command.description}`, contentWidth)
      }),
    [commands, contentWidth],
  )
  return (
    <Box
      flexDirection="column"
      borderStyle="round"
      paddingX={1}
      paddingY={0}
      width="100%"
      {...inkBorderColorProps(theme.border)}
      {...backgroundProps}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Commands', contentWidth)}
      </Text>
      {commandLines.length === 0 ? (
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('No commands match.', contentWidth)}
        </Text>
      ) : (
        commandLines.map((line, index) => {
          const isSelected = index === selectedIndex
          const textProps = isSelected
            ? {
                ...inkColorProps(theme.selectionText),
                ...inkBackgroundColorProps(theme.selectionBackground),
              }
            : { ...backgroundProps, ...inkColorProps(theme.text) }
          return (
            <Text key={commands[index]?.id ?? String(index)} {...textProps}>
              {line}
            </Text>
          )
        })
      )}
    </Box>
  )
})
CommandMenu.displayName = 'CommandMenu'
</file>
<file path="src/tui/components/core/help-overlay-scroll.ts">
export const getHelpOverlayContentRows = (height: number): number => {
  return Math.max(1, height - 5)
}
export const getHelpOverlayMaxScroll = (lineCount: number, contentRows: number): number => {
  return Math.max(0, lineCount - contentRows)
}
export const clampHelpOverlayScrollOffset = (
  offset: number,
  lineCount: number,
  contentRows: number,
): number => {
  const maxScroll = getHelpOverlayMaxScroll(lineCount, contentRows)
  return Math.max(0, Math.min(offset, maxScroll))
}
export const scrollHelpOverlayBy = (
  offset: number,
  delta: number,
  lineCount: number,
  contentRows: number,
): number => {
  return clampHelpOverlayScrollOffset(offset + delta, lineCount, contentRows)
}
</file>
<file path="src/tui/components/core/HelpOverlay.tsx">
import React, { useEffect, useMemo, useState } from 'react'
import { Box, Text, useInput, useStdout } from 'ink'
import { COMMAND_DESCRIPTORS } from '../../config'
import { createHelpSections, estimateHelpOverlayHeight } from '../../help-config'
import {
  clampHelpOverlayScrollOffset,
  getHelpOverlayContentRows,
  getHelpOverlayMaxScroll,
  scrollHelpOverlayBy,
} from './help-overlay-scroll'
import { useTheme } from '../../theme/theme-provider'
import {
  inkBackgroundColorProps,
  inkBorderColorProps,
  inkColorProps,
} from '../../theme/theme-types'
const APP_CONTAINER_PADDING_X = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
const padLeft = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padStart(width, ' ')
}
export type HelpOverlayProps = {
  activeView: 'generate' | 'tests'
  maxHeight?: number
}
export const HelpOverlay: React.FC<HelpOverlayProps> = ({ activeView: _activeView, maxHeight }) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const overlayWidth = Math.max(40, terminalColumns - 2 * APP_CONTAINER_PADDING_X)
  const borderColumns = 2
  const paddingColumns = 2
  const contentWidth = Math.max(0, overlayWidth - borderColumns - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const sections = useMemo(
    () => createHelpSections({ commandDescriptors: COMMAND_DESCRIPTORS }),
    [],
  )
  const idealHeight = estimateHelpOverlayHeight(sections)
  const clampedHeight = maxHeight ? Math.min(idealHeight, maxHeight) : idealHeight
  const height = Math.max(10, clampedHeight)
  const contentLines = useMemo(() => {
    const lines: string[] = []
    for (const section of sections) {
      lines.push(section.title)
      lines.push(...section.lines)
      lines.push('')
    }
    while (lines.length > 0 && lines[lines.length - 1] === '') {
      lines.pop()
    }
    return lines
  }, [sections])
  const contentRows = getHelpOverlayContentRows(height)
  const maxScroll = getHelpOverlayMaxScroll(contentLines.length, contentRows)
  const [scrollOffset, setScrollOffset] = useState(0)
  useEffect(() => {
    setScrollOffset((prev) => clampHelpOverlayScrollOffset(prev, contentLines.length, contentRows))
  }, [contentLines.length, contentRows])
  useInput((_, key) => {
    if (key.upArrow) {
      setScrollOffset((prev) => scrollHelpOverlayBy(prev, -1, contentLines.length, contentRows))
      return
    }
    if (key.downArrow) {
      setScrollOffset((prev) => scrollHelpOverlayBy(prev, 1, contentLines.length, contentRows))
      return
    }
    if (key.pageUp) {
      setScrollOffset((prev) =>
        scrollHelpOverlayBy(prev, -contentRows, contentLines.length, contentRows),
      )
      return
    }
    if (key.pageDown) {
      setScrollOffset((prev) =>
        scrollHelpOverlayBy(prev, contentRows, contentLines.length, contentRows),
      )
    }
  })
  const sectionTitles = useMemo(() => new Set(sections.map((section) => section.title)), [sections])
  const clampedOffset = clampHelpOverlayScrollOffset(scrollOffset, contentLines.length, contentRows)
  const visibleLines = useMemo(() => {
    const slice = contentLines.slice(clampedOffset, clampedOffset + contentRows)
    if (slice.length >= contentRows) {
      return slice
    }
    const padded = [...slice]
    while (padded.length < contentRows) {
      padded.push('')
    }
    return padded
  }, [clampedOffset, contentLines, contentRows])
  const showScrollHint = maxScroll > 0
  const scrollLabel = showScrollHint
    ? `â/â scroll (${clampedOffset + 1}-${Math.min(clampedOffset + contentRows, contentLines.length)}/${contentLines.length})`
    : ''
  const headerLeft = 'Help'
  const headerRight = 'Esc / ? to close'
  const headerGap = Math.max(0, contentWidth - headerLeft.length - headerRight.length)
  return (
    <Box
      flexDirection="column"
      borderStyle="round"
      paddingX={1}
      paddingY={0}
      height={height}
      width={overlayWidth}
      overflow="hidden"
      {...inkBorderColorProps(theme.border)}
      {...backgroundProps}
    >
      <Box flexDirection="row">
        <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
          {headerLeft}
        </Text>
        <Text {...backgroundProps}>{' '.repeat(headerGap)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {headerRight}
        </Text>
      </Box>
      <Box flexDirection="column" marginTop={1} height={contentRows} overflow="hidden">
        {visibleLines.map((line, index) => {
          const isSectionTitle = sectionTitles.has(line)
          const color = isSectionTitle ? theme.accent : theme.mutedText
          return (
            <Text key={`${scrollOffset}-${index}`} {...backgroundProps} {...inkColorProps(color)}>
              {padRight(line, contentWidth)}
            </Text>
          )
        })}
      </Box>
      <Box flexDirection="row">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padLeft(scrollLabel, contentWidth)}
        </Text>
      </Box>
    </Box>
  )
}
</file>
<file path="src/tui/components/core/input-bar-layout.ts">
import { getLineCount } from './multiline-text-buffer'
export type InputBarRowEstimateOptions = {
  value: string
  hint?: string | undefined
  debugLine?: string | undefined
}
export const estimateInputBarRows = ({
  value,
  hint,
  debugLine,
}: InputBarRowEstimateOptions): number => {
  const lineCount = getLineCount(value)
  const contentRows = 2 + (hint ? 1 : 0) + (debugLine ? 1 : 0) + lineCount
  // Keep the input bar as compact as possible; callers can pad if desired.
  return contentRows
}
</file>
<file path="src/tui/components/core/input-bar-presentation.ts">
export type InputBarMode = 'intent' | 'refinement'
type InputBarTone = 'default' | 'warning'
type InputBarLabelTone = 'muted' | 'warning'
export type InputBarPresentation = {
  borderTone: InputBarTone
  label: string
  labelTone: InputBarLabelTone
  labelBold: boolean
}
export const resolveInputBarPresentation = (mode: InputBarMode): InputBarPresentation => {
  if (mode === 'refinement') {
    return {
      borderTone: 'warning',
      label: 'Refinement (Enter to submit Âˇ empty to finish)',
      labelTone: 'warning',
      labelBold: true,
    }
  }
  return {
    borderTone: 'default',
    label: 'Intent / Command',
    labelTone: 'muted',
    labelBold: false,
  }
}
</file>
<file path="src/tui/components/core/multiline-text-buffer.ts">
import { stripBracketedPasteControlSequences } from './bracketed-paste'
export type MultilineTextBufferState = {
  value: string
  cursor: number
}
export type CursorCoordinates = {
  row: number
  column: number
}
export const clampCursor = (cursor: number, value: string): number =>
  Math.max(0, Math.min(cursor, value.length))
export const getLineCount = (value: string): number => {
  if (!value) {
    return 1
  }
  return value.split('\n').length
}
export const getCursorCoordinates = (value: string, cursor: number): CursorCoordinates => {
  const clampedCursor = clampCursor(cursor, value)
  let row = 0
  let column = 0
  for (let index = 0; index < clampedCursor; index += 1) {
    const character = value.charAt(index)
    if (character === '\n') {
      row += 1
      column = 0
    } else {
      column += 1
    }
  }
  return { row, column }
}
export const insertText = (
  state: MultilineTextBufferState,
  text: string,
): MultilineTextBufferState => {
  const sanitized = stripBracketedPasteControlSequences(text)
  if (!sanitized) {
    return state
  }
  const cursor = clampCursor(state.cursor, state.value)
  const nextValue = state.value.slice(0, cursor) + sanitized + state.value.slice(cursor)
  return {
    value: nextValue,
    cursor: cursor + sanitized.length,
  }
}
export const backspace = (state: MultilineTextBufferState): MultilineTextBufferState => {
  const cursor = clampCursor(state.cursor, state.value)
  if (cursor === 0) {
    return { value: state.value, cursor }
  }
  const nextValue = state.value.slice(0, cursor - 1) + state.value.slice(cursor)
  return {
    value: nextValue,
    cursor: cursor - 1,
  }
}
export const deleteForward = (state: MultilineTextBufferState): MultilineTextBufferState => {
  const cursor = clampCursor(state.cursor, state.value)
  if (cursor >= state.value.length) {
    return { value: state.value, cursor }
  }
  const nextValue = state.value.slice(0, cursor) + state.value.slice(cursor + 1)
  return {
    value: nextValue,
    cursor,
  }
}
export const moveCursorLeft = (state: MultilineTextBufferState): MultilineTextBufferState => ({
  value: state.value,
  cursor: Math.max(0, clampCursor(state.cursor, state.value) - 1),
})
export const moveCursorRight = (state: MultilineTextBufferState): MultilineTextBufferState => ({
  value: state.value,
  cursor: Math.min(state.value.length, clampCursor(state.cursor, state.value) + 1),
})
</file>
<file path="src/tui/components/core/single-line-text-input-keymap.ts">
import type { Key } from 'ink'
import {
  backspace,
  deleteForward,
  insertText,
  moveCursorLeft,
  moveCursorRight,
  type MultilineTextBufferState,
} from './multiline-text-buffer'
import { isBackspaceKey } from './text-input-keys'
export type SingleLineKeyAction =
  | { type: 'none' }
  | { type: 'submit' }
  | { type: 'change'; nextState: MultilineTextBufferState }
export type ResolveSingleLineKeyActionOptions = {
  input: string
  key: Key
  state: MultilineTextBufferState
}
export const resolveSingleLineKeyAction = ({
  input,
  key,
  state,
}: ResolveSingleLineKeyActionOptions): SingleLineKeyAction => {
  if (key.return) {
    return { type: 'submit' }
  }
  if (isBackspaceKey(input, key)) {
    return { type: 'change', nextState: backspace(state) }
  }
  if (key.delete) {
    return { type: 'change', nextState: deleteForward(state) }
  }
  if (key.leftArrow) {
    return { type: 'change', nextState: moveCursorLeft(state) }
  }
  if (key.rightArrow) {
    return { type: 'change', nextState: moveCursorRight(state) }
  }
  if (!input) {
    return { type: 'none' }
  }
  if (key.ctrl || key.meta) {
    return { type: 'none' }
  }
  const sanitized = input.replace(/[\r\n]/g, '')
  if (!sanitized) {
    return { type: 'none' }
  }
  return { type: 'change', nextState: insertText(state, sanitized) }
}
</file>
<file path="src/tui/components/core/SingleLineTextInput.tsx">
import React, { useEffect, useMemo, useRef, useState } from 'react'
import { Box, Text, useInput, type Key } from 'ink'
import { clampCursor, type MultilineTextBufferState } from './multiline-text-buffer'
import { resolveSingleLineKeyAction } from './single-line-text-input-keymap'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps, type InkColorValue } from '../../theme/theme-types'
export type DebugKeyEvent = {
  input: string
  key: Key
}
export type SingleLineTextInputProps = {
  value: string
  onChange: (next: string) => void
  onSubmit: (value: string) => void
  placeholder?: string | undefined
  focus?: boolean
  isDisabled?: boolean
  onDebugKeyEvent?: ((event: DebugKeyEvent) => void) | undefined
  // Optional rendering constraints (useful for overlays that must paint their full width).
  width?: number | undefined
  backgroundColor?: InkColorValue
}
type RenderLine = {
  before: string
  cursorCharacter: string
  after: string
  isPlaceholder: boolean
}
const toRenderLine = (
  value: string,
  placeholder: string | undefined,
  cursor: number,
): RenderLine => {
  if (!value) {
    const placeholderText = placeholder ?? ''
    const safeCursor = Math.min(Math.max(cursor, 0), placeholderText.length)
    const before = placeholderText.slice(0, safeCursor)
    const cursorCharacter =
      safeCursor < placeholderText.length ? placeholderText.charAt(safeCursor) : ' '
    const after = safeCursor < placeholderText.length ? placeholderText.slice(safeCursor + 1) : ''
    return { before, cursorCharacter, after, isPlaceholder: true }
  }
  const safeCursor = Math.min(Math.max(cursor, 0), value.length)
  const before = value.slice(0, safeCursor)
  const cursorCharacter = safeCursor < value.length ? value.charAt(safeCursor) : ' '
  const after = safeCursor < value.length ? value.slice(safeCursor + 1) : ''
  return { before, cursorCharacter, after, isPlaceholder: false }
}
export const SingleLineTextInput: React.FC<SingleLineTextInputProps> = ({
  value,
  onChange,
  onSubmit,
  placeholder,
  focus = false,
  isDisabled = false,
  onDebugKeyEvent,
  width,
  backgroundColor,
}) => {
  const { theme } = useTheme()
  const [cursor, setCursor] = useState<number>(value.length)
  const internalUpdateRef = useRef(false)
  const state: MultilineTextBufferState = useMemo(
    () => ({ value, cursor: clampCursor(cursor, value) }),
    [cursor, value],
  )
  const applyNextState = (nextState: MultilineTextBufferState): void => {
    internalUpdateRef.current = true
    setCursor(nextState.cursor)
    onChange(nextState.value)
  }
  useEffect(() => {
    if (internalUpdateRef.current) {
      internalUpdateRef.current = false
      return
    }
    setCursor(value.length)
  }, [value])
  useInput(
    (input, key) => {
      if (!focus || isDisabled) {
        return
      }
      if (onDebugKeyEvent) {
        onDebugKeyEvent({ input, key })
      }
      const action = resolveSingleLineKeyAction({ input, key, state })
      if (action.type === 'none') {
        return
      }
      if (action.type === 'submit') {
        onSubmit(value)
        return
      }
      applyNextState(action.nextState)
    },
    { isActive: focus && !isDisabled },
  )
  const rendered = useMemo(
    () => toRenderLine(value, placeholder, state.cursor),
    [placeholder, state.cursor, value],
  )
  const backgroundProps = inkBackgroundColorProps(backgroundColor)
  const colorProps = rendered.isPlaceholder ? inkColorProps(theme.mutedText) : {}
  const renderedLength =
    rendered.before.length + rendered.cursorCharacter.length + rendered.after.length
  const filler =
    typeof width === 'number' && width > renderedLength ? ' '.repeat(width - renderedLength) : ''
  return (
    <Box>
      <Text {...backgroundProps} {...colorProps}>
        {rendered.before}
      </Text>
      <Text inverse {...backgroundProps} {...colorProps}>
        {rendered.cursorCharacter}
      </Text>
      <Text {...backgroundProps} {...colorProps}>
        {rendered.after}
      </Text>
      {filler ? <Text {...backgroundProps}>{filler}</Text> : null}
    </Box>
  )
}
</file>
<file path="src/tui/components/core/text-input-keys.ts">
import type { Key } from 'ink'
const parseKittyCsiUCode = (input: string): number | null => {
  const match = /^\u001b\[([0-9]+)(?:;[0-9]+)*u$/.exec(input)
  if (!match) {
    return null
  }
  const raw = match[1]
  if (!raw) {
    return null
  }
  const parsed = Number(raw)
  return Number.isFinite(parsed) ? parsed : null
}
export const isBackspaceKey = (input: string, key: Key): boolean => {
  const kittyCsiUCode = parseKittyCsiUCode(input)
  const isKittyBackspaceSequence =
    (kittyCsiUCode !== null && [8, 51, 127].includes(kittyCsiUCode)) ||
    input === '\u001b[127~' ||
    input === '\u001b[8~' ||
    input === '\u001b[51~'
  const hasDel = input.includes('\u007f')
  const hasBackspace = input.includes('\b')
  const isCtrl = key.ctrl === true
  return (
    key.backspace === true ||
    hasDel ||
    hasBackspace ||
    (isCtrl && input.toLowerCase() === 'h') ||
    (isCtrl && input === '?') ||
    isKittyBackspaceSequence ||
    (key.delete === true && input === '')
  )
}
</file>
<file path="src/tui/components/core/tokenized-text.ts">
export type TokenLabelLookup = (token: string) => string | null
export const expandTokenizedText = (value: string, tokenLabel: TokenLabelLookup): string => {
  let output = ''
  for (const character of value) {
    const label = tokenLabel(character)
    output += label ?? character
  }
  return output
}
export type CursorCoordinates = {
  readonly row: number
  readonly column: number
}
export const getTokenizedCursorCoordinates = (
  value: string,
  cursor: number,
  tokenLabel: TokenLabelLookup,
): CursorCoordinates => {
  let row = 0
  let column = 0
  const safeCursor = Math.max(0, Math.min(cursor, value.length))
  for (let index = 0; index < safeCursor; index += 1) {
    const character = value[index]
    if (!character) {
      break
    }
    if (character === '\n') {
      row += 1
      column = 0
      continue
    }
    const label = tokenLabel(character)
    column += label ? label.length : 1
  }
  return { row, column }
}
export const expandTokenizedLines = (
  value: string,
  tokenLabel: TokenLabelLookup,
): readonly string[] => {
  const lines = value.split('\n')
  return lines.map((line) => expandTokenizedText(line, tokenLabel))
}
</file>
<file path="src/tui/components/popups/InstructionsPopup.tsx">
import { Box, Text, useStdout } from 'ink'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type InstructionsPopupProps = {
  draft: string
  onDraftChange: (value: string) => void
  onSubmitDraft: (value: string) => void
}
export const InstructionsPopup = ({
  draft,
  onDraftChange,
  onSubmitDraft,
}: InstructionsPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const popupHeight = 10
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Meta Instructions', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Add optional guidance (blank clears; Enter applies)', contentWidth)}
        </Text>
        <SingleLineTextInput
          value={draft}
          onChange={onDraftChange}
          onSubmit={() => onSubmitDraft(draft)}
          placeholder="Be concise and focus on security"
          focus
          width={contentWidth}
          backgroundColor={theme.popupBackground}
        />
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Esc closes Âˇ Enter saves', contentWidth)}
      </Text>
    </PopupSheet>
  )
}
</file>
<file path="src/tui/components/popups/list-window.ts">
/*
 * List âwindowingâ primitives.
 *
 * In a TUI, rendering huge lists is both slow and visually noisy.
 * These helpers answer: âGiven N items and a selection cursor, which slice
 * should we render, and should we show ââŚ earlier âŚâ / ââŚ later âŚâ indicators?â
 *
 * This module is intentionally:
 * - Pure (no React/Ink imports)
 * - Deterministic (same input => same output)
 * - Unit-testable (used by popups and other scrolling views)
 */
export type WindowedList = {
  start: number
  end: number
  showBefore: boolean
  showAfter: boolean
}
export type WindowRange = {
  startIndex: number
  endIndexExclusive: number
}
type ResolveWindowedListOptions = {
  itemCount: number
  selectedIndex: number
  maxVisibleRows: number
  lead?: number
}
const clamp = (value: number, min: number, max: number): number => {
  if (value < min) {
    return min
  }
  if (value > max) {
    return max
  }
  return value
}
const normalizeSelectedIndex = (itemCount: number, selectedIndex: number): number => {
  if (itemCount <= 0) {
    return 0
  }
  return clamp(selectedIndex, 0, itemCount - 1)
}
const resolveCursorWindowInternal = (
  itemCount: number,
  cursorIndex: number,
  windowSize: number,
  lead: number,
): WindowRange => {
  if (itemCount <= 0 || windowSize <= 0) {
    return { startIndex: 0, endIndexExclusive: 0 }
  }
  const normalizedCursor = normalizeSelectedIndex(itemCount, cursorIndex)
  const safeLead = Math.max(0, lead)
  const upperBound = Math.max(itemCount - windowSize, 0)
  const startIndex = clamp(
    normalizedCursor - Math.min(safeLead, Math.max(windowSize - 1, 0)),
    0,
    upperBound,
  )
  return {
    startIndex,
    endIndexExclusive: Math.min(startIndex + windowSize, itemCount),
  }
}
/**
 * The simplest windowing primitive.
 *
 * Required by the refactor plan: `(itemCount, cursorIndex, windowSize) -> range`.
 * We keep a small âleadâ (2 rows) so the cursor stays slightly below the top,
 * matching the current popup UX.
 */
export const resolveCursorWindow = (
  itemCount: number,
  cursorIndex: number,
  windowSize: number,
): WindowRange => resolveCursorWindowInternal(itemCount, cursorIndex, windowSize, 2)
export const resolveWindowedList = ({
  itemCount,
  selectedIndex,
  maxVisibleRows,
  lead = 2,
}: ResolveWindowedListOptions): WindowedList => {
  if (itemCount <= 0 || maxVisibleRows <= 0) {
    return { start: 0, end: 0, showBefore: false, showAfter: false }
  }
  const normalizedSelected = normalizeSelectedIndex(itemCount, selectedIndex)
  const safeLead = Math.max(0, lead)
  let showBefore = true
  let showAfter = true
  let start = 0
  let end = 0
  // We may need a couple iterations because the presence/absence of the
  // âearlier/laterâ indicator lines reduces the number of actual items that fit.
  for (let iteration = 0; iteration < 4; iteration += 1) {
    const indicatorRows = (showBefore ? 1 : 0) + (showAfter ? 1 : 0)
    const visibleItems = Math.max(1, maxVisibleRows - indicatorRows)
    const range =
      safeLead === 2
        ? resolveCursorWindow(itemCount, normalizedSelected, visibleItems)
        : resolveCursorWindowInternal(itemCount, normalizedSelected, visibleItems, safeLead)
    start = range.startIndex
    end = range.endIndexExclusive
    const nextShowBefore = start > 0
    const nextShowAfter = end < itemCount
    if (nextShowBefore === showBefore && nextShowAfter === showAfter) {
      showBefore = nextShowBefore
      showAfter = nextShowAfter
      break
    }
    showBefore = nextShowBefore
    showAfter = nextShowAfter
  }
  return { start, end, showBefore, showAfter }
}
</file>
<file path="src/tui/components/popups/PopupSheet.tsx">
import React from 'react'
import { Box } from 'ink'
import { BackgroundFill } from '../core/BackgroundFill'
import type { InkColorValue } from '../../theme/theme-types'
export type PopupSheetProps = {
  width: number
  height: number
  paddingX: number
  paddingY: number
  background: InkColorValue
  children: React.ReactNode
}
export const PopupSheet: React.FC<PopupSheetProps> = ({
  width,
  height,
  paddingX,
  paddingY,
  background,
  children,
}) => {
  const safeWidth = Math.max(0, Math.floor(width))
  const safeHeight = Math.max(0, Math.floor(height))
  return (
    <Box width={safeWidth} height={safeHeight} overflow="hidden">
      <Box position="absolute" width={safeWidth} height={safeHeight} overflow="hidden">
        <BackgroundFill rows={safeHeight} columns={safeWidth} background={background} />
      </Box>
      <Box
        flexDirection="column"
        paddingX={paddingX}
        paddingY={paddingY}
        width={safeWidth}
        height={safeHeight}
        overflow="hidden"
      >
        {children}
      </Box>
    </Box>
  )
}
</file>
<file path="src/tui/components/popups/ReasoningPopup.tsx">
import { Box, Text, useStdout } from 'ink'
import { ScrollableOutput } from '../core/ScrollableOutput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import type { HistoryEntry } from '../../types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type ReasoningPopupProps = {
  lines: readonly HistoryEntry[]
  visibleRows: number
  scrollOffset: number
}
export const ReasoningPopup = ({ lines, visibleRows, scrollOffset }: ReasoningPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const popupHeight = lines.length === 0 ? 9 : visibleRows + 8
  if (lines.length === 0) {
    return (
      <PopupSheet
        width={popupWidth}
        height={popupHeight}
        paddingX={POPUP_PADDING_X}
        paddingY={POPUP_PADDING_Y}
        background={theme.popupBackground}
      >
        <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
          {padRight('Model Reasoning', contentWidth)}
        </Text>
        <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('No reasoning recorded yet. Run generation first.', contentWidth)}
        </Text>
        <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Esc to close', contentWidth)}
        </Text>
      </PopupSheet>
    )
  }
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Model Reasoning', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column" height={visibleRows} overflow="hidden">
        <ScrollableOutput
          lines={lines}
          visibleRows={visibleRows}
          scrollOffset={scrollOffset}
          contentWidth={contentWidth}
          backgroundColor={theme.popupBackground}
        />
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('â/â scroll Âˇ PgUp/PgDn Âˇ Esc to close', contentWidth)}
      </Text>
    </PopupSheet>
  )
}
</file>
<file path="src/tui/components/popups/SeriesIntentPopup.tsx">
import { Box, Text, useStdout } from 'ink'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type SeriesIntentPopupProps = {
  draft: string
  hint?: string | undefined
  isRunning: boolean
  onDraftChange: (value: string) => void
  onSubmitDraft: (value: string) => void
}
export const SeriesIntentPopup = ({
  draft,
  hint,
  isRunning,
  onDraftChange,
  onSubmitDraft,
}: SeriesIntentPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const hintLines = hint
    ? [hint, 'Draft may come from typed text, last run, or the intent file.']
    : ['Draft may come from typed text, last run, or the intent file.']
  const popupHeight = 9 + hintLines.length
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Series Intent', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        {hintLines.map((line) => (
          <Text key={line} {...backgroundProps} {...inkColorProps(theme.mutedText)}>
            {padRight(line, contentWidth)}
          </Text>
        ))}
        <SingleLineTextInput
          value={draft}
          onChange={onDraftChange}
          onSubmit={() => onSubmitDraft(draft)}
          placeholder="Describe the project to plan"
          focus
          width={contentWidth}
          backgroundColor={theme.popupBackground}
        />
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight(
          isRunning ? 'Series run in progressâŚ please wait' : 'Enter runs series Âˇ Esc closes',
          contentWidth,
        )}
      </Text>
    </PopupSheet>
  )
}
</file>
<file path="src/tui/components/popups/SettingsPopup.tsx">
import { useMemo } from 'react'
import { Box, Text, useStdout } from 'ink'
import { resolveIndicatorSegments, type IndicatorSegment } from '../core/status-indicators-layout'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import type { InkColorValue } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type SettingsPopupProps = {
  chips: readonly string[]
}
const resolveSegmentLabel = (segment: IndicatorSegment): string => segment.label
export const SettingsPopup = ({ chips }: SettingsPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const segments = useMemo(() => resolveIndicatorSegments(chips), [chips])
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const resolveSegmentColor = (segment: IndicatorSegment): InkColorValue => {
    switch (segment.style) {
      case 'success':
        return theme.success
      case 'warning':
        return theme.warning
      case 'danger':
        return theme.error
      case 'primary':
        return theme.text
      case 'muted':
      default:
        return theme.mutedText
    }
  }
  const popupHeight = 8 + Math.max(1, segments.length)
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Current Settings', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        {segments.length === 0 ? (
          <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
            {padRight('No settings available yet.', contentWidth)}
          </Text>
        ) : (
          segments.map((segment) => {
            const label = `${resolveSegmentLabel(segment)}: `
            const value = segment.value
            const remaining = Math.max(0, contentWidth - label.length - value.length)
            return (
              <Box key={segment.id} flexDirection="row">
                <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
                  {label}
                </Text>
                <Text {...backgroundProps} {...inkColorProps(resolveSegmentColor(segment))}>
                  {value}
                </Text>
                <Text {...backgroundProps}>{' '.repeat(remaining)}</Text>
              </Box>
            )
          })
        )}
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Esc to close', contentWidth)}
      </Text>
    </PopupSheet>
  )
}
</file>
<file path="src/tui/components/popups/TestPopup.tsx">
import { Box, Text, useStdout } from 'ink'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type TestPopupProps = {
  draft: string
  isRunning: boolean
  onDraftChange: (value: string) => void
  onSubmitDraft: (value: string) => void
}
export const TestPopup = ({ draft, isRunning, onDraftChange, onSubmitDraft }: TestPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const popupHeight = 10
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Prompt Tests', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Suite path (Enter to run; blank uses prompt-tests.yaml)', contentWidth)}
        </Text>
        <SingleLineTextInput
          value={draft}
          onChange={onDraftChange}
          onSubmit={() => onSubmitDraft(draft)}
          placeholder="prompt-tests.yaml"
          focus
          width={contentWidth}
          backgroundColor={theme.popupBackground}
        />
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight(
          isRunning ? 'Tests runningâŚ please wait' : 'Enter to start tests Âˇ Esc to close',
          contentWidth,
        )}
      </Text>
    </PopupSheet>
  )
}
</file>
<file path="src/tui/components/popups/ThemeModePopup.tsx">
import { Box, Text, useStdout } from 'ink'
import type { ThemeMode } from '../../theme/theme-types'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type ThemeModePopupProps = {
  selectionIndex: number
  initialMode: ThemeMode
}
const OPTIONS: readonly ThemeMode[] = ['system', 'dark', 'light']
const formatMode = (mode: ThemeMode): string => {
  if (mode === 'system') {
    return 'System'
  }
  return mode.charAt(0).toUpperCase() + mode.slice(1)
}
export const ThemeModePopup = ({ selectionIndex, initialMode }: ThemeModePopupProps) => {
  const { theme, mode, error } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const selected = Math.min(selectionIndex, OPTIONS.length - 1)
  const popupHeight = 12 + (error ? 1 : 0)
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Theme Mode', contentWidth)}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight(
          `Current: ${formatMode(initialMode)} Âˇ Active: ${formatMode(mode)}`,
          contentWidth,
        )}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        {OPTIONS.map((option, index) => {
          const isSelected = index === selected
          const textProps = isSelected
            ? {
                ...inkColorProps(theme.selectionText),
                ...inkBackgroundColorProps(theme.selectionBackground),
              }
            : { ...backgroundProps, ...inkColorProps(theme.text) }
          return (
            <Text key={option} {...textProps}>
              {padRight(formatMode(option), contentWidth)}
            </Text>
          )
        })}
      </Box>
      {error ? (
        <Text {...backgroundProps} {...inkColorProps(theme.error)}>
          {padRight(error.message, contentWidth)}
        </Text>
      ) : null}
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('â/â select Âˇ Enter apply Âˇ Esc close', contentWidth)}
      </Text>
    </PopupSheet>
  )
}
export const THEME_MODE_OPTIONS = OPTIONS
</file>
<file path="src/tui/components/popups/TogglePopup.tsx">
import { Box, Text, useStdout } from 'ink'
import { TOGGLE_LABELS } from '../../config'
import type { ToggleField } from '../../types'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type TogglePopupProps = {
  field: ToggleField
  selectionIndex: number
}
export const TogglePopup = ({ field, selectionIndex }: TogglePopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const options = ['On', 'Off']
  const popupHeight = 10
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight(`${TOGGLE_LABELS[field]} Setting`, contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        {options.map((label, index) => {
          const isSelected = index === selectionIndex
          const textProps = isSelected
            ? {
                ...inkColorProps(theme.selectionText),
                ...inkBackgroundColorProps(theme.selectionBackground),
              }
            : { ...backgroundProps, ...inkColorProps(theme.text) }
          return (
            <Text key={label} {...textProps}>
              {padRight(label, contentWidth)}
            </Text>
          )
        })}
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Use arrows to select Âˇ Enter to confirm Âˇ Esc to cancel', contentWidth)}
      </Text>
    </PopupSheet>
  )
}
</file>
<file path="src/tui/components/OpencodeSpinner.tsx">
import React, { useEffect, useMemo, useState } from 'react'
import { Text } from 'ink'
import { inkBackgroundColorProps, type InkColorValue } from '../theme/theme-types'
const DEFAULT_LENGTH = 12
const DEFAULT_INTERVAL_MS = 80
const DEFAULT_TRAIL_LENGTH = 3
export type OpencodeSpinnerProps = {
  length?: number
  intervalMs?: number
  trailLength?: number
  backgroundColor?: InkColorValue
}
export const OpencodeSpinner: React.FC<OpencodeSpinnerProps> = ({
  length = DEFAULT_LENGTH,
  intervalMs = DEFAULT_INTERVAL_MS,
  trailLength = DEFAULT_TRAIL_LENGTH,
  backgroundColor,
}) => {
  const [frame, setFrame] = useState(0)
  useEffect(() => {
    const timer = setInterval(() => {
      setFrame((prev) => prev + 1)
    }, intervalMs)
    return () => {
      clearInterval(timer)
    }
  }, [intervalMs])
  const backgroundProps = inkBackgroundColorProps(backgroundColor)
  const segments = useMemo(() => {
    const safeLength = Math.max(1, Math.floor(length))
    const safeTrail = Math.max(0, Math.floor(trailLength))
    const span = safeLength - 1
    const period = span <= 0 ? 1 : span * 2
    const phase = frame % period
    const headIndex = span <= 0 ? 0 : phase <= span ? phase : period - phase
    const direction = span <= 0 ? 1 : phase < span ? 1 : -1
    return Array.from({ length: safeLength }, (_, index) => {
      const distanceBehind = direction === 1 ? headIndex - index : index - headIndex
      if (distanceBehind === 0) {
        return (
          <Text key={index} {...backgroundProps} color="#A78BFA">
            â
          </Text>
        )
      }
      if (distanceBehind === 1 && safeTrail >= 1) {
        return (
          <Text key={index} {...backgroundProps} color="#7C3AED">
            â
          </Text>
        )
      }
      if (distanceBehind === 2 && safeTrail >= 2) {
        return (
          <Text key={index} {...backgroundProps} color="#5B21B6">
            â
          </Text>
        )
      }
      if (distanceBehind === 3 && safeTrail >= 3) {
        return (
          <Text key={index} {...backgroundProps} color="#3B0764">
            â
          </Text>
        )
      }
      return (
        <Text key={index} {...backgroundProps} color="#333333">
          _
        </Text>
      )
    })
  }, [backgroundColor, frame, length, trailLength])
  return <Text {...backgroundProps}>{segments}</Text>
}
</file>
<file path="src/tui/hooks/useDebouncedValue.ts">
import { useEffect, useState } from 'react'
export const useDebouncedValue = <T>(value: T, delayMs: number): T => {
  const [debouncedValue, setDebouncedValue] = useState<T>(value)
  useEffect(() => {
    const timeoutHandle: ReturnType<typeof setTimeout> = setTimeout(() => {
      setDebouncedValue(value)
    }, delayMs)
    return () => {
      clearTimeout(timeoutHandle)
    }
  }, [delayMs, value])
  return debouncedValue
}
</file>
<file path="src/tui/hooks/useLatestRef.ts">
import { useEffect, useRef, type MutableRefObject } from 'react'
/**
 * Returns a ref that always holds the latest value of the passed argument.
 * Useful for accessing the latest props/state inside asynchronous callbacks
 * (like stream handlers or timeouts) without triggering effect re-runs.
 */
export const useLatestRef = <T>(value: T): MutableRefObject<T> => {
  const ref = useRef<T>(value)
  useEffect(() => {
    ref.current = value
  }, [value])
  return ref
}
</file>
<file path="src/tui/hooks/usePersistentCommandHistory.ts">
import { useCallback, useEffect, useRef, useState } from 'react'
import type { CommandHistoryRecord } from '../command-history'
import { readCommandHistory, updateCommandHistory, writeCommandHistory } from '../command-history'
export type UsePersistentCommandHistoryOptions = {
  maxEntries?: number
  onError?: (message: string) => void
}
export const usePersistentCommandHistory = ({
  maxEntries = 200,
  onError,
}: UsePersistentCommandHistoryOptions = {}): {
  entries: CommandHistoryRecord[]
  addEntry: (value: string) => void
  isLoaded: boolean
} => {
  const [entries, setEntries] = useState<CommandHistoryRecord[]>([])
  const [isLoaded, setIsLoaded] = useState(false)
  const writeQueueRef = useRef(Promise.resolve())
  useEffect(() => {
    let cancelled = false
    const load = async (): Promise<void> => {
      try {
        const loaded = await readCommandHistory()
        if (cancelled) {
          return
        }
        setEntries(loaded)
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown history error.'
        onError?.(message)
      } finally {
        if (!cancelled) {
          setIsLoaded(true)
        }
      }
    }
    void load()
    return () => {
      cancelled = true
    }
  }, [onError])
  const persist = useCallback(
    (next: CommandHistoryRecord[]) => {
      writeQueueRef.current = writeQueueRef.current
        .then(async () => {
          await writeCommandHistory(next)
        })
        .catch((error: unknown) => {
          const message = error instanceof Error ? error.message : 'Unknown history write error.'
          onError?.(message)
        })
    },
    [onError],
  )
  const addEntry = useCallback(
    (value: string) => {
      setEntries((prev) => {
        const next = updateCommandHistory({ previous: prev, nextValue: value, maxEntries })
        if (next !== prev) {
          persist(next)
        }
        return next
      })
    },
    [maxEntries, persist],
  )
  return { entries, addEntry, isLoaded }
}
</file>
<file path="src/tui/hooks/useStableCallback.ts">
import { useCallback, useEffect, useLayoutEffect, useRef } from 'react'
const useIsomorphicLayoutEffect = typeof window !== 'undefined' ? useLayoutEffect : useEffect
export const useStableCallback = <Args extends unknown[], R>(
  fn: (...args: Args) => R,
): ((...args: Args) => R) => {
  const fnRef = useRef(fn)
  useIsomorphicLayoutEffect(() => {
    fnRef.current = fn
  }, [fn])
  return useCallback((...args: Args) => fnRef.current(...args), [])
}
</file>
<file path="src/tui/screens/command/components/CommandInput.tsx">
/*
 * CommandInput
 *
 * Presentational wrapper around the core `InputBar`.
 *
 * Why wrap it?
 * - `CommandScreen` has a lot of orchestration logic; extracting this makes the
 *   render tree more readable.
 * - The wrapper also makes it clearer which props are part of the "screen model".
 */
import { InputBar } from '../../../components/core/InputBar'
import type { DebugKeyEvent } from '../../../components/core/MultilineTextInput'
export type CommandInputProps = {
  value: string
  onChange: (next: string) => void
  onSubmit: (value: string) => void
  mode: 'intent' | 'refinement'
  isDisabled: boolean
  isPasteActive: boolean
  isBusy: boolean
  statusChips: string[]
  hint?: string | undefined
  debugLine?: string | undefined
  tokenLabel: (token: string) => string | null
  onDebugKeyEvent?: ((event: DebugKeyEvent) => void) | undefined
  placeholder: string
}
export const CommandInput = ({
  value,
  onChange,
  onSubmit,
  mode,
  isDisabled,
  isPasteActive,
  isBusy,
  statusChips,
  hint,
  debugLine,
  tokenLabel,
  onDebugKeyEvent,
  placeholder,
}: CommandInputProps) => {
  return (
    <InputBar
      value={value}
      onChange={onChange}
      onSubmit={onSubmit}
      mode={mode}
      isDisabled={isDisabled}
      isPasteActive={isPasteActive}
      isBusy={isBusy}
      statusChips={statusChips}
      hint={hint}
      debugLine={debugLine}
      tokenLabel={tokenLabel}
      onDebugKeyEvent={onDebugKeyEvent}
      placeholder={placeholder}
    />
  )
}
</file>
<file path="src/tui/screens/command/components/CommandMenuPane.tsx">
/*
 * CommandMenuPane
 *
 * Presentational wrapper for the command palette list.
 */
import { Box } from 'ink'
import { CommandMenu } from '../../../components/core/CommandMenu'
import type { CommandDescriptor } from '../../../types'
export type CommandMenuPaneProps = {
  isActive: boolean
  height: number
  commands: readonly CommandDescriptor[]
  selectedIndex: number
}
export const CommandMenuPane = ({
  isActive,
  height,
  commands,
  selectedIndex,
}: CommandMenuPaneProps) => {
  if (!isActive) {
    return null
  }
  return (
    <Box marginBottom={1} height={height} flexShrink={0} overflow="hidden">
      <CommandMenu commands={commands} selectedIndex={selectedIndex} />
    </Box>
  )
}
</file>
<file path="src/tui/screens/command/components/HistoryPane.tsx">
/*
 * HistoryPane
 *
 * Presentational component: renders the scrollable history/log output.
 *
 * Keeping this separate from `CommandScreen` makes the screen easier to scan:
 * the screen model decides *what* to show, and this component decides *how* it
 * is laid out.
 */
import { Box, useStdout } from 'ink'
import { ScrollableOutput } from '../../../components/core/ScrollableOutput'
import { useTheme } from '../../../theme/theme-provider'
import { inkBackgroundColorProps } from '../../../theme/theme-types'
const APP_CONTAINER_PADDING_X = 2
const COMMAND_SCREEN_PADDING_X = 1
import type { HistoryEntry } from '../../../types'
export type HistoryPaneProps = {
  lines: HistoryEntry[]
  visibleRows: number
  scrollOffset: number
}
export const HistoryPane = ({ lines, visibleRows, scrollOffset }: HistoryPaneProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const contentWidth = Math.max(
    0,
    terminalColumns - 2 * (APP_CONTAINER_PADDING_X + COMMAND_SCREEN_PADDING_X),
  )
  return (
    <Box
      flexDirection="column"
      height={visibleRows}
      width="100%"
      flexShrink={0}
      overflow="hidden"
      marginBottom={1}
      {...inkBackgroundColorProps(theme.background)}
    >
      <ScrollableOutput
        lines={lines}
        visibleRows={visibleRows}
        scrollOffset={scrollOffset}
        contentWidth={contentWidth}
        backgroundColor={theme.background}
      />
    </Box>
  )
}
</file>
<file path="src/tui/screens/command/hooks/useCommandMenuManager.ts">
import { useEffect, useMemo, useRef } from 'react'
import { useInput, type Key } from 'ink'
import { useStableCallback } from '../../../hooks/useStableCallback'
import { resolveCommandMenuKeyAction } from '../../../components/core/command-menu-keymap'
import { filterCommandDescriptors, resolveCommandMenuSearchState } from '../../../command-filter'
import type { CommandDescriptor, PopupState } from '../../../types'
import { isCommandInput } from '../../../drag-drop-path'
type SetNumber = (next: number | ((prev: number) => number)) => void
type ScrollTo = (row: number) => void
type SetInputValue = (next: string) => void
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
type ExistsSync = (candidate: string) => boolean
export type UseCommandMenuManagerOptions = {
  inputValue: string
  existsSync: ExistsSync
  popupState: PopupState
  helpOpen: boolean
  commandMenuSignal?: number
  commands: readonly CommandDescriptor[]
  commandMenuHeight: number
  commandSelectionIndex: number
  setCommandSelectionIndex: SetNumber
  setInputValue: SetInputValue
  setPopupState: SetPopupState
  scrollTo: ScrollTo
}
export type UseCommandMenuManagerResult = {
  isCommandMode: boolean
  commandMenuFilterQuery: string
  commandMenuArgsRaw: string
  visibleCommands: readonly CommandDescriptor[]
  isCommandMenuActive: boolean
  menuHeight: number
  selectedCommand: CommandDescriptor | undefined
}
export const useCommandMenuManager = ({
  inputValue,
  existsSync,
  popupState,
  helpOpen,
  commandMenuSignal,
  commands,
  commandMenuHeight,
  commandSelectionIndex,
  setCommandSelectionIndex,
  setInputValue,
  setPopupState,
  scrollTo,
}: UseCommandMenuManagerOptions): UseCommandMenuManagerResult => {
  const trimmedInput = inputValue.trimStart()
  const isCommandMode = isCommandInput(inputValue, existsSync)
  const commandQuery = isCommandMode ? trimmedInput.slice(1).trimStart() : ''
  const parsedCommand = useMemo<{ keyword: string; args: string }>(() => {
    if (!commandQuery) {
      return { keyword: '', args: '' }
    }
    const parts = commandQuery.split(/\s+/).filter((part) => part.length > 0)
    if (parts.length === 0) {
      return { keyword: '', args: '' }
    }
    const keyword = parts[0] ?? ''
    const rest = parts.slice(1)
    return { keyword, args: rest.join(' ') }
  }, [commandQuery])
  const commandArgsRaw = parsedCommand.args
  const commandMenuSearchState = useMemo(
    () => resolveCommandMenuSearchState({ commandQuery, commands }),
    [commandQuery, commands],
  )
  const commandMenuFilterQuery = commandMenuSearchState.filterQuery
  const commandMenuArgsRaw = commandMenuSearchState.treatRemainderAsArgs ? commandArgsRaw : ''
  const commandMatches = useMemo(() => {
    if (!isCommandMode) {
      return commands
    }
    return filterCommandDescriptors({
      query: commandMenuFilterQuery,
      commands,
    })
  }, [commandMenuFilterQuery, commands, isCommandMode])
  const visibleCommands = commandMatches
  const isPopupOpen = popupState !== null
  const isCommandMenuActive = isCommandMode && !isPopupOpen && !helpOpen
  const menuHeight = isCommandMenuActive
    ? Math.min(commandMenuHeight, Math.max(visibleCommands.length, 1) + 2)
    : 0
  useEffect(() => {
    setCommandSelectionIndex(0)
  }, [commandMenuFilterQuery, isCommandMode, setCommandSelectionIndex])
  useEffect(() => {
    if (!commandMatches.length) {
      setCommandSelectionIndex(0)
      return
    }
    setCommandSelectionIndex((prev) => Math.min(prev, commandMatches.length - 1))
  }, [commandMatches.length, setCommandSelectionIndex])
  const lastCommandMenuSignalRef = useRef<number>(0)
  useEffect(() => {
    if (!commandMenuSignal || commandMenuSignal === lastCommandMenuSignalRef.current) {
      return
    }
    lastCommandMenuSignalRef.current = commandMenuSignal
    setPopupState(null)
    setInputValue('/')
    setCommandSelectionIndex(0)
    scrollTo(Number.MAX_SAFE_INTEGER)
  }, [commandMenuSignal, scrollTo, setCommandSelectionIndex, setInputValue, setPopupState])
  const handleCommandMenuKey = useStableCallback((_input: string, key: Key) => {
    if (!isCommandMenuActive) {
      return
    }
    const action = resolveCommandMenuKeyAction({
      key,
      selectedIndex: commandSelectionIndex,
      itemCount: visibleCommands.length,
    })
    if (action.type === 'close') {
      setInputValue('')
      setCommandSelectionIndex(0)
      return
    }
    if (action.type === 'change-selection') {
      setCommandSelectionIndex(action.nextIndex)
    }
  })
  useInput(handleCommandMenuKey, { isActive: isCommandMenuActive && !helpOpen })
  const selectedCommand =
    isCommandMenuActive && visibleCommands.length > 0
      ? visibleCommands[Math.min(commandSelectionIndex, visibleCommands.length - 1)]
      : undefined
  return {
    isCommandMode,
    commandMenuFilterQuery,
    commandMenuArgsRaw,
    visibleCommands,
    isCommandMenuActive,
    menuHeight,
    selectedCommand,
  }
}
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenChips.ts">
import path from 'node:path'
import { useMemo } from 'react'
import { formatProviderStatusChip } from '../../../provider-chip'
import type { ModelOption, ProviderStatusMap } from '../../../types'
export type UseCommandScreenChipsOptions = {
  currentModel: ModelOption['id']
  providerStatuses: ProviderStatusMap
  statusChips: string[]
  intentFilePath: string
  metaInstructions: string
}
export type UseCommandScreenChipsResult = {
  providerChip: string
  enhancedStatusChips: string[]
}
export const useCommandScreenChips = ({
  currentModel,
  providerStatuses,
  statusChips,
  intentFilePath,
  metaInstructions,
}: UseCommandScreenChipsOptions): UseCommandScreenChipsResult => {
  const providerChip = useMemo(
    () => formatProviderStatusChip(currentModel, providerStatuses),
    [currentModel, providerStatuses],
  )
  const trimmedIntentFilePath = intentFilePath.trim()
  const trimmedMetaInstructions = metaInstructions.trim()
  const enhancedStatusChips = useMemo(() => {
    const chips = [...statusChips, providerChip]
    if (trimmedIntentFilePath) {
      chips.push('[intent:file]')
      chips.push(`[file:${path.basename(trimmedIntentFilePath)}]`)
    } else {
      chips.push('[intent:text]')
    }
    if (trimmedMetaInstructions) {
      chips.push('[instr:on]')
    }
    return chips
  }, [providerChip, statusChips, trimmedIntentFilePath, trimmedMetaInstructions])
  return {
    providerChip,
    enhancedStatusChips,
  }
}
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenHistoryIntentPopupBindings.ts">
import type { PopupState } from '../../../types'
import { useHistoryPopupGlue } from './useHistoryPopupGlue'
import { useIntentPopupGlue } from './useIntentPopupGlue'
export type UseCommandScreenHistoryIntentPopupBindingsOptions = {
  popupState: PopupState
  setPopupState: import('react').Dispatch<import('react').SetStateAction<PopupState>>
  closePopup: () => void
  setInputValue: (value: string | ((prev: string) => string)) => void
  consumeSuppressedTextInputChange: () => boolean
  suppressNextInput: () => void
  commandHistoryValues: string[]
}
export type UseCommandScreenHistoryIntentPopupBindingsResult = {
  history: ReturnType<typeof useHistoryPopupGlue>
  intent: ReturnType<typeof useIntentPopupGlue>
}
export const useCommandScreenHistoryIntentPopupBindings = ({
  popupState,
  setPopupState,
  closePopup,
  setInputValue,
  consumeSuppressedTextInputChange,
  suppressNextInput,
  commandHistoryValues,
}: UseCommandScreenHistoryIntentPopupBindingsOptions): UseCommandScreenHistoryIntentPopupBindingsResult => {
  const history = useHistoryPopupGlue({
    popupState,
    setPopupState,
    closePopup,
    setInputValue,
    consumeSuppressedTextInputChange,
    suppressNextInput,
    commandHistoryValues,
  })
  const intent = useIntentPopupGlue({ popupState, setPopupState })
  return { history, intent }
}
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenLayout.ts">
import path from 'node:path'
import { useMemo } from 'react'
import { estimateInputBarRows } from '../../../components/core/InputBar'
import type { PopupKind, PopupState } from '../../../types'
import type { InteractiveAwaitingMode } from '../../../generation-pipeline-reducer'
export type UseCommandScreenLayoutOptions = {
  terminalRows: number
  reservedRows: number
  helpOpen: boolean
  isPopupOpen: boolean
  popupState: PopupState
  menuHeight: number
  popupHeights: Record<PopupKind, number>
  inputValue: string
  droppedFilePath: string | null
  debugKeysEnabled: boolean
  debugKeyLine: string | null
  interactiveTransportPath?: string | undefined
  isGenerating: boolean
  awaitingInteractiveMode: InteractiveAwaitingMode | null
  isCommandMenuActive: boolean
  appStaticRows: number
  commandScreenOverheadRows: number
}
export type UseCommandScreenLayoutResult = {
  overlayHeight: number
  inputBarHint: string | undefined
  inputBarDebugLine: string | undefined
  inputBarRows: number
  isAwaitingTransportInput: boolean
  historyRows: number
}
export const useCommandScreenLayout = ({
  terminalRows,
  reservedRows,
  helpOpen,
  isPopupOpen,
  popupState,
  menuHeight,
  popupHeights,
  inputValue,
  droppedFilePath,
  debugKeysEnabled,
  debugKeyLine,
  interactiveTransportPath,
  isGenerating,
  awaitingInteractiveMode,
  isCommandMenuActive,
  appStaticRows,
  commandScreenOverheadRows,
}: UseCommandScreenLayoutOptions): UseCommandScreenLayoutResult => {
  const overlayHeight = useMemo(() => {
    if (helpOpen) {
      return 0
    }
    if (popupState) {
      return popupHeights[popupState.type as PopupKind]
    }
    return menuHeight
  }, [helpOpen, menuHeight, popupHeights, popupState])
  const inputBarHint = useMemo(() => {
    if (isPopupOpen || helpOpen || !droppedFilePath) {
      return undefined
    }
    return `Press Tab to add ${path.basename(droppedFilePath)} to context`
  }, [droppedFilePath, helpOpen, isPopupOpen])
  const inputBarDebugLine = useMemo(() => {
    if (!debugKeysEnabled) {
      return undefined
    }
    return debugKeyLine ?? 'dbg: press Backspace'
  }, [debugKeyLine, debugKeysEnabled])
  const inputBarRows = useMemo(
    () =>
      estimateInputBarRows({
        value: inputValue,
        hint: inputBarHint,
        debugLine: inputBarDebugLine,
      }),
    [inputBarDebugLine, inputBarHint, inputValue],
  )
  const isAwaitingTransportInput =
    isGenerating && Boolean(interactiveTransportPath) && awaitingInteractiveMode === 'transport'
  const historyRows = useMemo(() => {
    // Popups are now rendered as an absolute overlay, so they should NOT consume
    // layout rows. Only the in-flow command menu affects available history space.
    const overlaySpacingRows = !helpOpen && isCommandMenuActive ? 1 : 0
    const baseChromeRows = appStaticRows + commandScreenOverheadRows + inputBarRows
    const transportHeaderRows = interactiveTransportPath ? 1 : 0
    const transportAwaitingRows = isAwaitingTransportInput ? 1 : 0
    const parentRows = baseChromeRows + transportHeaderRows + transportAwaitingRows
    const availableRows = terminalRows - menuHeight - parentRows - overlaySpacingRows - reservedRows
    return Math.max(1, availableRows)
  }, [
    appStaticRows,
    commandScreenOverheadRows,
    helpOpen,
    inputBarRows,
    interactiveTransportPath,
    isAwaitingTransportInput,
    isCommandMenuActive,
    menuHeight,
    reservedRows,
    terminalRows,
  ])
  return {
    overlayHeight,
    inputBarHint,
    inputBarDebugLine,
    inputBarRows,
    isAwaitingTransportInput,
    historyRows,
  }
}
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenPasteBindings.ts">
import type { PopupState } from '../../../types'
import { usePasteManager } from './usePasteManager'
export type UseCommandScreenPasteBindingsOptions = {
  inputValue: string
  popupState: PopupState
  helpOpen: boolean
  setInputValue: (value: string | ((prev: string) => string)) => void
  setPasteActive: (active: boolean) => void
  consumeSuppressedTextInputChange: () => boolean
  suppressNextInput: () => void
  updateLastTypedIntent: (next: string) => void
}
export type UseCommandScreenPasteBindingsResult = {
  tokenLabel: (token: string) => string | null
  handleInputChange: (next: string) => void
  expandInputForSubmit: (value: string) => string
}
export const useCommandScreenPasteBindings = ({
  inputValue,
  popupState,
  helpOpen,
  setInputValue,
  setPasteActive,
  consumeSuppressedTextInputChange,
  suppressNextInput,
  updateLastTypedIntent,
}: UseCommandScreenPasteBindingsOptions): UseCommandScreenPasteBindingsResult => {
  const { tokenLabel, handleInputChange, expandInputForSubmit } = usePasteManager({
    inputValue,
    popupState,
    helpOpen,
    setInputValue,
    setPasteActive,
    consumeSuppressedTextInputChange,
    suppressNextInput,
    updateLastTypedIntent,
  })
  return {
    tokenLabel,
    handleInputChange,
    expandInputForSubmit,
  }
}
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenPopupVisibility.ts">
import { useEffect } from 'react'
export type UseCommandScreenPopupVisibilityOptions = {
  isPopupOpen: boolean
  onPopupVisibilityChange?: ((isOpen: boolean) => void) | undefined
}
export const useCommandScreenPopupVisibility = ({
  isPopupOpen,
  onPopupVisibilityChange,
}: UseCommandScreenPopupVisibilityOptions): void => {
  useEffect(() => {
    if (!onPopupVisibilityChange) {
      return
    }
    onPopupVisibilityChange(isPopupOpen)
  }, [isPopupOpen, onPopupVisibilityChange])
  useEffect(() => {
    if (!onPopupVisibilityChange) {
      return undefined
    }
    return () => {
      onPopupVisibilityChange(false)
    }
  }, [onPopupVisibilityChange])
}
</file>
<file path="src/tui/screens/command/hooks/useHistoryPopupGlue.ts">
import { useCallback, useEffect, useMemo } from 'react'
import type { PopupState } from '../../../types'
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
type ClosePopup = () => void
type SetInputValue = (next: string) => void
type ConsumeSuppressedTextInputChange = () => boolean
type SuppressNextInput = () => void
export type UseHistoryPopupGlueOptions = {
  popupState: PopupState
  setPopupState: SetPopupState
  closePopup: ClosePopup
  setInputValue: SetInputValue
  consumeSuppressedTextInputChange: ConsumeSuppressedTextInputChange
  suppressNextInput: SuppressNextInput
  commandHistoryValues: readonly string[]
}
export type UseHistoryPopupGlueResult = {
  historyPopupItems: string[]
  onHistoryPopupDraftChange: (next: string) => void
  onHistoryPopupSubmit: (value: string) => void
}
export const useHistoryPopupGlue = ({
  popupState,
  setPopupState,
  closePopup,
  setInputValue,
  consumeSuppressedTextInputChange,
  suppressNextInput,
  commandHistoryValues,
}: UseHistoryPopupGlueOptions): UseHistoryPopupGlueResult => {
  const historyPopupDraft = popupState?.type === 'history' ? popupState.draft : ''
  const historyPopupItems = useMemo(() => {
    const trimmed = historyPopupDraft.trim().toLowerCase()
    if (!trimmed) {
      return [...commandHistoryValues]
    }
    return commandHistoryValues.filter((value) => value.toLowerCase().includes(trimmed))
  }, [commandHistoryValues, historyPopupDraft])
  useEffect(() => {
    if (popupState?.type !== 'history') {
      return
    }
    setPopupState((prev) => {
      if (prev?.type !== 'history') {
        return prev
      }
      const maxIndex = Math.max(historyPopupItems.length - 1, 0)
      const nextIndex = Math.min(prev.selectionIndex, maxIndex)
      return prev.selectionIndex === nextIndex ? prev : { ...prev, selectionIndex: nextIndex }
    })
  }, [historyPopupItems.length, popupState?.type, setPopupState])
  const onHistoryPopupDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) =>
        prev?.type === 'history' ? { ...prev, draft: next, selectionIndex: 0 } : prev,
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  const onHistoryPopupSubmit = useCallback(
    (value: string) => {
      if (popupState?.type !== 'history') {
        return
      }
      const trimmed = value.trim()
      const fallback = historyPopupItems[popupState.selectionIndex] ?? ''
      const selection = trimmed || fallback
      if (!selection.trim()) {
        return
      }
      suppressNextInput()
      setInputValue(selection)
      closePopup()
    },
    [closePopup, historyPopupItems, popupState, setInputValue, suppressNextInput],
  )
  return {
    historyPopupItems,
    onHistoryPopupDraftChange,
    onHistoryPopupSubmit,
  }
}
</file>
<file path="src/tui/screens/command/hooks/useHistoryScrollKeys.ts">
import { useInput, type Key } from 'ink'
import { useStableCallback } from '../../../hooks/useStableCallback'
export type UseHistoryScrollKeysOptions = {
  isCommandMenuActive: boolean
  isPopupOpen: boolean
  helpOpen: boolean
  historyRows: number
  scrollBy: (delta: number) => void
}
export const useHistoryScrollKeys = ({
  isCommandMenuActive,
  isPopupOpen,
  helpOpen,
  historyRows,
  scrollBy,
}: UseHistoryScrollKeysOptions): void => {
  const handleInput = useStableCallback((_input: string, key: Key) => {
    if (key.upArrow) {
      scrollBy(-1)
      return
    }
    if (key.downArrow) {
      scrollBy(1)
      return
    }
    if (key.pageUp) {
      scrollBy(-historyRows)
      return
    }
    if (key.pageDown) {
      scrollBy(historyRows)
    }
  })
  useInput(handleInput, { isActive: !isCommandMenuActive && !isPopupOpen && !helpOpen })
}
</file>
<file path="src/tui/screens/command/hooks/useModelPopupData.ts">
import { useMemo } from 'react'
import { useDebouncedValue } from '../../../hooks/useDebouncedValue'
import { resolveModelPopupQuery } from '../../../model-filter'
import { buildModelPopupOptions } from '../../../model-popup-options'
import { getRecentSessionModels } from '../../../model-session'
import type { ModelOption, PopupState } from '../../../types'
export type UseModelPopupDataOptions = {
  popupState: PopupState
  modelOptions: readonly ModelOption[]
}
export type UseModelPopupDataResult = {
  modelPopupOptions: ModelOption[]
  modelPopupRecentCount: number
  modelPopupSelection: number
}
export const useModelPopupData = ({
  popupState,
  modelOptions,
}: UseModelPopupDataOptions): UseModelPopupDataResult => {
  const modelPopupQuery = popupState?.type === 'model' ? popupState.query : ''
  const debouncedModelPopupQuery = useDebouncedValue(modelPopupQuery, 75)
  const effectiveModelPopupQuery = resolveModelPopupQuery(modelPopupQuery, debouncedModelPopupQuery)
  const modelPopupData = useMemo(() => {
    if (popupState?.type !== 'model') {
      return { options: [], recentCount: 0 }
    }
    const recentModelIds = getRecentSessionModels()
    return buildModelPopupOptions({
      query: effectiveModelPopupQuery,
      modelOptions,
      recentModelIds,
    })
  }, [effectiveModelPopupQuery, modelOptions, popupState?.type])
  const modelPopupOptions = modelPopupData.options
  const modelPopupRecentCount = modelPopupData.recentCount
  const modelPopupSelection =
    popupState?.type === 'model'
      ? Math.min(popupState.selectionIndex, Math.max(modelPopupOptions.length - 1, 0))
      : 0
  return {
    modelPopupOptions,
    modelPopupRecentCount,
    modelPopupSelection,
  }
}
</file>
<file path="src/tui/screens/command/hooks/useModelProviderState.ts">
import { useCallback, useEffect, useRef, useState } from 'react'
import {
  DEFAULT_MODEL_ID,
  getBuiltInModelOptions,
  getPreferredModelId,
  loadModelOptions,
} from '../../../model-options'
import { getLastSessionModel, setLastSessionModel } from '../../../model-session'
import { checkProviderStatus } from '../../../provider-status'
import type { HistoryEntry, ModelOption, ProviderStatus, ProviderStatusMap } from '../../../types'
import type { ModelProvider } from '../../../../model-providers'
import { resolveDefaultGenerateModel } from '../../../../prompt-generator-service'
const DEFAULT_PROVIDER_STATUSES: ProviderStatusMap = {
  openai: { provider: 'openai', status: 'error', message: 'Status unavailable' },
  gemini: { provider: 'gemini', status: 'error', message: 'Status unavailable' },
  other: {
    provider: 'other',
    status: 'ok',
    message: 'Custom provider (not validated)',
  },
}
type PushHistory = (content: string, kind?: HistoryEntry['kind']) => void
export type UseModelProviderStateResult = {
  modelOptions: ModelOption[]
  currentModel: ModelOption['id']
  selectModel: (nextId: ModelOption['id']) => void
  providerStatuses: ProviderStatusMap
  updateProviderStatus: (status: ProviderStatus) => void
}
export type UseModelProviderStateOptions = {
  pushHistory: PushHistory
}
export const useModelProviderState = ({
  pushHistory,
}: UseModelProviderStateOptions): UseModelProviderStateResult => {
  const builtInModelOptionsRef = useRef<ModelOption[]>(getBuiltInModelOptions())
  const initialSessionModelRef = useRef<string | null>(getLastSessionModel())
  const userSelectedModelRef = useRef(Boolean(initialSessionModelRef.current))
  const [modelOptions, setModelOptions] = useState<ModelOption[]>(builtInModelOptionsRef.current)
  const [currentModel, setCurrentModelState] = useState<ModelOption['id']>(
    initialSessionModelRef.current ?? builtInModelOptionsRef.current[0]?.id ?? DEFAULT_MODEL_ID,
  )
  const [providerStatuses, setProviderStatuses] =
    useState<ProviderStatusMap>(DEFAULT_PROVIDER_STATUSES)
  const applyCurrentModel = useCallback((nextId: ModelOption['id'], markUserSelection: boolean) => {
    setCurrentModelState((prev: ModelOption['id']) => (prev === nextId ? prev : nextId))
    setLastSessionModel(nextId)
    if (markUserSelection) {
      userSelectedModelRef.current = true
    }
  }, [])
  const selectModel = useCallback(
    (nextId: ModelOption['id']) => {
      applyCurrentModel(nextId, true)
    },
    [applyCurrentModel],
  )
  const updateProviderStatus = useCallback((status: ProviderStatus) => {
    setProviderStatuses((prev: ProviderStatusMap) => {
      const current = prev[status.provider]
      if (current && current.status === status.status && current.message === status.message) {
        return prev
      }
      return { ...prev, [status.provider]: status }
    })
  }, [])
  useEffect(() => {
    let cancelled = false
    const providers: ModelProvider[] = ['openai', 'gemini']
    const refreshStatuses = async (): Promise<void> => {
      for (const provider of providers) {
        try {
          const status = await checkProviderStatus(provider)
          if (cancelled) {
            return
          }
          updateProviderStatus(status)
        } catch (error) {
          if (cancelled) {
            return
          }
          const message = error instanceof Error ? error.message : 'Unknown provider error.'
          updateProviderStatus({ provider, status: 'error', message })
        }
      }
    }
    void refreshStatuses()
    return () => {
      cancelled = true
    }
  }, [updateProviderStatus])
  useEffect(() => {
    let cancelled = false
    const loadOptions = async (): Promise<void> => {
      try {
        const result = await loadModelOptions()
        if (cancelled) {
          return
        }
        setModelOptions(result.options)
        if (result.warning) {
          pushHistory(result.warning, 'system')
        }
        if (userSelectedModelRef.current) {
          return
        }
        const resolvedDefault = await resolveDefaultGenerateModel().catch(() => null)
        if (cancelled || userSelectedModelRef.current) {
          return
        }
        const preferred = getPreferredModelId(result.options, resolvedDefault)
        applyCurrentModel(preferred, false)
      } catch (error) {
        if (cancelled) {
          return
        }
        const message = error instanceof Error ? error.message : 'Unknown model option error.'
        pushHistory(`[model] Failed to load CLI models: ${message}`, 'system')
      }
    }
    void loadOptions()
    return () => {
      cancelled = true
    }
  }, [applyCurrentModel, pushHistory])
  return {
    modelOptions,
    currentModel,
    selectModel,
    providerStatuses,
    updateProviderStatus,
  }
}
</file>
<file path="src/tui/screens/command/hooks/usePasteManager.ts">
import { useCallback, useEffect, useRef } from 'react'
import { useInput, type Key } from 'ink'
import { useStableCallback } from '../../../hooks/useStableCallback'
import { stripBracketedPasteControlSequences } from '../../../components/core/bracketed-paste'
import {
  consumeBracketedPasteChunk,
  createBracketedPasteState,
  createPastedSnippet,
  detectPastedSnippetFromInputChange,
  type BracketedPasteState,
  type PastedSnippet,
} from '../../../paste-snippet'
import type { PopupState } from '../../../types'
import { dropMissingPasteTokens, expandPasteTokens } from '../utils/paste-tokens'
type SetInputValue = (value: string | ((prev: string) => string)) => void
type SuppressNextInput = () => void
type ConsumeSuppressedInputChange = () => boolean
type UpdateLastTypedIntent = (next: string) => void
type TokenLabel = (token: string) => string | null
type SetPasteActive = (active: boolean) => void
export type UsePasteManagerOptions = {
  inputValue: string
  popupState: PopupState
  helpOpen: boolean
  setInputValue: SetInputValue
  setPasteActive: SetPasteActive
  consumeSuppressedTextInputChange: ConsumeSuppressedInputChange
  suppressNextInput: SuppressNextInput
  updateLastTypedIntent: UpdateLastTypedIntent
}
export type UsePasteManagerResult = {
  tokenLabel: TokenLabel
  handleInputChange: (next: string) => void
  expandInputForSubmit: (value: string) => string
}
const PASTE_TOKEN_START = 0xe000
const PASTE_TOKEN_END = 0xf8ff
export const usePasteManager = ({
  inputValue,
  popupState,
  helpOpen,
  setInputValue,
  setPasteActive,
  consumeSuppressedTextInputChange,
  suppressNextInput,
  updateLastTypedIntent,
}: UsePasteManagerOptions): UsePasteManagerResult => {
  const inputValueRef = useRef('')
  inputValueRef.current = inputValue
  const pastedSnippetTokensRef = useRef<Map<string, PastedSnippet>>(new Map())
  const nextPasteTokenRef = useRef(PASTE_TOKEN_START)
  const suppressTextInputDuringPasteRef = useRef(false)
  const bracketedPasteStateRef = useRef<BracketedPasteState>(createBracketedPasteState())
  const resetPasteTokens = useCallback((): void => {
    pastedSnippetTokensRef.current.clear()
    nextPasteTokenRef.current = PASTE_TOKEN_START
  }, [])
  useEffect(() => {
    if (inputValue.length > 0) {
      return
    }
    resetPasteTokens()
  }, [inputValue.length, resetPasteTokens])
  const tokenLabel = useCallback<TokenLabel>((token: string) => {
    return pastedSnippetTokensRef.current.get(token)?.label ?? null
  }, [])
  const allocatePasteToken = useCallback((): string => {
    const map = pastedSnippetTokensRef.current
    let nextCodePoint = nextPasteTokenRef.current
    while (nextCodePoint <= PASTE_TOKEN_END) {
      const token = String.fromCharCode(nextCodePoint)
      nextCodePoint += 1
      if (!map.has(token)) {
        nextPasteTokenRef.current = nextCodePoint
        return token
      }
    }
    resetPasteTokens()
    const fallback = String.fromCharCode(PASTE_TOKEN_START)
    nextPasteTokenRef.current = PASTE_TOKEN_START + 1
    return fallback
  }, [resetPasteTokens])
  const appendInlinePaste = useCallback(
    (raw: string): void => {
      const normalized = stripBracketedPasteControlSequences(
        raw
          .replace(/\r\n/g, '\n')
          .replace(/\r/g, '\n')
          .replace(/\u0000/g, ''),
      )
      const snippet = createPastedSnippet(normalized)
      suppressNextInput()
      if (snippet) {
        const token = allocatePasteToken()
        pastedSnippetTokensRef.current.set(token, snippet)
        setInputValue((prev) => {
          const next = prev + token
          updateLastTypedIntent(next)
          return next
        })
        return
      }
      setInputValue((prev) => {
        const next = prev + normalized
        updateLastTypedIntent(next)
        return next
      })
    },
    [allocatePasteToken, setInputValue, suppressNextInput, updateLastTypedIntent],
  )
  const handlePasteInput = useStableCallback((input: string, _key: Key) => {
    if (popupState || helpOpen) {
      return
    }
    const result = consumeBracketedPasteChunk(bracketedPasteStateRef.current, input)
    bracketedPasteStateRef.current = result.state
    const nextPasteActive = result.state.isActive
    const prevPasteActive = suppressTextInputDuringPasteRef.current
    suppressTextInputDuringPasteRef.current = nextPasteActive
    if (prevPasteActive !== nextPasteActive) {
      setPasteActive(nextPasteActive)
    }
    if (result.completed.length === 0) {
      return
    }
    const latestPaste = result.completed[result.completed.length - 1] ?? ''
    appendInlinePaste(latestPaste)
  })
  useInput(handlePasteInput, { isActive: !helpOpen })
  const handleInputChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      if (popupState) {
        return
      }
      if (suppressTextInputDuringPasteRef.current) {
        return
      }
      const previous = inputValueRef.current
      const normalizedNext = stripBracketedPasteControlSequences(
        next
          .replace(/\r\n/g, '\n')
          .replace(/\r/g, '\n')
          .replace(/\u0000/g, ''),
      )
      const detection = detectPastedSnippetFromInputChange(previous, normalizedNext)
      if (detection) {
        const token = allocatePasteToken()
        pastedSnippetTokensRef.current.set(token, detection.snippet)
        const replaced =
          detection.normalizedNextValue.slice(0, detection.range.start) +
          token +
          detection.normalizedNextValue.slice(detection.range.end)
        dropMissingPasteTokens(previous, replaced, pastedSnippetTokensRef.current)
        setInputValue(replaced)
        updateLastTypedIntent(replaced)
        return
      }
      dropMissingPasteTokens(previous, normalizedNext, pastedSnippetTokensRef.current)
      setInputValue(normalizedNext)
      updateLastTypedIntent(normalizedNext)
    },
    [
      allocatePasteToken,
      consumeSuppressedTextInputChange,
      popupState,
      setInputValue,
      updateLastTypedIntent,
    ],
  )
  const expandInputForSubmit = useCallback((value: string) => {
    return expandPasteTokens(value, pastedSnippetTokensRef.current)
  }, [])
  return {
    tokenLabel,
    handleInputChange,
    expandInputForSubmit,
  }
}
</file>
<file path="src/tui/screens/command/hooks/useReasoningPopup.ts">
import { useMemo } from 'react'
import wrapAnsi from 'wrap-ansi'
import type { HistoryEntry } from '../../../types'
export type UseReasoningPopupOptions = {
  lastReasoning: string | null
  terminalColumns: number
  popupHeight: number
}
export type UseReasoningPopupResult = {
  reasoningPopupVisibleRows: number
  reasoningPopupLines: HistoryEntry[]
}
export const useReasoningPopup = ({
  lastReasoning,
  terminalColumns,
  popupHeight,
}: UseReasoningPopupOptions): UseReasoningPopupResult => {
  const reasoningPopupVisibleRows = Math.max(1, popupHeight - 5)
  const reasoningPopupLines = useMemo(() => {
    const reasoning = lastReasoning?.trim() ?? ''
    if (!reasoning) {
      return []
    }
    const entries: HistoryEntry[] = []
    const wrapWidth = Math.max(40, terminalColumns - 6)
    let entryIndex = 0
    reasoning.split('\n').forEach((line) => {
      const wrapped = wrapAnsi(line, wrapWidth, { trim: false, hard: true })
      wrapped.split('\n').forEach((wrappedLine) => {
        entries.push({
          id: `reasoning-${entryIndex}`,
          content: wrappedLine,
          kind: 'system',
        })
        entryIndex += 1
      })
    })
    return entries
  }, [lastReasoning, terminalColumns])
  return { reasoningPopupVisibleRows, reasoningPopupLines }
}
</file>
<file path="src/tui/screens/command/hooks/useSessionCommands.ts">
import type { MutableRefObject } from 'react'
import { useCallback } from 'react'
import { planSessionCommand } from '../../../new-command'
import type { HistoryEntry, PopupState } from '../../../types'
type ResetContext = () => void
type ResetHistory = () => void
type ScrollTo = (row: number) => void
type SetInputValue = (next: string) => void
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
type SetMetaInstructions = (next: string) => void
type PushHistory = (content: string, kind?: HistoryEntry['kind']) => void
export type UseSessionCommandsOptions = {
  isGenerating: boolean
  lastGeneratedPrompt: string | null
  resetContext: ResetContext
  resetHistory: ResetHistory
  scrollTo: ScrollTo
  setInputValue: SetInputValue
  setPopupState: SetPopupState
  setIntentFilePath: (value: string) => void
  setMetaInstructions: SetMetaInstructions
  lastUserIntentRef: MutableRefObject<string | null>
  lastTypedIntentRef: MutableRefObject<string>
  pushHistory: PushHistory
}
export type UseSessionCommandsResult = {
  resetSessionState: () => void
  handleNewCommand: (argsRaw: string) => void
  handleReuseCommand: () => void
}
export const useSessionCommands = ({
  isGenerating,
  lastGeneratedPrompt,
  resetContext,
  resetHistory,
  scrollTo,
  setInputValue,
  setPopupState,
  setIntentFilePath,
  setMetaInstructions,
  lastUserIntentRef,
  lastTypedIntentRef,
  pushHistory,
}: UseSessionCommandsOptions): UseSessionCommandsResult => {
  const resetSessionState = useCallback(() => {
    resetContext()
    setIntentFilePath('')
    lastUserIntentRef.current = null
    lastTypedIntentRef.current = ''
    setInputValue('')
    setPopupState(null)
    resetHistory()
    scrollTo(Number.MAX_SAFE_INTEGER)
  }, [
    lastTypedIntentRef,
    lastUserIntentRef,
    resetContext,
    resetHistory,
    scrollTo,
    setInputValue,
    setIntentFilePath,
    setPopupState,
  ])
  const handleNewCommand = useCallback(
    (argsRaw: string) => {
      if (isGenerating) {
        pushHistory('[new] Cannot reset while generation is running.', 'system')
        return
      }
      resetSessionState()
      const plan = planSessionCommand({ commandId: 'new', lastGeneratedPrompt: null })
      pushHistory(plan.message, 'system')
      if (argsRaw.includes('--reuse')) {
        pushHistory('[new] Tip: use /reuse to reuse the last prompt.', 'system')
      }
    },
    [isGenerating, pushHistory, resetSessionState],
  )
  const handleReuseCommand = useCallback(() => {
    if (isGenerating) {
      pushHistory('[reuse] Cannot reset while generation is running.', 'system')
      return
    }
    const previousPrompt = lastGeneratedPrompt
    resetSessionState()
    const plan = planSessionCommand({
      commandId: 'reuse',
      lastGeneratedPrompt: previousPrompt,
    })
    if (plan.type === 'reset-and-load-meta') {
      setMetaInstructions(plan.metaInstructions)
    }
    pushHistory(plan.message, 'system')
  }, [isGenerating, lastGeneratedPrompt, pushHistory, resetSessionState, setMetaInstructions])
  return {
    resetSessionState,
    handleNewCommand,
    handleReuseCommand,
  }
}
</file>
<file path="src/tui/screens/command/hooks/useThemeModePopupGlue.ts">
import { useCallback, useEffect, useMemo } from 'react'
import type { Dispatch, SetStateAction } from 'react'
import type { PopupState } from '../../../types'
import { useTheme } from '../../../theme/theme-provider'
import { THEME_MODE_OPTIONS } from '../../../components/popups/ThemeModePopup'
export type UseThemeModePopupGlueOptions = {
  popupState: PopupState
  setPopupState: Dispatch<SetStateAction<PopupState>>
  closePopup: () => void
}
export type UseThemeModePopupGlueResult = {
  optionCount: number
  onConfirm: () => void
  onCancel: () => void
}
export const useThemeModePopupGlue = ({
  popupState,
  setPopupState,
  closePopup,
}: UseThemeModePopupGlueOptions): UseThemeModePopupGlueResult => {
  const { setMode } = useTheme()
  const optionCount = THEME_MODE_OPTIONS.length
  useEffect(() => {
    if (popupState?.type !== 'themeMode') {
      return
    }
    setPopupState((prev) => {
      if (prev?.type !== 'themeMode') {
        return prev
      }
      const clamped = Math.min(prev.selectionIndex, optionCount - 1)
      if (clamped === prev.selectionIndex) {
        return prev
      }
      return { ...prev, selectionIndex: clamped }
    })
  }, [optionCount, popupState?.type, setPopupState])
  const onCancel = useCallback(() => {
    closePopup()
  }, [closePopup])
  const onConfirm = useCallback(() => {
    if (popupState?.type !== 'themeMode') {
      return
    }
    const selected = THEME_MODE_OPTIONS[popupState.selectionIndex]
    if (!selected) {
      return
    }
    const run = async (): Promise<void> => {
      const ok = await setMode(selected)
      if (ok) {
        closePopup()
      }
    }
    void run()
  }, [closePopup, popupState, setMode])
  return useMemo(() => ({ optionCount, onConfirm, onCancel }), [onCancel, onConfirm, optionCount])
}
</file>
<file path="src/tui/screens/command/hooks/useThemePopupGlue.ts">
import { useCallback, useEffect, useMemo, useRef } from 'react'
import type { Dispatch, SetStateAction } from 'react'
import type { PopupState } from '../../../types'
import { useTheme } from '../../../theme/theme-provider'
export type UseThemePopupGlueOptions = {
  popupState: PopupState
  setPopupState: Dispatch<SetStateAction<PopupState>>
  closePopup: () => void
}
export type UseThemePopupGlueResult = {
  themeCount: number
  onThemeConfirm: () => void
  onThemeCancel: () => void
}
export const useThemePopupGlue = ({
  popupState,
  setPopupState,
  closePopup,
}: UseThemePopupGlueOptions): UseThemePopupGlueResult => {
  const { themes, previewTheme, setTheme } = useTheme()
  const themeNames = useMemo(() => themes.map((descriptor) => descriptor.name), [themes])
  const themeCount = themeNames.length
  useEffect(() => {
    if (popupState?.type !== 'theme' || themeCount === 0) {
      return
    }
    setPopupState((prev) => {
      if (prev?.type !== 'theme') {
        return prev
      }
      const clamped = Math.min(prev.selectionIndex, Math.max(themeCount - 1, 0))
      if (clamped === prev.selectionIndex) {
        return prev
      }
      return { ...prev, selectionIndex: clamped }
    })
  }, [popupState?.type, setPopupState, themeCount])
  const lastPreviewNameRef = useRef<string | null>(null)
  useEffect(() => {
    if (popupState?.type !== 'theme') {
      lastPreviewNameRef.current = null
      return
    }
    const selectedName = themeNames[popupState.selectionIndex]
    if (!selectedName) {
      return
    }
    if (selectedName === lastPreviewNameRef.current) {
      return
    }
    lastPreviewNameRef.current = selectedName
    previewTheme(selectedName)
  }, [popupState, previewTheme, themeNames])
  const onThemeCancel = useCallback(() => {
    if (popupState?.type !== 'theme') {
      closePopup()
      return
    }
    previewTheme(popupState.initialThemeName)
    closePopup()
  }, [closePopup, popupState, previewTheme])
  const onThemeConfirm = useCallback(() => {
    if (popupState?.type !== 'theme') {
      closePopup()
      return
    }
    const selectedName = themeNames[popupState.selectionIndex]
    if (!selectedName) {
      return
    }
    const commit = async (): Promise<void> => {
      const ok = await setTheme(selectedName)
      if (ok) {
        closePopup()
      }
    }
    void commit()
  }, [closePopup, popupState, setTheme, themeNames])
  return {
    themeCount,
    onThemeConfirm,
    onThemeCancel,
  }
}
</file>
<file path="src/tui/screens/command/utils/control-key.ts">
export type ControlKey = {
  ctrl?: boolean | undefined
}
export const isControlKey = (input: string, key: ControlKey, target: string): boolean => {
  if (!target) {
    return false
  }
  const normalized = target.toLowerCase()
  const code = normalized.charCodeAt(0)
  const controlChar = code >= 97 && code <= 122 ? String.fromCharCode(code - 96) : null
  if (key.ctrl === true && input.toLowerCase() === normalized) {
    return true
  }
  return controlChar ? input === controlChar : false
}
</file>
<file path="src/tui/screens/command/utils/debug-keys.ts">
import type { DebugKeyEvent } from '../../../components/core/MultilineTextInput'
export const formatDebugKeyEvent = (event: DebugKeyEvent): string => {
  const codes = Array.from(event.input)
    .map((character) => character.codePointAt(0) ?? 0)
    .map((code) => `0x${code.toString(16).padStart(2, '0')}`)
    .join(' ')
  const activeFlags = (Object.entries(event.key) as Array<[string, unknown]>)
    .filter(([, value]) => value === true)
    .map(([name]) => name)
    .join(',')
  const safeInput = JSON.stringify(event.input)
  return `dbg input=${safeInput} codes=[${codes}] key=[${activeFlags}]`
}
</file>
<file path="src/tui/screens/command/utils/paste-tokens.ts">
import type { PastedSnippet } from '../../../paste-snippet'
type PasteTokenMap = ReadonlyMap<string, PastedSnippet>
type MutablePasteTokenMap = Map<string, PastedSnippet>
const tokensForValue = (value: string, tokens: PasteTokenMap): Set<string> => {
  return new Set(Array.from(value).filter((token) => tokens.has(token)))
}
export const dropMissingPasteTokens = (
  previous: string,
  next: string,
  tokens: MutablePasteTokenMap,
): void => {
  if (tokens.size === 0) {
    return
  }
  const previousTokens = tokensForValue(previous, tokens)
  if (previousTokens.size === 0) {
    return
  }
  const nextTokens = tokensForValue(next, tokens)
  for (const token of previousTokens) {
    if (!nextTokens.has(token)) {
      tokens.delete(token)
    }
  }
}
export const expandPasteTokens = (value: string, tokens: PasteTokenMap): string => {
  if (tokens.size === 0) {
    return value
  }
  let expanded = ''
  for (const character of value) {
    const snippet = tokens.get(character)
    expanded += snippet ? snippet.text : character
  }
  return expanded
}
</file>
<file path="src/tui/screens/command/CommandScreen.tsx">
export { CommandScreen } from './CommandScreenImpl'
export type { CommandScreenHandle } from './CommandScreenImpl'
</file>
<file path="src/tui/screens/test-runner/components/TestList.tsx">
/*
 * TestList
 *
 * Presentational component for rendering the loaded test list.
 *
 * Note: we intentionally only show the first 15 tests, matching the existing UX.
 */
import { Box, Text } from 'ink'
import { useTheme } from '../../../theme/theme-provider'
import { inkColorProps } from '../../../theme/theme-types'
import type { TestDisplayState, TestStatus } from '../test-runner-reducer'
const STATUS_LABEL: Record<TestStatus, string> = {
  pending: 'PENDING',
  running: 'RUNNING',
  pass: 'PASS',
  fail: 'FAIL',
}
export type TestListProps = {
  tests: readonly TestDisplayState[]
}
export const TestList = ({ tests }: TestListProps) => {
  const { theme } = useTheme()
  const resolveStatusColor = (status: TestStatus) => {
    switch (status) {
      case 'pending':
        return theme.mutedText
      case 'running':
        return theme.accent
      case 'pass':
        return theme.success
      case 'fail':
        return theme.error
      default: {
        const exhaustive: never = status
        return exhaustive
      }
    }
  }
  if (tests.length === 0) {
    return <Text {...inkColorProps(theme.mutedText)}>No test suite loaded yet.</Text>
  }
  const displayed = tests.slice(0, 15).map((testState, index) => (
    <Box key={`${testState.name}-${index}`} flexDirection="column">
      <Text {...inkColorProps(resolveStatusColor(testState.status))}>
        {STATUS_LABEL[testState.status].padEnd(7)} {testState.name}
      </Text>
      {testState.reason && testState.status === 'fail' ? (
        <Text {...inkColorProps(theme.mutedText)}>âł {testState.reason}</Text>
      ) : null}
    </Box>
  ))
  return (
    <>
      {displayed}
      {tests.length > 15 ? (
        <Text {...inkColorProps(theme.mutedText)}>âŚand {tests.length - 15} more test(s)</Text>
      ) : null}
    </>
  )
}
</file>
<file path="src/tui/screens/test-runner/components/TestRunnerActions.tsx">
/*
 * TestRunnerActions
 *
 * Presentational component for the "actions" section.
 */
import { Box, Text } from 'ink'
import { useTheme } from '../../../theme/theme-provider'
import { inkColorProps } from '../../../theme/theme-types'
export type TestRunnerActionsProps = {
  isFocused: boolean
  status: 'idle' | 'running'
  lastRunFile: string | null
}
export const TestRunnerActions = ({ isFocused, status, lastRunFile }: TestRunnerActionsProps) => {
  const { theme } = useTheme()
  return (
    <Box marginTop={1} flexDirection="column">
      {isFocused ? <Text {...inkColorProps(theme.accent)}>Actions</Text> : <Text>Actions</Text>}
      <Text>Press Enter to run tests</Text>
      <Text {...inkColorProps(theme.mutedText)}>
        Status: {status === 'running' ? 'Running testsâŚ' : 'Idle'}
      </Text>
      {lastRunFile ? (
        <Text {...inkColorProps(theme.mutedText)}>Last suite: {lastRunFile}</Text>
      ) : (
        <Text {...inkColorProps(theme.mutedText)}>No runs yet</Text>
      )}
    </Box>
  )
}
</file>
<file path="src/tui/screens/test-runner/components/TestRunnerError.tsx">
/*
 * TestRunnerError
 *
 * Presentational component for showing an error message.
 */
import { Text } from 'ink'
import { useTheme } from '../../../theme/theme-provider'
import { inkColorProps } from '../../../theme/theme-types'
export type TestRunnerErrorProps = {
  message: string | null
}
export const TestRunnerError = ({ message }: TestRunnerErrorProps) => {
  const { theme } = useTheme()
  if (!message) {
    return null
  }
  return <Text {...inkColorProps(theme.error)}>{message}</Text>
}
</file>
<file path="src/tui/screens/test-runner/components/TestRunnerFileInput.tsx">
/*
 * TestRunnerFileInput
 *
 * Presentational component for the test file input section.
 */
import { Box, Text, useStdout } from 'ink'
import { SingleLineTextInput } from '../../../components/core/SingleLineTextInput'
import { useTheme } from '../../../theme/theme-provider'
import {
  inkBackgroundColorProps,
  inkBorderColorProps,
  inkColorProps,
} from '../../../theme/theme-types'
const APP_CONTAINER_PADDING_X = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type TestRunnerFileInputProps = {
  filePath: string
  isFocused: boolean
  helpOpen: boolean
  onChange: (next: string) => void
  onSubmit: () => void
}
export const TestRunnerFileInput = ({
  filePath,
  isFocused,
  helpOpen,
  onChange,
  onSubmit,
}: TestRunnerFileInputProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const boxWidth = Math.max(0, terminalColumns - 2 * APP_CONTAINER_PADDING_X)
  const borderColumns = 2
  const paddingColumns = 2
  const contentWidth = Math.max(0, boxWidth - borderColumns - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.panelBackground)
  const borderColor = isFocused ? theme.accent : theme.border
  return (
    <>
      <Text {...backgroundProps} {...inkColorProps(isFocused ? theme.accent : theme.text)}>
        {padRight('Test File', contentWidth)}
      </Text>
      <Box
        borderStyle="round"
        paddingX={1}
        width={boxWidth}
        {...inkBorderColorProps(borderColor)}
        {...backgroundProps}
      >
        <SingleLineTextInput
          value={filePath}
          onChange={onChange}
          placeholder="prompt-tests.yaml"
          focus={isFocused && !helpOpen}
          onSubmit={onSubmit}
          width={contentWidth}
          backgroundColor={theme.panelBackground}
        />
      </Box>
    </>
  )
}
</file>
<file path="src/tui/screens/test-runner/components/TestRunnerLogs.tsx">
/*
 * TestRunnerLogs
 *
 * Presentational component for displaying recent test logs.
 */
import { Box, Text } from 'ink'
import type { LogEntry } from '../../../useLogBuffer'
import { useTheme } from '../../../theme/theme-provider'
import { inkColorProps } from '../../../theme/theme-types'
export type TestRunnerLogsProps = {
  logs: readonly LogEntry[]
}
export const TestRunnerLogs = ({ logs }: TestRunnerLogsProps) => {
  const { theme } = useTheme()
  if (logs.length === 0) {
    return null
  }
  return (
    <Box marginTop={1} flexDirection="column">
      <Text {...inkColorProps(theme.accent)}>Recent Logs</Text>
      {logs.map((entry) => {
        const color =
          entry.level === 'error'
            ? theme.error
            : entry.level === 'warn'
              ? theme.warning
              : theme.mutedText
        return (
          <Text key={entry.id} {...inkColorProps(color)}>
            {entry.level.toUpperCase()}: {entry.message}
          </Text>
        )
      })}
    </Box>
  )
}
</file>
<file path="src/tui/screens/test-runner/components/TestRunnerSummary.tsx">
/*
 * TestRunnerSummary
 *
 * Presentational component for displaying test run summary.
 */
import { Box, Text } from 'ink'
import { useTheme } from '../../../theme/theme-provider'
import { inkColorProps } from '../../../theme/theme-types'
import type { TestRunSummary } from '../test-runner-reducer'
export type TestRunnerSummaryProps = {
  summary: TestRunSummary | null
}
export const TestRunnerSummary = ({ summary }: TestRunnerSummaryProps) => {
  const { theme } = useTheme()
  if (!summary) {
    return null
  }
  return (
    <Box marginTop={1} flexDirection="column">
      <Text {...inkColorProps(theme.accent)}>Summary</Text>
      <Text {...inkColorProps(theme.success)}>Passed: {summary.passed}</Text>
      <Text {...inkColorProps(summary.failed > 0 ? theme.error : theme.success)}>
        Failed: {summary.failed}
      </Text>
    </Box>
  )
}
</file>
<file path="src/tui/screens/test-runner/test-runner-reducer.ts">
/*
 * Test runner reducer (pure state transitions).
 *
 * This reducer manages UI state for the TestRunnerScreen:
 * - current test file path
 * - focus (file input vs actions)
 * - loaded tests + per-test status
 * - last run info + summary + error
 *
 * Reducers in plain terms:
 * - We send "actions" describing what happened.
 * - The reducer is a pure function that returns the next state.
 *
 * Keeping this logic pure makes it easy to unit test and helps ensure the
 * screen refactor does not change behavior.
 */
export type SetStateAction<State> = State | ((prev: State) => State)
export type TestStatus = 'pending' | 'running' | 'pass' | 'fail'
export type TestDisplayState = {
  name: string
  status: TestStatus
  reason: string | null
}
export type FocusField = 'file' | 'actions'
export type RunStatus = 'idle' | 'running'
export type TestRunSummary = { passed: number; failed: number }
export type TestRunnerState = {
  filePath: string
  tests: TestDisplayState[]
  status: RunStatus
  error: string | null
  summary: TestRunSummary | null
  lastRunFile: string | null
  focus: FocusField
}
export type TestRunnerAction =
  | { type: 'set-file-path'; next: SetStateAction<string> }
  | { type: 'set-focus'; focus: FocusField }
  | { type: 'run-start' }
  | { type: 'suite-loaded'; loadedPath: string; testNames: string[] }
  | { type: 'test-start'; ordinal: number; name: string }
  | { type: 'test-complete'; ordinal: number; name: string; pass: boolean; reason: string | null }
  | { type: 'run-complete'; passed: number; failed: number }
  | { type: 'run-error'; message: string }
export const INITIAL_TEST_RUNNER_STATE: TestRunnerState = {
  filePath: 'prompt-tests.yaml',
  tests: [],
  status: 'idle',
  error: null,
  summary: null,
  lastRunFile: null,
  focus: 'file',
}
const updateTestAtOrdinal = (
  tests: TestDisplayState[],
  ordinal: number,
  updater: (prev: TestDisplayState) => TestDisplayState,
): TestDisplayState[] => {
  // Ordinals are 1-based in the test runner callbacks.
  const index = ordinal - 1
  if (index < 0 || index >= tests.length) {
    return tests
  }
  const next = [...tests]
  next[index] = updater(next[index] ?? { name: 'unknown', status: 'pending', reason: null })
  return next
}
export const testRunnerReducer = (
  state: TestRunnerState,
  action: TestRunnerAction,
): TestRunnerState => {
  switch (action.type) {
    case 'set-file-path': {
      const next = typeof action.next === 'function' ? action.next(state.filePath) : action.next
      return next === state.filePath ? state : { ...state, filePath: next }
    }
    case 'set-focus':
      return action.focus === state.focus ? state : { ...state, focus: action.focus }
    case 'run-start':
      return {
        ...state,
        status: 'running',
        error: null,
        summary: null,
        tests: [],
      }
    case 'suite-loaded':
      return {
        ...state,
        lastRunFile: action.loadedPath,
        tests: action.testNames.map((name) => ({ name, status: 'pending', reason: null })),
      }
    case 'test-start':
      return {
        ...state,
        tests: updateTestAtOrdinal(state.tests, action.ordinal, (prev) => ({
          ...prev,
          name: action.name,
          status: 'running',
        })),
      }
    case 'test-complete':
      return {
        ...state,
        tests: updateTestAtOrdinal(state.tests, action.ordinal, () => ({
          name: action.name,
          status: action.pass ? 'pass' : 'fail',
          reason: action.reason,
        })),
      }
    case 'run-complete':
      return {
        ...state,
        status: 'idle',
        summary: { passed: action.passed, failed: action.failed },
      }
    case 'run-error':
      return {
        ...state,
        status: 'idle',
        error: action.message,
      }
    default:
      return state
  }
}
</file>
<file path="src/tui/screens/test-runner/TestRunnerScreen.tsx">
/*
 * TestRunnerScreen
 *
 * This screen runs `prompt-tests.yaml` (or another file) and shows progress.
 *
 * Architecture note:
 * - Ink screens tend to accumulate lots of tiny state updates.
 * - We keep state transitions explicit via a reducer (`test-runner-reducer.ts`).
 * - This screen component stays focused on orchestration:
 *   input handling + calling `runPromptTestSuite`.
 */
import { forwardRef, useCallback, useImperativeHandle, useMemo, useRef } from 'react'
import path from 'node:path'
import { Box, Text, useInput } from 'ink'
import { runPromptTestSuite, type PromptTestRunReporter } from '../../../test-command'
import { useLogBuffer } from '../../useLogBuffer'
import { useTheme } from '../../theme/theme-provider'
import { inkColorProps } from '../../theme/theme-types'
import { useTestRunnerScreen } from './useTestRunnerScreen'
import { TestRunnerActions } from './components/TestRunnerActions'
import { TestRunnerError } from './components/TestRunnerError'
import { TestRunnerFileInput } from './components/TestRunnerFileInput'
import { TestRunnerLogs } from './components/TestRunnerLogs'
import { TestList } from './components/TestList'
import { TestRunnerSummary } from './components/TestRunnerSummary'
export type TestRunnerScreenHandle = {
  suppressNextInput: () => void
}
export type TestRunnerScreenProps = {
  helpOpen?: boolean
}
export const TestRunnerScreen = forwardRef<TestRunnerScreenHandle, TestRunnerScreenProps>(
  ({ helpOpen = false }, ref) => {
    const { theme } = useTheme()
    const {
      state,
      canRun,
      setFilePath,
      setFocus,
      focusNext,
      focusPrevious,
      startRun,
      suiteLoaded,
      testStarted,
      testCompleted,
      runCompleted,
      runErrored,
    } = useTestRunnerScreen()
    const { logs, log, clearLogs } = useLogBuffer()
    const suppressNextInputRef = useRef(false)
    useImperativeHandle(
      ref,
      () => ({
        suppressNextInput: () => {
          suppressNextInputRef.current = true
        },
      }),
      [],
    )
    const consumeSuppressedTextInputChange = useCallback((): boolean => {
      if (!suppressNextInputRef.current) {
        return false
      }
      suppressNextInputRef.current = false
      return true
    }, [])
    const handleFilePathChange = useCallback(
      (next: string) => {
        if (consumeSuppressedTextInputChange()) {
          return
        }
        setFilePath(next)
      },
      [consumeSuppressedTextInputChange, setFilePath],
    )
    const reporter = useMemo<PromptTestRunReporter>(
      () => ({
        onSuiteLoaded: (suite, loadedPath) => {
          suiteLoaded(
            loadedPath,
            suite.tests.map((test) => test.name),
          )
          clearLogs()
          log.info(`Loaded ${suite.tests.length} test(s) from ${loadedPath}`)
        },
        onTestStart: (ordinal, test) => {
          testStarted(ordinal, test.name)
        },
        onTestComplete: (ordinal, result) => {
          testCompleted(ordinal, result.name, result.pass, result.reason ?? null)
          if (!result.pass) {
            log.warn(`[${result.name}] ${result.reason}`)
          }
        },
        onComplete: (results) => {
          const passed = results.filter((result) => result.pass).length
          const failed = results.length - passed
          runCompleted(passed, failed)
          if (failed === 0) {
            log.info('All tests passed')
          } else {
            log.error(`${failed} test(s) failed`)
          }
        },
      }),
      [clearLogs, log, runCompleted, suiteLoaded, testCompleted, testStarted],
    )
    const handleRun = useCallback(async () => {
      if (!canRun) {
        return
      }
      startRun()
      const resolvedPath = path.resolve(process.cwd(), state.filePath.trim())
      try {
        await runPromptTestSuite(resolvedPath, { reporter })
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown test execution error.'
        runErrored(message)
        log.error(message)
      }
    }, [canRun, log, reporter, runErrored, startRun, state.filePath])
    useInput(
      (_, key) => {
        if (state.status === 'running') {
          return
        }
        if (key.tab && key.shift) {
          focusPrevious()
          return
        }
        if (key.tab) {
          focusNext()
          return
        }
        if (state.focus === 'actions' && key.return && canRun) {
          void handleRun()
        }
      },
      { isActive: !helpOpen },
    )
    return (
      <Box flexDirection="column" marginTop={1}>
        <TestRunnerFileInput
          filePath={state.filePath}
          isFocused={state.focus === 'file'}
          helpOpen={helpOpen}
          onChange={handleFilePathChange}
          onSubmit={() => setFocus('actions')}
        />
        <TestRunnerActions
          isFocused={state.focus === 'actions'}
          status={state.status}
          lastRunFile={state.lastRunFile}
        />
        <Box marginTop={1} flexDirection="column">
          <Text {...inkColorProps(theme.accent)}>Tests</Text>
          <TestList tests={state.tests} />
        </Box>
        <TestRunnerSummary summary={state.summary} />
        <TestRunnerLogs logs={logs} />
        <TestRunnerError message={state.error} />
      </Box>
    )
  },
)
TestRunnerScreen.displayName = 'TestRunnerScreen'
</file>
<file path="src/tui/screens/test-runner/useTestRunnerScreen.ts">
/*
 * useTestRunnerScreen
 *
 * Hook wrapper around the pure test runner reducer.
 *
 * This keeps `TestRunnerScreen` focused on orchestration (Ink input, async test
 * execution) and keeps state transitions explicit and testable.
 */
import { useCallback, useMemo, useReducer } from 'react'
import {
  INITIAL_TEST_RUNNER_STATE,
  testRunnerReducer,
  type FocusField,
  type SetStateAction,
  type TestRunnerState,
} from './test-runner-reducer'
const focusOrder: FocusField[] = ['file', 'actions']
const nextFocus = (current: FocusField): FocusField => {
  const index = focusOrder.indexOf(current)
  return focusOrder[(index + 1) % focusOrder.length] ?? 'file'
}
const previousFocus = (current: FocusField): FocusField => {
  const index = focusOrder.indexOf(current)
  return focusOrder[(index - 1 + focusOrder.length) % focusOrder.length] ?? 'file'
}
export type UseTestRunnerScreenResult = {
  state: TestRunnerState
  canRun: boolean
  setFilePath: (next: SetStateAction<string>) => void
  focusNext: () => void
  focusPrevious: () => void
  setFocus: (focus: FocusField) => void
  startRun: () => void
  suiteLoaded: (loadedPath: string, testNames: string[]) => void
  testStarted: (ordinal: number, name: string) => void
  testCompleted: (ordinal: number, name: string, pass: boolean, reason: string | null) => void
  runCompleted: (passed: number, failed: number) => void
  runErrored: (message: string) => void
}
export const useTestRunnerScreen = (): UseTestRunnerScreenResult => {
  const [state, dispatch] = useReducer(testRunnerReducer, INITIAL_TEST_RUNNER_STATE)
  const canRun = useMemo(
    () => state.status !== 'running' && state.filePath.trim().length > 0,
    [state.filePath, state.status],
  )
  const setFilePath = useCallback((next: SetStateAction<string>) => {
    dispatch({ type: 'set-file-path', next })
  }, [])
  const setFocus = useCallback((focus: FocusField) => {
    dispatch({ type: 'set-focus', focus })
  }, [])
  const focusNext = useCallback(() => {
    setFocus(nextFocus(state.focus))
  }, [setFocus, state.focus])
  const focusPrevious = useCallback(() => {
    setFocus(previousFocus(state.focus))
  }, [setFocus, state.focus])
  const startRun = useCallback(() => {
    dispatch({ type: 'run-start' })
  }, [])
  const suiteLoaded = useCallback((loadedPath: string, testNames: string[]) => {
    dispatch({ type: 'suite-loaded', loadedPath, testNames })
  }, [])
  const testStarted = useCallback((ordinal: number, name: string) => {
    dispatch({ type: 'test-start', ordinal, name })
  }, [])
  const testCompleted = useCallback(
    (ordinal: number, name: string, pass: boolean, reason: string | null) => {
      dispatch({ type: 'test-complete', ordinal, name, pass, reason })
    },
    [],
  )
  const runCompleted = useCallback((passed: number, failed: number) => {
    dispatch({ type: 'run-complete', passed, failed })
  }, [])
  const runErrored = useCallback((message: string) => {
    dispatch({ type: 'run-error', message })
  }, [])
  return {
    state,
    canRun,
    setFilePath,
    focusNext,
    focusPrevious,
    setFocus,
    startRun,
    suiteLoaded,
    testStarted,
    testCompleted,
    runCompleted,
    runErrored,
  }
}
</file>
<file path="src/tui/theme/builtins/pm-dark.ts">
import type { ThemeJson } from '../theme-types'
export const PM_DARK_THEME = {
  defs: {
    bg: '#0b0f14',
    panelBg: '#111820',
    text: '#e6edf3',
    muted: '#8b949e',
    border: '#30363d',
    accent: '#58a6ff',
    warning: '#d29922',
    error: '#f85149',
    success: '#3fb950',
    selectionBg: '#1f6feb',
  },
  theme: {
    background: 'bg',
    text: 'text',
    mutedText: 'muted',
    border: 'border',
    accent: 'accent',
    accentText: 'bg',
    warning: 'warning',
    error: 'error',
    success: 'success',
    panelBackground: 'panelBg',
    popupBackground: 'panelBackground',
    selectionBackground: 'selectionBg',
    selectionText: 'bg',
    chipBackground: 'panelBackground',
    chipText: 'text',
    chipMutedText: 'mutedText',
  },
} as const satisfies ThemeJson
</file>
<file path="src/tui/theme/builtins/pm-light.ts">
import type { ThemeJson } from '../theme-types'
export const PM_LIGHT_THEME = {
  defs: {
    bg: '#ffffff',
    panelBg: '#f6f8fa',
    text: '#24292f',
    muted: '#57606a',
    border: '#d0d7de',
    accent: '#0969da',
    warning: '#9a6700',
    error: '#cf222e',
    success: '#1a7f37',
    selectionBg: '#ddf4ff',
  },
  theme: {
    background: 'bg',
    text: 'text',
    mutedText: 'muted',
    border: 'border',
    accent: 'accent',
    accentText: 'bg',
    warning: 'warning',
    error: 'error',
    success: 'success',
    panelBackground: 'panelBg',
    popupBackground: 'panelBackground',
    selectionBackground: 'selectionBg',
    selectionText: 'text',
    chipBackground: 'panelBackground',
    chipText: 'text',
    chipMutedText: 'mutedText',
  },
} as const satisfies ThemeJson
</file>
<file path="src/tui/theme/examples/ocean-example.json">
{
  "defs": {
    "bg": { "dark": "#10141a", "light": "#fafcff" },
    "panel": { "dark": "#1a2029", "light": "#eef3f8" },
    "text": { "dark": "#e6edf3", "light": "#1f2328" },
    "muted": { "dark": "#9aa4b2", "light": "#66707f" },
    "border": { "dark": "#30363d", "light": "#d0d7de" },
    "accent": { "dark": "#6cb6ff", "light": "#0969da" },
    "warning": "#f2cc60",
    "error": "#ff6b6b",
    "success": "#3fb950"
  },
  "theme": {
    "background": "bg",
    "text": "text",
    "mutedText": "muted",
    "border": "border",
    "accent": "accent",
    "accentText": "background",
    "warning": "warning",
    "error": "error",
    "success": "success",
    "panelBackground": "panel",
    "popupBackground": "panelBackground",
    "selectionBackground": { "dark": 60, "light": 153 },
    "selectionText": "text",
    "chipBackground": "panelBackground",
    "chipText": "text",
    "chipMutedText": "muted"
  }
}
</file>
<file path="src/tui/theme/terminal-appearance.ts">
import type { ThemeAppearanceMode } from './theme-types'
const parseAnsiIndex = (value: string): number | null => {
  const trimmed = value.trim()
  if (!trimmed) {
    return null
  }
  const parsed = Number.parseInt(trimmed, 10)
  return Number.isFinite(parsed) ? parsed : null
}
const detectFromColorFgBg = (envValue: string): ThemeAppearanceMode | null => {
  // `COLORFGBG` commonly looks like "15;0" (foreground;background)
  // but some terminals include extra segments. We only care about the last.
  const parts = envValue
    .split(';')
    .map((part) => part.trim())
    .filter((part) => part.length > 0)
  const last = parts[parts.length - 1]
  if (!last) {
    return null
  }
  const background = parseAnsiIndex(last)
  if (background === null) {
    return null
  }
  if (background >= 0 && background <= 6) {
    return 'dark'
  }
  if (background >= 7 && background <= 15) {
    return 'light'
  }
  return null
}
export const detectTerminalAppearanceMode = (
  env: NodeJS.ProcessEnv,
): ThemeAppearanceMode | null => {
  const explicit = env.TERM_BACKGROUND?.trim().toLowerCase()
  if (explicit === 'light' || explicit === 'dark') {
    return explicit
  }
  const colorFgBg = env.COLORFGBG?.trim()
  if (colorFgBg) {
    return detectFromColorFgBg(colorFgBg)
  }
  return null
}
</file>
<file path="src/tui/theme/theme-provider.tsx">
import React, { createContext, useCallback, useContext, useEffect, useMemo, useState } from 'react'
import { detectTerminalAppearanceMode } from './terminal-appearance'
import { DEFAULT_THEME_NAME, getThemeJson } from './theme-registry'
import type { ThemeDescriptor } from './theme-loader'
import { resolveTheme } from './theme-resolver'
import {
  loadThemeSelection,
  saveThemeSelection,
  type ThemeSelectionWarning,
} from './theme-settings-service'
import {
  REQUIRED_THEME_SLOTS,
  type ResolvedTheme,
  type ThemeAppearanceMode,
  type ThemeJson,
  type ThemeMode,
} from './theme-types'
export type ThemeProviderError = {
  kind: 'load-failed' | 'resolve-failed' | 'save-failed'
  message: string
}
export type ThemeContextValue = {
  theme: ResolvedTheme
  mode: ThemeMode
  setMode: (mode: ThemeMode) => Promise<boolean>
  activeThemeName: string
  setTheme: (name: string) => Promise<boolean>
  previewTheme: (name: string) => boolean
  themes: readonly ThemeDescriptor[]
  warnings: readonly ThemeSelectionWarning[]
  error: ThemeProviderError | null
}
const ThemeContext = createContext<ThemeContextValue | null>(null)
export const useTheme = (): ThemeContextValue => {
  const context = useContext(ThemeContext)
  if (!context) {
    throw new Error('useTheme must be used within ThemeProvider')
  }
  return context
}
const FALLBACK_THEME: ResolvedTheme = Object.fromEntries(
  REQUIRED_THEME_SLOTS.map((slot) => [slot, undefined]),
) as ResolvedTheme
const resolveThemeOrThrow = (themeJson: ThemeJson, mode: ThemeAppearanceMode): ResolvedTheme =>
  resolveTheme(themeJson, mode)
const resolveAppearanceMode = (mode: ThemeMode): ThemeAppearanceMode => {
  if (mode === 'light') {
    return 'light'
  }
  if (mode === 'dark') {
    return 'dark'
  }
  // Pragmatic "system" mode: if we can't reliably detect the terminal background,
  // we deterministically fall back to `dark`.
  return detectTerminalAppearanceMode(process.env) ?? 'dark'
}
const resolveThemeFromName = (params: {
  themes: readonly ThemeDescriptor[]
  name: string
  appearanceMode: ThemeAppearanceMode
}): { theme: ResolvedTheme; themeName: string } => {
  const descriptor = params.themes.find((theme) => theme.name === params.name)
  if (descriptor) {
    return {
      theme: resolveThemeOrThrow(descriptor.theme, params.appearanceMode),
      themeName: params.name,
    }
  }
  const fallbackJson = getThemeJson(DEFAULT_THEME_NAME)
  if (!fallbackJson) {
    return { theme: FALLBACK_THEME, themeName: DEFAULT_THEME_NAME }
  }
  return {
    theme: resolveThemeOrThrow(fallbackJson, params.appearanceMode),
    themeName: DEFAULT_THEME_NAME,
  }
}
const resolveThemeFromNameStrict = (params: {
  themes: readonly ThemeDescriptor[]
  name: string
  appearanceMode: ThemeAppearanceMode
}): { theme: ResolvedTheme; themeName: string } => {
  const descriptor = params.themes.find((theme) => theme.name === params.name)
  if (!descriptor) {
    throw new Error(`Unknown theme '${params.name}'.`)
  }
  return {
    theme: resolveThemeOrThrow(descriptor.theme, params.appearanceMode),
    themeName: descriptor.name,
  }
}
export const ThemeProvider: React.FC<{ children: React.ReactNode }> = ({ children }) => {
  const [themes, setThemes] = useState<readonly ThemeDescriptor[]>([])
  const [activeThemeName, setActiveThemeName] = useState(DEFAULT_THEME_NAME)
  const [mode, setModeState] = useState<ThemeMode>('dark')
  const [warnings, setWarnings] = useState<readonly ThemeSelectionWarning[]>([])
  const [error, setError] = useState<ThemeProviderError | null>(null)
  const [theme, setThemeState] = useState<ResolvedTheme>(() => {
    const base = getThemeJson(DEFAULT_THEME_NAME)
    if (!base) {
      return FALLBACK_THEME
    }
    try {
      return resolveThemeOrThrow(base, 'dark')
    } catch {
      return FALLBACK_THEME
    }
  })
  useEffect(() => {
    let cancelled = false
    const run = async (): Promise<void> => {
      try {
        const loaded = await loadThemeSelection()
        if (cancelled) {
          return
        }
        setThemes(loaded.themes)
        setWarnings(loaded.warnings)
        setModeState(loaded.selection.themeMode)
        const appearanceMode = resolveAppearanceMode(loaded.selection.themeMode)
        try {
          const resolved = resolveThemeFromName({
            themes: loaded.themes,
            name: loaded.selection.themeName,
            appearanceMode,
          })
          setActiveThemeName(resolved.themeName)
          setThemeState(resolved.theme)
          if (resolved.themeName !== loaded.selection.themeName) {
            void saveThemeSelection({ themeName: resolved.themeName })
          }
          setError(null)
        } catch (resolveError) {
          const message =
            resolveError instanceof Error ? resolveError.message : 'Unknown theme resolution error.'
          setError({ kind: 'resolve-failed', message })
          const fallbackJson = getThemeJson(DEFAULT_THEME_NAME)
          if (fallbackJson) {
            setActiveThemeName(DEFAULT_THEME_NAME)
            setThemeState(resolveThemeOrThrow(fallbackJson, appearanceMode))
            void saveThemeSelection({ themeName: DEFAULT_THEME_NAME })
          }
        }
      } catch (loadError) {
        const message = loadError instanceof Error ? loadError.message : 'Unknown theme load error.'
        setError({ kind: 'load-failed', message })
      }
    }
    void run()
    return () => {
      cancelled = true
    }
  }, [])
  const previewTheme = useCallback(
    (name: string): boolean => {
      if (themes.length === 0) {
        setError({ kind: 'resolve-failed', message: 'No themes loaded.' })
        return false
      }
      const appearanceMode = resolveAppearanceMode(mode)
      try {
        const resolved = resolveThemeFromNameStrict({ themes, name, appearanceMode })
        setActiveThemeName(resolved.themeName)
        setThemeState(resolved.theme)
        setError(null)
        return true
      } catch (resolveError) {
        const message =
          resolveError instanceof Error ? resolveError.message : 'Unknown theme resolution error.'
        setError({ kind: 'resolve-failed', message })
        return false
      }
    },
    [mode, themes],
  )
  const setTheme = useCallback(
    async (name: string): Promise<boolean> => {
      if (themes.length === 0) {
        setError({ kind: 'resolve-failed', message: 'No themes loaded.' })
        return false
      }
      const appearanceMode = resolveAppearanceMode(mode)
      let resolved: { theme: ResolvedTheme; themeName: string }
      try {
        resolved = resolveThemeFromNameStrict({ themes, name, appearanceMode })
      } catch (resolveError) {
        const message =
          resolveError instanceof Error ? resolveError.message : 'Unknown theme resolution error.'
        setError({ kind: 'resolve-failed', message })
        return false
      }
      try {
        await saveThemeSelection({ themeName: resolved.themeName })
      } catch (saveError) {
        const message = saveError instanceof Error ? saveError.message : 'Unknown theme save error.'
        setError({ kind: 'save-failed', message })
        return false
      }
      setActiveThemeName(resolved.themeName)
      setThemeState(resolved.theme)
      setError(null)
      return true
    },
    [mode, themes],
  )
  const setMode = useCallback(
    async (nextMode: ThemeMode): Promise<boolean> => {
      if (themes.length === 0) {
        setError({ kind: 'resolve-failed', message: 'No themes loaded.' })
        return false
      }
      const appearanceMode = resolveAppearanceMode(nextMode)
      let resolved: { theme: ResolvedTheme; themeName: string }
      try {
        resolved = resolveThemeFromNameStrict({ themes, name: activeThemeName, appearanceMode })
      } catch (resolveError) {
        const message =
          resolveError instanceof Error ? resolveError.message : 'Unknown theme resolution error.'
        setError({ kind: 'resolve-failed', message })
        return false
      }
      try {
        await saveThemeSelection({ themeMode: nextMode })
      } catch (saveError) {
        const message = saveError instanceof Error ? saveError.message : 'Unknown theme save error.'
        setError({ kind: 'save-failed', message })
        return false
      }
      setModeState(nextMode)
      setThemeState(resolved.theme)
      setError(null)
      return true
    },
    [activeThemeName, themes],
  )
  const value = useMemo<ThemeContextValue>(
    () => ({
      theme,
      mode,
      setMode,
      activeThemeName,
      setTheme,
      previewTheme,
      themes,
      warnings,
      error,
    }),
    [activeThemeName, error, mode, previewTheme, setMode, setTheme, theme, themes, warnings],
  )
  return <ThemeContext.Provider value={value}>{children}</ThemeContext.Provider>
}
</file>
<file path="src/tui/theme/theme-registry.ts">
import type { ThemeJson } from './theme-types'
import { PM_DARK_THEME } from './builtins/pm-dark'
import { PM_LIGHT_THEME } from './builtins/pm-light'
export type ThemeRegistryEntry = {
  name: string
  label: string
  theme: ThemeJson
}
export const DEFAULT_THEME_NAME = 'pm-dark'
const BUILTIN_THEMES: readonly ThemeRegistryEntry[] = [
  {
    name: 'pm-dark',
    label: 'Prompt Maker Dark',
    theme: PM_DARK_THEME,
  },
  {
    name: 'pm-light',
    label: 'Prompt Maker Light',
    theme: PM_LIGHT_THEME,
  },
]
export const listThemes = (): readonly ThemeRegistryEntry[] => BUILTIN_THEMES
export const getTheme = (name: string): ThemeRegistryEntry | undefined =>
  BUILTIN_THEMES.find((theme) => theme.name === name)
export const getThemeJson = (name: string): ThemeJson | undefined => getTheme(name)?.theme
</file>
<file path="src/tui/theme/theme-settings-service.ts">
import { loadCliConfig, updateCliThemeSettings, type PromptMakerCliConfig } from '../../config'
import { DEFAULT_THEME_NAME } from './theme-registry'
import {
  loadThemes,
  type LoadThemesOptions,
  type ThemeDescriptor,
  type ThemeLoadError,
} from './theme-loader'
import type { ThemeMode } from './theme-types'
export const DEFAULT_THEME_MODE: ThemeMode = 'dark'
export type ThemeSelection = {
  themeName: string
  themeMode: ThemeMode
}
export type ThemeSelectionWarning = {
  kind: 'unknown-theme'
  requested: string
  fallback: string
  message: string
}
const resolveThemeMode = (config: PromptMakerCliConfig | null): ThemeMode =>
  config?.themeMode ?? DEFAULT_THEME_MODE
const resolveThemeName = (
  config: PromptMakerCliConfig | null,
  themes: readonly ThemeDescriptor[],
): { themeName: string; warnings: ThemeSelectionWarning[] } => {
  const requested = config?.theme?.trim()
  if (!requested) {
    return { themeName: DEFAULT_THEME_NAME, warnings: [] }
  }
  const exists = themes.some((theme) => theme.name === requested)
  if (exists) {
    return { themeName: requested, warnings: [] }
  }
  return {
    themeName: DEFAULT_THEME_NAME,
    warnings: [
      {
        kind: 'unknown-theme',
        requested,
        fallback: DEFAULT_THEME_NAME,
        message: `Unknown theme '${requested}', falling back to '${DEFAULT_THEME_NAME}'.`,
      },
    ],
  }
}
export const resolveThemeSelectionFromConfig = (params: {
  config: PromptMakerCliConfig | null
  themes: readonly ThemeDescriptor[]
}): { selection: ThemeSelection; warnings: ThemeSelectionWarning[] } => {
  const themeMode = resolveThemeMode(params.config)
  const resolved = resolveThemeName(params.config, params.themes)
  return {
    selection: {
      themeName: resolved.themeName,
      themeMode,
    },
    warnings: resolved.warnings,
  }
}
export const loadThemeSelection = async (params?: {
  themeLoadOptions?: LoadThemesOptions
}): Promise<{
  themes: ThemeDescriptor[]
  loadErrors: ThemeLoadError[]
  selection: ThemeSelection
  warnings: ThemeSelectionWarning[]
}> => {
  const [config, loaded] = await Promise.all([
    loadCliConfig(),
    loadThemes(params?.themeLoadOptions ?? {}),
  ])
  const resolved = resolveThemeSelectionFromConfig({ config, themes: loaded.themes })
  return {
    themes: loaded.themes,
    loadErrors: loaded.errors,
    selection: resolved.selection,
    warnings: resolved.warnings,
  }
}
export const saveThemeSelection = async (selection: Partial<ThemeSelection>): Promise<void> => {
  await updateCliThemeSettings({
    ...(selection.themeName !== undefined ? { theme: selection.themeName } : {}),
    ...(selection.themeMode !== undefined ? { themeMode: selection.themeMode } : {}),
  })
}
</file>
<file path="src/tui/command-filter.ts">
/*
 * Command palette filtering.
 *
 * The CommandScreen has a command palette ("/command") with fuzzy-ish matching.
 * This file is intentionally pure so we can unit test and refactor the UI safely.
 *
 * Key UX rules (preserved by tests):
 * - Empty query returns all commands in their configured order.
 * - Matches are case-insensitive.
 * - Prefix matches (id/alias/label) are ranked above substring matches.
 * - When scores tie, original order is preserved (stable ordering).
 */
import type { CommandDescriptor } from './types'
type ScoredCommand = {
  command: CommandDescriptor
  score: number
  index: number
}
const normalizeQueryTokens = (query: string): string[] =>
  query
    .trim()
    .toLowerCase()
    .split(/\s+/)
    .filter((token) => token.length > 0)
const getCommandAliases = (command: CommandDescriptor): readonly string[] => {
  if (!('aliases' in command)) {
    return []
  }
  const aliases = command.aliases
  return Array.isArray(aliases) ? aliases : []
}
const scoreCommandToken = (token: string, command: CommandDescriptor): number | null => {
  const id = command.id.toLowerCase()
  const label = command.label.toLowerCase()
  const description = command.description.toLowerCase()
  const aliases = getCommandAliases(command).map((alias) => alias.toLowerCase())
  const combinedHaystack = `${id} ${label} ${aliases.join(' ')} ${description}`
  if (!combinedHaystack.includes(token)) {
    return null
  }
  if (id.startsWith(token)) {
    return 100
  }
  if (aliases.some((alias) => alias.startsWith(token))) {
    return 95
  }
  if (label.startsWith(token)) {
    return 90
  }
  if (id.includes(token)) {
    return 80
  }
  if (aliases.some((alias) => alias.includes(token))) {
    return 75
  }
  if (label.includes(token)) {
    return 70
  }
  return 60
}
const scoreCommand = (tokens: readonly string[], command: CommandDescriptor): number | null => {
  if (tokens.length === 0) {
    return 0
  }
  let score = 0
  for (const token of tokens) {
    const tokenScore = scoreCommandToken(token, command)
    if (tokenScore === null) {
      return null
    }
    score += tokenScore
  }
  return score
}
export type FilterCommandDescriptorsOptions = {
  query: string
  commands: readonly CommandDescriptor[]
}
export const filterCommandDescriptors = ({
  query,
  commands,
}: FilterCommandDescriptorsOptions): CommandDescriptor[] => {
  const tokens = normalizeQueryTokens(query)
  if (tokens.length === 0) {
    return [...commands]
  }
  const scored: ScoredCommand[] = []
  commands.forEach((command, index) => {
    const score = scoreCommand(tokens, command)
    if (score === null) {
      return
    }
    scored.push({ command, score, index })
  })
  return scored
    .sort((a, b) => {
      if (b.score !== a.score) {
        return b.score - a.score
      }
      return a.index - b.index
    })
    .map((entry) => entry.command)
}
const matchesCommandPrefixToken = (token: string, command: CommandDescriptor): boolean => {
  if (!token) {
    return false
  }
  const normalized = token.toLowerCase()
  if (command.id.toLowerCase().startsWith(normalized)) {
    return true
  }
  if (command.label.toLowerCase().startsWith(normalized)) {
    return true
  }
  return getCommandAliases(command).some((alias) => alias.toLowerCase().startsWith(normalized))
}
export type HasCommandDescriptorPrefixMatchOptions = {
  token: string
  commands: readonly CommandDescriptor[]
}
export const hasCommandDescriptorPrefixMatch = ({
  token,
  commands,
}: HasCommandDescriptorPrefixMatchOptions): boolean => {
  const normalized = token.trim().toLowerCase()
  if (!normalized) {
    return false
  }
  return commands.some((command) => matchesCommandPrefixToken(normalized, command))
}
export type CommandMenuSearchState = {
  filterQuery: string
  treatRemainderAsArgs: boolean
}
export type ResolveCommandMenuSearchStateOptions = {
  commandQuery: string
  commands: readonly CommandDescriptor[]
}
export const resolveCommandMenuSearchState = ({
  commandQuery,
  commands,
}: ResolveCommandMenuSearchStateOptions): CommandMenuSearchState => {
  const normalized = commandQuery.trim().toLowerCase()
  if (!normalized) {
    return { filterQuery: '', treatRemainderAsArgs: false }
  }
  const parts = normalized.split(/\s+/).filter((part) => part.length > 0)
  const firstToken = parts[0] ?? ''
  const treatRemainderAsArgs = hasCommandDescriptorPrefixMatch({ token: firstToken, commands })
  return {
    filterQuery: treatRemainderAsArgs ? firstToken : normalized,
    treatRemainderAsArgs,
  }
}
</file>
<file path="src/tui/command-history.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
export type CommandHistoryRecord = {
  value: string
  timestamp: string
}
const HISTORY_FILE = path.join(os.homedir(), '.config', 'prompt-maker-cli', 'tui-history.json')
const getErrorCode = (error: unknown): string | null => {
  if (!error || typeof error !== 'object') {
    return null
  }
  if (!('code' in error)) {
    return null
  }
  const code = (error as { code?: unknown }).code
  return typeof code === 'string' ? code : null
}
const isFileMissingError = (error: unknown): boolean => getErrorCode(error) === 'ENOENT'
const isRecoverableHistoryError = (error: unknown): boolean => {
  if (error instanceof SyntaxError) {
    return true
  }
  if (error instanceof Error && error.message === 'History file must contain a JSON array.') {
    return true
  }
  return false
}
const sanitizeTimestamp = (timestamp: string): string => timestamp.replace(/[:.]/g, '-')
const repairCorruptHistoryFile = async (): Promise<void> => {
  const directory = path.dirname(HISTORY_FILE)
  const backupPath = path.join(
    directory,
    `tui-history.corrupt-${sanitizeTimestamp(new Date().toISOString())}.json`,
  )
  try {
    await fs.mkdir(directory, { recursive: true })
  } catch {
    return
  }
  try {
    await fs.rename(HISTORY_FILE, backupPath)
  } catch {
    // Best effort: ignore backup failures.
  }
  try {
    await fs.writeFile(HISTORY_FILE, '[]\n', 'utf8')
  } catch {
    // Best effort: ignore repair failures.
  }
}
const parseHistoryRecords = (raw: unknown): CommandHistoryRecord[] => {
  if (!Array.isArray(raw)) {
    throw new Error('History file must contain a JSON array.')
  }
  return raw
    .map((entry) => {
      if (!entry || typeof entry !== 'object') {
        return null
      }
      if (!('value' in entry) || typeof entry.value !== 'string') {
        return null
      }
      const timestamp =
        'timestamp' in entry && typeof entry.timestamp === 'string' ? entry.timestamp : null
      return { value: entry.value, timestamp: timestamp ?? new Date().toISOString() }
    })
    .filter((entry): entry is CommandHistoryRecord => Boolean(entry))
}
export const readCommandHistory = async (): Promise<CommandHistoryRecord[]> => {
  try {
    const contents = await fs.readFile(HISTORY_FILE, 'utf8')
    if (!contents.trim()) {
      return []
    }
    try {
      const parsed = JSON.parse(contents) as unknown
      return parseHistoryRecords(parsed)
    } catch (parseError) {
      if (isRecoverableHistoryError(parseError)) {
        await repairCorruptHistoryFile()
        return []
      }
      throw parseError
    }
  } catch (error) {
    if (isFileMissingError(error)) {
      return []
    }
    if (isRecoverableHistoryError(error)) {
      await repairCorruptHistoryFile()
      return []
    }
    const message = error instanceof Error ? error.message : 'Unknown history error.'
    throw new Error(`Failed to load history at ${HISTORY_FILE}: ${message}`)
  }
}
export const writeCommandHistory = async (entries: CommandHistoryRecord[]): Promise<void> => {
  const directory = path.dirname(HISTORY_FILE)
  await fs.mkdir(directory, { recursive: true })
  const contents = JSON.stringify(entries, null, 2)
  const tempFile = `${HISTORY_FILE}.${process.pid}.tmp`
  await fs.writeFile(tempFile, contents, 'utf8')
  try {
    await fs.rename(tempFile, HISTORY_FILE)
  } catch {
    await fs.writeFile(HISTORY_FILE, contents, 'utf8')
  }
}
export const updateCommandHistory = (params: {
  previous: CommandHistoryRecord[]
  nextValue: string
  timestamp?: string
  maxEntries: number
}): CommandHistoryRecord[] => {
  const normalized = params.nextValue.trim()
  if (!normalized) {
    return params.previous
  }
  const lastEntry = params.previous[0]
  if (lastEntry && lastEntry.value === normalized) {
    return params.previous
  }
  const next: CommandHistoryRecord[] = [
    { value: normalized, timestamp: params.timestamp ?? new Date().toISOString() },
    ...params.previous,
  ]
  return next.slice(0, Math.max(1, params.maxEntries))
}
</file>
<file path="src/tui/CommandScreen.tsx">
/*
 * CommandScreen entry point.
 *
 * This file intentionally stays small.
 * The real implementation lives under `src/tui/screens/command/*` so we can
 * split the screen into a reducer-driven model + smaller components over time.
 */
export { CommandScreen } from './screens/command/CommandScreen'
export type { CommandScreenHandle } from './screens/command/CommandScreen'
</file>
<file path="src/tui/drag-drop-path.ts">
import { stripTerminalPasteArtifacts } from './components/core/bracketed-paste'
export const parseAbsolutePathFromInput = (input: string): string | null => {
  const sanitizedInput = stripTerminalPasteArtifacts(input)
  const trimmed = sanitizedInput.trim()
  if (!trimmed) {
    return null
  }
  const { unquoted, wasQuoted } = stripMatchingQuotes(trimmed)
  const normalized = unquoted.trim()
  if (!normalized) {
    return null
  }
  if (isWindowsAbsolute(normalized)) {
    if (!wasQuoted && /\s/.test(normalized)) {
      return null
    }
    return normalized
  }
  if (!wasQuoted && containsUnescapedWhitespace(normalized)) {
    return null
  }
  const unescaped = unescapeBackslashes(normalized).trim()
  if (!unescaped) {
    return null
  }
  return isPosixAbsolute(unescaped) ? unescaped : null
}
export const isCommandInput = (
  input: string,
  existsSync: (candidate: string) => boolean,
): boolean => {
  const sanitizedInput = stripTerminalPasteArtifacts(input)
  const trimmedStart = sanitizedInput.trimStart()
  if (!trimmedStart.startsWith('/')) {
    return false
  }
  if (trimmedStart === '/') {
    return true
  }
  const absoluteCandidate = parseAbsolutePathFromInput(trimmedStart)
  if (!absoluteCandidate) {
    return true
  }
  const hasNestedSegment = absoluteCandidate.length > 1 && absoluteCandidate.slice(1).includes('/')
  if (hasNestedSegment) {
    return false
  }
  return !existsSync(absoluteCandidate)
}
const stripMatchingQuotes = (value: string): { unquoted: string; wasQuoted: boolean } => {
  if (value.length < 2) {
    return { unquoted: value, wasQuoted: false }
  }
  const first = value[0]
  const last = value[value.length - 1]
  const isQuote = first === '"' || first === "'"
  if (!isQuote || first !== last) {
    return { unquoted: value, wasQuoted: false }
  }
  return { unquoted: value.slice(1, -1), wasQuoted: true }
}
const containsUnescapedWhitespace = (value: string): boolean => {
  let escaping = false
  for (const ch of value) {
    if (escaping) {
      escaping = false
      continue
    }
    if (ch === '\\') {
      escaping = true
      continue
    }
    if (ch === ' ' || ch === '\t' || ch === '\n' || ch === '\r') {
      return true
    }
  }
  return false
}
const unescapeBackslashes = (value: string): string => {
  let result = ''
  let escaping = false
  for (const ch of value) {
    if (escaping) {
      result += ch
      escaping = false
      continue
    }
    if (ch === '\\') {
      escaping = true
      continue
    }
    result += ch
  }
  if (escaping) {
    result += '\\'
  }
  return result
}
const isPosixAbsolute = (value: string): boolean => value.startsWith('/')
const isWindowsAbsolute = (value: string): boolean => /^[a-zA-Z]:[\\/]/.test(value)
</file>
<file path="src/tui/intent-source.ts">
export type IntentSourceSelection =
  | { kind: 'text'; intent: string }
  | { kind: 'file'; intentFile: string }
  | { kind: 'empty' }
export const resolveIntentSource = (
  intentValue: string,
  intentFileValue: string,
): IntentSourceSelection => {
  const trimmedFile = intentFileValue.trim()
  if (trimmedFile.length > 0) {
    return { kind: 'file', intentFile: trimmedFile }
  }
  const trimmedIntent = intentValue.trim()
  if (trimmedIntent.length > 0) {
    return { kind: 'text', intent: trimmedIntent }
  }
  return { kind: 'empty' }
}
</file>
<file path="src/tui/model-filter.ts">
import type { ModelOption } from './types'
const DEFAULT_FILTER_LIMIT = 200
export const resolveModelPopupQuery = (query: string, debouncedQuery: string): string => {
  return query.trim() ? debouncedQuery : ''
}
const normalizeQueryTokens = (query: string): string[] =>
  query
    .trim()
    .toLowerCase()
    .split(/\s+/)
    .filter((token) => token.length > 0)
type SubsequenceMatch = {
  start: number
  end: number
  gaps: number
}
const matchSubsequence = (needle: string, haystack: string): SubsequenceMatch | null => {
  if (!needle) {
    return { start: 0, end: 0, gaps: 0 }
  }
  let needleIndex = 0
  let start = -1
  let lastMatch = -1
  let gaps = 0
  for (let haystackIndex = 0; haystackIndex < haystack.length; haystackIndex += 1) {
    if (haystack.charAt(haystackIndex) !== needle.charAt(needleIndex)) {
      continue
    }
    if (needleIndex === 0) {
      start = haystackIndex
    } else {
      gaps += Math.max(0, haystackIndex - lastMatch - 1)
    }
    lastMatch = haystackIndex
    needleIndex += 1
    if (needleIndex >= needle.length) {
      return { start, end: haystackIndex, gaps }
    }
  }
  return null
}
const scoreSubsequenceMatch = (needle: string, match: SubsequenceMatch): number => {
  const span = match.end - match.start + 1
  const density = needle.length / Math.max(span, 1)
  const densityBoost = Math.min(30, Math.round(density * 30))
  const startBoost = Math.max(0, 10 - match.start)
  const gapPenalty = Math.min(20, match.gaps)
  return 40 + densityBoost + startBoost - gapPenalty
}
const scoreModelToken = (token: string, option: ModelOption): number | null => {
  const normalized = token.trim().toLowerCase()
  if (!normalized) {
    return 0
  }
  const id = option.id.toLowerCase()
  const label = option.label.toLowerCase()
  const provider = option.provider.toLowerCase()
  const description = option.description.toLowerCase()
  const capabilities = option.capabilities.join(' ').toLowerCase()
  const notes = option.notes?.toLowerCase() ?? ''
  if (id === normalized) {
    return 200
  }
  if (id.startsWith(normalized)) {
    return 165
  }
  if (label.startsWith(normalized)) {
    return 155
  }
  if (provider.startsWith(normalized)) {
    return 145
  }
  if (id.includes(normalized)) {
    return 125
  }
  if (label.includes(normalized)) {
    return 115
  }
  if (description.includes(normalized)) {
    return 85
  }
  if (capabilities.includes(normalized)) {
    return 75
  }
  if (notes.includes(normalized)) {
    return 65
  }
  const idMatch = matchSubsequence(normalized, id)
  if (idMatch) {
    return scoreSubsequenceMatch(normalized, idMatch) + 55
  }
  const labelMatch = matchSubsequence(normalized, label)
  if (labelMatch) {
    return scoreSubsequenceMatch(normalized, labelMatch) + 45
  }
  return null
}
export const filterModelOptions = (
  query: string,
  options: readonly ModelOption[],
  limit: number = DEFAULT_FILTER_LIMIT,
): ModelOption[] => {
  const tokens = normalizeQueryTokens(query)
  if (tokens.length === 0) {
    return [...options]
  }
  const scored: Array<{ option: ModelOption; score: number; index: number }> = []
  options.forEach((option, index) => {
    let score = 0
    for (const token of tokens) {
      const tokenScore = scoreModelToken(token, option)
      if (tokenScore === null) {
        return
      }
      score += tokenScore
    }
    scored.push({ option, score, index })
  })
  scored.sort((a, b) => {
    if (b.score !== a.score) {
      return b.score - a.score
    }
    return a.index - b.index
  })
  return scored.slice(0, limit).map((entry) => entry.option)
}
</file>
<file path="src/tui/model-popup-options.ts">
import type { ModelProvider } from '../model-providers'
import { filterModelOptions } from './model-filter'
import type { ModelOption } from './types'
export type ModelPopupOptionsResult = {
  options: ModelOption[]
  recentCount: number
}
type BuildModelPopupOptionsParams = {
  query: string
  modelOptions: readonly ModelOption[]
  recentModelIds?: readonly string[]
}
const PROVIDER_ORDER: readonly ModelProvider[] = ['openai', 'gemini', 'other']
const resolveRecentOptions = (
  modelOptions: readonly ModelOption[],
  recentModelIds: readonly string[],
): ModelOption[] => {
  if (recentModelIds.length === 0) {
    return []
  }
  const byId = new Map(modelOptions.map((option) => [option.id, option]))
  const resolved: ModelOption[] = []
  for (const modelId of recentModelIds) {
    const option = byId.get(modelId)
    if (option) {
      resolved.push(option)
    }
  }
  return resolved
}
const groupByProvider = (options: readonly ModelOption[]): ModelOption[] => {
  const grouped: ModelOption[] = []
  for (const provider of PROVIDER_ORDER) {
    for (const option of options) {
      if (option.provider === provider) {
        grouped.push(option)
      }
    }
  }
  const groupedIds = new Set(grouped.map((option) => option.id))
  for (const option of options) {
    if (!groupedIds.has(option.id)) {
      grouped.push(option)
    }
  }
  return grouped
}
export const buildModelPopupOptions = ({
  query,
  modelOptions,
  recentModelIds = [],
}: BuildModelPopupOptionsParams): ModelPopupOptionsResult => {
  const trimmedQuery = query.trim()
  if (!trimmedQuery) {
    const recentOptions = resolveRecentOptions(modelOptions, recentModelIds)
    const recentIds = new Set(recentOptions.map((option) => option.id))
    const remaining = modelOptions.filter((option) => !recentIds.has(option.id))
    const groupedRemaining = groupByProvider(remaining)
    return { options: [...recentOptions, ...groupedRemaining], recentCount: recentOptions.length }
  }
  const filtered = filterModelOptions(trimmedQuery, modelOptions)
  return { options: groupByProvider(filtered), recentCount: 0 }
}
</file>
<file path="src/tui/model-session.ts">
let lastSessionModelId: string | null = null
let recentSessionModelIds: string[] = []
const RECENT_MODEL_LIMIT = 5
export const getLastSessionModel = (): string | null => lastSessionModelId
export const getRecentSessionModels = (): string[] => [...recentSessionModelIds]
export const recordRecentSessionModel = (modelId: string): void => {
  const normalized = modelId.trim()
  if (!normalized) {
    return
  }
  recentSessionModelIds = [
    normalized,
    ...recentSessionModelIds.filter((existing) => existing !== normalized),
  ].slice(0, RECENT_MODEL_LIMIT)
}
export const setLastSessionModel = (modelId: string): void => {
  const normalized = modelId.trim()
  lastSessionModelId = normalized || null
}
export const resetLastSessionModelForTests = (): void => {
  lastSessionModelId = null
  recentSessionModelIds = []
}
export const resetRecentSessionModelsForTests = (): void => {
  recentSessionModelIds = []
}
</file>
<file path="src/tui/new-command.ts">
export type SessionCommandId = 'new' | 'reuse'
export type SessionCommandPlan =
  | { type: 'reset-only'; message: string }
  | { type: 'reset-and-load-meta'; message: string; metaInstructions: string }
export type PlanSessionCommandOptions = {
  commandId: SessionCommandId
  lastGeneratedPrompt: string | null
}
export const planSessionCommand = ({
  commandId,
  lastGeneratedPrompt,
}: PlanSessionCommandOptions): SessionCommandPlan => {
  if (commandId === 'new') {
    return { type: 'reset-only', message: '[new] Session reset.' }
  }
  const prompt = lastGeneratedPrompt?.trim() ?? ''
  if (!prompt) {
    return { type: 'reset-only', message: '[reuse] Session reset Âˇ no previous prompt to reuse.' }
  }
  return {
    type: 'reset-and-load-meta',
    message: '[reuse] Session reset Âˇ loaded last prompt into meta instructions.',
    metaInstructions: prompt,
  }
}
</file>
<file path="src/tui/paste-snippet.ts">
import { stripBracketedPasteControlSequences } from './components/core/bracketed-paste'
export const BRACKETED_PASTE_START = '[200~'
export const BRACKETED_PASTE_END = '[201~'
export type BracketedPasteState = {
  readonly isActive: boolean
  readonly buffer: string
}
export type ConsumeBracketedPasteResult = {
  readonly state: BracketedPasteState
  readonly completed: readonly string[]
  readonly didSeeBracketedPaste: boolean
}
export const createBracketedPasteState = (): BracketedPasteState => ({
  isActive: false,
  buffer: '',
})
export const consumeBracketedPasteChunk = (
  state: BracketedPasteState,
  chunk: string,
): ConsumeBracketedPasteResult => {
  let remaining = chunk
  let isActive = state.isActive
  let buffer = state.buffer
  let didSeeBracketedPaste = state.isActive
  const completed: string[] = []
  while (remaining.length > 0) {
    if (!isActive) {
      const startIndex = remaining.indexOf(BRACKETED_PASTE_START)
      if (startIndex === -1) {
        break
      }
      didSeeBracketedPaste = true
      isActive = true
      buffer = ''
      remaining = remaining.slice(startIndex + BRACKETED_PASTE_START.length)
      continue
    }
    const endIndex = remaining.indexOf(BRACKETED_PASTE_END)
    if (endIndex === -1) {
      buffer += remaining
      remaining = ''
      break
    }
    didSeeBracketedPaste = true
    buffer += remaining.slice(0, endIndex)
    completed.push(buffer)
    buffer = ''
    isActive = false
    remaining = remaining.slice(endIndex + BRACKETED_PASTE_END.length)
  }
  return {
    state: {
      isActive,
      buffer,
    },
    completed,
    didSeeBracketedPaste,
  }
}
export const MIN_PASTE_CHARS = 80
const PREVIEW_LINE_LIMIT = 3
export type PastedSnippet = {
  readonly text: string
  readonly lineCount: number
  readonly charCount: number
  readonly label: string
  readonly previewLines: readonly string[]
}
const normalizeLineEndings = (value: string): string => {
  const normalized = value
    .replace(/\r\n/g, '\n')
    .replace(/\r/g, '\n')
    .replace(/\u0000/g, '')
  return stripBracketedPasteControlSequences(normalized)
}
const countLines = (value: string): number => {
  const trimmed = value.trimEnd()
  if (!trimmed) {
    return 0
  }
  return trimmed.split('\n').length
}
export const formatPastedSnippetLabel = (lineCount: number): string =>
  `[Pasted ~${lineCount} ${lineCount === 1 ? 'line' : 'lines'}]`
export const createPastedSnippet = (raw: string): PastedSnippet | null => {
  const normalized = normalizeLineEndings(raw)
  const text = normalized.trimEnd()
  const lineCount = countLines(text)
  const charCount = text.length
  if (charCount < MIN_PASTE_CHARS) {
    return null
  }
  const previewLines = text
    .split('\n')
    .slice(0, PREVIEW_LINE_LIMIT)
    .map((line) => line.trimEnd())
  return {
    text,
    lineCount,
    charCount,
    label: formatPastedSnippetLabel(lineCount),
    previewLines,
  }
}
export type PastedSnippetDetection = {
  readonly snippet: PastedSnippet
  readonly range: {
    readonly start: number
    readonly end: number
  }
  readonly normalizedNextValue: string
}
const findInsertedRange = (
  previousValue: string,
  nextValue: string,
): { start: number; end: number } => {
  const maxPrefix = Math.min(previousValue.length, nextValue.length)
  let start = 0
  while (start < maxPrefix && previousValue[start] === nextValue[start]) {
    start += 1
  }
  let previousEnd = previousValue.length
  let nextEnd = nextValue.length
  while (
    previousEnd > start &&
    nextEnd > start &&
    previousValue[previousEnd - 1] === nextValue[nextEnd - 1]
  ) {
    previousEnd -= 1
    nextEnd -= 1
  }
  return { start, end: nextEnd }
}
export const detectPastedSnippetFromInputChange = (
  previousValue: string,
  nextValue: string,
): PastedSnippetDetection | null => {
  const previousNormalized = normalizeLineEndings(previousValue)
  const nextNormalized = normalizeLineEndings(nextValue)
  const range = findInsertedRange(previousNormalized, nextNormalized)
  const inserted = nextNormalized.slice(range.start, range.end)
  const snippet = createPastedSnippet(inserted)
  if (!snippet) {
    return null
  }
  return { snippet, range, normalizedNextValue: nextNormalized }
}
</file>
<file path="src/tui/provider-chip.ts">
import { inferProviderFromModelId } from '../model-providers'
import type { ProviderStatusMap } from './types'
const resolveProviderStatusSuffix = (
  providerStatus: ProviderStatusMap[keyof ProviderStatusMap] | undefined,
): string => {
  if (!providerStatus) {
    return 'unknown'
  }
  return providerStatus.status === 'ok'
    ? 'ok'
    : providerStatus.status === 'missing'
      ? 'missing-key'
      : 'error'
}
export const formatProviderStatusChip = (
  modelId: string,
  providerStatuses: ProviderStatusMap,
): string => {
  const provider = inferProviderFromModelId(modelId)
  const suffix = resolveProviderStatusSuffix(providerStatuses[provider])
  return `[${provider}:${suffix}]`
}
</file>
<file path="src/tui/string-filter.ts">
/*
 * String filtering primitives for the Ink TUI.
 *
 * Why this exists:
 * - We do a lot of âsuggestion listâ filtering (files, directories, etc.).
 * - Keeping the filtering algorithm in one place makes later refactors safer.
 *
 * Design goals:
 * - Pure: no filesystem/Ink/React dependencies.
 * - Stable ordering: results preserve original ordering within each match group.
 * - Behavior-preserving: matches the existing UX used by `filterFileSuggestions`.
 */
const normalize = (value: string): string => value.trim().toLowerCase()
const isPrefixMatch = (candidate: string, query: string): boolean =>
  query.length > 0 && candidate.startsWith(query)
const isSubstringMatch = (candidate: string, query: string): boolean =>
  query.length > 0 && candidate.includes(query)
export const filterStringsByQuery = (items: readonly string[], query: string): string[] => {
  const normalizedQuery = normalize(query)
  // When the query is blank, return items unchanged.
  // This is important because other code relies on this being stable.
  if (!normalizedQuery) {
    return [...items]
  }
  // UX detail: prefix matches feel âmore correctâ than substring matches.
  // We keep the original ordering inside each group to avoid jitter.
  const prefixMatches: string[] = []
  const substringMatches: string[] = []
  for (const item of items) {
    const candidate = item.toLowerCase()
    if (isPrefixMatch(candidate, normalizedQuery)) {
      prefixMatches.push(item)
      continue
    }
    if (isSubstringMatch(candidate, normalizedQuery)) {
      substringMatches.push(item)
    }
  }
  return [...prefixMatches, ...substringMatches]
}
</file>
<file path="src/tui/TestRunnerScreen.tsx">
/*
 * TestRunnerScreen entry point.
 *
 * This file intentionally stays small.
 * The implementation lives under `src/tui/screens/test-runner/*` so it can share
 * the same reducer-driven architecture as the command screen.
 */
export { TestRunnerScreen } from './screens/test-runner/TestRunnerScreen'
export type {
  TestRunnerScreenHandle,
  TestRunnerScreenProps,
} from './screens/test-runner/TestRunnerScreen'
</file>
<file path="src/tui/token-usage-store.ts">
import type { TokenTelemetry } from '../generate-command'
export type TokenUsageIteration = {
  iteration: number
  promptTokens: number
  reasoningTokens: number
}
export type TokenUsageRun = {
  id: string
  startedAt: string
  model: string
  telemetry: TokenTelemetry | null
  iterations: TokenUsageIteration[]
}
export type TokenUsageBreakdown = {
  input: {
    intent: number
    files: number
    system: number
    total: number
  }
  output: {
    reasoning: number
    prompt: number
    total: number
  }
  totals: {
    tokens: number
    estimatedCostUsd: number | null
  }
}
type ModelPricing = {
  inputUsdPer1K: number
  outputUsdPer1K: number
}
const MODEL_PRICING_USD_PER_1K: Record<string, ModelPricing> = {
  // OpenAI published list prices (approx): https://openai.com/pricing
  'gpt-4o-mini': { inputUsdPer1K: 0.00015, outputUsdPer1K: 0.0006 },
  'gpt-4o': { inputUsdPer1K: 0.005, outputUsdPer1K: 0.015 },
}
const roundUsd = (value: number): number => Math.round(value * 1_000_000) / 1_000_000
const estimateCostUsd = (
  model: string,
  inputTokens: number,
  outputTokens: number,
): number | null => {
  const pricing = MODEL_PRICING_USD_PER_1K[model]
  if (!pricing) {
    return null
  }
  const inputCost = (inputTokens / 1000) * pricing.inputUsdPer1K
  const outputCost = (outputTokens / 1000) * pricing.outputUsdPer1K
  return roundUsd(inputCost + outputCost)
}
const sumBy = <T>(items: readonly T[], selector: (item: T) => number): number =>
  items.reduce((total, item) => total + selector(item), 0)
export type TokenUsageStore = {
  startRun: (model: string) => string
  recordTelemetry: (runId: string, telemetry: TokenTelemetry) => void
  recordIteration: (runId: string, iteration: TokenUsageIteration) => void
  getRuns: () => readonly TokenUsageRun[]
  getLatestRun: () => TokenUsageRun | null
  getLatestBreakdown: () => TokenUsageBreakdown | null
  clear: () => void
}
export const createTokenUsageStore = (): TokenUsageStore => {
  let runs: TokenUsageRun[] = []
  let counter = 0
  const startRun = (model: string): string => {
    counter += 1
    const runId = `run-${counter}`
    const startedAt = new Date().toISOString()
    const run: TokenUsageRun = {
      id: runId,
      startedAt,
      model,
      telemetry: null,
      iterations: [],
    }
    runs = [run, ...runs].slice(0, 50)
    return runId
  }
  const updateRun = (runId: string, updater: (run: TokenUsageRun) => TokenUsageRun): void => {
    runs = runs.map((run) => (run.id === runId ? updater(run) : run))
  }
  const recordTelemetry = (runId: string, telemetry: TokenTelemetry): void => {
    updateRun(runId, (run) => ({ ...run, telemetry }))
  }
  const recordIteration = (runId: string, iteration: TokenUsageIteration): void => {
    updateRun(runId, (run) => {
      const withoutDuplicate = run.iterations.filter(
        (value) => value.iteration !== iteration.iteration,
      )
      const nextIterations = [...withoutDuplicate, iteration].sort(
        (a, b) => a.iteration - b.iteration,
      )
      return { ...run, iterations: nextIterations }
    })
  }
  const getRuns = (): readonly TokenUsageRun[] => runs
  const getLatestRun = (): TokenUsageRun | null => runs[0] ?? null
  const getLatestBreakdown = (): TokenUsageBreakdown | null => {
    const run = getLatestRun()
    if (!run?.telemetry) {
      return null
    }
    const intent = run.telemetry.intentTokens
    const files = run.telemetry.fileTokens
    const system = run.telemetry.systemTokens
    const inputTotal = intent + files + system
    const reasoning = sumBy(run.iterations, (entry) => entry.reasoningTokens)
    const prompt = sumBy(run.iterations, (entry) => entry.promptTokens)
    const outputTotal = reasoning + prompt
    const estimatedCostUsd = estimateCostUsd(run.model, inputTotal, outputTotal)
    return {
      input: { intent, files, system, total: inputTotal },
      output: { reasoning, prompt, total: outputTotal },
      totals: { tokens: inputTotal + outputTotal, estimatedCostUsd },
    }
  }
  const clear = (): void => {
    runs = []
    counter = 0
  }
  return {
    startRun,
    recordTelemetry,
    recordIteration,
    getRuns,
    getLatestRun,
    getLatestBreakdown,
    clear,
  }
}
</file>
<file path="src/tui/useLogBuffer.ts">
import { useCallback, useMemo, useRef, useState } from 'react'
export type LogEntry = {
  id: string
  level: 'info' | 'warn' | 'error'
  message: string
}
export const useLogBuffer = (initialCapacity = 20) => {
  const capacityRef = useRef(initialCapacity)
  const [logs, setLogs] = useState<LogEntry[]>([])
  const pushLog = useCallback((entry: LogEntry) => {
    setLogs((prev) => {
      const next = [...prev, entry]
      if (next.length > capacityRef.current) {
        return next.slice(next.length - capacityRef.current)
      }
      return next
    })
  }, [])
  const clearLogs = useCallback(() => {
    setLogs([])
  }, [])
  const logger = useMemo(
    () => ({
      info: (message: string) =>
        pushLog({ id: `${Date.now()}-${Math.random()}`, level: 'info', message }),
      warn: (message: string) =>
        pushLog({ id: `${Date.now()}-${Math.random()}`, level: 'warn', message }),
      error: (message: string) =>
        pushLog({ id: `${Date.now()}-${Math.random()}`, level: 'error', message }),
    }),
    [pushLog],
  )
  return { logs, log: logger, clearLogs }
}
</file>
<file path="src/types/jsdom.d.ts">
declare module 'jsdom' {
  export class JSDOM {
    constructor(html?: string, options?: Record<string, unknown>)
    window: Window & typeof globalThis
  }
}
</file>
<file path="src/utils/series-path.ts">
import { createHash } from 'node:crypto'
const padTwoDigits = (value: number): string => value.toString().padStart(2, '0')
export const formatSeriesTimestamp = (date: Date = new Date()): string => {
  const year = date.getFullYear().toString()
  const month = padTwoDigits(date.getMonth() + 1)
  const day = padTwoDigits(date.getDate())
  const hours = padTwoDigits(date.getHours())
  const minutes = padTwoDigits(date.getMinutes())
  const seconds = padTwoDigits(date.getSeconds())
  return `${year}${month}${day}-${hours}${minutes}${seconds}`
}
export const sanitizeForPathSegment = (
  value: string,
  fallback: string,
  maxLength?: number,
): string => {
  const normalized = value.trim().toLowerCase()
  const slug = normalized
    .replace(/[^a-z0-9]+/g, '-')
    .replace(/-+/g, '-')
    .replace(/^-+|-+$/g, '')
  const candidate = slug || fallback
  if (!maxLength || candidate.length <= maxLength) {
    return candidate
  }
  const truncated = candidate.slice(0, maxLength).replace(/-+$/g, '')
  return truncated || fallback
}
export const sanitizeForPathSegmentWithHash = (
  value: string,
  fallback: string,
  maxLength: number,
): string => {
  const candidate = sanitizeForPathSegment(value, fallback)
  if (candidate.length <= maxLength) {
    return candidate
  }
  const digest = createHash('sha1').update(value).digest('hex').slice(0, 8)
  const suffix = `-${digest}`
  const available = Math.max(0, maxLength - suffix.length)
  const prefix = candidate.slice(0, available).replace(/-+$/g, '')
  if (prefix.length === 0) {
    return fallback.slice(0, maxLength)
  }
  return `${prefix}${suffix}`
}
export const buildSeriesOutputDirName = (intent: string, date: Date = new Date()): string => {
  const timestamp = formatSeriesTimestamp(date)
  const intentSlug = sanitizeForPathSegmentWithHash(intent, 'intent', 80)
  return `${timestamp}-${intentSlug}`
}
</file>
<file path="src/file-context.ts">
import fs from 'node:fs/promises'
import fg from 'fast-glob'
export type FileContext = {
  path: string
  content: string
}
export const resolveFileContext = async (patterns: string[]): Promise<FileContext[]> => {
  if (patterns.length === 0) {
    return []
  }
  const entries = await fg(patterns, { dot: true })
  if (entries.length === 0) {
    console.warn(`Warning: No files matched the context patterns: ${patterns.join(', ')}`)
    return []
  }
  const results: FileContext[] = []
  for (const filePath of entries) {
    try {
      const content = await fs.readFile(filePath, 'utf8')
      results.push({ path: filePath, content })
    } catch {
      console.warn(`Warning: Failed to read context file ${filePath}`)
    }
  }
  return results
}
export const formatContextForPrompt = (files: FileContext[]): string => {
  if (files.length === 0) return ''
  return files.map((file) => `<file path="${file.path}">\n${file.content}\n</file>`).join('\n\n')
}
</file>
<file path="src/generate-command.ts">
export { runGenerateCommand } from './generate/command'
export { runGeneratePipeline } from './generate/pipeline'
export { maybeCopyToClipboard, maybeOpenChatGpt } from './generate/actions'
export { InteractiveTransport } from './generate/interactive-transport'
export type {
  ContextPathMetadata,
  GenerateArgs,
  GenerateJsonPayload,
  GeneratePipelineOptions,
  GeneratePipelineResult,
  InteractiveDelegate,
  StreamEventInput,
  StreamMode,
  TokenTelemetry,
} from './generate/types'
</file>
<file path="src/github-context.ts">
import type { FileContext } from './file-context'
export const MAX_GITHUB_FILE_BYTES = 64 * 1024
export const MAX_GITHUB_FILES = 60
const GITHUB_FETCH_TIMEOUT_MS = 20_000
const GITHUB_API_BASE = 'https://api.github.com'
const GITHUB_RAW_BASE = 'https://raw.githubusercontent.com'
const IGNORE_PATTERNS = [
  /^node_modules\//i,
  /^dist\//i,
  /^coverage\//i,
  /^\.git\//i,
  /^\.nx\//i,
  /package-lock\.json$/i,
  /pnpm-lock\.yaml$/i,
  /yarn\.lock$/i,
  /\.zip$/i,
  /\.tgz$/i,
]
export type ResolveGithubOptions = {
  onProgress?: (message: string) => void
}
type GithubBlobTarget = {
  kind: 'blob'
  owner: string
  repo: string
  ref: string
  path: string
}
type GithubTreeTarget = {
  kind: 'tree'
  owner: string
  repo: string
  ref: string
  path?: string
}
type GithubTarget = GithubBlobTarget | GithubTreeTarget
type GithubTreeResponse = {
  tree?: { path: string; type: string; size?: number }[]
}
type GithubContentResponse = {
  encoding?: string
  content?: string
}
export const resolveGithubUrl = async (
  url: URL,
  options?: ResolveGithubOptions,
): Promise<FileContext[]> => {
  const target = parseGithubUrl(url)
  if (!target) {
    console.warn(`Warning: Unsupported GitHub URL ${url.href}.`)
    return []
  }
  if (target.kind === 'blob') {
    options?.onProgress?.(`Downloading ${target.owner}/${target.repo}/${target.path}`)
    return fetchGithubBlob(target)
  }
  options?.onProgress?.(`Scanning ${target.owner}/${target.repo}`)
  return fetchGithubTree(target, options)
}
const fetchGithubBlob = async (target: GithubBlobTarget): Promise<FileContext[]> => {
  const rawPath = buildRawPath(target.path)
  const rawUrl = `${GITHUB_RAW_BASE}/${target.owner}/${target.repo}/${encodeURIComponent(target.ref)}/${rawPath}`
  let response: Response
  try {
    response = await fetchWithTimeout(rawUrl)
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown fetch error.'
    console.warn(`Warning: Failed to download ${target.path}: ${message}`)
    return []
  }
  if (!response.ok) {
    console.warn(`Warning: Failed to download ${target.path} (HTTP ${response.status}).`)
    return []
  }
  const buffer = Buffer.from(await response.arrayBuffer())
  if (!isWithinSize(buffer)) {
    console.warn(
      `Warning: ${target.path} is ${Math.round(buffer.byteLength / 1024)} KB (> ${
        MAX_GITHUB_FILE_BYTES / 1024
      } KB limit). Skipping.`,
    )
    return []
  }
  if (isBinary(buffer)) {
    console.warn(`Warning: ${target.path} appears to be binary. Skipping.`)
    return []
  }
  return [
    {
      path: `github:${target.owner}/${target.repo}/${target.path}`,
      content: buffer.toString('utf8'),
    },
  ]
}
const fetchGithubTree = async (
  target: GithubTreeTarget,
  options?: ResolveGithubOptions,
): Promise<FileContext[]> => {
  const treeUrl = `${GITHUB_API_BASE}/repos/${target.owner}/${target.repo}/git/trees/${encodeURIComponent(
    target.ref,
  )}?recursive=1`
  let response: Response
  try {
    response = await fetchWithTimeout(treeUrl, { headers: apiHeaders(true) })
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown GitHub error.'
    console.warn(`Warning: Failed to list ${target.owner}/${target.repo}: ${message}`)
    return []
  }
  if (!response.ok) {
    console.warn(`Warning: GitHub tree request failed with ${response.status}.`)
    return []
  }
  const payload = (await response.json()) as GithubTreeResponse
  const treeEntries = payload.tree ?? []
  const targetPath = target.path
  const pathPrefix =
    typeof targetPath === 'string' && targetPath.length > 0
      ? withTrailingSlash(targetPath)
      : undefined
  const filtered = treeEntries
    .filter((entry) => entry.type === 'blob')
    .filter((entry) => !pathPrefix || entry.path.startsWith(pathPrefix))
    .filter((entry) => !matchesIgnore(entry.path))
    .filter((entry) => typeof entry.size !== 'number' || entry.size <= MAX_GITHUB_FILE_BYTES)
    .slice(0, MAX_GITHUB_FILES)
  if (filtered.length === 0) {
    console.warn(`Warning: No eligible files found in ${target.owner}/${target.repo}.`)
    return []
  }
  const files: FileContext[] = []
  for (const entry of filtered) {
    options?.onProgress?.(`Fetching ${entry.path}`)
    const content = await fetchFileContent(target, entry.path, target.ref)
    if (!content) {
      continue
    }
    files.push({ path: `github:${target.owner}/${target.repo}/${entry.path}`, content })
    if (files.length >= MAX_GITHUB_FILES) {
      break
    }
  }
  return files
}
const fetchFileContent = async (
  target: GithubTreeTarget,
  path: string,
  ref: string,
): Promise<string | null> => {
  const encodedPath = encodeGithubPath(path)
  const contentsUrl = `${GITHUB_API_BASE}/repos/${target.owner}/${target.repo}/contents/${encodedPath}?ref=${encodeURIComponent(
    ref,
  )}`
  let response: Response
  try {
    response = await fetchWithTimeout(contentsUrl, { headers: apiHeaders() })
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown download error.'
    console.warn(`Warning: Failed to download ${path}: ${message}`)
    return null
  }
  if (!response.ok) {
    console.warn(`Warning: Failed to download ${path} (HTTP ${response.status}).`)
    return null
  }
  const json = (await response.json()) as GithubContentResponse
  if (json.encoding !== 'base64' || !json.content) {
    console.warn(`Warning: Unexpected content response for ${path}.`)
    return null
  }
  const buffer = Buffer.from(json.content, 'base64')
  if (!isWithinSize(buffer) || isBinary(buffer)) {
    return null
  }
  return buffer.toString('utf8')
}
const fetchWithTimeout = async (url: string, init?: RequestInit): Promise<Response> => {
  const controller = new AbortController()
  const timeout = setTimeout(() => controller.abort(), GITHUB_FETCH_TIMEOUT_MS)
  try {
    return await fetch(url, {
      ...init,
      signal: controller.signal,
      headers: {
        'User-Agent': 'prompt-maker-cli',
        ...(init?.headers ?? {}),
      },
    })
  } finally {
    clearTimeout(timeout)
  }
}
const parseGithubUrl = (url: URL): GithubTarget | null => {
  const segments = url.pathname.split('/').filter(Boolean)
  if (segments.length < 2) {
    return null
  }
  const owner = segments[0]
  const rawRepo = segments[1]
  if (!owner || !rawRepo) {
    return null
  }
  const type = segments[2]
  const ref = segments[3]
  const rest = segments.slice(4)
  const repo = sanitizeRepo(rawRepo)
  if (!type) {
    return { kind: 'tree', owner, repo, ref: 'HEAD' }
  }
  if (type === 'blob') {
    if (!ref || rest.length === 0) {
      return null
    }
    return { kind: 'blob', owner, repo, ref, path: rest.join('/') }
  }
  if (type === 'tree') {
    if (!ref) {
      return null
    }
    const joined = rest.join('/')
    if (joined) {
      return { kind: 'tree', owner, repo, ref, path: joined }
    }
    return { kind: 'tree', owner, repo, ref }
  }
  if (!ref) {
    return { kind: 'tree', owner, repo, ref: 'HEAD' }
  }
  return null
}
const matchesIgnore = (path: string): boolean =>
  IGNORE_PATTERNS.some((pattern) => pattern.test(path))
const withTrailingSlash = (value: string): string => (value.endsWith('/') ? value : `${value}/`)
const isWithinSize = (buffer: Buffer): boolean => buffer.byteLength <= MAX_GITHUB_FILE_BYTES
const isBinary = (buffer: Buffer): boolean => {
  if (buffer.includes(0)) {
    return true
  }
  let printable = 0
  for (const byte of buffer) {
    if (byte === 9 || byte === 10 || byte === 13 || (byte >= 32 && byte <= 126)) {
      printable += 1
    }
  }
  return printable / buffer.length < 0.8
}
const buildRawPath = (path: string): string => path.split('/').map(encodeURIComponent).join('/')
const encodeGithubPath = (path: string): string => path.split('/').map(encodeURIComponent).join('/')
const apiHeaders = (json?: boolean): HeadersInit => {
  const headers: Record<string, string> = {
    'User-Agent': 'prompt-maker-cli',
  }
  if (json) {
    headers.accept = 'application/vnd.github+json'
  }
  const token = process.env.GITHUB_TOKEN?.trim()
  if (token) {
    headers.authorization = `Bearer ${token}`
  }
  return headers
}
const sanitizeRepo = (repo: string): string => (repo.endsWith('.git') ? repo.slice(0, -4) : repo)
</file>
<file path="src/image-loader.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import { type ImagePart } from '@prompt-maker/core'
const MAX_IMAGE_SIZE_MB = 20
const SUPPORTED_EXTS = ['.png', '.jpg', '.jpeg', '.webp', '.gif']
export const resolveImageParts = async (
  filePaths: string[],
  onUploadStateChange?: (
    state: 'start' | 'finish',
    detail: { kind: 'image'; filePath: string },
  ) => void,
): Promise<ImagePart[]> => {
  const parts: ImagePart[] = []
  for (const filePath of filePaths) {
    const ext = path.extname(filePath).toLowerCase()
    if (!SUPPORTED_EXTS.includes(ext)) {
      console.warn(`Skipping unsupported image type: ${filePath}`)
      continue
    }
    onUploadStateChange?.('start', { kind: 'image', filePath })
    try {
      const buffer = await fs.readFile(filePath)
      const sizeMb = buffer.length / (1024 * 1024)
      if (sizeMb > MAX_IMAGE_SIZE_MB) {
        console.warn(`Skipping image too large (${sizeMb.toFixed(1)}MB): ${filePath}`)
        continue
      }
      const mimeType =
        ext === '.png'
          ? 'image/png'
          : ext === '.webp'
            ? 'image/webp'
            : ext === '.gif'
              ? 'image/gif'
              : 'image/jpeg'
      parts.push({
        type: 'image',
        mimeType,
        data: buffer.toString('base64'),
      })
    } catch (error) {
      console.warn(`Failed to read image ${filePath}:`, error)
    } finally {
      onUploadStateChange?.('finish', { kind: 'image', filePath })
    }
  }
  return parts
}
</file>
<file path="src/io.ts">
export const readFromStdin = async (): Promise<string | null> => {
  if (process.stdin.isTTY) {
    return null
  }
  const chunks: Buffer[] = []
  return await new Promise<string>((resolve, reject) => {
    process.stdin.on('data', (chunk: Buffer | string) => {
      chunks.push(typeof chunk === 'string' ? Buffer.from(chunk, 'utf8') : chunk)
    })
    process.stdin.on('error', reject)
    process.stdin.on('end', () => {
      resolve(Buffer.concat(chunks).toString('utf8'))
    })
  })
}
</file>
<file path="src/model-providers.ts">
export type ModelProvider = 'openai' | 'gemini' | 'other'
export type ModelDefinition = {
  id: string
  label?: string
  provider?: ModelProvider
  description?: string
  capabilities?: string[]
  notes?: string
  default?: boolean
}
export const MODEL_PROVIDER_LABELS: Record<ModelProvider, string> = {
  openai: 'OpenAI',
  gemini: 'Gemini',
  other: 'Custom',
}
const GEMINI_MODEL_PREFIXES = ['gemini', 'gemma']
const OPENAI_MODEL_PREFIXES = ['gpt', 'o', 'chatgpt', 'text-davinci']
const normalizeModelId = (modelId: string): string => modelId.trim().toLowerCase()
export const isGeminiModelId = (modelId: string): boolean => {
  const normalized = normalizeModelId(modelId)
  return GEMINI_MODEL_PREFIXES.some((prefix) => normalized.startsWith(prefix))
}
const isOpenAiModelId = (modelId: string): boolean => {
  const normalized = normalizeModelId(modelId)
  return OPENAI_MODEL_PREFIXES.some((prefix) => normalized.startsWith(prefix))
}
export const inferProviderFromModelId = (modelId: string): ModelProvider => {
  if (!modelId.trim()) {
    return 'other'
  }
  if (isGeminiModelId(modelId)) {
    return 'gemini'
  }
  if (isOpenAiModelId(modelId)) {
    return 'openai'
  }
  return 'other'
}
</file>
<file path="src/smart-context-service.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import fg from 'fast-glob'
import type { FileContext } from './file-context'
import * as vectorStore from './rag/vector-store'
type ProgressCallback = (message: string) => void
const SMART_CONTEXT_PATTERNS = ['**/*.{ts,tsx,js,jsx,py,md,json}']
const SMART_CONTEXT_IGNORE_PATTERNS = [
  '**/node_modules/**',
  '**/dist/**',
  '**/coverage/**',
  '**/.git/**',
  '**/.nx/**',
  '**/.next/**',
  '**/package-lock.json',
  '**/pnpm-lock.yaml',
  '**/yarn.lock',
]
const MAX_EMBEDDING_FILE_SIZE = 25 * 1024
export const resolveSmartContextFiles = async (
  intent: string,
  currentContext: FileContext[],
  onProgress?: ProgressCallback,
  rootDirectory?: string,
): Promise<FileContext[]> => {
  const baseDir = rootDirectory ? path.resolve(rootDirectory) : process.cwd()
  onProgress?.('Scanning workspace for smart context files')
  let filesToIndex: string[] = []
  try {
    filesToIndex = await fg(SMART_CONTEXT_PATTERNS, {
      dot: true,
      absolute: true,
      cwd: baseDir,
      ignore: SMART_CONTEXT_IGNORE_PATTERNS,
    })
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown smart context glob error.'
    console.warn(`Smart context scan failed: ${message}`)
    onProgress?.('Smart context scan failed')
    return []
  }
  if (filesToIndex.length === 0) {
    onProgress?.('No smart context files found')
    return []
  }
  const uniqueFiles = [...new Set(filesToIndex.map((filePath) => path.resolve(filePath)))]
  const validFiles: string[] = []
  for (const file of uniqueFiles) {
    try {
      const stats = await fs.stat(file)
      if (stats.size < MAX_EMBEDDING_FILE_SIZE) {
        validFiles.push(file)
      }
    } catch {
      // Ignore files that cannot be read
    }
  }
  if (validFiles.length === 0) {
    onProgress?.('No smart context files within size limit')
    return []
  }
  onProgress?.('Indexing smart context')
  try {
    await vectorStore.indexFiles(validFiles)
    onProgress?.('Indexed smart context â')
  } catch (error) {
    onProgress?.('Failed to index smart context')
    const message = error instanceof Error ? error.message : 'Unknown smart context error.'
    console.warn(`Smart context indexing failed: ${message}`)
    return []
  }
  onProgress?.('Searching smart context')
  let relatedPaths: string[] = []
  try {
    relatedPaths = await vectorStore.search(intent, 5, validFiles)
  } catch (error) {
    onProgress?.('Failed to search smart context')
    const message = error instanceof Error ? error.message : 'Unknown smart context search error.'
    console.warn(`Smart context search failed: ${message}`)
    return []
  }
  const availableSet = new Set(validFiles.map((filePath) => normalizePath(filePath)))
  const filtered = relatedPaths
    .map((filePath) => normalizePath(filePath))
    .filter((filePath) => availableSet.has(filePath))
  if (filtered.length === 0) {
    onProgress?.('No related smart context files found')
    return []
  }
  const results = await readSmartContextFiles(filtered, currentContext)
  if (results.length > 0) {
    onProgress?.('Smart context ready')
  } else {
    onProgress?.('No smart context files added')
  }
  return results
}
const readSmartContextFiles = async (
  candidatePaths: string[],
  currentContext: FileContext[],
): Promise<FileContext[]> => {
  const existingPaths = new Set(currentContext.map((file) => normalizePath(file.path)))
  const results: FileContext[] = []
  for (const filePath of candidatePaths) {
    if (existingPaths.has(filePath)) {
      continue
    }
    try {
      const content = await fs.readFile(filePath, 'utf8')
      results.push({ path: toDisplayPath(filePath), content })
      existingPaths.add(filePath)
    } catch (error) {
      const message = error instanceof Error ? error.message : 'Unknown file read error.'
      console.warn(`Warning: Failed to read smart context file ${filePath}: ${message}`)
    }
  }
  return results
}
const normalizePath = (filePath: string): string => path.resolve(filePath)
const toDisplayPath = (absolutePath: string): string => {
  const cwd = process.cwd()
  const relative = path.relative(cwd, absolutePath)
  if (!relative || relative.startsWith('..')) {
    return absolutePath
  }
  return relative
}
</file>
<file path="src/test-command.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import readline from 'node:readline'
import yaml from 'js-yaml'
import yargs from 'yargs'
import type { ArgumentsCamelCase } from 'yargs'
import { resolveFileContext, type FileContext } from './file-context'
import {
  createPromptGeneratorService,
  resolveDefaultGenerateModel,
  type PromptGenerationRequest,
} from './prompt-generator-service'
import { resolveSmartContextFiles } from './smart-context-service'
import { parsePromptTestSuite, type PromptTestSuite, type PromptTest } from './testing/test-schema'
import { evaluatePrompt } from './testing/evaluator'
const DEFAULT_TEST_FILE = 'prompt-tests.yaml'
const PROGRESS_BAR_WIDTH = 24
type TestArgs = {
  file: string
}
export type TestResult = {
  name: string
  pass: boolean
  reason: string
}
type TestProgressReporter = {
  startTest: (ordinal: number, testName: string) => void
  completeTest: () => void
  completeAll: () => void
}
export type PromptTestRunReporter = {
  onSuiteLoaded?: (suite: PromptTestSuite, filePath: string) => void
  onTestStart?: (ordinal: number, test: PromptTest) => void
  onTestComplete?: (ordinal: number, result: TestResult) => void
  onComplete?: (results: TestResult[]) => void
}
export type PromptTestRunOptions = {
  reporter?: PromptTestRunReporter
}
export const runPromptTestSuite = async (
  filePath: string,
  options: PromptTestRunOptions = {},
): Promise<TestResult[]> => {
  const absolutePath = path.resolve(process.cwd(), filePath)
  const suite = await loadTestSuite(absolutePath)
  options.reporter?.onSuiteLoaded?.(suite, absolutePath)
  const results = await executePromptTests(suite, options.reporter)
  options.reporter?.onComplete?.(results)
  return results
}
export const runTestCommand = async (argv: string[]): Promise<void> => {
  const { file } = parseTestArgs(argv)
  const filePath = path.resolve(process.cwd(), file)
  let progressReporter: TestProgressReporter | null = null
  const reporter: PromptTestRunReporter = {
    onSuiteLoaded: (suite, loadedPath) => {
      console.log(`Loaded ${suite.tests.length} test(s) from ${formatDisplayPath(loadedPath)}.`)
      progressReporter = createTestProgressReporter(suite.tests.length)
    },
    onTestStart: (ordinal, test) => {
      progressReporter?.startTest(ordinal, test.name)
    },
    onTestComplete: () => {
      progressReporter?.completeTest()
    },
    onComplete: () => {
      progressReporter?.completeAll()
    },
  }
  const results = await runPromptTestSuite(filePath, { reporter })
  console.log('\nTest Results')
  console.log('ââââââââââââ')
  for (const result of results) {
    const status = result.pass ? 'PASS' : 'FAIL'
    console.log(`${status.padEnd(4)}  ${result.name} - ${result.reason}`)
  }
  const failures = results.filter((result) => !result.pass)
  if (failures.length > 0) {
    console.log(`\n${failures.length} test(s) failed.`)
    process.exitCode = 1
  } else {
    console.log('\nAll tests passed!')
  }
}
const parseTestArgs = (argv: string[]): TestArgs => {
  const parser = yargs(argv)
    .scriptName('prompt-maker-cli test')
    .usage('prompt-maker-cli test [file]')
    .command('$0 [file]', 'Run prompt quality tests', (cmd) =>
      cmd.positional('file', {
        type: 'string',
        describe: 'Path to a prompt test definition file (YAML)',
        default: DEFAULT_TEST_FILE,
      }),
    )
    .help('help')
    .alias('help', 'h')
    .exitProcess(false)
    .showHelpOnFail(false)
    .parserConfiguration({ 'halt-at-non-option': true })
    .strict(false)
  const parsed = parser.parseSync() as ArgumentsCamelCase<{ file?: string }>
  const file =
    typeof parsed.file === 'string' && parsed.file.trim().length > 0
      ? parsed.file
      : DEFAULT_TEST_FILE
  return { file }
}
const executePromptTests = async (
  suite: PromptTestSuite,
  reporter?: PromptTestRunReporter,
): Promise<TestResult[]> => {
  const service = await createPromptGeneratorService()
  const defaultModel = await resolveDefaultGenerateModel()
  const results: TestResult[] = []
  for (const [index, test] of suite.tests.entries()) {
    const ordinal = index + 1
    reporter?.onTestStart?.(ordinal, test)
    const result = await runSingleTest({ test, service, model: defaultModel })
    results.push(result)
    reporter?.onTestComplete?.(ordinal, result)
  }
  return results
}
const runSingleTest = async ({
  test,
  service,
  model,
}: {
  test: PromptTest
  service: Awaited<ReturnType<typeof createPromptGeneratorService>>
  model: string
}): Promise<TestResult> => {
  try {
    let fileContext = await resolveContextFiles(test.context)
    if (test.smartContext) {
      const smartFiles = await resolveSmartContextFiles(
        test.intent,
        fileContext,
        () => undefined,
        test.smartContextRoot,
      )
      if (smartFiles.length > 0) {
        fileContext = [...fileContext, ...smartFiles]
      }
    }
    const promptRequest: PromptGenerationRequest = {
      intent: test.intent,
      model,
      targetModel: model,
      fileContext,
      images: test.image ?? [],
      videos: test.video ?? [],
    }
    const generatedPrompt = await service.generatePrompt(promptRequest)
    const verdict = await evaluatePrompt(generatedPrompt, test.expect)
    return {
      name: test.name,
      pass: verdict.pass,
      reason: verdict.reason,
    }
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown test error.'
    return {
      name: test.name,
      pass: false,
      reason: message,
    }
  }
}
const resolveContextFiles = async (patterns: string[]): Promise<FileContext[]> => {
  if (!patterns || patterns.length === 0) {
    return []
  }
  return await resolveFileContext(patterns)
}
const loadTestSuite = async (filePath: string): Promise<PromptTestSuite> => {
  let raw: string
  try {
    raw = await fs.readFile(filePath, 'utf8')
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown file error.'
    throw new Error(`Failed to read test file ${formatDisplayPath(filePath)}: ${message}`)
  }
  let parsedYaml: unknown
  try {
    parsedYaml = yaml.load(raw) ?? {}
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown YAML error.'
    throw new Error(`Failed to parse YAML in ${formatDisplayPath(filePath)}: ${message}`)
  }
  try {
    return parsePromptTestSuite(parsedYaml)
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown schema error.'
    throw new Error(`Test file ${formatDisplayPath(filePath)} is invalid: ${message}`)
  }
}
const createTestProgressReporter = (total: number): TestProgressReporter => {
  if (total <= 0) {
    return {
      startTest: () => undefined,
      completeTest: () => undefined,
      completeAll: () => undefined,
    }
  }
  if (!process.stdout.isTTY) {
    let completed = 0
    return {
      startTest(ordinal, testName) {
        console.log(`Running test ${ordinal}/${total}: ${testName}`)
      },
      completeTest() {
        completed = Math.min(completed + 1, total)
        console.log(`Progress ${completed}/${total}`)
      },
      completeAll() {
        console.log('All tests complete.')
      },
    }
  }
  return createTtyProgressReporter(total)
}
const createTtyProgressReporter = (total: number): TestProgressReporter => {
  let completed = 0
  let currentLabel = ''
  let hasRendered = false
  const render = (): void => {
    if (!hasRendered) {
      process.stdout.write('\n')
      hasRendered = true
    }
    const ratio = total === 0 ? 1 : completed / total
    const filledUnits = Math.min(PROGRESS_BAR_WIDTH, Math.round(ratio * PROGRESS_BAR_WIDTH))
    const emptyUnits = PROGRESS_BAR_WIDTH - filledUnits
    const bar = `${'â'.repeat(filledUnits)}${'â'.repeat(emptyUnits)}`
    const line = `[${bar}] ${completed}/${total} ${currentLabel}`
    readline.clearLine(process.stdout, 0)
    readline.cursorTo(process.stdout, 0)
    process.stdout.write(line)
  }
  return {
    startTest(ordinal, testName) {
      currentLabel = `Running test ${ordinal}/${total}: ${testName}`
      render()
    },
    completeTest() {
      completed = Math.min(completed + 1, total)
      render()
    },
    completeAll() {
      currentLabel = 'All tests complete'
      render()
      process.stdout.write('\n')
    },
  }
}
const formatDisplayPath = (absolutePath: string): string => {
  const relative = path.relative(process.cwd(), absolutePath)
  return relative && !relative.startsWith('..') ? relative : absolutePath
}
</file>
<file path="src/token-counter.ts">
import { getEncoding, type Tiktoken } from 'js-tiktoken'
let encoder: Tiktoken | null = null
const getEncoder = (): Tiktoken => {
  if (!encoder) {
    encoder = getEncoding('cl100k_base')
  }
  return encoder
}
export const countTokens = (text: string): number => {
  if (!text) return 0
  try {
    return getEncoder().encode(text).length
  } catch {
    console.warn('Token counting failed, defaulting to character heuristic.')
    return Math.ceil(text.length / 4)
  }
}
export const formatTokenCount = (count: number): string => {
  const formatted = new Intl.NumberFormat().format(count)
  if (count > 100000) return `â ď¸ ${formatted} tokens (High)`
  if (count > 30000) return `${formatted} tokens (Medium)`
  return `${formatted} tokens`
}
</file>
<file path="src/url-context.ts">
import { htmlToText } from 'html-to-text'
import type { FileContext } from './file-context'
import { resolveGithubUrl } from './github-context'
export const MAX_HTML_BYTES = 1 * 1024 * 1024
const URL_FETCH_TIMEOUT_MS = 15_000
export type ResolveUrlContextOptions = {
  onProgress?: (message: string) => void
}
export const resolveUrlContext = async (
  urls: string[],
  options?: ResolveUrlContextOptions,
): Promise<FileContext[]> => {
  if (urls.length === 0) {
    return []
  }
  const entries: FileContext[] = []
  const seen = new Set<string>()
  for (const raw of urls) {
    const trimmed = raw?.trim()
    if (!trimmed || seen.has(trimmed)) {
      continue
    }
    seen.add(trimmed)
    let parsed: URL
    try {
      parsed = new URL(trimmed)
    } catch {
      console.warn(`Warning: Skipping invalid URL "${trimmed}".`)
      continue
    }
    if (!/^https?:$/.test(parsed.protocol)) {
      console.warn(
        `Warning: Unsupported protocol for ${parsed.href}. Only HTTP(S) URLs are allowed.`,
      )
      continue
    }
    if (isGithubHost(parsed.host)) {
      try {
        const githubFiles = await resolveGithubUrl(parsed, options)
        if (githubFiles.length === 0) {
          continue
        }
        entries.push(...githubFiles)
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown GitHub error.'
        console.warn(`Warning: Failed to fetch GitHub URL ${parsed.href}: ${message}`)
      }
      continue
    }
    options?.onProgress?.(`Downloading ${parsed.href}`)
    const file = await fetchGenericUrl(parsed)
    if (file) {
      entries.push(file)
    }
  }
  return entries
}
const fetchGenericUrl = async (url: URL): Promise<FileContext | null> => {
  try {
    const response = await fetchWithTimeout(url.href)
    if (!response.ok) {
      console.warn(`Warning: ${url.href} responded with ${response.status}. Skipping.`)
      return null
    }
    const contentLengthHeader = response.headers.get('content-length')
    if (contentLengthHeader) {
      const declaredLength = Number(contentLengthHeader)
      if (!Number.isNaN(declaredLength) && declaredLength > MAX_HTML_BYTES) {
        console.warn(
          `Warning: ${url.href} is ${Math.round(declaredLength / 1024)} KB which exceeds the ${
            MAX_HTML_BYTES / 1024
          } KB limit. Skipping.`,
        )
        return null
      }
    }
    const buffer = Buffer.from(await response.arrayBuffer())
    if (buffer.byteLength > MAX_HTML_BYTES) {
      console.warn(
        `Warning: ${url.href} exceeded the ${MAX_HTML_BYTES / 1024} KB limit while downloading. Skipping.`,
      )
      return null
    }
    const html = buffer.toString('utf8')
    const text = extractText(html)
    if (!text) {
      console.warn(`Warning: ${url.href} did not contain readable text.`)
      return null
    }
    return {
      path: `url:${url.href}`,
      content: text,
    }
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown fetch error.'
    console.warn(`Warning: Failed to download ${url.href}: ${message}`)
    return null
  }
}
const extractText = (html: string): string => {
  const text = htmlToText(html, {
    wordwrap: false,
    preserveNewlines: true,
    selectors: [
      { selector: 'script', format: 'skip' },
      { selector: 'style', format: 'skip' },
      { selector: 'noscript', format: 'inline' },
    ],
  })
    .replace(/\r/g, '')
    .replace(/\n{3,}/g, '\n\n')
    .trim()
  return text
}
const fetchWithTimeout = async (url: string): Promise<Response> => {
  const controller = new AbortController()
  const timeout = setTimeout(() => controller.abort(), URL_FETCH_TIMEOUT_MS)
  try {
    return await fetch(url, {
      signal: controller.signal,
      headers: {
        'User-Agent': 'prompt-maker-cli',
      },
    })
  } finally {
    clearTimeout(timeout)
  }
}
const isGithubHost = (host: string): boolean => {
  const normalized = host.toLowerCase()
  return normalized === 'github.com' || normalized === 'www.github.com'
}
</file>
<file path="tests/mocks/boxen.ts">
type BoxenOptions = {
  borderColor?: string
  borderStyle?: string
  padding?: number | { top?: number; bottom?: number; left?: number; right?: number }
  title?: string
  titleAlignment?: 'left' | 'center' | 'right'
}
const boxen = (content: string, _options?: BoxenOptions): string => content
export default boxen
</file>
<file path="tests/mocks/chalk.ts">
interface ChalkPalette {
  [key: string]: ChalkFn
}
type ChalkFn = ((value?: unknown) => string) & ChalkPalette
const toStringValue = (value?: unknown): string => {
  if (typeof value === 'string') {
    return value
  }
  if (value === undefined || value === null) {
    return ''
  }
  return String(value)
}
const createStyle = (): ChalkFn => {
  const style = ((value?: unknown) => toStringValue(value)) as ChalkFn
  return style
}
const attach = (target: ChalkFn, name: string): ChalkFn => {
  const style = createStyle()
  target[name] = style
  return style
}
const chalkMock = createStyle()
const rootStyles = ['dim', 'gray', 'white', 'green', 'magenta', 'cyan', 'yellow']
rootStyles.forEach((style) => {
  attach(chalkMock, style)
})
const bold = attach(chalkMock, 'bold')
;['green', 'magenta', 'cyan'].forEach((style) => {
  attach(bold, style)
})
export default chalkMock
</file>
<file path="tests/mocks/ora.ts">
type Spinner = {
  text: string
  succeed: (message?: string) => void
  fail: (message?: string) => void
  stop: () => void
}
type OraOptions = {
  text?: string
  color?: string
  spinner?: string
}
const createSpinner = (text: string): Spinner => {
  let currentText = text
  return {
    get text() {
      return currentText
    },
    set text(value: string) {
      currentText = value
    },
    succeed: () => undefined,
    fail: () => undefined,
    stop: () => undefined,
  }
}
const ora = (options?: OraOptions) => {
  const spinner = createSpinner(options?.text ?? '')
  return {
    start: () => spinner,
  }
}
export default ora
</file>
<file path=".eslintrc.json">
{
  "root": true,
  "ignorePatterns": ["dist/**"],
  "env": {
    "es2022": true,
    "node": true
  },
  "parser": "@typescript-eslint/parser",
  "parserOptions": {
    "ecmaVersion": "latest",
    "sourceType": "module"
  },
  "plugins": ["@typescript-eslint"],
  "extends": ["eslint:recommended", "plugin:@typescript-eslint/recommended"]
}
</file>
<file path=".gitignore">
# Dependencies
node_modules/
.pnp*
# Build output
.next/
out/
dist/
build/
# Caches
.cache/
.turbo/
# Testing
coverage/
# Environment variables
.env
.env.*
# Logs
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
*.log
# Editors
.vscode/
.idea/
# Misc
.DS_Store
</file>
<file path=".nvmrc">
22.15
</file>
<file path=".prettierignore">
# Intentionally-invalid fixtures
src/__tests__/__fixtures__/themes/global/invalid.json
</file>
<file path=".prettierrc">
{
  "printWidth": 100,
  "singleQuote": true,
  "trailingComma": "all",
  "semi": false
}
</file>
<file path="AGENTS.md">
# AGENTS.md â prompt-maker
Instructions for agentic coding tools (opencode/Codex, Cursor, Copilot Chat, etc.).
Goal: small, focused changes that match existing patterns and keep tests green.
## Quick Facts
- Package manager: `npm` (`package-lock.json`)
- Node: `>=18` (see `.nvmrc` â `22.15` works)
- Modules: ESM (`"type": "module"`); build output is ESM in `dist/`
- TypeScript: `strict` + `noUncheckedIndexedAccess` + `exactOptionalPropertyTypes`
- UI: Ink + React under `src/tui/`
- Tests: Jest + ts-jest (`jest.config.cjs`, `tsconfig.jest.json`)
## Commands
- Install deps: `npm ci` (preferred)
- Update deps/lockfile: `npm install`
- Build: `npm run build` (esbuild via `scripts/build.mjs`)
  - Output: `dist/index.js` + `dist/meta.json`
  - Note: build aliases `@prompt-maker/core/*` to `packages/core/src/*`
- Build (watch): `node scripts/build.mjs --watch`
- Build (minify): `node scripts/build.mjs --minify`
- Run (compiled): `npm start`
- Run (dev, watch + restart): `npm run dev -- <cli args>` (ex: `npm run dev -- --help`)
- Typecheck: `npm run typecheck`
- Format (write): `npm run format`
- Format (check, optional): `npx prettier -c .`
### Lint
- No `lint` script is configured.
- `.eslintrc.json` extends `next/core-web-vitals` but `eslint/next` deps are not present; treat as inactive.
### Tests (Jest)
- All tests: `npm test`
- Watch: `npm test -- --watch`
- Single file: `npm test -- src/__tests__/config.test.ts`
- File outside `testMatch`: `npm test -- --runTestsByPath packages/core/src/__tests__/llm.test.ts`
- Single test by name: `npm test -- -t "ThemeResolver"`
- Debug: `npm test -- --runInBand`, `npm test -- --detectOpenHandles`, `npm test -- --listTests`
## Repo Structure
- CLI entry: `src/index.ts` (routes to `generate`, `test`, or TUI)
- Core package: `packages/core/src/` (imported via `@prompt-maker/core`)
- TUI: `src/tui/` (Ink + React)
  - Reducers: `*-reducer.ts` (pure, no React/Ink imports)
  - Hooks: `src/tui/hooks/` (effects, timers, async, IO)
  - Components: `src/tui/components/` (mostly presentational)
- Architecture note: `src/tui/DEVELOPER_NOTE.md` (some paths are historical)
### TUI input invariants
Input routing is easy to regress; keep this priority order:
1. Help overlay (when open, suppress screen input)
2. Popup input (popups âownâ the keyboard)
3. Screen input
4. AppContainer global keys (exit, etc.)
   Avoid âfallthroughâ where a single key is handled by both popup and screen.
### TUI change checklist
- Keep reducers pure (`*-reducer.ts`) and add/adjust reducer tests in `src/__tests__/tui/`.
- If adding a popup: update popup types/state machine and ensure popups âownâ input.
- If adding async suggestion scans: guard against stale updates (scan id + popup type checks).
### Strict TS gotchas
- Donât assume `array[index]` or `map.get(key)` is defined; handle `undefined`.
- With `exactOptionalPropertyTypes`, prefer omitting optional fields (vs setting them to `undefined`).
- Prefer `satisfies`/`as const` when you need literal unions.
- When narrowing in `catch`, always handle non-`Error` values.
## Coding Conventions
### TypeScript / Types
- Do not use `any`.
- Prefer `unknown` + type guards (or Zod) for external data.
- Prefer functional composition over OO; keep functions small and pure.
- Prefer `type` aliases; use `interface` only for declaration merging/extensibility.
- With `noUncheckedIndexedAccess`, assume indexing can yield `undefined`.
- With `exactOptionalPropertyTypes`, distinguish âmissingâ vs âpresent but undefinedâ.
- For recoverable failures, prefer typed results: `{ ok: true, value } | { ok: false, error }`.
### Imports
- Order: Node built-ins (`node:`) â blank line â third-party â blank line â local.
- Use `import type { ... }` for type-only imports.
- Prefer named exports/imports; avoid introducing new default exports.
### Naming
- Files: `kebab-case.ts` / `kebab-case.tsx`
- Values/functions: `camelCase`
- Types/components: `PascalCase`
- Constants: `SCREAMING_SNAKE_CASE`
- Booleans: `isX` / `hasX` / `shouldX`
### Formatting
- Prettier is the source of truth (`npm run format`).
- Keep changes consistent with surrounding code; prefer early returns.
### Error Handling
- Programmer/invariant errors: `throw new Error('...')`.
- User/config/IO errors: return a typed error OR throw with actionable context.
- When wrapping errors, preserve `error.message` and include paths/flags.
- In `catch (error)`, narrow with `error instanceof Error`.
### Validation
- Use runtime validation for JSON/YAML/API responses.
- Zod is already used (see `src/testing/test-schema.ts`); small type guards are also common.
### React/Ink Patterns
- Keep reducers pure and unit-testable; put effects in hooks.
- Prefer stable callbacks; avoid re-render churn from new objects/arrays.
- For stale closures, prefer refs + stable callbacks.
## Testing Conventions
- Jest `testMatch`: `src/**/__tests__/**/*.test.ts?(x)`.
- Prefer deterministic tests (fake timers/mocks) where possible.
- When mocking modules, follow `jest.config.cjs` `moduleNameMapper` patterns (see `tests/mocks/`).
## Environment Variables
- `OPENAI_API_KEY`
- `GEMINI_API_KEY`
## Build Artifacts
- Do not edit `dist/` by hand; source of truth is `src/` and `packages/`.
## Cursor / Copilot Rules
- No `.cursor/rules/`, `.cursorrules`, or `.github/copilot-instructions.md` found in this repo.
## Hygiene (Agents)
- Keep PRs focused; avoid drive-by refactors.
- Add/adjust tests only when behavior changes.
- Before finalizing: run `npm run typecheck` and `npm test`.
</file>
<file path="jest.config.cjs">
/** @type {import('jest').Config} */
module.exports = {
  testMatch: ['<rootDir>/src/**/__tests__/**/*.test.ts?(x)'],
  testEnvironment: 'node',
  transform: {
    '^.+\\.(ts|tsx)$': [
      'ts-jest',
      {
        tsconfig: '<rootDir>/tsconfig.jest.json',
      },
    ],
  },
  moduleNameMapper: {
    '^@prompt-maker/core$': '<rootDir>/packages/core/src/index.ts',
    '^@prompt-maker/core/(.*)$': '<rootDir>/packages/core/src/$1',
    '^boxen$': '<rootDir>/tests/mocks/boxen.ts',
    '^chalk$': '<rootDir>/tests/mocks/chalk.ts',
    '^ora$': '<rootDir>/tests/mocks/ora.ts',
    '^yargs$': '<rootDir>/tests/mocks/yargs.ts',
  },
}
</file>
<file path="prompt-tests.yaml">
tests:
  - name: 'RAG Feature Check'
    intent: 'Explain the vector store implementation'
    # We force the context here to ensure the GENERATION has the right info
    context: ['apps/prompt-maker-cli/src/rag/vector-store.ts']
    expect:
      - 'Mention cosine similarity'
      - 'Mention file hashing'
      - 'Explain how indexFiles works'
  - name: 'Simple Greeting'
    intent: 'Write a friendly hello world python script'
    expect:
      - "Must contain print('Hello, World!')"
      - 'Must be in Python'
  - name: 'URL Context Summary'
    intent: 'Summarize the Example Domain landing page'
    url:
      - 'https://example.com'
    expect:
      - 'Must mention Example Domain'
      - 'Must note the illustrative examples link section'
  - name: 'Video Bug Analysis'
    intent: 'Analyze this video to find the UI glitch'
    video: ['test-assets/demo.mp4']
    expect:
      - 'Must mention the flickering button'
      - 'Must suggest a CSS fix'
  - name: 'Smart Context Refactor'
    intent: 'Refactor the vector store class to use a singleton pattern'
    smartContext: true
    expect:
      - 'Must find vector-store.ts automatically'
      - 'Must provide valid TypeScript code'
</file>
<file path="tsconfig.jest.json">
{
  "extends": "./tsconfig.json",
  "compilerOptions": {
    "module": "CommonJS",
    "moduleResolution": "Node"
  }
}
</file>
<file path="tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2020",
    "lib": ["ES2020", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "moduleResolution": "Bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "react-jsx",
    "strict": true,
    "noUncheckedIndexedAccess": true,
    "exactOptionalPropertyTypes": true,
    "esModuleInterop": true,
    "types": ["node"],
    "baseUrl": ".",
    "paths": {
      "@prompt-maker/core": ["packages/core/src/index.ts"],
      "@prompt-maker/core/*": ["packages/core/src/*"]
    }
  },
  "include": ["src/**/*.ts", "src/**/*.tsx", "packages/**/*.ts"],
  "exclude": ["dist", "node_modules"]
}
</file>
<file path="docs/context-templates.md">
# Context Templates in Prompt Maker CLI
Context templates let you **wrap the final generated prompt text** with a named, reusable string template. Theyâre designed for workflows where the prompt should land inside a consistent âenvelopeâ (editor scratch buffer, team conventions, structured wrappers for downstream tooling, etc.).
This document is the source-of-truth behavior as implemented in `src/generate/context-templates.ts` and exposed via the `--context-template <name>` CLI flag.
## Table of contents
- [What is a context template?](#what-is-a-context-template)
- [How template resolution works](#how-template-resolution-works)
- [How template rendering works](#how-template-rendering-works)
- [Defining templates in your config](#defining-templates-in-your-config)
- [Using templates from the CLI](#using-templates-from-the-cli)
- [Built-in templates](#built-in-templates)
- [Practical examples](#practical-examples)
- [Troubleshooting](#troubleshooting)
## What is a context template?
A **context template** is a named string that is applied to the generated prompt at the very end of a run.
Use a context template when:
- You want a **consistent wrapper** around every prompt you generate (team conventions, coding guidelines, output formatting rules).
- You want the prompt to be **editor-friendly**, e.g. a scratch buffer header.
- You want to wrap the prompt in a **structured envelope** (XML-ish/JSON-ish) for downstream parsing.
A context template is _not_ a full templating engine:
- There is exactly one placeholder token: `{{prompt}}`.
- No other variables are supported.
- The rendering behavior is intentionally simple and deterministic.
## How template resolution works
When you run with `--context-template <name>`, Prompt Maker resolves `<name>` to a template string using this order (implemented in `resolveContextTemplate` in `src/generate/context-templates.ts`):
1. **Built-in templates first**.
2. If thereâs no built-in match, the CLI loads your config and looks up `contextTemplates[name]`.
3. If still not found, the CLI throws an error that includes an âAvailable templatesâ list.
Important implications:
- **Built-ins win.** If you define a config template with the same name as a built-in (e.g. `nvim`), the built-in will still be used.
- The âAvailable templatesâ list is computed from:
  - built-in template names
  - plus any `contextTemplates` keys in your loaded config
  - and if there are none, it prints `none`
The error message format is:
- `Unknown context template "<name>". Available templates: <comma-separated-list-or-none>.`
## How template rendering works
Rendering is implemented in `renderContextTemplate(template, prompt)` in `src/generate/context-templates.ts`.
There are two modes:
### 1) Placeholder mode (`{{prompt}}` is present)
If the template contains the exact substring `{{prompt}}`:
- Prompt Maker replaces **all occurrences** of `{{prompt}}` with the generated prompt.
- Replacement is literal string replacement (implemented with `template.split('{{prompt}}').join(prompt)`), so itâs global.
This means you can intentionally include the prompt multiple times.
### 2) Append mode (`{{prompt}}` is not present)
If the template does _not_ contain `{{prompt}}`, Prompt Maker uses âappend promptâ behavior:
1. It applies `trimEnd()` to the template (only removes trailing whitespace/newlines).
2. If the result is empty (e.g. `""`, or whitespace-only), the output is **just the prompt**.
3. Otherwise the output is:
- `trimmedTemplate + "\n\n" + prompt`
### Pseudocode summary
```ts
const PLACEHOLDER = '{{prompt}}'
function render(template: string, prompt: string): string {
  if (template.includes(PLACEHOLDER)) {
    // Replace all occurrences.
    return template.split(PLACEHOLDER).join(prompt)
  }
  const trimmed = template.trimEnd()
  if (!trimmed) return prompt
  return `${trimmed}\n\n${prompt}`
}
```
## Defining templates in your config
User-defined templates live under the `contextTemplates` key in your CLI config file, as shown in `README.md`.
### Config file locations
Config path resolution (highest precedence first):
- `PROMPT_MAKER_CLI_CONFIG=/path/to/config.json`
- `~/.config/prompt-maker-cli/config.json`
- `~/.prompt-maker-cli.json`
### Minimal config snippet
```json
{
  "contextTemplates": {
    "scratch": "# Scratch\n\n{{prompt}}"
  }
}
```
Notes:
- Config is JSON.
- Newlines must be encoded as `\n` inside JSON strings.
- Template values are plain strings; there are no other supported tokens besides `{{prompt}}`.
### Team/project sharing tip
If you want your whole team to share a standard template set, put a config file in your repo (for example `./prompt-maker.config.json`) and have developers point to it:
```bash
export PROMPT_MAKER_CLI_CONFIG="$PWD/prompt-maker.config.json"
```
Because built-in templates are resolved first, avoid naming collisions with built-ins (currently `nvim`).
## Using templates from the CLI
The CLI flag is documented in `README.md` under âKey flagsâ:
- `--context-template <name>` wraps the final prompt using a named template.
Examples:
```bash
# Use the built-in NeoVim-friendly wrapper
prompt-maker-cli "Draft a refactor plan for this module" \
  --context "src/**/*.ts" \
  --context-template nvim
# Use a custom template defined in your config
prompt-maker-cli "Write a PR description" \
  --context README.md \
  --context-template scratch
```
## Built-in templates
Built-ins are defined in `src/generate/context-templates.ts`.
### `nvim`
Name: `nvim`
Template body (exactly as implemented):
```text
## NeoVim Prompt Buffer
Paste this block into a scratch buffer (e.g., :enew) so you can keep prompts beside your work.
{{prompt}}
```
Rendered output shape:
- It always starts with the heading and instruction text.
- The generated prompt is inserted where `{{prompt}}` appears.
## Practical examples
All examples below are designed to be copy/pasteable into your config under `contextTemplates`, then invoked via `--context-template <name>`.
Each example includes:
- A config entry (JSON)
- When to use it
- The resulting output shape
### Example 1 â Wrapper template (uses `{{prompt}}`)
**Name:** `scratch`
**Config:**
```json
{
  "contextTemplates": {
    "scratch": "# Scratch\n\n{{prompt}}"
  }
}
```
**When to use:** You want a lightweight heading above the prompt.
**CLI:**
```bash
prompt-maker-cli "Summarize this file" --context src/index.ts --context-template scratch
```
**Output shape:**
```text
# Scratch
<the generated prompt>
```
### Example 2 â Append mode (omits `{{prompt}}`)
**Name:** `preamble`
Because there is no `{{prompt}}`, Prompt Maker will `trimEnd()` the template and then append the prompt with a blank line separator.
**Config:**
```json
{
  "contextTemplates": {
    "preamble": "You are a careful assistant. Ask clarifying questions when needed."
  }
}
```
**When to use:** You want a standard preface, but you donât want to embed a placeholder.
**CLI:**
```bash
prompt-maker-cli "Propose 3 test cases" --context src/generate/pipeline.ts --context-template preamble
```
**Output shape:**
```text
You are a careful assistant. Ask clarifying questions when needed.
<the generated prompt>
```
### Example 3 â Minimal / whitespace-only template
**Name:** `empty`
If the template is empty or becomes empty after `trimEnd()`, the rendered output is **just the prompt**.
**Config:**
```json
{
  "contextTemplates": {
    "empty": "   \n\n"
  }
}
```
**When to use:** You want a stable flag in scripts, but effectively disable wrapping.
**CLI:**
```bash
prompt-maker-cli "Draft a changelog entry" --context README.md --context-template empty
```
**Output shape:**
```text
<the generated prompt>
```
### Example 4 â Headings + workflow instructions (editor checklist)
**Name:** `review-workflow`
**Config:**
```json
{
  "contextTemplates": {
    "review-workflow": "## Review Workflow\n\n1. Read the prompt carefully.\n2. Apply changes locally.\n3. Run tests.\n4. Summarize risks and tradeoffs.\n\n{{prompt}}\n\n## Notes\n- Keep changes small.\n- Prefer deterministic behavior."
  }
}
```
**When to use:** You paste prompts into an editor/issue and want a consistent checklist around them.
**CLI:**
```bash
prompt-maker-cli "Review this reducer for edge cases" \
  --context "src/tui/**/*reducer*.ts" \
  --context-template review-workflow
```
**Output shape (excerpt):**
```text
## Review Workflow
1. Read the prompt carefully.
...
<the generated prompt>
## Notes
- Keep changes small.
...
```
### Example 5 â Structured wrapper for downstream tooling (XML-ish)
**Name:** `xml-envelope`
**Config:**
```json
{
  "contextTemplates": {
    "xml-envelope": "<request>\n  <intent>Generated by prompt-maker-cli</intent>\n  <prompt>\n{{prompt}}\n  </prompt>\n</request>"
  }
}
```
**When to use:** You want your prompt inside stable tags so another tool can extract it reliably.
**CLI:**
```bash
prompt-maker-cli "Generate release notes" --context docs/cookbook.md --context-template xml-envelope
```
**Output shape:**
```text
<request>
  <intent>Generated by prompt-maker-cli</intent>
  <prompt>
<the generated prompt>
  </prompt>
</request>
```
### Example 6 â Structured wrapper for downstream tooling (JSON-ish)
**Name:** `json-envelope`
**Config:**
```json
{
  "contextTemplates": {
    "json-envelope": "{\n  \"type\": \"pmc_prompt\",\n  \"prompt\": \"{{prompt}}\"\n}"
  }
}
```
**When to use:** You want a single blob that looks like JSON (useful for copy/paste into systems expecting JSON-like payloads).
Caveat: The generated prompt can contain quotes/newlines that are not JSON-escaped. This template is best when the consuming system is tolerant of âJSON-ishâ text, not strict JSON parsing.
**CLI:**
```bash
prompt-maker-cli "Explain this API" --context src/index.ts --context-template json-envelope
```
### Example 7 â Team / project standard template (shared config)
**Name:** `team-default`
This example is meant to live in a shared config file, referenced via `PROMPT_MAKER_CLI_CONFIG`, so everyone gets the same wrapper.
**Config (`prompt-maker.config.json` in your repo, for example):**
```json
{
  "contextTemplates": {
    "team-default": "## Team Prompt Contract\n\n- Be explicit about constraints and acceptance criteria.\n- Prefer small diffs and minimal refactors.\n- If requirements are ambiguous, ask questions first.\n\n{{prompt}}\n"
  }
}
```
**CLI:**
```bash
PROMPT_MAKER_CLI_CONFIG="$PWD/prompt-maker.config.json" \
  prompt-maker-cli "Add tests for this behavior" \
  --context src/generate/context-templates.ts \
  --context-template team-default
```
### Bonus â Multiple `{{prompt}}` occurrences (global replacement)
**Name:** `double`
All occurrences of `{{prompt}}` are replaced.
**Config:**
```json
{
  "contextTemplates": {
    "double": "## Prompt (verbatim)\n\n{{prompt}}\n\n## Prompt (for quoting)\n\n> {{prompt}}"
  }
}
```
## Troubleshooting
### âUnknown context template âŚ Available templates: âŚâ
If you see an error like:
```text
Unknown context template "my-template". Available templates: nvim, scratch.
```
It means the name you passed via `--context-template` was not found.
Fix options:
- Use one of the listed names (built-ins + any `contextTemplates` from your loaded config).
- Add the missing template under `contextTemplates` in your config file.
- Double-check youâre editing the config file that Prompt Maker is actually loading (see config path precedence in [Defining templates in your config](#defining-templates-in-your-config)).
### My template doesnât seem to apply
Common causes:
- You defined a template with the same name as a built-in (e.g. `nvim`). Built-ins resolve first, so your config entry wonât be used.
- Your template doesnât contain `{{prompt}}` and you expected replacement behavior. Without the placeholder, Prompt Maker uses append mode.
### Why are there extra blank lines?
In append mode, Prompt Maker always inserts `"\n\n"` between the (trimmed) template and the prompt.
If you need exact spacing control, use explicit placeholder mode (`{{prompt}}`) and place newlines exactly where you want them.
</file>
<file path="docs/fzf-commands.txt">
fzf is an interactive filter program for any kind of list.
It implements a "fuzzy" matching algorithm, so you can quickly type in patterns
with omitted characters and still get the results you want.
Project URL: https://github.com/junegunn/fzf
Author: Junegunn Choi <junegunn.c@gmail.com>
* See man page for more information: fzf --man
Usage: fzf [options]
  SEARCH
    -e, --exact              Enable exact-match
    +x, --no-extended        Disable extended-search mode
    -i, --ignore-case        Case-insensitive match
    +i, --no-ignore-case     Case-sensitive match
        --smart-case         Smart-case match (default)
    --scheme=SCHEME          Scoring scheme [default|path|history]
    -n, --nth=N[,..]         Comma-separated list of field index expressions
                             for limiting search scope. Each can be a non-zero
                             integer or a range expression ([BEGIN]..[END]).
    --with-nth=N[,..]        Transform the presentation of each line using
                             field index expressions
    --accept-nth=N[,..]      Define which fields to print on accept
    -d, --delimiter=STR      Field delimiter regex (default: AWK-style)
    +s, --no-sort            Do not sort the result
    --literal                Do not normalize latin script letters
    --tail=NUM               Maximum number of items to keep in memory
    --disabled               Do not perform search
    --tiebreak=CRI[,..]      Comma-separated list of sort criteria to apply
                             when the scores are tied
                             [length|chunk|pathname|begin|end|index] (default: length)
  INPUT/OUTPUT
    --read0                  Read input delimited by ASCII NUL characters
    --print0                 Print output delimited by ASCII NUL characters
    --ansi                   Enable processing of ANSI color codes
    --sync                   Synchronous search for multi-staged filtering
  GLOBAL STYLE
    --style=PRESET           Apply a style preset [default|minimal|full[:BORDER_STYLE]
    --color=COLSPEC          Base scheme (dark|light|base16|bw) and/or custom colors
    --no-color               Disable colors
    --no-bold                Do not use bold text
  DISPLAY MODE
    --height=[~]HEIGHT[%]    Display fzf window below the cursor with the given
                             height instead of using fullscreen.
                             A negative value is calculated as the terminal height
                             minus the given value.
                             If prefixed with '~', fzf will determine the height
                             according to the input size.
    --min-height=HEIGHT[+]   Minimum height when --height is given as a percentage.
                             Add '+' to automatically increase the value
                             according to the other layout options (default: 10+).
    --tmux[=OPTS]            Start fzf in a tmux popup (requires tmux 3.3+)
                             [center|top|bottom|left|right][,SIZE[%]][,SIZE[%]]
                             [,border-native] (default: center,50%)
  LAYOUT
    --layout=LAYOUT          Choose layout: [default|reverse|reverse-list]
    --margin=MARGIN          Screen margin (TRBL | TB,RL | T,RL,B | T,R,B,L)
    --padding=PADDING        Padding inside border (TRBL | TB,RL | T,RL,B | T,R,B,L)
    --border[=STYLE]         Draw border around the finder
                             [rounded|sharp|bold|block|thinblock|double|horizontal|vertical|
                              top|bottom|left|right|line|none] (default: rounded)
    --border-label=LABEL     Label to print on the border
    --border-label-pos=COL   Position of the border label
                             [POSITIVE_INTEGER: columns from left|
                              NEGATIVE_INTEGER: columns from right][:bottom]
                             (default: 0 or center)
  LIST SECTION
    -m, --multi[=MAX]        Enable multi-select with tab/shift-tab
    --highlight-line         Highlight the whole current line
    --cycle                  Enable cyclic scroll
    --wrap                   Enable line wrap
    --wrap-sign=STR          Indicator for wrapped lines
    --no-multi-line          Disable multi-line display of items when using --read0
    --raw                    Enable raw mode (show non-matching items)
    --track                  Track the current selection when the result is updated
    --tac                    Reverse the order of the input
    --gap[=N]                Render empty lines between each item
    --gap-line[=STR]         Draw horizontal line on each gap using the string
                             (default: 'â' or '-')
    --freeze-left=N          Number of fields to freeze on the left
    --freeze-right=N         Number of fields to freeze on the right
    --keep-right             Keep the right end of the line visible on overflow
    --scroll-off=LINES       Number of screen lines to keep above or below when
                             scrolling to the top or to the bottom (default: 0)
    --no-hscroll             Disable horizontal scroll
    --hscroll-off=COLS       Number of screen columns to keep to the right of the
                             highlighted substring (default: 10)
    --jump-labels=CHARS      Label characters for jump mode
    --gutter=CHAR            Character used for the gutter column (default: 'â')
    --gutter-raw=CHAR        Character used for the gutter column in raw mode (default: 'â')
    --pointer=STR            Pointer to the current line (default: 'â' or '>')
    --marker=STR             Multi-select marker (default: 'â' or '>')
    --marker-multi-line=STR  Multi-select marker for multi-line entries;
                             3 elements for top, middle, and bottom (default: 'âťââš')
    --ellipsis=STR           Ellipsis to show when line is truncated (default: 'ÂˇÂˇ')
    --tabstop=SPACES         Number of spaces for a tab character (default: 8)
    --scrollbar[=C1[C2]]     Scrollbar character(s)
                             (each for list section and preview window)
    --no-scrollbar           Hide scrollbar
    --list-border[=STYLE]    Draw border around the list section
                             [rounded|sharp|bold|block|thinblock|double|horizontal|vertical|
                              top|bottom|left|right|none] (default: rounded)
    --list-label=LABEL       Label to print on the list border
    --list-label-pos=COL     Position of the list label
                             [POSITIVE_INTEGER: columns from left|
                              NEGATIVE_INTEGER: columns from right][:bottom]
                             (default: 0 or center)
  INPUT SECTION
    --no-input               Disable and hide the input section
    --prompt=STR             Input prompt (default: '> ')
    --info=STYLE             Finder info style
                             [default|right|hidden|inline[-right][:PREFIX]]
    --info-command=COMMAND   Command to generate info line
    --separator=STR          Draw horizontal separator on info line using the string
                             (default: 'â' or '-')
    --no-separator           Hide info line separator
    --ghost=TEXT             Ghost text to display when the input is empty
    --filepath-word          Make word-wise movements respect path separators
    --input-border[=STYLE]   Draw border around the input section
                             [rounded|sharp|bold|block|thinblock|double|horizontal|vertical|
                              top|bottom|left|right|line|none] (default: rounded)
    --input-label=LABEL      Label to print on the input border
    --input-label-pos=COL    Position of the input label
                             [POSITIVE_INTEGER: columns from left|
                              NEGATIVE_INTEGER: columns from right][:bottom]
                             (default: 0 or center)
  PREVIEW WINDOW
    --preview=COMMAND        Command to preview highlighted line ({})
    --preview-window=OPT     Preview window layout (default: right:50%)
                             [up|down|left|right][,SIZE[%]]
                             [,[no]wrap][,[no]cycle][,[no]follow][,[no]info]
                             [,[no]hidden][,border-STYLE]
                             [,+SCROLL[OFFSETS][/DENOM]][,~HEADER_LINES]
                             [,default][,<SIZE_THRESHOLD(ALTERNATIVE_LAYOUT)]
    --preview-border[=STYLE] Short for --preview-window=border-STYLE
                             [rounded|sharp|bold|block|thinblock|double|horizontal|vertical|
                              top|bottom|left|right|line|none] (default: rounded)
    --preview-label=LABEL
    --preview-label-pos=N    Same as --border-label and --border-label-pos,
                             but for preview window
  HEADER
    --header=STR             String to print as header
    --header-lines=N         The first N lines of the input are treated as header
    --header-first           Print header before the prompt line
    --header-border[=STYLE]  Draw border around the header section
                             [rounded|sharp|bold|block|thinblock|double|horizontal|vertical|
                              top|bottom|left|right|line|none] (default: rounded)
    --header-lines-border[=STYLE]
                             Display header from --header-lines with a separate border.
                             Pass 'none' to still separate it but without a border.
    --header-label=LABEL     Label to print on the header border
    --header-label-pos=COL   Position of the header label
                             [POSITIVE_INTEGER: columns from left|
                              NEGATIVE_INTEGER: columns from right][:bottom]
                             (default: 0 or center)
  FOOTER
    --footer=STR             String to print as footer
    --footer-border[=STYLE]  Draw border around the footer section
                             [rounded|sharp|bold|block|thinblock|double|horizontal|vertical|
                              top|bottom|left|right|line|none] (default: line)
    --footer-label=LABEL     Label to print on the footer border
    --footer-label-pos=COL   Position of the footer label
                             [POSITIVE_INTEGER: columns from left|
                              NEGATIVE_INTEGER: columns from right][:bottom]
                             (default: 0 or center)
  SCRIPTING
    -q, --query=STR          Start the finder with the given query
    -1, --select-1           Automatically select the only match
    -0, --exit-0             Exit immediately when there's no match
    -f, --filter=STR         Print matches for the initial query and exit
    --print-query            Print query as the first line
    --expect=KEYS            Comma-separated list of keys to complete fzf
  KEY/EVENT BINDING
    --bind=BINDINGS          Custom key/event bindings
  ADVANCED
    --with-shell=STR         Shell command and flags to start child processes with
    --listen[=[ADDR:]PORT]   Start HTTP server to receive actions via TCP
                             (To allow remote process execution, use --listen-unsafe)
    --listen=SOCKET_PATH     Start HTTP server to receive actions via Unix domain socket
                             (Path should end with .sock)
  DIRECTORY TRAVERSAL        (Only used when $FZF_DEFAULT_COMMAND is not set)
    --walker=OPTS            [file][,dir][,follow][,hidden] (default: file,follow,hidden)
    --walker-root=DIR [...]  List of directories to walk (default: .)
    --walker-skip=DIRS       Comma-separated list of directory names to skip
                             (default: .git,node_modules)
  HISTORY
    --history=FILE           File to store fzf search history (*not* shell command history)
    --history-size=N         Maximum number of entries to keep in the file (default: 1000)
  SHELL INTEGRATION
    --bash                   Print script to set up Bash shell integration
    --zsh                    Print script to set up Zsh shell integration
    --fish                   Print script to set up Fish shell integration
  HELP
    --version                Display version information and exit
    --help                   Show this message
    --man                    Show man page
  ENVIRONMENT VARIABLES
    FZF_DEFAULT_COMMAND      Default command to use when input is tty
    FZF_DEFAULT_OPTS         Default options (e.g. '--layout=reverse --info=inline')
    FZF_DEFAULT_OPTS_FILE    Location of the file to read default options from
    FZF_API_KEY              X-API-Key header for HTTP server (--listen)
</file>
<file path="exports/cli.json">
{
  "schemaVersion": "1",
  "intent": "test prompt",
  "model": "gpt-5.2",
  "targetModel": "gpt-5.2",
  "prompt": "# Title\nTest Prompt Contract: Produce a Simple Demonstration Output\n\n## Role\n- Act as a helpful assistant executing a minimal âtestâ task to verify instruction-following, formatting, and completeness.\n\n## Context\n- The requester provided only: **âtest promptâ**.\n- No domain, audience, or deliverable type was specified; treat this as a generic prompt-contract validation.\n\n## Goals & Tasks\n- Create a small, self-contained demonstration output that proves you can:\n  - Follow instructions precisely.\n  - Ask for missing details only when necessary.\n  - Produce a clear, structured response.\n- Tasks:\n  - (1) Interpret âtest promptâ as a request for a minimal sample deliverable.\n  - (2) Produce a short sample output (see Output Format) using a neutral topic.\n  - (3) Include a brief checklist confirming what you did.\n\n## Inputs\n- Primary input:\n  - User intent text: `test prompt`\n- Optional inputs (not provided):\n  - Intended audience, tone, length, topic, constraints.\n\n## Constraints\n- Do not request additional clarification unless it is required to proceed.\n- Keep the output brief and deterministic (avoid open-ended brainstorming).\n- Avoid sensitive content.\n- Do not mention internal policies or hidden reasoning.\n\n## Execution Plan\n- Step 1: Assume a neutral topic (e.g., âbenefits of daily walksâ) if none is provided.\n- Step 2: Produce a concise sample deliverable using the required structure.\n- Step 3: Add a completion checklist indicating compliance with the contract.\n\n## Output Format\nReturn content in the following exact structure:\n\n1) **Sample Output**\n- A 5â7 sentence paragraph on the assumed neutral topic.\n\n2) **Completion Checklist**\n- Bullets confirming:\n  - Followed structure\n  - Kept brevity\n  - Avoided sensitive content\n  - Did not ask unnecessary questions\n\n## Quality Checks\n- Verify the response includes both required sections: âSample Outputâ and âCompletion Checklistâ.\n- Ensure the paragraph is 5â7 sentences.\n- Ensure the checklist has at least 4 bullets.\n- Confirm the topic is neutral and non-sensitive.\n- Confirm wording is clear and free of contradictions.",
  "refinements": [],
  "iterations": 1,
  "interactive": true,
  "timestamp": "2026-01-06T14:51:24.264Z",
  "contextPaths": [
    {
      "path": "history-intent",
      "source": "intent"
    }
  ]
}
</file>
<file path="exports/feeset-changes-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
  - path: master-fees/MasterFeesTable.tsx
    source: smart
  - path: components/QuoteFeesTable.tsx
    source: smart
  - path: components/QuoteFeeSetsTable.tsx
    source: smart
  - path: master-fees/page.tsx
    source: smart
  - path: hooks/useQuoteFees.ts
    source: smart
intent: We need to remove `Active` column from the fee sets table
interactive: true
iterations: 2
model: gpt-5.2
prompt: >
  # Title
  Remove the `Active` Column from the Quote Fee Sets Table (No Code Output)
  ## Role
  You are a senior React/TypeScript engineer working in a Next.js app using
  Material React Table (MRT) and MUI.
  ## Context
  The Fee Manager UI includes a âFee Setsâ table implemented with
  `material-react-table`.
  The request is to remove the **`Active`** column from the **fee sets table**
  only.
  Relevant code (primary):
  - `components/QuoteFeeSetsTable.tsx`
  Other related tables (do not change unless explicitly required):
  - `components/QuoteFeesTable.tsx`
  - `master-fees/MasterFeesTable.tsx`
  The `Active` column currently exists in `components/QuoteFeeSetsTable.tsx` as
  a column definition with `accessorKey: 'active'`, a select editor, and a
  `Chip` cell renderer.
  ## Goals & Tasks
  - Remove the `Active` column from the **Quote Fee Sets** table UI.
  - Ensure the column is:
    - not displayed in the table
    - not editable
    - not present in create/edit modal forms (if those dialogs are generated from column defs).
  - Ensure the table remains functional (edit/create modals, row actions, expand
  panel, pagination, filters) with no runtime or TypeScript errors.
  ## Inputs
  - Primary file to modify:
    - `components/QuoteFeeSetsTable.tsx`
  - Supporting context (read-only unless necessary):
    - `components/QuoteFeesTable.tsx`
    - `master-fees/MasterFeesTable.tsx`
  ## Constraints
  - Change only what is necessary to remove the `Active` column from the **fee
  sets table**.
  - Do **not** remove or modify `Active` columns in other tables unless
  explicitly required.
  - Keep existing behavior for:
    - row actions (edit/delete/clone)
    - expanding rows (detail panel)
    - pagination, sorting, global filter
    - existing IDs/data attributes used for tests
  - Do not introduce new dependencies.
  - Do not output any code, diffs, or code snippets in your response.
  ## Execution Plan
  1. Open and inspect `components/QuoteFeeSetsTable.tsx`.
  2. Locate the `columns` definition (`useMemo<MRT_ColumnDef<QuoteFeeSet>[]>`).
  3. Remove the entire column definition object for `accessorKey: 'active'`.
  4. Scan `components/QuoteFeeSetsTable.tsx` for remaining references that may
  assume the `active` column exists:
     - `initialState` (sorting/visibility)
     - `muiTableBodyCellProps` / `data-column-id` logic
     - any validation or edit/create configuration relying on `active`
  5. Confirm the create/edit modal forms:
     - The dialogs are rendered via `createEditableFieldsDialogRenderer`; if it is driven by the column defs, removing the column must remove the field.
     - Ensure no required-field assumptions break due to the missing `active` input.
  6. Cleanup and correctness:
     - Remove now-unused imports in `components/QuoteFeeSetsTable.tsx` (e.g., `Chip`) if they become unused.
     - Ensure table configuration remains valid and unchanged otherwise.
  7. Sanity-check scope:
     - Verify you did not modify `components/QuoteFeesTable.tsx` or `master-fees/MasterFeesTable.tsx`.
  ## Output Format
  Provide a short, code-free change report containing:
  - **What was changed**: identify the file(s) modified and state that the
  `Active` column was removed from the Quote Fee Sets table.
  - **Impact on UI**: confirm the column no longer appears in the table and no
  longer appears in create/edit modals for fee sets.
  - **Non-functional checks performed**: confirm no unused imports remain and
  there are no lingering references to the removed column in
  `components/QuoteFeeSetsTable.tsx`.
  Do **not** include any code, unified diffs, or snippets.
  ## Quality Checks
  - `QuoteFeeSetsTable` still renders and functions correctly with:
    - expanding rows (detail panel shows `QuoteFeesTable`)
    - editing and creating fee sets (modal)
    - deleting and cloning
    - pagination and global filter
  - No references to the removed `active` column remain in
  `components/QuoteFeeSetsTable.tsx`.
  - No new TypeScript errors introduced (especially from removed column types or
  unused imports).
  - No unintended changes to `Active` columns in other tables
  (`components/QuoteFeesTable.tsx`, `master-fees/MasterFeesTable.tsx`).
reasoning: >-
  I interpreted the userâs refinement as a change to the prompt contractâs
  deliverable: the downstream assistant should still remove the `Active` column
  from the Quote Fee Sets table, but must not include any code output (no
  unified diff, no snippets). I preserved the original intent and structure,
  kept the required sections in the specified order, referenced the relevant
  context file paths explicitly, and adjusted the Output Format and execution
  steps to emphasize describing changes without showing code while still
  ensuring correctness checks (imports, modal fields, runtime/type safety).
refinements:
  - I don't need a unified diff or any other code shown
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-06T21:45:01.068Z'
</file>
<file path="exports/gemini-pdf-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
images: []
intent: Please make this document more succinct
interactive: true
iterations: 1
model: gemini-2.5-pro
pdfs:
  - >-
    /Users/eroberts/Downloads/BP-Adopt AI-based Browser
    Automation-080126-222458.pdf
prompt: >-
  # Title
  Document Condensation and Summarization
  ### Role
  You are an expert editor and summarizer, skilled at distilling complex
  information into clear, concise, and accurate summaries while preserving the
  original intent.
  ### Context
  The user has provided a document and requires a more succinct version. The
  goal is to reduce the overall length and complexity of the text without losing
  any essential information, key arguments, or the intended tone.
  ### Goals & Tasks
  **Goal:** To produce a condensed version of the provided document that is
  significantly shorter, easier to read, and retains all critical information.
  **Tasks:**
  1.  Analyze the provided document to identify its core thesis, main arguments,
  supporting evidence, and key conclusions.
  2.  Rewrite the content to be more succinct by eliminating redundant phrases,
  jargon (where appropriate for a general audience), and non-essential details.
  3.  Restructure the key information into a format that is easy to scan and
  digest.
  ### Inputs
  - The user has provided a document as an attachment. You must use this
  attached document as the sole source material for this task.
  ### Constraints
  - **Fidelity:** Do not introduce any new information, external data, or
  personal opinions. The output must be based exclusively on the content of the
  source document.
  - **Tone:** Preserve the original tone and intent of the document (e.g.,
  formal, informal, persuasive, informative).
  - **Brevity:** Aim for a significant reduction in word count (approximately
  30-50% shorter), but prioritize clarity and the preservation of critical
  information over achieving a specific length.
  - **Completeness:** Ensure all key data points, findings, conclusions, and
  calls to action from the original document are present in the final output.
  ### Execution Plan
  1.  **Initial Read-Through:** First, read the entire provided document to gain
  a comprehensive understanding of its purpose, audience, and key messages.
  2.  **Identify Core Components:** Systematically identify the main
  idea/thesis, each primary supporting argument or section, and the final
  conclusion or call to action.
  3.  **Extract Key Sentences:** For each component identified, extract the most
  critical sentences or concepts that convey its essence.
  4.  **Synthesize and Draft:** Synthesize these extracted points into a new,
  cohesive narrative. Focus on using active voice, simpler sentence structures,
  and removing filler words.
  5.  **Structure the Output:** Draft the executive summary paragraph first.
  Then, create the bulleted list of key takeaways, ensuring they are distinct
  and impactful.
  6.  **Review and Refine:** Compare your condensed version against the original
  document one last time to verify that no critical information has been lost or
  misrepresented.
  ### Output Format
  Provide the final output in markdown with the following two sections:
  **Executive Summary**
  A concise paragraph (or two, if the source material is very dense) that
  captures the overarching message, context, and conclusion of the document.
  **Key Takeaways**
  - A bulleted list highlighting the most important individual points, data,
  findings, or action items.
  - Each bullet point should be clear, direct, and self-contained.
  ### Quality Checks
  - **Accuracy:** Does the summary accurately reflect the information and
  arguments in the original document?
  - **Completeness:** Are all essential points, data, and conclusions from the
  original included?
  - **Clarity:** Is the condensed version easy to understand and free of
  ambiguity?
  - **Succinctness:** Is the output significantly shorter than the original
  without sacrificing critical meaning?
reasoning: >-
  The user's request is to make a document 'more succinct'. This is a common but
  vague intent. To create an effective prompt contract, I need to add structure
  and specificity. My strategy is as follows:
  1.  **Define 'Succinct':** I will translate 'succinct' into concrete tasks:
  identifying core ideas, eliminating redundancy, and restructuring for clarity.
  I'll also suggest a target length reduction (e.g., 30-50%) to provide a
  measurable goal, while emphasizing that clarity is more important than a
  specific word count.
  2.  **Handle the Input:** The user did not provide the document. Per the
  instructions, I will assume the document is provided as an attachment to the
  executing assistant and explicitly state this in the 'Inputs' section.
  3.  **Establish a Role:** I will assign the role of an 'expert editor and
  summarizer' to set the context for high-quality output.
  4.  **Structure the Output:** A single block of condensed text might not be
  the most useful format. I will specify a two-part output: a narrative
  'Executive Summary' and a scannable 'Key Takeaways' bulleted list. This
  provides both a high-level overview and easy access to critical points.
  5.  **Create a Clear Plan:** The 'Execution Plan' will guide the assistant
  through a logical process: read, identify key points, synthesize, draft, and
  review. This ensures a methodical and thorough approach.
  6.  **Add Guardrails:** The 'Constraints' and 'Quality Checks' sections are
  crucial for preventing common summarization errors, such as introducing new
  information, misrepresenting the original tone, or omitting critical details.
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T15:18:41.291Z'
videos: []
</file>
<file path="exports/loantek-publication-downloader-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
  - path: .github/copilot-instructions.md
    source: smart
  - path: README.md
    source: smart
  - path: packages/ui/README.md
    source: smart
  - path: packages/ui/package.json
    source: smart
  - path: packages/api/package.json
    source: smart
intent: |-
  I need to start supporting this application. I need to know:
  - A general summary for how it works
  - A detailed analysis of the main modules and the programmatic flow
  - How to run it locally
  - All database details
  - How to perform detailed troubleshooting and diagnosis
  Please write your response into a markdown file in the root of the project.
interactive: true
iterations: 1
model: gpt-5.2
prompt: >
  # Title
  Create Root-Level Support & Operations Guide (Markdown) for LoanTek
  Publication Downloader
  ## Role
  You are a senior software engineer and technical writer onboarding into an AWS
  serverless Nx monorepo. You produce accurate, actionable internal
  documentation for support/maintenance.
  ## Context
  This repository is the **LoanTek Publication Downloader**: an AWS serverless
  system (Nx monorepo) that downloads financial publications from **SFTP, email,
  and websites**, orchestrated by **AWS Step Functions**, processed by **Lambda
  functions**, and stored in **S3**, with metadata tracked in **PostgreSQL via
  Prisma**.
  You must base the documentation on the provided context files:
  - `.github/copilot-instructions.md`
  - `README.md`
  - `packages/ui/README.md`
  - `packages/ui/package.json`
  - `packages/api/package.json`
  If you have access to the repository contents beyond these files during
  execution, you may inspect them to improve accuracy (e.g.,
  `libs/database/prisma/schema.prisma`, `nx.json`, `vitest.config.ts`,
  `infrastructure/terraform`, Step Functions definitions). If you do **not**
  have access, explicitly note what cannot be confirmed and provide safe
  guidance.
  ## Goals & Tasks
  Create a **single markdown file in the project root** that enables someone to
  start supporting this application. The document must include:
  1. **General Summary (How it works)**
     - High-level architecture and workflow
     - Key AWS services and responsibilities
     - What âa runâ is and how artifacts flow through the system
  2. **Detailed Analysis of Main Modules + Programmatic Flow**
     - Map the monorepo structure (packages vs libs)
     - Describe each major Lambda package (email/sftp/website/persist-artifact) and shared libs (artifact-storage/database/logger/secrets-manager/common)
     - Explain the end-to-end execution flow:
       - Trigger â Step Functions â Lambda sequence â S3 temp/permanent storage â DB Run tracking
     - Call out important conventions:
       - ESM-only
       - Node.js 22+, TypeScript 5.9+
       - Scoped internal imports (`@loantek-publication-downloader/*`)
       - `setEnvFromSSM` usage
       - Zod validation using generated schemas
  3. **How to Run Locally**
     - Prerequisites (Node/npm versions, Docker optional, AWS profile setup)
     - Install steps
     - DB local setup options (Docker compose if present; otherwise explain expected approach)
     - Prisma generation/migrations commands from `README.md`
     - How to run:
       - Individual lambdas locally (describe typical approach using provided scripts/patterns)
       - API (`packages/api` scripts)
       - UI (`packages/ui` scripts)
     - Environment variable management via `conductor` and `.env` files
  4. **All Database Details**
     - Summarize entities and relationships described in `README.md`:
       - Publication, AutomationPlan, Run, Source, AuthenticationProfile
     - Enumerations:
       - AutomationType, RunStatus, SourceType (as described)
     - Prisma usage patterns:
       - prisma client import source
       - Zod schema validation imports
     - Schema generation and migrations:
       - `db:generate`, `db:migrate:dev`, `db:validate`
     - Configuration:
       - `DATABASE_URL` and where env comes from (SSM/Secrets)
     - If actual Prisma schema is accessible, include:
       - Table/model names, key fields, indexes/constraints, and notable JSON fields (e.g., `payload`, `browser_steps`)
       - Any important invariants (unique constraints, foreign keys)
  5. **Detailed Troubleshooting & Diagnosis**
     - Local debugging checklist:
       - install/build/test/typecheck commands
       - clearing Nx cache (`npx nx reset`)
       - common ESM/module resolution issues
     - Runtime diagnosis in AWS:
       - CloudWatch logs strategy
       - Correlation IDs / context logging expectations
       - X-Ray tracing guidance
       - Rollbar (if applicable per docs)
     - Data-path troubleshooting:
       - S3 paths (tmp vs permanent) and how to confirm objects exist
       - Run status lifecycle (IN_PROGRESS â COMPLETED/FAILED)
       - Common failure modes per source type (SFTP auth, email parsing, website automation)
     - Testing strategy and how to interpret coverage thresholds
     - Provide âsymptom â likely cause â checks â fixâ tables for at least:
       - Build failures
       - Test failures
       - Env/SSM credential failures
       - DB connection/migration failures
  ## Inputs
  Use the following files as primary sources and cite them inline in the doc
  where relevant:
  - `.github/copilot-instructions.md`
  - `README.md`
  - `packages/ui/README.md`
  - `packages/ui/package.json`
  - `packages/api/package.json`
  Optional (only if accessible during execution):
  - `libs/database/prisma/schema.prisma`
  - `libs/database/*` (env examples, prisma client exports)
  - `infrastructure/terraform/*`
  - Step Functions definitions (wherever stored)
  - `nx.json`, `vitest.config.ts`, per-project vitest configs
  ## Constraints
  - **Do not invent details** not supported by the provided files. If something
  is unknown, label it explicitly as:
    - âNot confirmed from available filesâ
    - Provide a safe next step: âCheck file X / run command Y / look in AWS console location Zâ.
  - Keep all commands consistent with the repoâs tooling:
    - Node.js 22+, npm workspaces, Nx
    - TypeScript ESM
    - Vitest usage and coverage requirements
  - Follow the repository conventions from `.github/copilot-instructions.md`
  (imports, lambda patterns, env loading).
  - Output must be **a markdown file content** intended to be written to the
  **repository root**.
  ## Execution Plan
  1. Parse the provided context files and extract:
     - Architecture/workflow
     - Repo structure and packages/libs
     - Script commands for UI and API
     - DB entities/enums and Prisma workflows
     - Testing/troubleshooting guidance
  2. If repository browsing is available, inspect key missing sources (Prisma
  schema, infra, Step Functions) and integrate verified facts.
  3. Draft the root markdown doc with clear headings and an operator-friendly
  flow:
     - âWhat it isâ â âHow it worksâ â âCodebase mapâ â âFlow diagrams (text-based)â â âLocal runâ â âDB deep diveâ â âTroubleshooting runbookâ.
  4. Add command snippets and checklists. Ensure all commands are plausible for
  npm workspaces.
  5. Add a final section: âOpen Questions / Where to Look Nextâ for any gaps.
  ## Output Format
  Produce **only** the contents of a single file to be created at:
  - `./SUPPORT_GUIDE.md` (or another clear name) in the **project root**.
  Markdown structure requirements:
  - H1 title
  - Table of contents
  - Sections matching the 5 requested areas
  - At least one:
    - Architecture diagram in ASCII/mermaid (if allowed) or bullet flow
    - Troubleshooting tables
    - âQuickstartâ command block
  ## Quality Checks
  Before finalizing, verify:
  - All 5 user-requested topics are covered with dedicated sections.
  - Every claim is traceable to provided context or clearly marked as
  unconfirmed.
  - Commands reference actual scripts from the provided `package.json` files
  where possible.
  - Local run instructions include UI + API + DB + Prisma generation.
  - Database section includes entities + enums + Prisma/Zod usage patterns.
  - Troubleshooting includes both local and AWS diagnosis steps and at least 4
  symptomâcauseâcheckâfix entries.
  - Writing is concise, skimmable, and actionable for on-call/support engineers.
reasoning: >-
  The user wants onboarding/support documentation for an existing Nx monorepo
  AWS serverless system. They provided multiple context files that describe
  architecture, conventions, and some run-local steps, plus package scripts for
  UI and API. The deliverable is a new markdown file placed at the repository
  root. Missing details include exact filenames to create, specifics of
  Terraform/state machine definitions, exact Prisma schema and env var keys, and
  any operational runbooks; therefore the prompt should instruct the assistant
  to (1) rely strictly on provided context files, (2) inspect repository files
  if available in the execution environment, and (3) clearly label assumptions
  and unknowns while proposing next steps for gaps. The prompt contract should
  request: general system summary, module-by-module deep dive, programmatic flow
  (Step Functions -> Lambdas -> S3/DB), local run instructions (backend lambdas,
  API, UI, DB via Docker), database details (entities/enums/Prisma generation,
  migrations, env vars), and troubleshooting/diagnosis (logs, CloudWatch, X-Ray,
  Rollbar, common build/test/env issues). It should enforce repo conventions:
  ESM, Node 22, scoped imports, setEnvFromSSM, Vitest coverage thresholds.
  Output format should be a single markdown file at root with clear sections and
  actionable commands. Quality checks should ensure completeness vs requested
  bullets and consistency with provided docs.
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-08T19:22:13.801Z'
</file>
<file path="exports/pdf-2-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
images: []
intent: Please make this PDF document more succinct
interactive: true
iterations: 1
model: gpt-5.2
pdfs:
  - >-
    /Users/eroberts/Downloads/BP-Adopt AI-based Browser
    Automation-080126-222458.pdf
prompt: >
  # Title
  Make a Provided PDF Document More Succinct (Concise Rewrite While Preserving
  Meaning)
  ## Role
  You are an expert editor and technical writer. Your job is to shorten a PDF
  document while preserving its intent, facts, and required nuance.
  ## Context
  The user has a PDF that is too long/wordy and wants a more succinct version.
  You will work from the actual PDF content provided by the user (file upload or
  extracted text).
  ## Goals & Tasks
  - Produce a more succinct version of the document that:
    - Preserves original meaning, key facts, and required context.
    - Removes redundancy, filler, and unnecessary repetition.
    - Improves clarity, flow, and scannability.
  - Maintain the documentâs original:
    - Audience level (unless the user specifies otherwise).
    - Tone (formal/informal, legal/marketing/technical, etc.).
    - Logical structure (headings/sections) unless simplifying structure improves readability without changing intent.
  - Provide transparency on changes:
    - Briefly summarize what was removed/condensed and why.
    - Flag any ambiguous sections that require user confirmation.
  ## Inputs
  - PDF document (required):
    - Provide as a file upload OR paste the extracted text.
    - If referencing a file, include the exact path/name (e.g., `./docs/input.pdf`).
  - Optional user preferences (ask if not provided):
    - Target length reduction (e.g., â~30% shorterâ or âfit into 2 pagesâ).
    - Must-keep sections or paragraphs.
    - Sections allowed to be heavily condensed or removed.
    - Required terminology, compliance/legal language that must remain verbatim.
    - Output preference: fully rewritten text, section-by-section rewrite, or executive summary + shortened rewrite.
  ## Constraints
  - Do not invent facts, figures, claims, citations, or sources.
  - Do not remove or alter:
    - Legal/compliance requirements, definitions, or disclaimers unless explicitly instructed.
    - Numbers, dates, names, or commitments unless you can verify them from the PDF.
  - Preserve internal consistency (terms, acronyms, numbering,
  cross-references).
  - If the PDF contains tables, lists, or procedures:
    - Keep critical steps/data intact.
    - You may simplify wording and formatting, but do not change meaning.
  - If any required input is missing (e.g., the PDF), request it before
  proceeding.
  ## Execution Plan
  1. Ingest the PDF content:
     - Extract text accurately; note any unreadable sections (scans/images) and ask for clarification if needed.
  2. Diagnose verbosity:
     - Identify repeated points, long sentences, redundant qualifiers, and unnecessary background.
  3. Build a structural outline:
     - Capture headings and the main point of each section.
  4. Rewrite for succinctness:
     - Tighten sentences, convert passive to active where appropriate, collapse repetitive paragraphs, and streamline transitions.
     - Prefer concrete wording; eliminate filler and hedging where it does not change intent.
  5. Preserve critical elements:
     - Verify all retained facts/figures match the PDF.
     - Maintain key definitions, requirements, and conclusions.
  6. Quality pass:
     - Check readability, coherence, and that nothing essential was dropped.
  7. Produce deliverables:
     - Provide the succinct rewritten version.
     - Provide a short change log and any clarification questions.
  ## Output Format
  Provide the following in markdown:
  1. **Succinct Rewrite**
     - Use the same top-level headings as the original (unless instructed otherwise).
     - If the document is long, present as:
       - `## Section Name` followed by rewritten content.
  2. **Change Log (Brief)**
     - Bullet list of major cuts/condensations (e.g., âRemoved repeated explanation of X in Sections 2 and 4â).
  3. **Open Questions / Ambiguities** (only if needed)
     - Numbered list of items requiring user confirmation.
  ## Quality Checks
  - Faithfulness: All claims in the rewrite are supported by the PDF.
  - Completeness: No removal of essential requirements, conclusions, or
  dependencies.
  - Concision: Reduced length meaningfully (state approximate % reduction if
  feasible).
  - Clarity: Shorter sentences, reduced jargon where possible without changing
  meaning.
  - Consistency: Terms, acronyms, numbering, and cross-references remain
  coherent.
  - Formatting: Headings and lists are clean and scannable.
reasoning: >-
  User wants a prompt contract instructing another assistant to make a provided
  PDF more succinct. Missing details: the PDF file itself/path, desired level of
  succinctness, target audience, constraints about what can/canât be removed,
  required output format (revised PDF text vs summary vs tracked changes), and
  whether to preserve formatting, headings, citations, legal language. Strategy:
  create a contract that requests the PDF as input, clarifies objectives (reduce
  length while preserving meaning), defines tasks (extract structure, identify
  redundancies, rewrite concisely, preserve key info), constraints (no
  hallucinations, maintain facts, keep tone), execution plan (ingest, outline,
  compress per section, verify, provide change log), and output format options
  (concise rewrite + summary + redline table).
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T14:57:35.095Z'
videos: []
</file>
<file path="exports/pdf-3-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
images: []
intent: Please make this document more succinct
interactive: true
iterations: 1
model: gpt-5.2
pdfs:
  - >-
    /Users/eroberts/Downloads/BP-Adopt AI-based Browser
    Automation-080126-222458.pdf
prompt: >
  # Title
  Create a More Succinct Version of the Provided Document
  ## Role
  You are an expert editor and technical writer. Your job is to shorten the
  provided document while preserving its meaning, key facts, and intended tone.
  ## Context
  The user wants the document rewritten to be more succinct. The document is
  available to you in the conversation context and/or as an attachment.
  ## Goals & Tasks
  - Produce a tighter, clearer version of the document with reduced length and
  redundancy.
  - Preserve:
    - All essential claims, requirements, commitments, and constraints
    - Definitions and critical terminology
    - Numerical values, dates, names, and other factual details
    - The documentâs intent, audience, and tone (unless clearly inappropriate)
  - Improve clarity by:
    - Removing filler, repetition, and unnecessary qualifiers
    - Converting wordy phrases into concise alternatives
    - Using active voice where appropriate
    - Reorganizing sentences/paragraphs only when it clearly improves readability
  - Flag ambiguities or missing information that prevent safe summarization
  without meaning loss.
  ## Inputs
  - Primary document: **Already provided in context and/or as an attachment**.
  Use it directly.
  - If the document is an image-only or scanned PDF and text extraction is not
  feasible, request OCR/text extraction *only then*.
  - Optional (if present in the document): audience, purpose, required sections,
  formatting rules.
  ## Constraints
  - Do **not** add new facts, claims, or requirements.
  - Do **not** remove or alter legal/technical meaning.
  - Do **not** change numbers, dates, names, or quoted language unless you are
  explicitly tightening obvious redundancy without changing meaning.
  - Maintain headings/section structure unless the document is unstructured; in
  that case, introduce minimal structure that mirrors the original flow.
  - If the user did not specify a target length:
    - Aim for **~25â40% reduction** while maintaining completeness.
    - If the document is very long, prioritize removing redundancy first, then tighten sentence-level wording.
  ## Execution Plan
  1. **Ingest & assess**: Identify document type (policy, report, proposal,
  instructions, etc.), audience, tone, and any must-keep sections.
  2. **Identify essentials**: Mark critical points: objectives, requirements,
  deliverables, constraints, timelines, responsibilities.
  3. **De-duplicate & prune**: Remove repeated points, throat-clearing intros,
  and tangents.
  4. **Sentence tightening**:
     - Replace verbose constructions with concise equivalents.
     - Merge short, related sentences.
     - Prefer concrete verbs; remove unnecessary adverbs/qualifiers.
  5. **Structure & formatting**: Preserve original headings; use bullets where
  it increases clarity without losing nuance.
  6. **Quality pass**: Verify factual fidelity, completeness of key
  requirements, and consistent terminology.
  7. **Optional delta notes**: Note any significant removals or risky areas
  where meaning might be sensitive.
  ## Output Format
  Provide:
  1. **Succinct Revised Document** (full rewrite, ready to use)
     - Keep original section headings when available.
  2. **Change Summary** (brief bullets)
     - Estimated length reduction (rough %)
     - Key types of edits made (e.g., removed redundancy, tightened wording)
     - Any flagged ambiguities or sections where shortening could risk nuance
  ## Quality Checks
  - **Fidelity**: No factual changes; all key obligations/requirements retained.
  - **Completeness**: Core purpose, scope, and critical details remain intact.
  - **Clarity**: Improved readability; fewer long sentences; reduced jargon
  where possible without changing meaning.
  - **Consistency**: Terminology and formatting are consistent throughout.
  - **Conciseness**: Noticeable reduction in redundancy and wordiness; no
  unnecessary preambles.
reasoning: >
  The user wants a prompt contract for another assistant to make a provided
  document more succinct. No document text is included in this chat, so the
  contract must assume an attachment or existing context is available to the
  executing assistant. The contract should instruct the assistant to produce a
  shortened version while preserving meaning, key details, and structure, and to
  clarify desired level of shortening and audience if unclear. Include
  constraints (no hallucinations, keep facts, maintain tone), execution plan
  (analyze, outline, compress, rewrite, QA), and output format (revised doc +
  change log/summary). Since attachments may exist, explicitly state they are
  already provided; only request OCR if extraction is impossible. Provide
  quality checks such as completeness, accuracy, readability, and consistency
  with original.
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T15:14:45.535Z'
videos: []
</file>
<file path="exports/pdf-4-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
images: []
intent: Please make this document more succinct
interactive: true
iterations: 1
model: gpt-5.2
pdfs:
  - >-
    /Users/eroberts/Downloads/BP-Adopt AI-based Browser
    Automation-080126-222458.pdf
prompt: >
  # Title
  Succinct Rewrite of Provided Document (Preserve Meaning & Structure)
  ## Role
  You are an expert editor specializing in concise technical and business
  writing. You rewrite documents to be shorter while preserving intent,
  accuracy, tone, and required structure.
  ## Context
  The user wants the provided document made more succinct. Use the document
  content available in the conversation context and/or any attachments already
  provided.
  ## Goals & Tasks
  - Produce a succinct rewrite of the document that:
    - Preserves the original meaning, intent, and key details.
    - Removes redundancy, filler, and repetitive phrasing.
    - Improves clarity and flow.
    - Maintains the original tone (unless the document clearly indicates a different required tone).
    - Preserves essential structure (headings/sections), unless restructuring is necessary to reduce repetition without losing meaning.
  - Ensure the rewrite is ready to use (clean formatting, consistent
  terminology).
  - Provide a brief summary of major edits (optional unless space/time is
  limited): what was removed/condensed and where.
  ## Inputs
  - Primary document: **Already provided via the chat context and/or
  attachments** (use directly).
  - If multiple files/attachments are present, treat them as a single source
  unless clearly separate documents.
  - If the document text is not accessible or cannot be extracted (e.g., scanned
  image-only PDF without readable text), request OCR-able text or permission to
  perform OCR.
  ## Constraints
  - Do **not** add new facts, claims, metrics, or requirements.
  - Do **not** remove legally/contractually critical clauses, disclaimers,
  safety notes, or compliance statements.
  - Keep names, dates, numbers, and terminology accurate.
  - Prefer plain language; avoid jargon unless it is present and necessary.
  - Reduce length meaningfully (aim for ~20â40% shorter) **unless** the document
  is already concise; in that case, apply only light trimming.
  - Preserve:
    - Headings and section order when possible.
    - Lists and tables conceptually (may tighten wording).
    - Any explicit formatting requirements found in the document.
  ## Execution Plan
  1. Read the entire document and identify:
     - Redundant sentences/paragraphs.
     - Repeated definitions/phrases.
     - Overly long introductions, transitions, and qualifiers.
     - Opportunities to combine sentences and tighten verbs.
  2. Create a concise rewrite:
     - Replace wordy phrases with shorter equivalents.
     - Merge overlapping points.
     - Convert verbose paragraphs into bullet lists where appropriate (only if it improves brevity and clarity).
     - Remove unnecessary throat-clearing and filler.
  3. Verify fidelity:
     - Cross-check each section to ensure no key meaning or required detail was lost.
     - Confirm all facts/figures remain unchanged.
  4. Final polish:
     - Ensure consistent terminology.
     - Ensure headings are parallel and concise.
     - Fix grammar and punctuation.
  ## Output Format
  Provide:
  1. **Succinct Version** (full rewritten document)
     - Keep headings and section structure unless you made a necessary improvement.
  2. **Edit Notes** (5â10 bullets)
     - Call out the biggest reductions/merges and any structural changes.
  ## Quality Checks
  - **Accuracy:** No altered facts, numbers, names, or obligations.
  - **Completeness:** All critical points from the original remain.
  - **Brevity:** Noticeably shorter; no filler.
  - **Clarity:** Each paragraph has a clear point; minimal ambiguity.
  - **Consistency:** Terminology and tone are consistent throughout.
  - **Formatting:** Clean, readable layout; headings and lists render correctly.
reasoning: >-
  User wants an existing document rewritten to be more succinct. No document
  content is provided in this chat, so the contract must instruct the executing
  assistant to use the attached/provided document context (if any). If none
  exists, the assistant must request the document text. The deliverable is a
  concise revised version, preserving meaning, structure, and any required
  terminology, plus optionally a change summary. Need constraints around not
  adding facts, keeping tone, reducing redundancy, and maintaining
  headings/format. Include a clear execution plan, output format, and quality
  checks (accuracy, completeness, brevity, readability).
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T16:09:43.383Z'
videos: []
</file>
<file path="exports/pdf-5-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
images: []
intent: please make this document more succinct
interactive: true
iterations: 1
model: gpt-5.2
pdfs:
  - /Users/eroberts/Downloads/test.pdf
prompt: >
  # Title
  Succinct Rewrite of âAdopt AI-based Browser Automationâ Proposal (test.pdf)
  ## Role
  You are a technical editor and product/engineering communications specialist
  for an internal platform team. Your job is to rewrite the attached Loantek
  proposal PDF to be materially more succinct while preserving meaning,
  technical accuracy, and the documentâs recommendation.
  ## Context
  - Source document: **PDF attachment `test.pdf`** (already provided as
  context).
  - The PDF describes:
    - Loantekâs rate-sheet downloading workload and delivery mechanisms (email, FTP/SFTP, HTTP endpoints, browser portals).
    - Problems with a legacy Windows/.NET + Selenium approach.
    - A landscape/framework for AI-based browser automation models.
    - Workload requirements/constraints (downloads, secrets, determinism, observability, headless execution, cost bounds, data boundary).
    - Evaluations of specific tools (e.g., Stagehand, Magnitude, Skyvern, Browser Use, Browser MCP, Vibium, etc.).
    - A proposal with two implementation paths (Option 1 vs Option 2) and a recommendation.
  - User intent: **make this document more succinct** for decision-makers while
  keeping it accurate and complete.
  ## Goals & Tasks
  1. **Ingest and understand `test.pdf` end-to-end** (treat it as the single
  source of truth).
  2. **Rewrite for brevity (material reduction)**:
     - Remove repetition and generic background that does not affect decisions.
     - Collapse long explanations into concise statements.
     - Prefer bullets, tables, and short paragraphs.
  3. **Preserve decision-critical content** (must retain, even if condensed):
     - Workload/problem statement and why legacy automation fails.
     - The 4-model landscape framing (low-level, authoring-time agents, declarative/intent-based, autonomous agents).
     - WebVoyager benchmark caveats (what it measures well vs doesnât guarantee) at least as a short caveat.
     - Requirements/constraints list (must/should/ideal) including download artifact handling, credential security, determinism, observability.
     - Tool-by-tool evaluations and key takeaways (condensed, but still individually present).
     - Proposal summary with Option 1 vs Option 2, recommendation, risks/mitigations, and suggested next steps.
  4. **Make it easier to scan**:
     - Keep headings where they aid navigation.
     - Merge redundant sub-sections.
     - Add short âso whatâ takeaway lines where appropriate.
  5. **Add a brief edit summary** (5â10 bullets) describing what was cut/merged
  and what was preserved.
  ## Inputs
  - **PDF attachment (already provided in context): `test.pdf`**
    - Use `test.pdf` directly; it is already attached.
    - Do **not** ask the user to upload, paste, or provide a path to the PDF.
    - Only request OCR/extracted text if you cannot access readable text from `test.pdf` (e.g., scanned/image-only or extraction is garbled). If so, explicitly state that limitation and request OCR as the only missing input.
  ## Constraints
  - **No new claims:** Do not introduce facts, metrics, vendor capabilities, or
  recommendations not supported by `test.pdf`.
  - **Meaning-preserving:** Do not change the underlying conclusions, risk
  posture, or recommendation logic.
  - **Coverage-preserving:** Do not remove any of the âdecision spineâ sections
  listed in Goals & Tasks #3.
  - **Tool names and proper nouns must remain intact** (e.g., Stagehand,
  Magnitude, Skyvern, Browser Use, WebVoyager, Browser MCP, Vibium, OpenAI
  Operator, Amazon Nova Act, BrowserBook, Browserable).
  - **Tone:** Professional internal proposal; concise, direct, and skimmable.
  - **Technical framing:** Maintain the documentâs emphasis on deterministic,
  scheduled production workloads and constraints (artifact handling,
  observability, data boundaries).
  ## Execution Plan
  1. **Parse `test.pdf`**
     - Extract text.
     - If extraction is unreadable/garbled or the PDF appears image-only, stop and request OCR/extracted text, stating why.
  2. **Build a âdecision spineâ outline**
     - Problem â how browser automation works (only as needed) â landscape models â WebVoyager caveat â requirements/constraints â tool evaluations â proposal options â recommendation â risks/mitigations â next steps.
  3. **Compress aggressively without losing needed specificity**
     - Remove repeated explanations of Selenium brittleness; state once, then refer back.
     - Turn long narrative tool sections into a consistent compact template.
     - Convert requirements into a compact categorized list (must/should/ideal) retaining sub-points.
  4. **Standardize tool entries** (each tool should include):
     - 1 sentence: what it is.
     - 3â6 bullets: requirement fit (downloads/artifacts, secrets, headless/server-side, resilience, manageability).
     - 1â2 bullets: key takeaway / why fit or not.
  5. **Length reduction target**
     - Aim for **~40â60% fewer words** than `test.pdf` text content while preserving all decision-critical content.
     - If impossible without losing key detail, reduce as much as possible and explain what prevented further reduction.
  6. **Final editorial pass**
     - Ensure determinism, failure semantics, artifact handling, and observability remain explicit.
     - Ensure Option 1 vs Option 2 comparison remains clear and the recommendation is unchanged.
  ## Output Format
  Return Markdown with exactly these sections:
  1. **Rewritten Succinct Document**
     - Title.
     - Clear headings.
     - Bullets/tables where helpful (especially for requirements and tool comparisons).
  2. **Edit Summary (5â10 bullets)**
     - What you removed/merged.
     - What you preserved and why.
  ## Quality Checks
  - **Traceability:** Every non-obvious claim maps to wording in `test.pdf`.
  - **Completeness:** All decision-critical topics listed under Goals & Tasks #3
  are present.
  - **Brevity:** Achieves the 40â60% reduction target (or explains why not).
  - **Skimmability:** Recommendation, Option 1 vs Option 2, and Next Steps are
  quickly findable.
  - **Operational clarity:** Failures, retries, artifacts, observability outputs
  are described concretely.
  - **Consistency:** Terminology is consistent (e.g., âartifact handling,â
  âdeterminism,â âobservability,â âheadless execution,â âbounded costâ).
  ## Document Snapshot
  Use this section as a grounding check *before* rewriting. It must reflect
  `test.pdf` specifically.
  ### Specific Topics / Sections (5â10 bullets)
  - Legacy browser automation is a manually executed Windows/.NET executable
  rather than a managed headless server process.
  - Selenium operational overhead is highlighted (e.g., ChromeDriver updates and
  brittle DOM selector maintenance).
  - A four-model framework for AI-based browser automation is presented (from
  low-level primitives to autonomous agents).
  - WebVoyager benchmark is discussed as a capability ceiling signal with
  explicit limitations for production suitability.
  - A detailed requirements/constraints lens is defined (downloads/artifacts,
  credential security, determinism, observability, headless execution, cost
  bounds, data boundaries).
  - Multiple tools are evaluated with requirement fit callouts, including
  Vibium, Browser MCP, Browser Use, Stagehand, Magnitude, BrowserBook, Skyvern,
  Browserable, OpenAI Operator, and Amazon Nova Act.
  - Stagehand and Magnitude are framed as strong TypeScript-first candidates for
  intent-based automation with Playwright under the hood.
  - A two-option proposal is summarized: Option 1 (managed browser + prompt-list
  specs + Lambda runner) vs Option 2 (owned platform/control plane), with a
  recommendation to proceed with Option 1 and set âtripwiresâ for Option 2.
  ### Verbatim Quotes (3 quotes; 10â25 words each)
  - "The automation is implemented as a .NET application compiled into a Windows
  executable."
  - "DOM selectorâbased automation is brittle and prone to breakage when lenders
  make minor UI changes."
  - "Proceed with Option 1 (Minimum Viable Complexity) as the default near-term
  path."
  ### If Quotes Are Not Possible
  - If you cannot extract readable text from `test.pdf` (e.g., it is
  scanned/image-only), explicitly state that limitation and request
  OCR/extracted text as the only missing input. Do not ask for re-upload or file
  paths.
reasoning: >-
  I interpreted the userâs request as: refine the existing prompt contract (not
  execute the rewrite) to be explicitly grounded in the attached PDF and to add
  a new "Document Snapshot" section that evidences engagement with the PDF via
  topic bullets and verbatim quotes. I preserved the original structure and
  intent (succinct rewrite, preserve meaning and recommendation), while making
  instructions more concrete and testable. I ensured compliance with the
  non-negotiables: treat test.pdf as already provided, do not ask for
  upload/path, only request OCR if text is unreadable, and mention the filename
  verbatim. I added the required Document Snapshot section without removing any
  required sections, and I embedded verbatim quotes drawn from the provided PDF
  text; if the executing assistant cannot extract readable text, the plan
  instructs to request OCR and explain the limitation.
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T16:26:15.345Z'
videos: []
</file>
<file path="exports/pdf-6-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
images: []
intent: Please make this document more succinct
interactive: true
iterations: 1
model: gemini-2.5-pro
pdfs:
  - /Users/eroberts/Downloads/test.pdf
prompt: >-
  # Title
  Executive Summary of the 'Adopt AI-based Browser Automation' Strategy Document
  # Role
  You are a Senior Technical Strategist and expert communicator. Your expertise
  lies in distilling complex technical documents into clear, concise summaries
  for executive and engineering leadership audiences.
  # Context
  The user has provided a 22-page PDF document titled "Adopt AI-based Browser
  Automation" (`test.pdf`). This document details the problems with a legacy
  system for downloading mortgage rate sheets, evaluates the modern landscape of
  AI-driven browser automation tools, defines a set of requirements, analyzes
  several specific tools, and concludes with a formal proposal recommending a
  specific path forward.
  # Goals & Tasks
  Your goal is to create a succinct, one-page executive summary of the provided
  document, `test.pdf`. The summary should enable a leadership audience to
  quickly grasp the business problem, the proposed solution, the rationale, and
  the immediate action plan.
  - **Analyze:** Thoroughly read the attached PDF (`test.pdf`) to understand its
  core arguments, tool evaluations, and final proposal.
  - **Identify Key Themes:** Isolate the most critical information:
      - **Problem:** The operational drawbacks of the current legacy Selenium-based system (brittleness, maintenance overhead, manual execution).
      - **Requirements:** The key "must-have" criteria for a new solution (e.g., deterministic execution, reliable file downloads, security, observability).
      - **Recommendation:** The final proposed path forward, specifically "Option 1 (Minimum Viable Complexity)", and the chosen technology type (an intent-based framework like Stagehand).
      - **Next Steps:** The immediate plan, including the pilot scope, success criteria, and 4-6 week decision timeline.
  - **Synthesize & Draft:** Write a new, concise summary that synthesizes these
  identified themes. Do not simply copy-paste text from the source.
  - **Refine:** Ensure the language is professional, clear, and tailored for a
  leadership audience that needs to make a strategic decision.
  # Inputs
  - The primary input is the attached PDF document: `test.pdf`.
  - This document is already provided to you. Do not ask the user to upload or
  paste its contents.
  - The text within the PDF is machine-readable; no OCR is required.
  # Document Snapshot
  This section confirms the key contents of the provided `test.pdf` document.
  **Key Topics Covered:**
  -   Analysis of the legacy .NET/Selenium automation system's drawbacks.
  -   Categorization of the AI-Based Browser Automation Landscape into four
  models (e.g., AI-Augmented Primitives, Intent-Based Automation).
  -   Definition of core requirements for the rate sheet download workload,
  including must-haves like file download support and secure credential
  handling.
  -   Evaluation of multiple open-source and commercial tools (e.g., Stagehand,
  Magnitude, Skyvern) against the defined requirements.
  -   Introduction of the WebVoyager benchmark as a performance signal for AI
  agents.
  -   Proposal of two implementation paths: "Option 1 (Minimum Viable
  Complexity)" and "Option 2 (Kitchen Sink / Owned Platform)".
  -   A clear recommendation to proceed with Option 1.
  -   A detailed plan for a pilot project, including scope, success criteria,
  and a 4-6 week decision gate.
  **Verbatim Quotes:**
  -   "DOM selector-based automation is brittle and prone to breakage when
  lenders make minor UI changes."
  -   "Across the tools evaluated, the most meaningful differentiator is where
  decision-making livesâspecifically, how much of the what, when, and how of
  browser interaction is encoded by humans..."
  -   "Proceed with Option 1 (Minimum Viable Complexity) as the default
  near-term path, using an intent-based framework (Stagehand-style) integrated
  into Publication Downloader..."
  # Constraints
  - **Length:** The final summary must not exceed 500 words.
  - **Focus:** Concentrate on the strategic aspects: the problem, the final
  recommendation (Option 1), the business justification, and the pilot plan.
  Avoid deep technical dives into the specific tools that were evaluated but not
  chosen (e.g., BrowserBook, OpenAI Operator).
  - **Audience:** Write for a leadership audience (both technical and
  non-technical). Define necessary terms simply (e.g., explain that
  "intent-based automation" focuses on *what* to do, not *how*, to reduce
  brittleness).
  - **Tone:** Maintain a professional, objective, and decisive tone.
  # Execution Plan
  1.  **Initial Scan:** Quickly read through the entire document (`test.pdf`) to
  understand its structure and main sections.
  2.  **Detailed Analysis:** Carefully re-read the document, focusing on
  extracting the following:
      - The core business pain points with the "legacy Windows-based 'watcher'" described on pages 1-2.
      - The final recommendation for "Option 1 (Minimum Viable Complexity)" and its architecture on pages 18-20.
      - The rationale for choosing Option 1 (e.g., speed, reduced complexity, leveraging managed services, avoiding rebuilding a control plane).
      - The specific details of the "Suggested Next Steps" on pages 21-22, including the pilot scope, success criteria, and decision gate timeline.
  3.  **Drafting - Section by Section:**
      - **Problem:** Start by summarizing the issues with the legacy system.
      - **Solution & Recommendation:** Clearly state the recommendation to proceed with Option 1, describing it as an approach that uses a modern, intent-based framework (like Stagehand) with a managed browser runtime.
      - **Rationale:** Briefly explain *why* this option is recommended over building a full platform (Option 2).
      - **Next Steps:** Outline the pilot project as a clear, actionable plan.
  4.  **Review and Edit:** Review the drafted summary against the constraints.
  Edit for clarity, conciseness, and word count. Ensure the summary flows
  logically and accurately reflects the source document's proposal.
  # Output Format
  Produce a single markdown file with the following structure:
  ```markdown
  ## Executive Summary: Modernizing Browser Automation
  ### The Problem
  (A short paragraph summarizing the issues with the current legacy system: high
  maintenance, brittle, manual, etc., as detailed on page 1 of `test.pdf`.)
  ### The Recommendation
  (A clear statement of the recommendation to adopt Option 1. Describe what this
  entails at a high level: using an intent-based AI framework (Stagehand-style)
  integrated into the existing Publication Downloader, leveraging a third-party
  managed browser runtime.)
  ### Rationale
  (Use a bulleted list to explain why this path is recommended. Focus on
  business value: faster time-to-market, reduced operational burden, lower
  initial investment, and validating the approach before committing to a larger
  platform as described in the "Strengths" of Option 1.)
  ### Suggested Next Steps
  (A bulleted list outlining the immediate action plan from pages 21-22,
  including:
  - The scope of the pilot (1-2 lenders).
  - Key success criteria (e.g., reduced break/fix incidents, reliable artifact
  capture).
  - The timeline for the decision gate (4-6 weeks).)
  ```
  # Quality Checks
  - **Accuracy:** Does the summary correctly identify "Option 1 (Minimum Viable
  Complexity)" as the recommended path?
  - **Succinctness:** Is the final output under the 500-word limit?
  - **Clarity:** Is the summary easily understood by someone who has not read
  the full 22-page document?
  - **Completeness:** Does the summary include the problem, the recommended
  solution, the justification, and the immediate next steps?
  - **Audience Fit:** Is the tone and level of detail appropriate for an
  executive decision-making audience?
reasoning: >-
  The user requested to make the prompt contract non-generic and explicitly
  grounded in the provided PDF, `test.pdf`. I have fulfilled this by:
  1.  Adding a new `Document Snapshot` section that includes specific bullet
  points summarizing the document's content and three verbatim quotes, proving
  the document has been analyzed.
  2.  Explicitly mentioning the filename `test.pdf` in the `Context`, `Inputs`,
  and `Execution Plan` sections to ground the instructions.
  3.  Updating the `Inputs` section to firmly state that the PDF is already
  provided and should not be requested again, as per the user's hard
  requirement.
  4.  Refining the `Goals & Tasks`, `Constraints`, and `Execution Plan` to
  reference specific concepts, recommendations (like "Option 1"), and
  terminology (like "intent-based framework") found directly within `test.pdf`.
  5.  Adjusting the `Output Format` template to include hints that guide the
  assistant toward using the specific language and recommendations from the
  source document.
  This revision transforms the prompt from a generic summarization task into a
  highly specific contract tailored to the provided source material, ensuring
  the output is directly tied to the document's unique content and proposal.
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T16:36:20.682Z'
videos: []
</file>
<file path="exports/pdf-7-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
images: []
intent: please make this more succinct
interactive: true
iterations: 1
model: gpt-5.2-pro
pdfs:
  - /Users/eroberts/Downloads/test.pdf
prompt: >-
  # Title
  Succinct Rewrite of âAdopt AI-based Browser Automat i onâ (test.pdf)
  ## Role
  You are a technical editor for engineering proposals. Your job is to produce a
  significantly more succinct rewrite of the attached PDF while preserving
  decision-critical content, requirement strength levels, and the documentâs
  recommendation.
  ## Context
  The attached PDF (**test.pdf**) is an internal-style proposal about
  modernizing Loantekâs browser-based rate sheet downloads. It contrasts legacy
  Windows/Selenium âwatcherâ automation with newer AI-assisted approaches,
  defines workload requirements (must/should/ideal), evaluates specific tools,
  and recommends a path forward (Option 1 vs Option 2).
  ### Document Snapshot
  Use this snapshot to confirm you are grounded in **test.pdf** (and to guide
  what must be preserved).
  **Key topics/sections present in the document (5â10 bullets):**
  - Loantekâs current rate sheet ingestion context and why browser portals are
  needed (email/FTP/SFTP/HTTP + scripted portals).
  - Legacy browser automation drawbacks: Windows executable/manual VDI run,
  hardcoded lender steps requiring redeploys, Selenium/ChromeDriver drift,
  brittle selectors.
  - The âPublication Downloaderâ as a modernization inflection point prompting a
  re-evaluation of browser automation.
  - How traditional browser automation works (Selenium/Playwright): DOM
  selectors, scripted actions, waits/retries; brittleness under UI changes.
  - AI-based browser automation landscape framed as 4 models: low-level AI
  primitives; prompt-assisted authoring; declarative/intent-based automation;
  autonomous agent frameworks.
  - WebVoyager benchmark: what it measures well vs what it does not guarantee;
  used as capability ceiling signal.
  - Workload requirements & constraints with explicit **must/should/ideal**
  labels, including deterministic outcomes, observability, data boundaries, and
  headless/server suitability.
  - Tool evaluations and takeaways for: Vibium, Browser MCP, Browser Use,
  Stagehand, Magnitude, BrowserBook, Skyvern, Browserable, OpenAI Operator,
  Amazon Nova Act.
  - Proposal summary with two implementation paths (Option 1 outsource runtime
  vs Option 2 owned platform), trade-offs/risks, mitigations, and next steps
  (pilot + decision gate).
  **Verbatim quotes (10â25 words each; keep exactly as written):**
  - "The Loantek platform i s respons i ble for download i ng rate sheets from
  mortgage lenders ."
  - "The automat i on i s i mplemented as a . NET appl i cat i on comp i led i
  nto a W i ndows executable ."
  - "Proceed w i th Opt i on 1 (M i n i mum V i able Complex i ty) as the
  default near-term path"
  If you cannot access readable embedded text in **test.pdf** (e.g.,
  image-only/scanned PDF), state that explicitly and request OCR/extracted text
  as the only missing input.
  ## Goals & Tasks
  - Read **test.pdf** end-to-end and rewrite it to be materially more succinct
  while keeping the proposalâs technical and operational meaning intact.
  - Remove repetition and âthroat-clearingâ while preserving:
    - The problem statement and why the legacy âwatcherâ approach is risky/expensive.
    - The 4-model landscape (and the axis framing: authoring/execution/ownership).
    - The WebVoyager interpretation caveats.
    - The complete requirements list with original numbering and labels (**must/should/ideal**, including **5** and **5a**).
    - Each evaluated toolâs overview + requirements satisfaction indicators (â/â /â) + key takeaways (no meaning drift).
    - The proposalâs two options, the recommendation to pursue Option 1 near-term, âtripwiresâ for Option 2, risks/mitigations, and suggested next steps.
  - Fix obvious PDF text artifacts (e.g., broken spacing: âautomat i onâ, âw i
  thâ) to normal English **without changing meaning**.
  - Keep the tone: engineering proposal (clear, operationally grounded), not
  marketing.
  ## Inputs
  - PDF attachment already provided as context: **test.pdf**
    - Reference path (context only): `/Users/eroberts/Downloads/test.pdf`
  - Use the attached PDF directly as the source of truth.
  - Do **not** ask the user to paste/re-upload/provide the PDF or its path.
  - Only request OCR/extracted text if you cannot read usable text from the PDF;
  explicitly state that limitation.
  ## Constraints
  - Do **not** introduce new claims, requirements, tools, benchmark results,
  pricing, or vendor capabilities not supported by **test.pdf**.
  - Do **not** delete or weaken security/compliance/data-boundary requirements
  (e.g., credential handling, observability artifacts, deterministic outcomes).
  - Preserve the semantic strength of requirement keywords exactly: **must /
  should / ideal**.
  - Preserve the documentâs internal consistency, especially:
    - Which tools do/do not support download artifact capture.
    - Which tools are TypeScript-first vs Python-first.
    - Which options are recommended and why (Option 1 default near-term; Option 2 later if tripwires hit).
    - Any licensing note present (e.g., AGPL-3.0 mention around Skyvern) and its framing as a trade-off.
  - Prefer concise sentences, active voice, and de-duplicated phrasing.
  ## Execution Plan
  1. **Parse the document structure** from **test.pdf**:
     - List the major headings and ensure you keep them in roughly the same narrative order.
  2. **Compress section-by-section**:
     - Remove repeated rationale and overlong explanations.
     - Convert eligible paragraphs into bullet lists (especially requirements, tool comparisons, risks/mitigations, next steps).
  3. **Normalize and clean text**:
     - Repair broken words/spacing from PDF extraction.
     - Standardize headings, numbering, and â/â /â indicators.
  4. **Preserve decision-critical fidelity**:
     - Keep requirements numbering and labels.
     - Keep each toolâs âRequirements Satisfiedâ content aligned with the PDF.
     - Keep proposal details: Option 1 architecture elements (managed browser runtime, prompt-list specs, Step Functions/Lambdas runner) and Option 2 concept; retain âtripwires.â
  5. **Run a coverage/consistency pass** using the Quality Checks section.
  ## Output Format
  Provide **two deliverables** in Markdown.
  1) **Succinct Rewrite (Primary Output)**
  - Use clear headings and compact prose.
  - Maintain the documentâs general flow (do not reorganize into a totally new
  paper), covering:
    - Context + objective
    - How browser automation works today
    - AI-based browser automation landscape (4 models)
    - WebVoyager notes (what it measures / doesnât)
    - Workload requirements & constraints (must/should/ideal)
    - Research overview + per-tool summaries (Vibium, Browser MCP, Browser Use, Stagehand, Magnitude, BrowserBook, Skyvern, Browserable, OpenAI Operator, Amazon Nova Act)
    - Proposal summary (Option 1 vs Option 2), recommendation, risks/mitigations, suggested next steps
  2) **One-Page Executive Summary (Optional but Recommended)**
  - 8â15 bullets maximum.
  - Must include:
    - Why the legacy watcher/Selenium approach is failing operationally
    - The top workload requirements (must-haves)
    - The best-fitting candidates and why (as stated in the PDF)
    - The recommended path (Option 1) + pilot scope + decision gate timing
  ## Quality Checks
  - **Succinctness target:** reduce word count by ~40â60% vs **test.pdf** while
  retaining decision-critical content.
  - **Requirements integrity:** all numbered requirements remain, including
  **5** and **5a**, with correct **must/should/ideal** labels.
  - **Tool fidelity:** for each evaluated tool, â/â /â judgments remain
  consistent with **test.pdf** (especially download artifact access, execution
  model, and manageability).
  - **Recommendation preserved:** the rewrite still recommends Option 1 as the
  default near-term approach and retains the âtripwiresâ framing for Option 2.
  - **Operational clarity retained:** deterministic outcomes/failure semantics,
  observability artifacts (logs/screenshots/page snapshots),
  credential/security, data boundary/compliance, cost/runtime bounds are
  explicit.
  - **Text cleanup correctness:** spacing/broken-word artifacts are fixed
  without altering meaning; headings and lists render cleanly.
  - **No extraneous additions:** no new vendors, no unreferenced benchmark
  claims, no speculative capabilities beyond the PDF.
reasoning: >-
  I preserved the original prompt contract structure and required section order,
  while making it explicitly grounded in the provided attachment. To satisfy the
  userâs added âDocument Snapshotâ requirement without violating the mandated
  section order, I embedded a clearly titled âDocument Snapshotâ subsection
  inside the Context section. I referenced the attachment by filename (test.pdf)
  and treated it as already-available input, only permitting an OCR request if
  the PDF text is unreadable. I also tightened tasks and checks to be concrete
  and testable against the PDFâs specific content: Loantekâs legacy âwatcher,â
  Publication Downloader, the 4-model landscape, WebVoyager caveats, numbered
  requirements (including 5 and 5a), specific tool set, and the Option 1 vs
  Option 2 proposal with âtripwires.â
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T17:26:32.606Z'
videos: []
</file>
<file path="exports/pdf-document-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
intent: please make this document more succinct
interactive: true
iterations: 1
model: gpt-5.2
prompt: >-
  # Title
  Succinct Rewrite of Provided Document (Preserve Meaning)
  ## Role
  You are an expert editor specializing in concise rewriting. Your job is to
  shorten the provided document while preserving its meaning, accuracy, and
  required details.
  ## Context
  The user has a document that is too long or wordy and wants a more succinct
  version. The rewrite must maintain the original intent and any critical
  information, while improving clarity and reducing redundancy.
  ## Goals & Tasks
  - Produce a more succinct version of the document.
  - Preserve:
    - Core meaning, claims, and factual content
    - Necessary nuance, definitions, and any required terminology
    - Logical structure where helpful (unless reorganizing improves concision without losing meaning)
  - Improve:
    - Redundancy and repetition
    - Wordiness, filler, and indirect phrasing
    - Sentence length and readability
  - Optional (only if helpful and safe):
    - Convert verbose passages into bullets
    - Tighten headings and transitions
  ## Inputs
  Request/Use the following inputs from the user:
  - **Document text** to edit (paste in full).
  - Any **must-keep** sections, sentences, terms, or legal/compliance language.
  - Target constraints (if any):
    - Desired length (e.g., âcut by 30%â, âmax 400 wordsâ, âone pageâ)
    - Audience (executives, customers, internal team, academic, etc.)
    - Tone (formal, friendly, neutral) and voice (first person vs. third person)
    - Formatting requirements (keep headings, markdown, doc style, etc.)
  If constraints are not provided, default to:
  - Maintain original tone and formatting as much as possible.
  - Aim for a **~25â40% reduction** in word count without losing meaning.
  ## Constraints
  - Do **not** introduce new facts, claims, metrics, or promises.
  - Do **not** remove critical caveats, safety/compliance statements, or
  definitions.
  - Do **not** change meaning, scope, or intent.
  - Keep proper nouns, product names, and key terms consistent.
  - If a section is ambiguous about what can be removed, **flag it** rather than
  guessing.
  ## Execution Plan
  1. Read the entire document to understand purpose, audience, and key points.
  2. Identify redundancies, repeated ideas, filler phrases, and overly long
  sentences.
  3. Tighten language:
     - Prefer active voice when it reduces words without changing tone.
     - Replace phrases with single precise words.
     - Remove throat-clearing intros and obvious statements.
  4. Restructure minimally when it improves concision (e.g., merge similar
  paragraphs, use bullets).
  5. Verify fidelity: confirm all key points remain and nothing new was added.
  6. Provide the revised document and a brief summary of major edits.
  ## Output Format
  Return:
  1. **Succinct Version (Final)**: Provide the rewritten document in the same
  general format as the original (headings/bullets preserved unless improved).
  2. **Change Summary (Brief)**:
     - Bullet list of the main categories of reductions (e.g., removed redundancy, simplified sentences, condensed examples).
  3. **Metrics (if feasible)**:
     - Approx. original word count vs. revised word count and % reduction.
  4. **Questions/Flags (only if needed)**:
     - Any passages where removing text risked changing meaning; ask targeted clarification questions.
  ## Quality Checks
  Before finalizing, ensure:
  - Meaning and key details are preserved; no new information added.
  - The revised text is clearly shorter and more readable.
  - No critical disclaimers/requirements were dropped.
  - Terminology, names, and numbers (if present) match the original.
  - Formatting is clean and consistent.
  - Any necessary clarifications are explicitly flagged.
reasoning: >-
  User wants an optimized prompt contract for another assistant to make a
  document more succinct. Missing key input: the document itself and any
  constraints (target length, audience, tone, must-keep sections, formatting).
  Strategy: create a contract that requests the document and preferences,
  defines succinctness goals, preserves meaning, and provides an output format
  with a revised version plus a change summary. Include constraints to avoid
  adding new facts and to maintain required terminology. Provide an execution
  plan and quality checks for concision, fidelity, and readability.
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T14:37:00.263Z'
</file>
<file path="exports/pdf-issue.yaml">
contextPaths:
  - path: inline-intent
    source: intent
intent: Please make this information more succinct for me
interactive: true
iterations: 1
model: gpt-5.2
prompt: >-
  # Title
  Succinct Rewrite of Provided Information
  ## Role
  You are a concise editor and summarizer. Your job is to rewrite user-provided
  content to be more succinct while preserving meaning and key details.
  ## Context
  The user wants a shorter, clearer version of some information. The source text
  may be any length or format (paragraphs, notes, email draft, report excerpt,
  etc.). If the source text is missing, you must request it.
  ## Goals & Tasks
  - Produce a more succinct version of the provided text.
  - Preserve:
    - Core message and intent
    - Key facts, figures, dates, names, and constraints
    - Required nuance (e.g., caveats, exceptions) when it materially affects meaning
  - Improve clarity by:
    - Removing redundancies and filler
    - Simplifying wording without changing meaning
    - Tightening sentences and structure
  - If the user specifies a target length, audience, tone, or format, follow it.
  ## Inputs
  - **Source text to condense**: {{SOURCE_TEXT}}
  - **Optional preferences** (use if provided; otherwise infer conservatively):
    - Target length (e.g., âcut by 30%â, â<=120 wordsâ, âone paragraphâ): {{TARGET_LENGTH}}
    - Audience (e.g., internal team, executives, customers): {{AUDIENCE}}
    - Tone (e.g., neutral, formal, friendly, direct): {{TONE}}
    - Required format (e.g., bullets, email, memo, slide notes): {{FORMAT}}
    - Must-keep items (specific lines/terms that must remain): {{MUST_KEEP}}
  ## Constraints
  - Do **not** add new facts, claims, or assumptions.
  - Do **not** omit critical details that change decisions, obligations, or
  outcomes.
  - Maintain proper nouns and numbers accurately.
  - If the source is ambiguous or missing essential context, ask up to **3**
  clarifying questions before rewriting.
  - If {{SOURCE_TEXT}} is empty or not provided, request it and stop.
  ## Execution Plan
  1. Verify {{SOURCE_TEXT}} is present; if not, ask the user to paste it.
  2. Identify the core purpose (what the text is trying to communicate) and the
  must-keep details (facts, dates, actions, constraints).
  3. Remove repetition, throat-clearing, and low-information phrases.
  4. Reconstruct the text with tighter phrasing and clearer structure.
  5. Ensure fidelity: compare against the original for meaning, key facts, and
  completeness.
  6. Confirm compliance with any provided length/tone/format constraints.
  ## Output Format
  Provide:
  1. **Succinct version** (primary deliverable) in the requested {{FORMAT}}; if
  none, return as a single tight paragraph.
  2. **Key changes (optional, brief)**: 3â6 bullets noting major cuts/rewrites
  only if the user asked for explanation; otherwise omit.
  ## Quality Checks
  - **Fidelity**: No meaning drift; all critical facts retained.
  - **Brevity**: Noticeably shorter; no filler.
  - **Clarity**: Plain language; strong verbs; minimal jargon.
  - **Correctness**: Names, numbers, dates, and requirements match the source.
  - **Constraint adherence**: Meets target length/tone/format if provided.
reasoning: >-
  The user wants help making âthis informationâ more succinct, but they did not
  provide the actual text to condense. So the prompt contract must (a)
  request/accept the source text as an input, (b) define how to summarize
  (length targets, preserve meaning, keep key facts, remove redundancies), (c)
  specify optional preferences (audience, tone, maximum length, format), and (d)
  ensure the assistant only performs the succincting task when the text is
  provided. I will draft a prompt contract that can be executed later, with
  clear goals, constraints (no new info, maintain critical details), execution
  steps, output format (succinct version + optional bullets of key points), and
  quality checks (coverage, fidelity, length compliance).
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-08T22:26:25.496Z'
</file>
<file path="exports/prompt-export.json">
{
  "schemaVersion": "1",
  "intent": "test prompt",
  "model": "gpt-5.2",
  "targetModel": "gpt-5.2",
  "prompt": "# Title\nTest Prompt Contract: Produce a Simple Demonstration Output\n\n## Role\n- Act as a helpful assistant executing a minimal âtestâ task to verify instruction-following, formatting, and completeness.\n\n## Context\n- The requester provided only: **âtest promptâ**.\n- No domain, audience, or deliverable type was specified; treat this as a generic prompt-contract validation.\n\n## Goals & Tasks\n- Create a small, self-contained demonstration output that proves you can:\n  - Follow instructions precisely.\n  - Ask for missing details only when necessary.\n  - Produce a clear, structured response.\n- Tasks:\n  - (1) Interpret âtest promptâ as a request for a minimal sample deliverable.\n  - (2) Produce a short sample output (see Output Format) using a neutral topic.\n  - (3) Include a brief checklist confirming what you did.\n\n## Inputs\n- Primary input:\n  - User intent text: `test prompt`\n- Optional inputs (not provided):\n  - Intended audience, tone, length, topic, constraints.\n\n## Constraints\n- Do not request additional clarification unless it is required to proceed.\n- Keep the output brief and deterministic (avoid open-ended brainstorming).\n- Avoid sensitive content.\n- Do not mention internal policies or hidden reasoning.\n\n## Execution Plan\n- Step 1: Assume a neutral topic (e.g., âbenefits of daily walksâ) if none is provided.\n- Step 2: Produce a concise sample deliverable using the required structure.\n- Step 3: Add a completion checklist indicating compliance with the contract.\n\n## Output Format\nReturn content in the following exact structure:\n\n1) **Sample Output**\n- A 5â7 sentence paragraph on the assumed neutral topic.\n\n2) **Completion Checklist**\n- Bullets confirming:\n  - Followed structure\n  - Kept brevity\n  - Avoided sensitive content\n  - Did not ask unnecessary questions\n\n## Quality Checks\n- Verify the response includes both required sections: âSample Outputâ and âCompletion Checklistâ.\n- Ensure the paragraph is 5â7 sentences.\n- Ensure the checklist has at least 4 bullets.\n- Confirm the topic is neutral and non-sensitive.\n- Confirm wording is clear and free of contradictions.",
  "reasoning": "User provided minimal intent: \"test prompt\" and asked for a structured prompt contract. No concrete deliverable domain is specified, so create a generic test prompt contract that another assistant can execute, focusing on demonstrating compliance with the required sections, clarity, and testability. Include placeholders for missing inputs and a simple, safe task (e.g., produce a brief sample output and explain steps) without actually executing it. Ensure the prompt text contains the exact required sections in order and begins with a # Title. Return JSON with exactly two keys.",
  "refinements": [],
  "iterations": 1,
  "interactive": true,
  "timestamp": "2026-01-06T14:50:14.538Z",
  "contextPaths": [
    {
      "path": "inline-intent",
      "source": "intent"
    }
  ]
}
</file>
<file path="packages/core/src/index.ts">
export { callLLM, getEmbedding } from './lib/llm'
export type { Message, MessageContent, TextPart, ImagePart, VideoPart, PdfPart } from './lib/llm'
</file>
<file path="runs/prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
  - path: packages/ui/README.md
    source: smart
  - path: packages/ui/package.json
    source: smart
  - path: packages/ui/src/app/layout.tsx
    source: smart
  - path: packages/api/package.json
    source: smart
  - path: vitest.config.ts
    source: smart
images: []
intent: >-
  I need to know how to run the UI for this project locally.
  I have a `.env` file setup in the `packages/ui/env` folder, but I'm not sure
  this is the correct place. Please let me know if it is correct and if it is
  missing any information, just to make sure I have everything setup.
interactive: true
iterations: 2
model: gpt-5.2
pdfs: []
prompt: >
  # Title
  Local Development Runbook for LoanTek Publication Downloader UI + `.env`
  Placement Validation + AWS Connectivity Setup (Step-by-Step)
  ## Role
  You are a senior full-stack engineer experienced with Next.js (App Router),
  Node/TypeScript monorepos, and AWS (SSM Parameter Store, IAM, AWS CLI/SSO,
  credential resolution).
  ## Context
  The repository contains a UI package at `packages/ui`.
  You must use the provided context files to determine:
  - How the UI is intended to be run locally
  - How environment variables are loaded (local `.env`, generated `.env`, and/or
  AWS SSM)
  - Whether placing a `.env` file inside `packages/ui/env` is correct, and what
  the correct location should be
  - What AWS settings/steps are required locally to successfully connect to AWS
  services used by the UI (especially SSM Parameter Store) and how to validate
  connectivity
  Relevant files already provided (assume you can inspect them directly as
  context):
  - `packages/ui/README.md`
  - `packages/ui/package.json`
  - `packages/ui/src/app/layout.tsx`
  - `vitest.config.ts`
  Key known facts to leverage explicitly:
  - `packages/ui/README.md` documents running locally and AWS_PROFILE usage,
  plus SSM path convention `/qa/loantek/publication-downloader-ui/` derived from
  `APP_ENV`.
  - `packages/ui/package.json` defines scripts: `dev`, `start`, `env-sync`,
  `generate-env`.
  - `packages/ui/src/app/layout.tsx` calls `await loadEnv()` at runtime,
  implying server-side env loading.
  ## Goals & Tasks
  1. Provide step-by-step instructions to run the UI locally using the repoâs
  documented scripts.
  2. Determine and explain the correct `.env` file location(s) for local UI
  development.
     - Explicitly address the userâs current setup: `.env` inside `packages/ui/env`.
  3. Explain how env vars are expected to be sourced in this repo:
     - Next.js env loading expectations (app root env files)
     - From generated env via `npm run generate-env`
     - From AWS SSM via `npm run env-sync` and/or runtime loading mentioned in the README
  4. Add an ordered, âbasic principlesâ AWS connectivity setup section with
  exact steps to make AWS calls succeed locally.
     - Cover credential acquisition (AWS SSO or access keys), profile selection, region configuration, and verification.
     - Cover required access to SSM Parameter Store paths used by this UI and how to confirm permissions.
     - Tie steps to the repoâs conventions: `APP_ENV`, SSM path prefix, and `AWS_PROFILE` usage.
  5. Identify what information might be missing from the userâs env setup.
     - Provide a concrete checklist of likely required variables and how to discover them from code (without inventing names).
  6. Include troubleshooting steps for common failures:
     - Missing env, wrong env file location, missing AWS credentials, wrong profile, wrong region
     - SSM permission errors, parameter path mismatch, APP_ENV mismatch
     - Next dev server issues, New Relic preload issues from `NODE_OPTIONS` in scripts
  ## Inputs
  - Context files (use directly):
    - `packages/ui/README.md`
    - `packages/ui/package.json`
    - `packages/ui/src/app/layout.tsx`
    - `vitest.config.ts`
  - No additional user attachments are required.
  ## Constraints
  - Do not ask the user to re-upload or paste any of the provided files.
  - Only ask clarifying questions if the answer truly depends on unknowns not
  inferable from the provided context (e.g., whether their org uses AWS SSO vs
  access keys; which AWS account/role; intended APP_ENV).
  - Keep recommendations consistent with scripts and conventions shown in
  `packages/ui/package.json` and `packages/ui/README.md`.
  - Do not invent specific environment variable names unless you can justify
  them from code found in the provided files. If uncertain, provide a precise
  method to discover them.
  - Provide âexact steps, in orderâ for AWS connectivity from basics
  (credentials â config â verify identity â verify SSM access â run UI).
  ## Execution Plan
  1. Extract official local run steps from `packages/ui/README.md` and reflect
  them verbatim where applicable.
  2. Enumerate and explain UI scripts from `packages/ui/package.json`:
     - What `dev` does (including `NODE_OPTIONS` preload)
     - What `start` does
     - What `env-sync` does (SSM path argument pattern)
     - What `generate-env` does (source template location under `packages/ui/env`)
  3. Use `packages/ui/src/app/layout.tsx` to highlight runtime env loading via
  `loadEnv()` and explain implications (server-side execution during app
  startup).
  4. Determine correct `.env` placement based on Next.js conventions:
     - Explain which directory Next uses as the âproject rootâ (the Next app directory `packages/ui/`) and which `.env*` filenames are loaded.
     - Compare against `packages/ui/env/.env` and state whether it will be auto-loaded by Next.
  5. Provide **AWS connectivity setup (ordered)** from first principles:
     - Step 1: Confirm AWS tooling presence (AWS CLI) and baseline environment (Node/npm).
     - Step 2: Choose credential method:
       - If AWS SSO: configure SSO profile and login.
       - If access keys: configure credentials file and default/selected profile.
     - Step 3: Ensure region is set (env var or config), explain why region matters for SSM.
     - Step 4: Verify credentials resolution:
       - Command to confirm identity (e.g., STS caller identity) and interpret output.
     - Step 5: Verify SSM access:
       - Determine expected parameter path prefix from README: `/${APP_ENV}/loantek/publication-downloader-ui/`.
       - Provide commands to test listing/getting parameters under that path.
       - If denied, explain required IAM permissions at a high level (actions and resource patterns) without fabricating account IDs.
     - Step 6: Apply repo workflow:
       - Option A: Run `npm run env-sync` to populate `packages/ui/.env`.
       - Option B: Rely on runtime loading (as described in README) and ensure `APP_ENV` and `AWS_PROFILE` are set.
     - Step 7: Run the UI and validate expected behavior.
  6. Provide âmissing env var discoveryâ method:
     - Direct the assistant to search for `process.env` usage and inspect `packages/ui/src/lib/serverInit` (mentioned in `layout.tsx` import) if available; otherwise clearly state the limitation and provide search instructions.
  7. Troubleshooting matrix:
     - Symptom â likely cause â exact fix steps (including AWS_PROFILE/APP_ENV/region, SSM permissions, New Relic preload issues, wrong `.env` location).
  ## Output Format
  Provide a single markdown response with these sections (in this order):
  1. **Quick Start (Local UI)** â exact commands to run; include prerequisites.
  2. **Where the `.env` should live (and what `packages/ui/env` is for)** â
  explicit answer about `packages/ui/env/.env` vs `packages/ui/.env*` and how
  `generate-env` relates.
  3. **AWS Connectivity Setup (Exact Steps, In Order)**
     - Subsections: Credentials, Profile selection (`AWS_PROFILE`), Region, Verify identity, Verify SSM path access, Sync/load env, Run UI.
     - Include concrete terminal commands and what success/failure looks like.
  4. **Recommended Env Workflows** â (A) local-only, (B) AWS SSM-backed, (C)
  template-based `generate-env`.
  5. **How to Verify If Your `.env` Is Missing Anything** â concrete steps; how
  to locate required vars from code.
  6. **Troubleshooting** â table or bullet list: symptom â cause â fix.
  7. **Clarifying Questions (only if needed)** â maximum 3 short questions.
  ## Quality Checks
  - All commands must match scripts shown in `packages/ui/package.json` (e.g.,
  `npm run dev`, `npm run env-sync`, `npm run generate-env`).
  - Must explicitly reference and align with guidance in
  `packages/ui/README.md`:
    - `AWS_PROFILE=... npm run dev`
    - `APP_ENV` usage
    - SSM path convention `/qa/loantek/publication-downloader-ui/` and generalized `/${APP_ENV}/loantek/publication-downloader-ui/`.
  - Must directly answer whether `packages/ui/env/.env` is the correct place and
  explain what Next.js loads by default.
  - AWS section must be âfrom basic principlesâ and strictly ordered with
  verification steps and expected outcomes.
  - Must not claim specific env variable names unless verified from provided
  code; otherwise provide a method to discover.
  - Ensure the runbook is usable end-to-end for a developer on a fresh clone.
reasoning: >-
  I interpreted the refinement instruction as requiring the prompt contract to
  explicitly add an ordered, from-first-principles AWS setup path for local UI
  execution, including credentials sourcing (AWS_PROFILE), region,
  permissions/SSM path, and verification steps, while still grounding everything
  in the provided repo context (README/package scripts/layout loadEnv). I
  preserved the existing structure and goals, expanded the AWS section into
  concrete, sequential tasks (configure AWS CLI/SSO, set region, validate
  identity, validate SSM access, then run env-sync or rely on runtime loading),
  and added constraints to avoid inventing env var names while still providing
  exact operational steps and checks. No task execution is performed; it remains
  a contract for another assistant and references the provided context files by
  path.
refinements:
  - >-
    I also need to understand any AWS settings that I might need to add/change
    in order to get a connection. I need exact steps, in order, from basic
    principles
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T18:57:39.623Z'
videos: []
</file>
<file path="src/__tests__/__fixtures__/themes/adapt/opencode.json">
{
  "defs": {
    "surface": "#0b2a32"
  },
  "theme": {
    "background": "#001219",
    "text": "#e9d8a6",
    "textMuted": "#94d2bd",
    "backgroundPanel": "surface",
    "backgroundElement": "#005f73",
    "primary": "#0a9396",
    "border": "#005f73",
    "warning": "#ee9b00",
    "error": "#ae2012",
    "success": "#2a9d8f"
  }
}
</file>
<file path="src/__tests__/generate/interactive.test.ts">
import { runGenerationWorkflow } from '../../generate/interactive'
import type { StreamDispatcher } from '../../generate/stream'
import type { TokenTelemetry } from '../../generate/types'
describe('runGenerationWorkflow (interactive)', () => {
  it('passes pdfs through to PromptGenerationRequest', async () => {
    const generator = {
      generatePromptDetailed: jest.fn().mockResolvedValue({ prompt: 'ok', reasoning: 'r' }),
    }
    const stream: StreamDispatcher = {
      mode: 'none',
      emit: jest.fn(),
    }
    const telemetry: TokenTelemetry = {
      files: [],
      intentTokens: 0,
      fileTokens: 0,
      systemTokens: 0,
      totalTokens: 0,
    }
    await runGenerationWorkflow({
      service: generator as never,
      context: {
        intent: 'intent',
        refinements: [],
        model: 'gpt-4o-mini',
        targetModel: 'gpt-5.2',
        fileContext: [],
        images: [],
        videos: [],
        pdfs: ['/tmp/doc.pdf'],
        metaInstructions: '',
      },
      telemetry,
      interactiveMode: 'none',
      display: false,
      stream,
    })
    expect(generator.generatePromptDetailed).toHaveBeenCalledWith(
      expect.objectContaining({ pdfs: ['/tmp/doc.pdf'] }),
    )
  })
})
</file>
<file path="src/__tests__/generate/payload-io.test.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
import {
  GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
  type GenerateJsonPayload,
} from '../../generate/types'
import { loadGeneratePayloadFromFile, serializeGeneratePayload } from '../../generate/payload-io'
const createTempDir = async (): Promise<string> =>
  fs.mkdtemp(path.join(os.tmpdir(), 'prompt-maker-payload-io-'))
const SAMPLE_PAYLOAD: GenerateJsonPayload = {
  schemaVersion: GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
  intent: 'Build a payload loader',
  model: 'gpt-4.1',
  targetModel: 'gpt-4.1',
  prompt: 'Hello world',
  refinements: ['Be concise'],
  iterations: 1,
  interactive: false,
  timestamp: '1970-01-01T00:00:00.000Z',
  contextPaths: [{ path: 'README.md', source: 'file' }],
}
describe('payload-io', () => {
  it('round-trips JSON payloads', async () => {
    const dir = await createTempDir()
    try {
      const filePath = path.join(dir, 'payload.json')
      await fs.writeFile(filePath, serializeGeneratePayload(SAMPLE_PAYLOAD, 'json'), 'utf8')
      await expect(loadGeneratePayloadFromFile(filePath)).resolves.toEqual(SAMPLE_PAYLOAD)
    } finally {
      await fs.rm(dir, { recursive: true, force: true })
    }
  })
  it('round-trips YAML payloads', async () => {
    const dir = await createTempDir()
    try {
      const filePath = path.join(dir, 'payload.yaml')
      await fs.writeFile(filePath, serializeGeneratePayload(SAMPLE_PAYLOAD, 'yaml'), 'utf8')
      await expect(loadGeneratePayloadFromFile(filePath)).resolves.toEqual(SAMPLE_PAYLOAD)
    } finally {
      await fs.rm(dir, { recursive: true, force: true })
    }
  })
  it('throws on invalid JSON content', async () => {
    const dir = await createTempDir()
    try {
      const filePath = path.join(dir, 'payload.json')
      await fs.writeFile(filePath, '{ not-valid-json', 'utf8')
      await expect(loadGeneratePayloadFromFile(filePath)).rejects.toThrow('Failed to parse JSON')
    } finally {
      await fs.rm(dir, { recursive: true, force: true })
    }
  })
  it('throws on invalid payload shape', async () => {
    const dir = await createTempDir()
    try {
      const filePath = path.join(dir, 'payload.yml')
      await fs.writeFile(filePath, 'intent: hello\n', 'utf8')
      await expect(loadGeneratePayloadFromFile(filePath)).rejects.toThrow(
        'Invalid generate payload',
      )
    } finally {
      await fs.rm(dir, { recursive: true, force: true })
    }
  })
})
</file>
<file path="src/__tests__/generate/resume-loaded-stream-event.test.ts">
import type { StreamEventInput } from '../../generate/types'
test('resume.loaded is a valid StreamEventInput', () => {
  const event = {
    event: 'resume.loaded',
    source: 'history',
    reusedContextPaths: [{ path: 'docs/context-templates.md', source: 'file' }],
    missingContextPaths: [{ path: 'https://example.com', source: 'url' }],
  } satisfies StreamEventInput
  expect(event.event).toBe('resume.loaded')
})
</file>
<file path="src/__tests__/history/generate-history.test.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
import {
  loadGeneratePayloadFromHistory,
  parseFromHistorySelector,
} from '../../history/generate-history'
import {
  GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
  type GenerateJsonPayload,
} from '../../generate/types'
describe('generate-history helpers', () => {
  const tempRoots: string[] = []
  afterEach(async () => {
    await Promise.all(
      tempRoots.map(async (root) => {
        await fs.rm(root, { recursive: true, force: true })
      }),
    )
    tempRoots.splice(0, tempRoots.length)
  })
  const writeHistory = async (lines: string[]): Promise<string> => {
    const root = await fs.mkdtemp(path.join(os.tmpdir(), 'pm-generate-history-'))
    tempRoots.push(root)
    const historyPath = path.join(root, 'history.jsonl')
    await fs.writeFile(historyPath, `${lines.join('\n')}\n`, 'utf8')
    return historyPath
  }
  const createPayload = (overrides: Partial<GenerateJsonPayload>): GenerateJsonPayload => ({
    schemaVersion: GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
    intent: 'intent',
    model: 'model',
    targetModel: 'target-model',
    prompt: 'prompt',
    refinements: [],
    iterations: 1,
    interactive: false,
    timestamp: new Date(0).toISOString(),
    contextPaths: [{ path: 'file.txt', source: 'file' }],
    ...overrides,
  })
  test('parseFromHistorySelector parses last/last:N/N', () => {
    expect(parseFromHistorySelector('last')).toEqual({ fromEnd: 1, label: 'last' })
    expect(parseFromHistorySelector('last:2')).toEqual({ fromEnd: 2, label: 'last:2' })
    expect(parseFromHistorySelector('3')).toEqual({ fromEnd: 3, label: '3' })
  })
  test('parseFromHistorySelector rejects invalid selectors', () => {
    expect(() => parseFromHistorySelector('last:0')).toThrow(/Invalid --from-history selector/)
    expect(() => parseFromHistorySelector('wat')).toThrow(/Invalid --from-history selector/)
  })
  test('loadGeneratePayloadFromHistory enforces selector bounds', async () => {
    const payload = createPayload({ intent: 'only' })
    const historyPath = await writeHistory([JSON.stringify(payload)])
    await expect(loadGeneratePayloadFromHistory({ selector: '2', historyPath })).rejects.toThrow(
      /History selector is out of range/,
    )
  })
  test('loadGeneratePayloadFromHistory refuses unsupported schemaVersion', async () => {
    const supported = createPayload({ intent: 'supported' })
    const unsupported = { ...supported, schemaVersion: '999' }
    const historyPath = await writeHistory([JSON.stringify(supported), JSON.stringify(unsupported)])
    await expect(loadGeneratePayloadFromHistory({ selector: 'last', historyPath })).rejects.toThrow(
      /Unsupported history payload schemaVersion=999/,
    )
    await expect(loadGeneratePayloadFromHistory({ selector: '2', historyPath })).resolves.toEqual(
      supported,
    )
  })
})
</file>
<file path="src/__tests__/tui/budgets-persistence.test.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
const readJson = async (filePath: string): Promise<unknown> => {
  const contents = await fs.readFile(filePath, 'utf8')
  return JSON.parse(contents) as unknown
}
const writeJson = async (filePath: string, value: unknown): Promise<void> => {
  await fs.mkdir(path.dirname(filePath), { recursive: true })
  await fs.writeFile(filePath, `${JSON.stringify(value, null, 2)}\n`, 'utf8')
}
describe('budget settings persistence', () => {
  const envBefore = { ...process.env }
  afterEach(() => {
    process.env = { ...envBefore }
  })
  test('saving updates promptGenerator without rewriting other fields', async () => {
    const tempRoot = await fs.mkdtemp(path.join(os.tmpdir(), 'pm-budgets-persist-'))
    const configPath = path.join(tempRoot, 'config.json')
    await writeJson(configPath, {
      openaiApiKey: 'keep-me',
      promptGenerator: { defaultModel: 'gpt-4o', maxInputTokens: 500 },
    })
    process.env.PROMPT_MAKER_CLI_CONFIG = configPath
    jest.resetModules()
    const { updateCliPromptGeneratorSettings } = await import('../../config')
    await updateCliPromptGeneratorSettings({
      maxInputTokens: 123,
      maxContextTokens: 456,
      contextOverflowStrategy: 'drop-oldest',
    })
    const updated = await readJson(configPath)
    expect(updated).toMatchObject({
      openaiApiKey: 'keep-me',
      promptGenerator: {
        defaultModel: 'gpt-4o',
        maxInputTokens: 123,
        maxContextTokens: 456,
        contextOverflowStrategy: 'drop-oldest',
      },
    })
  })
  test('null patch deletes budget fields', async () => {
    const tempRoot = await fs.mkdtemp(path.join(os.tmpdir(), 'pm-budgets-persist-'))
    const configPath = path.join(tempRoot, 'config.json')
    await writeJson(configPath, {
      promptGenerator: {
        defaultModel: 'gpt-4o',
        maxInputTokens: 123,
        maxContextTokens: 456,
        contextOverflowStrategy: 'drop-oldest',
      },
    })
    process.env.PROMPT_MAKER_CLI_CONFIG = configPath
    jest.resetModules()
    const { updateCliPromptGeneratorSettings } = await import('../../config')
    await updateCliPromptGeneratorSettings({
      maxInputTokens: null,
      maxContextTokens: null,
      contextOverflowStrategy: null,
    })
    const updated = await readJson(configPath)
    expect(updated).toMatchObject({
      promptGenerator: { defaultModel: 'gpt-4o' },
    })
    expect(updated).not.toMatchObject({
      promptGenerator: {
        maxInputTokens: expect.anything(),
      },
    })
  })
})
</file>
<file path="src/__tests__/tui/budgets-settings.test.ts">
import { parseBudgetSettingsDraft } from '../../tui/budget-settings'
describe('budget settings draft parsing', () => {
  test('empty fields disable budgets', () => {
    expect(
      parseBudgetSettingsDraft({
        maxContextTokensDraft: '',
        maxInputTokensDraft: '',
        contextOverflowStrategyDraft: '',
      }),
    ).toEqual({
      ok: true,
      settings: {
        maxContextTokens: null,
        maxInputTokens: null,
        contextOverflowStrategy: null,
      },
    })
  })
  test('rejects non-integer tokens', () => {
    const result = parseBudgetSettingsDraft({
      maxContextTokensDraft: 'abc',
      maxInputTokensDraft: '',
      contextOverflowStrategyDraft: '',
    })
    expect(result.ok).toBe(false)
    if (result.ok) {
      throw new Error('Expected error result')
    }
    expect(result.errorMessage).toBe('Max context tokens must be a positive integer.')
  })
  test('defaults overflow to fail when budgets enabled', () => {
    expect(
      parseBudgetSettingsDraft({
        maxContextTokensDraft: '100',
        maxInputTokensDraft: '',
        contextOverflowStrategyDraft: '',
      }),
    ).toEqual({
      ok: true,
      settings: {
        maxContextTokens: 100,
        maxInputTokens: null,
        contextOverflowStrategy: 'fail',
      },
    })
  })
  test('preserves selected overflow strategy', () => {
    expect(
      parseBudgetSettingsDraft({
        maxContextTokensDraft: '100',
        maxInputTokensDraft: '200',
        contextOverflowStrategyDraft: 'drop-oldest',
      }),
    ).toEqual({
      ok: true,
      settings: {
        maxContextTokens: 100,
        maxInputTokens: 200,
        contextOverflowStrategy: 'drop-oldest',
      },
    })
  })
  test('allows setting overflow strategy without budgets', () => {
    expect(
      parseBudgetSettingsDraft({
        maxContextTokensDraft: '',
        maxInputTokensDraft: '',
        contextOverflowStrategyDraft: 'drop-url',
      }),
    ).toEqual({
      ok: true,
      settings: {
        maxContextTokens: null,
        maxInputTokens: null,
        contextOverflowStrategy: 'drop-url',
      },
    })
  })
})
</file>
<file path="src/__tests__/tui/context-popup-state-mutations.test.ts">
import {
  defocusSuggestionsIfEmpty,
  deriveSuggestionsViewModel,
  getAutoAddAbsolutePathCandidate,
} from '../../tui/screens/command/hooks/context-popup-glue/popup-state-mutations'
describe('context popup state mutations', () => {
  describe('deriveSuggestionsViewModel', () => {
    it('clamps suggested selection index and preserves focus when suggestions exist', () => {
      const model = deriveSuggestionsViewModel({
        draft: 'a',
        suggestedItems: ['a', 'b', 'c'],
        suggestedSelectionIndex: 99,
        suggestedFocused: true,
        filterSuggestions: (_query, suggestions) => suggestions,
      })
      expect(model.suggestions).toEqual(['a', 'b', 'c'])
      expect(model.selectionIndex).toBe(2)
      expect(model.focused).toBe(true)
      expect(model.shouldDefocus).toBe(false)
    })
    it('defocuses suggestions when the filtered list is empty', () => {
      const model = deriveSuggestionsViewModel({
        draft: 'a',
        suggestedItems: ['a', 'b'],
        suggestedSelectionIndex: 1,
        suggestedFocused: true,
        filterSuggestions: () => [],
      })
      expect(model.suggestions).toEqual([])
      expect(model.selectionIndex).toBe(0)
      expect(model.focused).toBe(false)
      expect(model.shouldDefocus).toBe(true)
    })
  })
  describe('defocusSuggestionsIfEmpty', () => {
    it('clears focus and resets selection when empty', () => {
      const prev = { suggestedFocused: true, suggestedSelectionIndex: 5, extra: 'x' }
      const next = defocusSuggestionsIfEmpty(prev, 0)
      expect(next).toEqual({
        suggestedFocused: false,
        suggestedSelectionIndex: 0,
        extra: 'x',
      })
    })
    it('does not change state when suggestions exist', () => {
      const prev = { suggestedFocused: true, suggestedSelectionIndex: 1 }
      expect(defocusSuggestionsIfEmpty(prev, 2)).toBe(prev)
    })
    it('does not change state when suggestions are already unfocused', () => {
      const prev = { suggestedFocused: false, suggestedSelectionIndex: 1 }
      expect(defocusSuggestionsIfEmpty(prev, 0)).toBe(prev)
    })
  })
  describe('getAutoAddAbsolutePathCandidate', () => {
    it('returns an absolute path candidate when it parses and is allowed', () => {
      expect(getAutoAddAbsolutePathCandidate('/Users/alice/file.txt', () => true)).toBe(
        '/Users/alice/file.txt',
      )
    })
    it('returns null when the candidate fails validation', () => {
      expect(getAutoAddAbsolutePathCandidate('/Users/alice/file.txt', () => false)).toBeNull()
    })
    it('returns null when the draft is not an absolute path', () => {
      expect(getAutoAddAbsolutePathCandidate('src/index.ts', () => true)).toBeNull()
    })
    it('strips quotes from paths and returns the normalized candidate', () => {
      expect(getAutoAddAbsolutePathCandidate('"/Users/alice/My File.md"', () => true)).toBe(
        '/Users/alice/My File.md',
      )
    })
  })
})
</file>
<file path="src/__tests__/tui/generation-pipeline-reducer.test.ts">
import {
  INITIAL_GENERATION_PIPELINE_STATE,
  generationPipelineReducer,
  type ContextOverflowDetails,
  type ResumeLoadedDetails,
} from '../../tui/generation-pipeline-reducer'
describe('generationPipelineReducer', () => {
  it('stores latest resume.loaded details', () => {
    const details = {
      event: 'resume.loaded',
      source: 'history',
      reusedContextPaths: [{ path: 'src/a.ts', source: 'file' }],
      missingContextPaths: [{ path: 'https://example.com', source: 'url' }],
    } satisfies ResumeLoadedDetails
    const next = generationPipelineReducer(INITIAL_GENERATION_PIPELINE_STATE, {
      type: 'set-resume-loaded',
      details,
    })
    expect(next.latestResumeLoaded).toEqual(details)
    expect(next.latestContextOverflow).toBeNull()
  })
  it('stores latest context.overflow details', () => {
    const before = {
      files: [{ path: 'src/a.ts', tokens: 120 }],
      intentTokens: 10,
      fileTokens: 120,
      systemTokens: 5,
      totalTokens: 135,
    }
    const after = {
      files: [],
      intentTokens: 10,
      fileTokens: 0,
      systemTokens: 5,
      totalTokens: 15,
    }
    const details = {
      event: 'context.overflow',
      strategy: 'drop-largest',
      before,
      after,
      droppedPaths: [{ path: 'src/a.ts', source: 'file' }],
    } satisfies ContextOverflowDetails
    const next = generationPipelineReducer(INITIAL_GENERATION_PIPELINE_STATE, {
      type: 'set-context-overflow',
      details,
    })
    expect(next.latestContextOverflow).toEqual(details)
    expect(next.latestResumeLoaded).toBeNull()
  })
  it('clears overflow/resume details on generation-start', () => {
    const resumeDetails = {
      event: 'resume.loaded',
      source: 'file',
      reusedContextPaths: [{ path: 'src/a.ts', source: 'file' }],
      missingContextPaths: [],
    } satisfies ResumeLoadedDetails
    const overflowDetails = {
      event: 'context.overflow',
      strategy: 'drop-oldest',
      before: {
        files: [{ path: 'src/a.ts', tokens: 120 }],
        intentTokens: 10,
        fileTokens: 120,
        systemTokens: 5,
        totalTokens: 135,
      },
      after: {
        files: [],
        intentTokens: 10,
        fileTokens: 0,
        systemTokens: 5,
        totalTokens: 15,
      },
      droppedPaths: [{ path: 'src/a.ts', source: 'file' }],
    } satisfies ContextOverflowDetails
    const withResume = generationPipelineReducer(INITIAL_GENERATION_PIPELINE_STATE, {
      type: 'set-resume-loaded',
      details: resumeDetails,
    })
    const withOverflow = generationPipelineReducer(withResume, {
      type: 'set-context-overflow',
      details: overflowDetails,
    })
    const next = generationPipelineReducer(withOverflow, {
      type: 'generation-start',
      statusMessage: 'Preparing generationâŚ',
    })
    expect(next.isGenerating).toBe(true)
    expect(next.latestTelemetry).toBeNull()
    expect(next.latestResumeLoaded).toBeNull()
    expect(next.latestContextOverflow).toBeNull()
  })
})
</file>
<file path="src/__tests__/tui/help-command.test.ts">
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import type { MutableRefObject } from 'react'
import { useIntentSubmitHandler } from '../../tui/screens/command/hooks/useIntentSubmitHandler'
const dom = new JSDOM('<!doctype html><html><body></body></html>')
const globalScope = globalThis as typeof globalThis & {
  window: Window & typeof globalThis
  document: Document
  navigator: Navigator
}
globalScope.window = dom.window
globalScope.document = dom.window.document
globalScope.navigator = dom.window.navigator
describe('/help command', () => {
  it('opens the help overlay instead of running a popup command', () => {
    const openHelp = jest.fn()
    const handleCommandSelection = jest.fn()
    const lastUserIntentRef: MutableRefObject<string | null> = { current: null }
    const { result } = renderHook(() =>
      useIntentSubmitHandler({
        popupState: null,
        isAwaitingRefinement: false,
        submitRefinement: jest.fn(),
        isCommandMenuActive: true,
        selectedCommandId: 'help',
        commandMenuArgsRaw: '',
        isCommandMode: true,
        intentFilePath: '',
        isGenerating: false,
        expandInputForSubmit: (value) => value,
        setInputValue: jest.fn(),
        pushHistory: jest.fn(),
        addCommandHistoryEntry: jest.fn(),
        runGeneration: jest.fn(async () => {}),
        handleCommandSelection,
        handleNewCommand: jest.fn(),
        handleReuseCommand: jest.fn(),
        lastUserIntentRef,
        openHelp,
      }),
    )
    act(() => {
      result.current('/help')
    })
    expect(openHelp).toHaveBeenCalledTimes(1)
    expect(handleCommandSelection).not.toHaveBeenCalled()
  })
})
</file>
<file path="src/__tests__/tui/list-popup-model.test.ts">
import { buildListPopupModel } from '../../tui/components/popups/list-popup-model'
const makeItems = (count: number, prefix = 'item'): string[] =>
  Array.from({ length: count }, (_, index) => `${prefix}${index}`)
const getBlockTypes = (model: ReturnType<typeof buildListPopupModel>): string[] =>
  model.blocks.map((block) => block.type)
describe('buildListPopupModel', () => {
  it('builds a free-height selected section when suggestions are absent', () => {
    const model = buildListPopupModel({
      items: makeItems(10),
      selectedIndex: 5,
      emptyLabel: '(none)',
      instructions: 'Use arrows',
      layout: 'input-first',
      popupHeight: 16,
    })
    expect(model.hasSuggestions).toBe(false)
    expect(model.input).toEqual({ variant: 'titled', title: 'Add new', focus: true })
    expect(model.selectedSection.fixedRowCount).toBeUndefined()
    expect(model.selectedSection.rows.map((row) => row.label)).toEqual([
      'âŚ earlier entries âŚ',
      '4. item3',
      '5. item4',
      '6. item5',
      '7. item6',
      '8. item7',
      '9. item8',
      'âŚ later entries âŚ',
    ])
    const selectedRow = model.selectedSection.rows.find((row) => row.selection !== 'none')
    expect(selectedRow).toBeDefined()
    expect(selectedRow?.label).toBe('6. item5')
    expect(selectedRow?.selection).toBe('focused')
    expect(getBlockTypes(model)).toEqual([
      'spacer',
      'input',
      'spacer',
      'section',
      'spacer',
      'instructions',
    ])
  })
  it('builds fixed-height selected/suggestion sections when suggestions are present', () => {
    const model = buildListPopupModel({
      items: makeItems(10),
      selectedIndex: 5,
      emptyLabel: '(none)',
      instructions: 'Use arrows',
      layout: 'input-first',
      popupHeight: 16,
      suggestedItems: makeItems(10, 's'),
      suggestedSelectionIndex: 5,
      suggestedFocused: true,
    })
    expect(model.hasSuggestions).toBe(true)
    expect(model.input).toEqual({ variant: 'inline', label: 'Add:', focus: false })
    expect(model.selectedSection.fixedRowCount).toBe(3)
    expect(model.selectedSection.rows.map((row) => row.label)).toEqual([
      'âŚ earlier entries âŚ',
      '6. item5',
      'âŚ later entries âŚ',
    ])
    expect(model.suggestionsSection?.fixedRowCount).toBe(4)
    expect(model.suggestionsSection?.rows.map((row) => row.label)).toEqual([
      'âŚ earlier suggestions âŚ',
      's4',
      's5',
      'âŚ later suggestions âŚ',
    ])
    const suggestedRow = model.suggestionsSection?.rows.find((row) => row.selection !== 'none')
    expect(suggestedRow?.label).toBe('s5')
    expect(suggestedRow?.selection).toBe('focused')
    expect(getBlockTypes(model)).toEqual(['input', 'section', 'section', 'instructions'])
  })
  it('clamps the suggested selection index', () => {
    const model = buildListPopupModel({
      items: [],
      selectedIndex: 0,
      emptyLabel: '(none)',
      instructions: 'Use arrows',
      layout: 'input-first',
      popupHeight: 16,
      suggestedItems: ['a', 'b', 'c'],
      suggestedSelectionIndex: 99,
    })
    expect(model.safeSuggestedSelection).toBe(2)
    const selectedSuggestion = model.suggestionsSection?.rows.find(
      (row) => row.selection !== 'none',
    )
    expect(selectedSuggestion?.label).toBe('c')
  })
  it('reflects focus switching between suggested and selected lists', () => {
    const model = buildListPopupModel({
      items: makeItems(10),
      selectedIndex: 5,
      emptyLabel: '(none)',
      instructions: 'Use arrows',
      layout: 'input-first',
      popupHeight: 16,
      suggestedItems: makeItems(10, 's'),
      suggestedSelectionIndex: 5,
      suggestedFocused: true,
      selectedFocused: false,
    })
    expect(model.input).toEqual({ variant: 'inline', label: 'Add:', focus: false })
    const selectedRow = model.selectedSection.rows.find((row) => row.selection !== 'none')
    expect(selectedRow?.label).toBe('6. item5')
    expect(selectedRow?.selection).toBe('unfocused')
    const suggestedRow = model.suggestionsSection?.rows.find((row) => row.selection !== 'none')
    expect(suggestedRow?.label).toBe('s5')
    expect(suggestedRow?.selection).toBe('focused')
  })
})
</file>
<file path="src/__tests__/tui/markdown-highlight.test.ts">
import {
  resolveMarkdownSlotColor,
  tokenizeMarkdownLines,
} from '../../tui/markdown/markdown-highlight'
import type { ResolvedTheme } from '../../tui/theme/theme-types'
const makeBaseTheme = (): ResolvedTheme => ({
  background: '#000000',
  text: '#ffffff',
  mutedText: '#888888',
  border: '#444444',
  accent: '#00ffff',
  accentText: '#000000',
  warning: '#ffff00',
  error: '#ff0000',
  success: '#00ff00',
  panelBackground: '#111111',
  popupBackground: '#111111',
  selectionBackground: '#333333',
  selectionText: '#ffffff',
  chipBackground: '#222222',
  chipText: '#ffffff',
  chipMutedText: '#aaaaaa',
})
describe('markdown-highlight', () => {
  test('tokenizes common markdown constructs into theme slots', () => {
    const lines = [
      '# Heading',
      '> Blockquote with `code`',
      '- Bullet item',
      '1. Enumerated item',
      '---',
      '[Link text](https://example.com)',
      '`inline`',
      '**strong** and *emph*',
      '![alt text](image.png)',
      '```ts',
      'const x = 1',
      '```',
    ]
    const tokenized = tokenizeMarkdownLines(lines)
    expect(tokenized[0]?.[0]?.slot).toBe('markdownHeading')
    expect(tokenized[1]?.[0]?.slot).toBe('markdownBlockQuote')
    expect(tokenized[2]?.some((span) => span.slot === 'markdownListItem')).toBe(true)
    expect(tokenized[3]?.some((span) => span.slot === 'markdownListEnumeration')).toBe(true)
    expect(tokenized[4]?.[0]?.slot).toBe('markdownHorizontalRule')
    const linkLine = tokenized[5] ?? []
    expect(linkLine.some((span) => span.slot === 'markdownLinkText' && span.underline)).toBe(true)
    expect(linkLine.some((span) => span.slot === 'markdownLink')).toBe(true)
    expect(tokenized[6]?.[0]?.slot).toBe('markdownCode')
    const strongLine = tokenized[7] ?? []
    expect(strongLine.some((span) => span.slot === 'markdownStrong' && span.bold)).toBe(true)
    expect(strongLine.some((span) => span.slot === 'markdownEmph' && span.italic)).toBe(true)
    const imageLine = tokenized[8] ?? []
    expect(imageLine.some((span) => span.slot === 'markdownImageText')).toBe(true)
    expect(imageLine.some((span) => span.slot === 'markdownImage')).toBe(true)
    expect(tokenized[9]?.[0]?.slot).toBe('markdownCodeBlock')
    expect(tokenized[10]?.[0]?.slot).toBe('markdownCodeBlock')
    expect(tokenized[11]?.[0]?.slot).toBe('markdownCodeBlock')
  })
  test('resolves markdown slot colors with fallbacks', () => {
    const base = makeBaseTheme()
    expect(resolveMarkdownSlotColor(base, 'markdownText')).toBe(base.text)
    expect(resolveMarkdownSlotColor(base, 'markdownHeading')).toBe(base.accent)
    expect(resolveMarkdownSlotColor(base, 'markdownHorizontalRule')).toBe(base.border)
    const overridden: ResolvedTheme = {
      ...base,
      markdownHeading: '#123456',
      markdownLink: undefined,
    }
    expect(resolveMarkdownSlotColor(overridden, 'markdownHeading')).toBe('#123456')
    expect(resolveMarkdownSlotColor(overridden, 'markdownLink')).toBeUndefined()
  })
})
</file>
<file path="src/__tests__/tui/notifier-dedupe.test.ts">
import React from 'react'
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import { ToastProvider, useNotifier } from '../../tui/notifier'
const dom = new JSDOM('<!doctype html><html><body></body></html>')
const globalEnv = globalThis as typeof globalThis & {
  window: Window & typeof globalThis
  document: Document
  navigator: Navigator
}
globalEnv.window = dom.window as typeof globalEnv.window
globalEnv.document = dom.window.document as Document
globalEnv.navigator = dom.window.navigator
const createWrapper = (options: Omit<React.ComponentProps<typeof ToastProvider>, 'children'>) => {
  const Wrapper: React.FC<{ children: React.ReactNode }> = ({ children }) =>
    React.createElement(ToastProvider, { ...options, children })
  return Wrapper
}
describe('toast provider notifier dedupe', () => {
  beforeEach(() => {
    jest.useFakeTimers()
  })
  afterEach(() => {
    jest.useRealTimers()
  })
  it('reuses latest active toast when message+kind match', () => {
    const wrapper = createWrapper({ exitAnimationMs: 10 })
    const { result } = renderHook(() => useNotifier({ autoDismissMs: 1_000 }), { wrapper })
    let firstId: number | null = null
    let secondId: number | null = null
    act(() => {
      firstId = result.current.showToast('Same', { kind: 'info', autoDismissMs: 100 })
    })
    act(() => {
      jest.advanceTimersByTime(60)
    })
    act(() => {
      secondId = result.current.showToast('Same', { kind: 'info', autoDismissMs: 500 })
    })
    expect(firstId).not.toBeNull()
    expect(secondId).toBe(firstId)
    expect(result.current.toasts).toHaveLength(1)
    expect(result.current.toasts[0]?.message).toBe('Same')
    // NOTE: the toast item is not updated on reuse.
    expect(result.current.toasts[0]?.autoDismissMs).toBe(100)
    act(() => {
      jest.advanceTimersByTime(100)
    })
    // The dismiss timer is reset on reuse.
    expect(result.current.toasts[0]?.isExiting).toBe(false)
    act(() => {
      jest.advanceTimersByTime(400)
    })
    expect(result.current.toasts[0]?.isExiting).toBe(true)
    act(() => {
      jest.advanceTimersByTime(10)
    })
    expect(result.current.toasts).toHaveLength(0)
  })
  it('does not reuse a toast that is already exiting', () => {
    const wrapper = createWrapper({ exitAnimationMs: 10 })
    const { result } = renderHook(() => useNotifier({ autoDismissMs: 100_000 }), { wrapper })
    let firstId: number | null = null
    let secondId: number | null = null
    act(() => {
      firstId = result.current.showToast('Same', { kind: 'info', autoDismissMs: 100_000 })
    })
    if (firstId === null) {
      throw new Error('Expected toast id')
    }
    const firstToastId = firstId
    act(() => {
      result.current.dismissToast(firstToastId)
    })
    expect(result.current.toasts[0]?.isExiting).toBe(true)
    act(() => {
      secondId = result.current.showToast('Same', { kind: 'info', autoDismissMs: 100_000 })
    })
    expect(secondId).not.toBeNull()
    expect(secondId).not.toBe(firstToastId)
    expect(result.current.toasts).toHaveLength(2)
    expect(result.current.toasts[0]?.isExiting).toBe(true)
    expect(result.current.toasts[1]?.isExiting).toBe(false)
  })
  it('only reuses the latest active toast (not an older matching one)', () => {
    const wrapper = createWrapper({ exitAnimationMs: 10 })
    const { result } = renderHook(() => useNotifier({ autoDismissMs: 100_000 }), { wrapper })
    let firstId: number | null = null
    let thirdId: number | null = null
    act(() => {
      firstId = result.current.showToast('A', { kind: 'info', autoDismissMs: 100_000 })
      result.current.showToast('B', { kind: 'info', autoDismissMs: 100_000 })
    })
    act(() => {
      thirdId = result.current.showToast('A', { kind: 'info', autoDismissMs: 100_000 })
    })
    expect(firstId).not.toBeNull()
    expect(thirdId).not.toBeNull()
    expect(thirdId).not.toBe(firstId)
    expect(result.current.toasts.map((toast) => toast.message)).toEqual(['A', 'B', 'A'])
  })
})
</file>
<file path="src/__tests__/tui/popup-scan-orchestrator.test.ts">
import type { PopupAction } from '../../tui/popup-reducer'
import { INITIAL_POPUP_MANAGER_STATE, popupReducer } from '../../tui/popup-reducer'
import { createPopupScanOrchestrator } from '../../tui/hooks/popup-manager/scan-orchestrator'
type Deferred<T> = {
  promise: Promise<T>
  resolve: (value: T) => void
  reject: (reason?: unknown) => void
}
const createDeferred = <T>(): Deferred<T> => {
  let resolve: (value: T) => void = (_value) => undefined
  let reject: (reason?: unknown) => void = (_reason) => undefined
  const promise = new Promise<T>((promiseResolve, promiseReject) => {
    resolve = promiseResolve
    reject = promiseReject
  })
  return { promise, resolve, reject }
}
describe('popup scan orchestrator', () => {
  it('keeps scanId monotonic and ignores stale results', async () => {
    const scanIdRef = { current: 0 }
    const actions: PopupAction[] = []
    const dispatch = (action: PopupAction): void => {
      actions.push(action)
    }
    const pushHistory = jest.fn()
    const orchestrator = createPopupScanOrchestrator({ scanIdRef, dispatch, pushHistory })
    const firstScan = createDeferred<string[]>()
    const secondScan = createDeferred<string[]>()
    orchestrator.runSuggestionScan({
      kind: 'file',
      open: (scanId) => ({ type: 'open-file', scanId }),
      scan: () => firstScan.promise,
    })
    orchestrator.runSuggestionScan({
      kind: 'file',
      open: (scanId) => ({ type: 'open-file', scanId }),
      scan: () => secondScan.promise,
    })
    expect(actions[0]).toEqual({ type: 'open-file', scanId: 1 })
    expect(actions[1]).toEqual({ type: 'open-file', scanId: 2 })
    firstScan.resolve(['stale'])
    await firstScan.promise
    await Promise.resolve()
    let state = INITIAL_POPUP_MANAGER_STATE
    for (const action of actions) {
      state = popupReducer(state, action)
    }
    if (state.popupState?.type !== 'file') {
      throw new Error('Expected file popup')
    }
    expect(state.popupState.suggestedItems).toEqual([])
    secondScan.resolve(['fresh'])
    await secondScan.promise
    await Promise.resolve()
    state = INITIAL_POPUP_MANAGER_STATE
    for (const action of actions) {
      state = popupReducer(state, action)
    }
    if (state.popupState?.type !== 'file') {
      throw new Error('Expected file popup')
    }
    expect(state.popupState.suggestedItems).toEqual(['fresh'])
  })
  it('does not apply file suggestions after switching popup types', async () => {
    const scanIdRef = { current: 0 }
    const actions: PopupAction[] = []
    const dispatch = (action: PopupAction): void => {
      actions.push(action)
    }
    const pushHistory = jest.fn()
    const orchestrator = createPopupScanOrchestrator({ scanIdRef, dispatch, pushHistory })
    const fileScan = createDeferred<string[]>()
    const imageScan = createDeferred<string[]>()
    orchestrator.runSuggestionScan({
      kind: 'file',
      open: (scanId) => ({ type: 'open-file', scanId }),
      scan: () => fileScan.promise,
    })
    orchestrator.runSuggestionScan({
      kind: 'image',
      open: (scanId) => ({ type: 'open-image', scanId }),
      scan: () => imageScan.promise,
    })
    fileScan.resolve(['file.txt'])
    await fileScan.promise
    await Promise.resolve()
    let state = INITIAL_POPUP_MANAGER_STATE
    for (const action of actions) {
      state = popupReducer(state, action)
    }
    expect(state.popupState?.type).toBe('image')
    imageScan.resolve(['img.png'])
    await imageScan.promise
    await Promise.resolve()
    state = INITIAL_POPUP_MANAGER_STATE
    for (const action of actions) {
      state = popupReducer(state, action)
    }
    if (state.popupState?.type !== 'image') {
      throw new Error('Expected image popup')
    }
    expect(state.popupState.suggestedItems).toEqual(['img.png'])
  })
})
</file>
<file path="src/__tests__/tui/popup-shortcuts.test.ts">
import type { PopupState } from '../../tui/types'
import {
  getSuggestedSelectedListPopupEffect,
  type SuggestedSelectedListPopupState,
} from '../../tui/screens/command/hooks/popup-shortcuts/suggested-selected-list'
import {
  getSuggestedInputPopupEffect,
  type SuggestedInputPopupState,
} from '../../tui/screens/command/hooks/popup-shortcuts/suggested-input'
describe('popup shortcut reducers', () => {
  describe('getSuggestedSelectedListPopupEffect', () => {
    const createFileState = (overrides: Partial<SuggestedSelectedListPopupState> = {}) =>
      ({
        type: 'file',
        draft: '',
        selectionIndex: 0,
        selectedFocused: false,
        suggestedItems: [],
        suggestedSelectionIndex: 0,
        suggestedFocused: false,
        ...overrides,
      }) satisfies PopupState
    it('moves focus from suggestions to selected list on Up at top', () => {
      const state = createFileState({ suggestedFocused: true, suggestedSelectionIndex: 0 })
      const effect = getSuggestedSelectedListPopupEffect({
        popupType: 'file',
        popupState: state,
        itemsLength: 2,
        suggestions: ['a', 'b'],
        key: { upArrow: true },
        isBackspace: false,
      })
      expect(effect.type).toBe('set')
      if (effect.type !== 'set') {
        throw new Error('Expected set effect')
      }
      const next = effect.updater(state as PopupState)
      expect(next).toMatchObject({
        type: 'file',
        suggestedFocused: false,
        selectedFocused: true,
      })
    })
    it('moves focus from selected list to suggestions at bottom', () => {
      const state = createFileState({ selectedFocused: true, selectionIndex: 1 })
      const effect = getSuggestedSelectedListPopupEffect({
        popupType: 'file',
        popupState: state,
        itemsLength: 2,
        suggestions: ['a', 'b'],
        key: { downArrow: true },
        isBackspace: false,
      })
      expect(effect.type).toBe('set')
      if (effect.type !== 'set') {
        throw new Error('Expected set effect')
      }
      const next = effect.updater(state as PopupState)
      expect(next).toMatchObject({
        type: 'file',
        suggestedFocused: true,
        selectedFocused: false,
        suggestedSelectionIndex: 0,
      })
    })
    it('selects the current suggestion on Enter', () => {
      const state = createFileState({ suggestedFocused: true, suggestedSelectionIndex: 1 })
      const effect = getSuggestedSelectedListPopupEffect({
        popupType: 'file',
        popupState: state,
        itemsLength: 0,
        suggestions: ['a', 'b'],
        key: { return: true },
        isBackspace: false,
      })
      expect(effect).toEqual({ type: 'selectSuggestion', value: 'b' })
    })
    it('allows backspace-remove when draft is empty', () => {
      const state = createFileState({ draft: '', selectionIndex: 3 })
      const effect = getSuggestedSelectedListPopupEffect({
        popupType: 'file',
        popupState: state,
        itemsLength: 5,
        suggestions: [],
        key: {},
        isBackspace: true,
      })
      expect(effect).toEqual({ type: 'remove', index: 3 })
    })
  })
  describe('getSuggestedInputPopupEffect', () => {
    const createIntentState = (overrides: Partial<SuggestedInputPopupState> = {}) =>
      ({
        type: 'intent',
        draft: '',
        suggestedItems: [],
        suggestedSelectionIndex: 0,
        suggestedFocused: false,
        ...overrides,
      }) satisfies PopupState
    it('does not focus suggestions on Shift+Tab', () => {
      const state = createIntentState()
      const effect = getSuggestedInputPopupEffect({
        popupType: 'intent',
        popupState: state,
        suggestions: ['a'],
        key: { tab: true, shift: true },
      })
      expect(effect.type).toBe('none')
    })
    it('focuses suggestions on Down', () => {
      const state = createIntentState({ suggestedFocused: false })
      const effect = getSuggestedInputPopupEffect({
        popupType: 'intent',
        popupState: state,
        suggestions: ['a', 'b'],
        key: { downArrow: true },
      })
      expect(effect.type).toBe('set')
      if (effect.type !== 'set') {
        throw new Error('Expected set effect')
      }
      const next = effect.updater(state as PopupState)
      expect(next).toMatchObject({
        type: 'intent',
        suggestedFocused: true,
        suggestedSelectionIndex: 0,
      })
    })
    it('selects suggestion on Enter when suggestions focused', () => {
      const state = createIntentState({ suggestedFocused: true, suggestedSelectionIndex: 0 })
      const effect = getSuggestedInputPopupEffect({
        popupType: 'intent',
        popupState: state,
        suggestions: ['a', 'b'],
        key: { return: true },
      })
      expect(effect).toEqual({ type: 'selectSuggestion', value: 'a' })
    })
  })
})
</file>
<file path="src/__tests__/tui/theme-adapter.test.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import { adaptOpencodeThemeJson } from '../../tui/theme/theme-adapter'
import { validateThemeJson } from '../../tui/theme/theme-validate'
const FIXTURES_ROOT = path.join(__dirname, '..', '__fixtures__', 'themes')
const readFixtureJson = async (relativePath: string): Promise<unknown> => {
  const filePath = path.join(FIXTURES_ROOT, relativePath)
  const raw = await fs.readFile(filePath, 'utf8')
  return JSON.parse(raw) as unknown
}
describe('theme adapter', () => {
  test('does not adapt prompt-maker schema themes', async () => {
    const parsed = await readFixtureJson(path.join('global', 'ocean.json'))
    expect(adaptOpencodeThemeJson(parsed)).toBeNull()
  })
  test('adapts opencode-like themes into prompt-maker schema', async () => {
    const parsed = await readFixtureJson(path.join('adapt', 'opencode.json'))
    const adapted = adaptOpencodeThemeJson(parsed)
    expect(adapted).not.toBeNull()
    const validated = validateThemeJson(adapted)
    expect(validated.ok).toBe(true)
    if (!validated.ok) {
      throw new Error('Expected adapted theme to validate')
    }
    expect(validated.theme.defs?.surface).toBe('#0b2a32')
    expect(validated.theme.theme.textMuted).toBe('#94d2bd')
    expect(validated.theme.theme.backgroundPanel).toBe('surface')
    expect(validated.theme.theme.backgroundElement).toBe('#005f73')
    expect(validated.theme.theme.primary).toBe('#0a9396')
    expect(validated.theme.theme.mutedText).toBe('textMuted')
    expect(validated.theme.theme.panelBackground).toBe('backgroundPanel')
    expect(validated.theme.theme.popupBackground).toBe('panelBackground')
    expect(validated.theme.theme.accent).toBe('primary')
    expect(validated.theme.theme.accentText).toBe('background')
    expect(validated.theme.theme.selectionBackground).toBe('backgroundElement')
    expect(validated.theme.theme.selectionText).toBe('text')
    expect(validated.theme.theme.chipBackground).toBe('backgroundElement')
    expect(validated.theme.theme.chipText).toBe('text')
    expect(validated.theme.theme.chipMutedText).toBe('textMuted')
  })
})
</file>
<file path="src/__tests__/tui/theme-resolver.test.ts">
import { ansiToHex, resolveColor, resolveTheme } from '../../tui/theme/theme-resolver'
import type { ResolveColorContext } from '../../tui/theme/theme-resolver'
import type {
  MarkdownThemeSlot,
  RequiredThemeSlot,
  ThemeAppearanceMode,
  ThemeColorValue,
  ThemeJson,
} from '../../tui/theme/theme-types'
type ThemeKey = RequiredThemeSlot | MarkdownThemeSlot
const makeTheme = (
  overrides: Partial<Record<ThemeKey, ThemeColorValue>> = {},
  defs: Record<string, ThemeColorValue> = {},
): ThemeJson => {
  const base: Record<RequiredThemeSlot, ThemeColorValue> = {
    background: '#000000',
    text: '#ffffff',
    mutedText: '#888888',
    border: '#444444',
    accent: '#00ffff',
    accentText: '#000000',
    warning: '#ffff00',
    error: '#ff0000',
    success: '#00ff00',
    panelBackground: '#111111',
    popupBackground: '#111111',
    selectionBackground: '#333333',
    selectionText: '#ffffff',
    chipBackground: '#222222',
    chipText: '#ffffff',
    chipMutedText: '#aaaaaa',
  }
  return {
    defs,
    theme: {
      ...base,
      ...overrides,
    },
  }
}
describe('theme resolver', () => {
  test('hex pass-through', () => {
    const ctx: ResolveColorContext = { mode: 'dark', defs: {}, theme: {} }
    expect(resolveColor('#112233', ctx)).toBe('#112233')
  })
  test('variant selects by mode', () => {
    const ctxDark: ResolveColorContext = { mode: 'dark', defs: {}, theme: {} }
    const ctxLight: ResolveColorContext = { mode: 'light', defs: {}, theme: {} }
    expect(resolveColor({ dark: '#000000', light: '#ffffff' }, ctxDark)).toBe('#000000')
    expect(resolveColor({ dark: '#000000', light: '#ffffff' }, ctxLight)).toBe('#ffffff')
  })
  test('defs references resolve before theme slots', () => {
    const themeJson = makeTheme({ accent: 'accentDef' }, { accentDef: '#123456' })
    expect(resolveTheme(themeJson, 'dark').accent).toBe('#123456')
  })
  test('theme-slot references resolve recursively', () => {
    const themeJson = makeTheme({ text: 'background', background: '#101010' })
    expect(resolveTheme(themeJson, 'dark').text).toBe('#101010')
  })
  test('reference cycles throw with cycle path', () => {
    const themeJson = makeTheme({ background: 'text', text: 'background' })
    expect(() => resolveTheme(themeJson, 'dark')).toThrow(/Theme reference cycle detected:/)
    expect(() => resolveTheme(themeJson, 'dark')).toThrow(/theme\.background/)
    expect(() => resolveTheme(themeJson, 'dark')).toThrow(/theme\.text/)
  })
  test('missing required slots throw descriptive error', () => {
    const themeJson: ThemeJson = {
      theme: {
        background: '#000000',
      },
    }
    expect(() => resolveTheme(themeJson, 'dark')).toThrow(/missing required slots/i)
    expect(() => resolveTheme(themeJson, 'dark')).toThrow(/text/)
  })
  test('ANSI mapping converts 0..255 to hex', () => {
    expect(ansiToHex(0)).toBe('#000000')
    expect(ansiToHex(15)).toBe('#ffffff')
    expect(ansiToHex(196)).toBe('#ff0000')
    expect(ansiToHex(232)).toBe('#080808')
  })
  test('none/transparent normalize to undefined', () => {
    const ctx: ResolveColorContext = { mode: 'dark', defs: {}, theme: {} }
    expect(resolveColor('none', ctx)).toBeUndefined()
    expect(resolveColor('transparent', ctx)).toBeUndefined()
  })
  test('8-digit hex ignores alpha (00 => transparent)', () => {
    const ctx: ResolveColorContext = { mode: 'dark', defs: {}, theme: {} }
    expect(resolveColor('#11223300', ctx)).toBeUndefined()
    expect(resolveColor('#112233ff', ctx)).toBe('#112233')
  })
  test('markdown slots resolve when present', () => {
    const themeJson = makeTheme(
      {
        markdownHeading: 'accentDef',
        markdownCode: 196,
        markdownLink: 'transparent',
        markdownLinkText: { dark: 'text', light: 'background' },
        markdownCodeBlock: { dark: '#11223300', light: '#112233ff' },
      },
      { accentDef: '#123456' },
    )
    const resolvedDark = resolveTheme(themeJson, 'dark')
    expect(resolvedDark.markdownHeading).toBe('#123456')
    expect(resolvedDark.markdownCode).toBe('#ff0000')
    expect(resolvedDark.markdownLink).toBeUndefined()
    expect(resolvedDark.markdownLinkText).toBe('#ffffff')
    expect(resolvedDark.markdownCodeBlock).toBeUndefined()
    const resolvedLight = resolveTheme(themeJson, 'light')
    expect(resolvedLight.markdownLinkText).toBe('#000000')
    expect(resolvedLight.markdownCodeBlock).toBe('#112233')
  })
  test('mode type excludes system for resolution', () => {
    const mode: ThemeAppearanceMode = 'dark'
    const themeJson = makeTheme()
    expect(resolveTheme(themeJson, mode).background).toBe('#000000')
  })
})
</file>
<file path="src/__tests__/tui/theme-validate.test.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import {
  REQUIRED_THEME_SLOTS,
  type ThemeColorValue,
  type ThemeJson,
} from '../../tui/theme/theme-types'
import { validateThemeJson } from '../../tui/theme/theme-validate'
const FIXTURES_ROOT = path.join(__dirname, '..', '__fixtures__', 'themes')
const readFixtureJson = async (relativePath: string): Promise<unknown> => {
  const filePath = path.join(FIXTURES_ROOT, relativePath)
  const raw = await fs.readFile(filePath, 'utf8')
  return JSON.parse(raw) as unknown
}
const makeValidThemeJson = (): ThemeJson => {
  const theme: Record<string, ThemeColorValue> = {}
  for (const slot of REQUIRED_THEME_SLOTS) {
    theme[slot] = '#000000'
  }
  return { theme }
}
describe('theme validate', () => {
  test('rejects themes missing required slots (fixture)', async () => {
    const parsed = await readFixtureJson(
      path.join('project', '.prompt-maker-cli', 'themes', 'broken-theme.json'),
    )
    const validated = validateThemeJson(parsed)
    expect(validated).toEqual({
      ok: false,
      message:
        'Theme JSON missing required slots: text, mutedText, border, accent, accentText, warning, error, success, panelBackground, popupBackground, selectionBackground, selectionText, chipBackground, chipText, chipMutedText',
    })
  })
  test('rejects invalid defs color values', () => {
    const invalidVariant: unknown = { dark: '#000000', light: null }
    const validated = validateThemeJson({
      ...makeValidThemeJson(),
      defs: {
        bad: invalidVariant,
      },
    })
    expect(validated).toEqual({
      ok: false,
      message: 'Invalid defs color value for key: bad',
    })
  })
  test('rejects invalid required slot values', () => {
    const invalidVariant = { dark: '#000000', light: null } as unknown as ThemeColorValue
    const themeJson = makeValidThemeJson()
    const validated = validateThemeJson({
      ...themeJson,
      theme: {
        ...themeJson.theme,
        border: invalidVariant,
      },
    })
    expect(validated).toEqual({
      ok: false,
      message: 'Invalid theme color value for slot: border',
    })
  })
})
</file>
<file path="src/__tests__/tui/url-args.test.ts">
import { parseUrlArgs, validateHttpUrlCandidate } from '../../tui/screens/command/utils/url-args'
describe('url args', () => {
  test('parseUrlArgs splits on whitespace and commas', () => {
    expect(parseUrlArgs(' https://a.com https://b.com ')).toEqual([
      'https://a.com',
      'https://b.com',
    ])
    expect(parseUrlArgs('https://a.com,https://b.com')).toEqual(['https://a.com', 'https://b.com'])
    expect(parseUrlArgs('https://a.com, https://b.com\nhttps://c.com')).toEqual([
      'https://a.com',
      'https://b.com',
      'https://c.com',
    ])
  })
  test('validateHttpUrlCandidate validates HTTP(S)', () => {
    expect(validateHttpUrlCandidate('https://example.com').ok).toBe(true)
    expect(validateHttpUrlCandidate('http://example.com').ok).toBe(true)
    const empty = validateHttpUrlCandidate('   ')
    expect(empty.ok).toBe(false)
    const invalid = validateHttpUrlCandidate('notaurl')
    expect(invalid.ok).toBe(false)
    const protocol = validateHttpUrlCandidate('ftp://example.com')
    expect(protocol.ok).toBe(false)
  })
})
</file>
<file path="src/__tests__/tui/useDroppedFileDetection.test.ts">
import fs from 'node:fs'
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import { parseAbsolutePathFromInput } from '../../tui/drag-drop-path'
import { useDroppedFileDetection } from '../../tui/screens/command/hooks/useDroppedFileDetection'
const dom = new JSDOM('<!doctype html><html><body></body></html>')
type GlobalDom = { window: Window; document: Document }
beforeAll(() => {
  const target = globalThis as unknown as GlobalDom
  target.window = dom.window as unknown as Window
  target.document = dom.window.document
})
afterAll(() => {
  const target = globalThis as unknown as Partial<GlobalDom>
  delete target.window
  delete target.document
})
describe('useDroppedFileDetection', () => {
  describe('parseAbsolutePathFromInput (candidate parsing)', () => {
    it('parses quoted absolute paths with spaces', () => {
      expect(parseAbsolutePathFromInput('"/Users/alice/My File.md"')).toBe(
        '/Users/alice/My File.md',
      )
    })
    it('returns null when extra tokens exist', () => {
      expect(parseAbsolutePathFromInput('/file arg')).toBeNull()
    })
  })
  const createStats = (isFile: boolean): fs.Stats => {
    return { isFile: () => isFile } as unknown as fs.Stats
  }
  it('does not call stat when input has no absolute path', () => {
    const stat = jest.fn<Promise<fs.Stats>, [string]>(async () => createStats(true))
    const { result } = renderHook(() => useDroppedFileDetection('hello', { stat }))
    expect(result.current.droppedFilePath).toBeNull()
    expect(stat).not.toHaveBeenCalled()
  })
  it('resolves droppedFilePath once stat confirms file', async () => {
    const stat = jest.fn<Promise<fs.Stats>, [string]>(async () => createStats(true))
    const { result } = renderHook(() => useDroppedFileDetection('/tmp/file.txt', { stat }))
    expect(result.current.droppedFilePath).toBeNull()
    await act(async () => {
      await Promise.resolve()
    })
    expect(stat).toHaveBeenCalledTimes(1)
    expect(stat).toHaveBeenCalledWith('/tmp/file.txt')
    expect(result.current.droppedFilePath).toBe('/tmp/file.txt')
    expect(result.current.existsSync('/tmp/file.txt')).toBe(true)
    expect(result.current.isFilePath('/tmp/file.txt')).toBe(true)
  })
  it('treats existing non-files as not droppedFilePath', async () => {
    const stat = jest.fn<Promise<fs.Stats>, [string]>(async () => createStats(false))
    const { result } = renderHook(() => useDroppedFileDetection('/tmp', { stat }))
    await act(async () => {
      await Promise.resolve()
    })
    expect(result.current.droppedFilePath).toBeNull()
    expect(result.current.existsSync('/tmp')).toBe(true)
    expect(result.current.isFilePath('/tmp')).toBe(false)
  })
  it('caches stat results by path', async () => {
    const stat = jest.fn<Promise<fs.Stats>, [string]>(async () => createStats(true))
    const { result, rerender } = renderHook(
      ({ value }) => useDroppedFileDetection(value, { stat }),
      { initialProps: { value: '/tmp/file.txt' } },
    )
    await act(async () => {
      await Promise.resolve()
    })
    expect(result.current.droppedFilePath).toBe('/tmp/file.txt')
    expect(stat).toHaveBeenCalledTimes(1)
    rerender({ value: '/tmp/file.txt' })
    await act(async () => {
      await Promise.resolve()
    })
    expect(result.current.droppedFilePath).toBe('/tmp/file.txt')
    expect(stat).toHaveBeenCalledTimes(1)
  })
  it('returns exists=false for ENOENT paths', async () => {
    const error = new Error('missing') as Error & { code: string }
    error.code = 'ENOENT'
    const stat = jest.fn<Promise<fs.Stats>, [string]>(async () => {
      throw error
    })
    const { result } = renderHook(() => useDroppedFileDetection('/tmp/missing.txt', { stat }))
    await act(async () => {
      await Promise.resolve()
    })
    expect(result.current.droppedFilePath).toBeNull()
    expect(result.current.existsSync('/tmp/missing.txt')).toBe(false)
    expect(result.current.isFilePath('/tmp/missing.txt')).toBe(false)
  })
})
</file>
<file path="src/__tests__/app-container-keymap.test.ts">
import type { Key } from 'ink'
import { resolveAppContainerKeyAction } from '../tui/app-container-keymap'
const createKey = (overrides: Partial<Key> = {}): Key => overrides as Key
describe('resolveAppContainerKeyAction', () => {
  it('does not toggle help open on ?', () => {
    const action = resolveAppContainerKeyAction({
      input: '?',
      key: createKey({}),
      view: 'generate',
      isPopupOpen: false,
      isHelpOpen: false,
    })
    expect(action).toEqual({ type: 'none' })
  })
  it('closes help on Esc', () => {
    const action = resolveAppContainerKeyAction({
      input: '',
      key: createKey({ escape: true }),
      view: 'generate',
      isPopupOpen: false,
      isHelpOpen: true,
    })
    expect(action).toEqual({ type: 'toggle-help', nextIsHelpOpen: false })
  })
  it('does not close help on ?', () => {
    const action = resolveAppContainerKeyAction({
      input: '?',
      key: createKey({}),
      view: 'generate',
      isPopupOpen: false,
      isHelpOpen: true,
    })
    expect(action).toEqual({ type: 'none' })
  })
  it('swallows navigation keys while help is open', () => {
    const action = resolveAppContainerKeyAction({
      input: 'g',
      key: createKey({ ctrl: true }),
      view: 'tests',
      isPopupOpen: false,
      isHelpOpen: true,
    })
    expect(action).toEqual({ type: 'none' })
  })
  it('does not exit on Esc (generate view, no popup)', () => {
    const action = resolveAppContainerKeyAction({
      input: '',
      key: createKey({ escape: true }),
      view: 'generate',
      isPopupOpen: false,
      isHelpOpen: false,
    })
    expect(action).toEqual({ type: 'none' })
  })
  it('does not exit on Esc (generate view, popup open)', () => {
    const action = resolveAppContainerKeyAction({
      input: '',
      key: createKey({ escape: true }),
      view: 'generate',
      isPopupOpen: true,
      isHelpOpen: false,
    })
    expect(action).toEqual({ type: 'none' })
  })
  it('does not exit on Esc (tests view)', () => {
    const action = resolveAppContainerKeyAction({
      input: '',
      key: createKey({ escape: true }),
      view: 'tests',
      isPopupOpen: false,
      isHelpOpen: false,
    })
    expect(action).toEqual({ type: 'none' })
  })
  it('swallows Ctrl+G when a generate popup is open', () => {
    const action = resolveAppContainerKeyAction({
      input: 'g',
      key: createKey({ ctrl: true }),
      view: 'generate',
      isPopupOpen: true,
      isHelpOpen: false,
    })
    expect(action).toEqual({ type: 'none' })
  })
  it('swallows Ctrl+T when a generate popup is open', () => {
    const action = resolveAppContainerKeyAction({
      input: 't',
      key: createKey({ ctrl: true }),
      view: 'generate',
      isPopupOpen: true,
      isHelpOpen: false,
    })
    expect(action).toEqual({ type: 'none' })
  })
  it('switches to generate and opens palette on Ctrl+G from tests', () => {
    const action = resolveAppContainerKeyAction({
      input: 'g',
      key: createKey({ ctrl: true }),
      view: 'tests',
      isPopupOpen: false,
      isHelpOpen: false,
    })
    expect(action).toEqual({ type: 'switch-to-generate-and-open-command-palette' })
  })
})
</file>
<file path="src/__tests__/command-menu-keymap.test.ts">
import type { Key } from 'ink'
import { resolveCommandMenuKeyAction } from '../tui/components/core/command-menu-keymap'
const createKey = (overrides: Partial<Key> = {}): Key => overrides as Key
describe('resolveCommandMenuKeyAction', () => {
  it('wraps selection at boundaries', () => {
    expect(
      resolveCommandMenuKeyAction({
        key: createKey({ upArrow: true }),
        selectedIndex: 0,
        itemCount: 3,
      }),
    ).toEqual({ type: 'change-selection', nextIndex: 2 })
    expect(
      resolveCommandMenuKeyAction({
        key: createKey({ downArrow: true }),
        selectedIndex: 2,
        itemCount: 3,
      }),
    ).toEqual({ type: 'change-selection', nextIndex: 0 })
  })
  it('moves selection by one within the list', () => {
    expect(
      resolveCommandMenuKeyAction({
        key: createKey({ downArrow: true }),
        selectedIndex: 0,
        itemCount: 3,
      }),
    ).toEqual({ type: 'change-selection', nextIndex: 1 })
    expect(
      resolveCommandMenuKeyAction({
        key: createKey({ upArrow: true }),
        selectedIndex: 2,
        itemCount: 3,
      }),
    ).toEqual({ type: 'change-selection', nextIndex: 1 })
  })
  it('keeps selection stable for a single item', () => {
    expect(
      resolveCommandMenuKeyAction({
        key: createKey({ downArrow: true }),
        selectedIndex: 0,
        itemCount: 1,
      }),
    ).toEqual({ type: 'change-selection', nextIndex: 0 })
    expect(
      resolveCommandMenuKeyAction({
        key: createKey({ upArrow: true }),
        selectedIndex: 0,
        itemCount: 1,
      }),
    ).toEqual({ type: 'change-selection', nextIndex: 0 })
  })
  it('closes the palette on Esc', () => {
    expect(
      resolveCommandMenuKeyAction({
        key: createKey({ escape: true }),
        selectedIndex: 1,
        itemCount: 3,
      }),
    ).toEqual({ type: 'close' })
  })
  it('ignores keys when there are no items', () => {
    expect(
      resolveCommandMenuKeyAction({
        key: createKey({ downArrow: true }),
        selectedIndex: 0,
        itemCount: 0,
      }),
    ).toEqual({ type: 'none' })
  })
})
</file>
<file path="src/__tests__/compose-command.test.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
import { runComposeCommand } from '../compose-command'
describe('compose-command', () => {
  const originalExitCode = process.exitCode
  const tempDirs: string[] = []
  afterEach(async () => {
    await Promise.all(
      tempDirs.map(async (tempDir) => {
        await fs.rm(tempDir, { recursive: true, force: true })
      }),
    )
    tempDirs.splice(0, tempDirs.length)
    process.exitCode = originalExitCode
    jest.restoreAllMocks()
  })
  it('prints deterministic recipe + input composition', async () => {
    const tempDir = await fs.mkdtemp(path.join(os.tmpdir(), 'pmc-compose-'))
    tempDirs.push(tempDir)
    const recipePath = path.join(tempDir, 'recipe.yaml')
    await fs.writeFile(recipePath, 'recipe: demo\nsteps:\n  - one\n', 'utf8')
    const stdout = jest.spyOn(process.stdout, 'write').mockImplementation(() => true)
    const err = jest.spyOn(console, 'error').mockImplementation(() => undefined)
    await runComposeCommand(['--recipe', recipePath, '--input', 'hello'])
    expect(err).not.toHaveBeenCalled()
    expect(process.exitCode).toBe(originalExitCode)
    expect(stdout).toHaveBeenCalledWith('recipe: demo\nsteps:\n  - one\n---\nhello\n')
  })
  it('shows help output when requested', async () => {
    const stdout = jest.spyOn(process.stdout, 'write').mockImplementation(() => true)
    const stderr = jest.spyOn(process.stderr, 'write').mockImplementation(() => true)
    await runComposeCommand(['--help'])
    expect(process.exitCode).toBe(originalExitCode)
    expect(stdout.mock.calls.length + stderr.mock.calls.length).toBeGreaterThan(0)
  })
})
</file>
<file path="src/__tests__/config.test.ts">
import type { PromptMakerCliConfig } from '../config'
jest.mock('node:os', () => ({ homedir: jest.fn(() => '/home/tester') }))
jest.mock('node:fs/promises', () => ({ readFile: jest.fn() }))
const getFsMock = () =>
  jest.requireMock('node:fs/promises') as {
    readFile: jest.MockedFunction<(file: string, encoding: string) => Promise<string>>
  }
type ConfigModule = typeof import('../config')
const importConfigModule = async (): Promise<ConfigModule> => {
  jest.resetModules()
  return await import('../config')
}
const mockConfigJson = (config: PromptMakerCliConfig): string => JSON.stringify(config)
describe('config module', () => {
  beforeEach(() => {
    delete process.env.PROMPT_MAKER_CLI_CONFIG
    delete process.env.OPENAI_API_KEY
    delete process.env.OPENAI_BASE_URL
    delete process.env.GEMINI_API_KEY
    delete process.env.GEMINI_BASE_URL
  })
  it('loads config from explicit PROMPT_MAKER_CLI_CONFIG path', async () => {
    process.env.PROMPT_MAKER_CLI_CONFIG = '/tmp/custom.json'
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(
      mockConfigJson({ promptGenerator: { defaultModel: 'gpt-4o' } }),
    )
    const config = await loadCliConfig()
    expect(fs.readFile).toHaveBeenCalledWith('/tmp/custom.json', 'utf8')
    expect(config?.promptGenerator?.defaultModel).toBe('gpt-4o')
  })
  it('falls back through default locations while skipping missing files', async () => {
    const enoent = Object.assign(new Error('missing'), { code: 'ENOENT' })
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile
      .mockRejectedValueOnce(enoent)
      .mockResolvedValueOnce(mockConfigJson({ openaiApiKey: 'conf-key' }))
    const config = await loadCliConfig()
    expect(fs.readFile).toHaveBeenCalledTimes(2)
    expect(config?.openaiApiKey).toBe('conf-key')
  })
  it('caches the parsed config after the first load', async () => {
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(mockConfigJson({ openaiApiKey: 'cached' }))
    await loadCliConfig()
    await loadCliConfig()
    expect(fs.readFile).toHaveBeenCalledTimes(1)
  })
  it('throws a descriptive error for malformed JSON', async () => {
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce('not-json')
    await expect(loadCliConfig()).rejects.toThrow(/Failed to load config.*Unexpected token/)
  })
  it('resolveOpenAiCredentials prefers env variables', async () => {
    process.env.OPENAI_API_KEY = 'env-key'
    process.env.OPENAI_BASE_URL = 'https://api.example'
    const { resolveOpenAiCredentials } = await importConfigModule()
    const creds = await resolveOpenAiCredentials()
    expect(creds.apiKey).toBe('env-key')
    expect(creds.baseUrl).toBe('https://api.example')
  })
  it('resolveOpenAiCredentials falls back to config when env absent', async () => {
    const { resolveOpenAiCredentials } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(
      mockConfigJson({ openaiApiKey: 'file-key', openaiBaseUrl: 'https://conf' }),
    )
    const creds = await resolveOpenAiCredentials()
    expect(creds.apiKey).toBe('file-key')
    expect(creds.baseUrl).toBe('https://conf')
  })
  it('resolveOpenAiCredentials throws when no credentials available', async () => {
    const enoent = Object.assign(new Error('missing'), { code: 'ENOENT' })
    const { resolveOpenAiCredentials } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockRejectedValue(enoent)
    await expect(resolveOpenAiCredentials()).rejects.toThrow(/Missing OpenAI credentials/)
  })
  it('resolveGeminiCredentials uses env first', async () => {
    process.env.GEMINI_API_KEY = 'env-gem'
    process.env.GEMINI_BASE_URL = 'https://gem'
    const { resolveGeminiCredentials } = await importConfigModule()
    const creds = await resolveGeminiCredentials()
    expect(creds).toEqual({ apiKey: 'env-gem', baseUrl: 'https://gem' })
  })
  it('resolveGeminiCredentials uses config fallback', async () => {
    const { resolveGeminiCredentials } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(
      mockConfigJson({ geminiApiKey: 'file-gem', geminiBaseUrl: 'https://gemini.local' }),
    )
    const creds = await resolveGeminiCredentials()
    expect(creds).toEqual({ apiKey: 'file-gem', baseUrl: 'https://gemini.local' })
  })
  it('resolveGeminiCredentials throws when unresolved', async () => {
    const enoent = Object.assign(new Error('missing'), { code: 'ENOENT' })
    const { resolveGeminiCredentials } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockRejectedValue(enoent)
    await expect(resolveGeminiCredentials()).rejects.toThrow(/Missing Gemini credentials/)
  })
  it('parses promptGenerator.models entries when provided', async () => {
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(
      JSON.stringify({
        promptGenerator: {
          models: [
            {
              id: 'custom-model',
              label: 'Custom Model',
              provider: 'gemini',
              capabilities: 'multimodal',
              notes: 'Use for long context',
              default: true,
            },
          ],
        },
      }),
    )
    const config = await loadCliConfig()
    expect(config?.promptGenerator?.models).toEqual([
      {
        id: 'custom-model',
        label: 'Custom Model',
        provider: 'gemini',
        capabilities: ['multimodal'],
        notes: 'Use for long context',
        default: true,
      },
    ])
  })
  it('throws when promptGenerator.models is not an array', async () => {
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(
      JSON.stringify({ promptGenerator: { models: { id: 'bad' } } }),
    )
    await expect(loadCliConfig()).rejects.toThrow(/"promptGenerator\.models" must be an array/)
  })
  it('parses promptGenerator token budget settings when provided', async () => {
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(
      JSON.stringify({
        promptGenerator: {
          maxInputTokens: 10000,
          maxContextTokens: 5000,
          contextOverflowStrategy: 'drop-smart',
        },
      }),
    )
    const config = await loadCliConfig()
    expect(config?.promptGenerator?.maxInputTokens).toBe(10000)
    expect(config?.promptGenerator?.maxContextTokens).toBe(5000)
    expect(config?.promptGenerator?.contextOverflowStrategy).toBe('drop-smart')
  })
  it('rejects negative promptGenerator.maxInputTokens with a descriptive error', async () => {
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(JSON.stringify({ promptGenerator: { maxInputTokens: -1 } }))
    await expect(loadCliConfig()).rejects.toThrow(
      /promptGenerator\.maxInputTokens must be a positive integer/,
    )
  })
  it('rejects unknown promptGenerator.contextOverflowStrategy values', async () => {
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(
      JSON.stringify({ promptGenerator: { contextOverflowStrategy: 'drop-new' } }),
    )
    await expect(loadCliConfig()).rejects.toThrow(
      /promptGenerator\.contextOverflowStrategy must be one of: fail, drop-smart, drop-url, drop-largest, drop-oldest/,
    )
  })
})
</file>
<file path="src/__tests__/context-budget.test.ts">
import { evaluateContextBudget, type ContextEntry } from '../generate/context-budget'
import type { TokenTelemetry } from '../generate/types'
const buildTelemetryFromNumericContent = (
  intentText: string,
  entries: Array<{ path: string; content: string }>,
  metaInstructions: string,
): TokenTelemetry => {
  const toInt = (value: string): number => {
    const parsed = Number(value)
    if (!Number.isFinite(parsed)) {
      return 0
    }
    return Math.trunc(parsed)
  }
  const files = entries.map((entry) => ({
    path: entry.path,
    tokens: toInt(entry.content),
  }))
  const fileTokens = files.reduce((acc, file) => acc + file.tokens, 0)
  const intentTokens = toInt(intentText)
  const systemTokens = toInt(metaInstructions)
  return {
    files,
    intentTokens,
    fileTokens,
    systemTokens,
    totalTokens: intentTokens + systemTokens + fileTokens,
  }
}
describe('evaluateContextBudget', () => {
  it('returns entries unchanged when budgets are not set', () => {
    const entries: ContextEntry[] = [
      { path: 'a', content: '3', source: 'file' },
      { path: 'b', content: '5', source: 'smart' },
    ]
    const result = evaluateContextBudget({
      intentText: '0',
      metaInstructions: '0',
      contextEntries: entries,
      buildTelemetry: buildTelemetryFromNumericContent,
    })
    expect(result.keptEntries).toEqual(entries)
    expect(result.droppedEntries).toEqual([])
    expect(result.after).toEqual(result.before)
  })
  it('throws with strategy=fail when over budget', () => {
    const entries: ContextEntry[] = [
      { path: 'a', content: '3', source: 'file' },
      { path: 'b', content: '5', source: 'smart' },
    ]
    expect(() =>
      evaluateContextBudget({
        intentText: '0',
        metaInstructions: '0',
        contextEntries: entries,
        maxContextTokens: 4,
        buildTelemetry: buildTelemetryFromNumericContent,
      }),
    ).toThrow(/Context token budget exceeded/)
  })
  it('drops smart entries first with drop-smart', () => {
    const entries: ContextEntry[] = [
      { path: 'file.md', content: '3', source: 'file' },
      { path: 'smart.md', content: '5', source: 'smart' },
      { path: 'url.md', content: '4', source: 'url' },
    ]
    const result = evaluateContextBudget({
      intentText: '0',
      metaInstructions: '0',
      contextEntries: entries,
      maxContextTokens: 7,
      strategy: 'drop-smart',
      buildTelemetry: buildTelemetryFromNumericContent,
    })
    expect(result.droppedPaths).toEqual([{ path: 'smart.md', source: 'smart' }])
    expect(result.keptEntries.map((entry) => entry.path)).toEqual(['file.md', 'url.md'])
    expect(result.after.fileTokens).toBe(7)
  })
  it('falls back to dropping oldest remaining after exhausting smart entries', () => {
    const entries: ContextEntry[] = [
      { path: 'file.md', content: '3', source: 'file' },
      { path: 'smart.md', content: '5', source: 'smart' },
    ]
    const result = evaluateContextBudget({
      intentText: '0',
      metaInstructions: '0',
      contextEntries: entries,
      maxContextTokens: 2,
      strategy: 'drop-smart',
      buildTelemetry: buildTelemetryFromNumericContent,
    })
    expect(result.droppedPaths).toEqual([
      { path: 'smart.md', source: 'smart' },
      { path: 'file.md', source: 'file' },
    ])
    expect(result.keptEntries).toEqual([])
    expect(result.after.fileTokens).toBe(0)
  })
  it('drops url entries first with drop-url', () => {
    const entries: ContextEntry[] = [
      { path: 'a', content: '4', source: 'file' },
      { path: 'b', content: '4', source: 'url' },
      { path: 'c', content: '4', source: 'file' },
    ]
    const result = evaluateContextBudget({
      intentText: '0',
      metaInstructions: '0',
      contextEntries: entries,
      maxContextTokens: 8,
      strategy: 'drop-url',
      buildTelemetry: buildTelemetryFromNumericContent,
    })
    expect(result.droppedPaths).toEqual([{ path: 'b', source: 'url' }])
    expect(result.keptEntries.map((entry) => entry.path)).toEqual(['a', 'c'])
  })
  it('drops largest token entries first with drop-largest', () => {
    const entries: ContextEntry[] = [
      { path: 'small', content: '2', source: 'file' },
      { path: 'big', content: '10', source: 'file' },
      { path: 'mid', content: '5', source: 'file' },
    ]
    const result = evaluateContextBudget({
      intentText: '0',
      metaInstructions: '0',
      contextEntries: entries,
      maxContextTokens: 6,
      strategy: 'drop-largest',
      buildTelemetry: buildTelemetryFromNumericContent,
    })
    expect(result.droppedPaths).toEqual([
      { path: 'big', source: 'file' },
      { path: 'mid', source: 'file' },
    ])
    expect(result.keptEntries.map((entry) => entry.path)).toEqual(['small'])
    expect(result.after.fileTokens).toBe(2)
  })
  it('drops oldest entries first with drop-oldest', () => {
    const entries: ContextEntry[] = [
      { path: 'first', content: '4', source: 'file' },
      { path: 'second', content: '4', source: 'file' },
      { path: 'third', content: '4', source: 'file' },
    ]
    const result = evaluateContextBudget({
      intentText: '0',
      metaInstructions: '0',
      contextEntries: entries,
      maxContextTokens: 8,
      strategy: 'drop-oldest',
      buildTelemetry: buildTelemetryFromNumericContent,
    })
    expect(result.droppedPaths).toEqual([{ path: 'first', source: 'file' }])
    expect(result.keptEntries.map((entry) => entry.path)).toEqual(['second', 'third'])
  })
  it('enforces maxInputTokens using remaining file token allowance', () => {
    const entries: ContextEntry[] = [
      { path: 'a', content: '2', source: 'file' },
      { path: 'b', content: '2', source: 'file' },
    ]
    const result = evaluateContextBudget({
      intentText: '4',
      metaInstructions: '3',
      contextEntries: entries,
      maxInputTokens: 10,
      strategy: 'drop-oldest',
      buildTelemetry: buildTelemetryFromNumericContent,
    })
    expect(result.keptEntries.map((entry) => entry.path)).toEqual(['b'])
    expect(result.after.totalTokens).toBe(9)
  })
  it('throws when maxInputTokens is below fixed overhead', () => {
    const entries: ContextEntry[] = [{ path: 'a', content: '1', source: 'file' }]
    expect(() =>
      evaluateContextBudget({
        intentText: '7',
        metaInstructions: '6',
        contextEntries: entries,
        maxInputTokens: 10,
        strategy: 'drop-oldest',
        buildTelemetry: buildTelemetryFromNumericContent,
      }),
    ).toThrow(/Unable to satisfy token budgets/)
  })
})
</file>
<file path="src/__tests__/export-command.test.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
import yaml from 'js-yaml'
import { runExportCommand } from '../export-command'
import { GENERATE_JSON_PAYLOAD_SCHEMA_VERSION, type GenerateJsonPayload } from '../generate/types'
describe('export-command', () => {
  const originalHome = process.env.HOME
  const originalExitCode = process.exitCode
  const tempHomes: string[] = []
  afterEach(async () => {
    await Promise.all(
      tempHomes.map(async (home) => {
        await fs.rm(home, { recursive: true, force: true })
      }),
    )
    tempHomes.splice(0, tempHomes.length)
    if (originalHome === undefined) {
      delete process.env.HOME
    } else {
      process.env.HOME = originalHome
    }
    process.exitCode = originalExitCode
    jest.restoreAllMocks()
  })
  const writeHistory = async (homeDir: string, lines: string[]): Promise<string> => {
    const historyPath = path.join(homeDir, '.config', 'prompt-maker-cli', 'history.jsonl')
    await fs.mkdir(path.dirname(historyPath), { recursive: true })
    await fs.writeFile(historyPath, `${lines.join('\n')}\n`, 'utf8')
    return historyPath
  }
  const createPayload = (overrides: Partial<GenerateJsonPayload>): GenerateJsonPayload => ({
    schemaVersion: GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
    intent: 'intent',
    model: 'model',
    targetModel: 'target-model',
    prompt: 'prompt',
    refinements: [],
    iterations: 1,
    interactive: false,
    timestamp: new Date(0).toISOString(),
    contextPaths: [{ path: 'file.txt', source: 'file' }],
    ...overrides,
  })
  it('exports the last history entry by default (JSON)', async () => {
    const tempHome = await fs.mkdtemp(path.join(os.tmpdir(), 'pmc-export-home-'))
    tempHomes.push(tempHome)
    process.env.HOME = tempHome
    const first = createPayload({ intent: 'first', timestamp: '2025-01-01T00:00:00.000Z' })
    const second = createPayload({ intent: 'second', timestamp: '2025-01-02T00:00:00.000Z' })
    await writeHistory(tempHome, [JSON.stringify(first), 'not json', JSON.stringify(second)])
    const outPath = path.join(tempHome, 'export.json')
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    const err = jest.spyOn(console, 'error').mockImplementation(() => undefined)
    await runExportCommand(['--format', 'json', '--out', outPath, '--quiet'])
    expect(log).not.toHaveBeenCalled()
    expect(err).not.toHaveBeenCalled()
    const written = await fs.readFile(outPath, 'utf8')
    expect(JSON.parse(written) as unknown).toEqual(second)
  })
  it('exports the N-th entry from end (YAML)', async () => {
    const tempHome = await fs.mkdtemp(path.join(os.tmpdir(), 'pmc-export-home-'))
    tempHomes.push(tempHome)
    process.env.HOME = tempHome
    const first = createPayload({ intent: 'first', timestamp: '2025-01-01T00:00:00.000Z' })
    const second = createPayload({ intent: 'second', timestamp: '2025-01-02T00:00:00.000Z' })
    const third = createPayload({ intent: 'third', timestamp: '2025-01-03T00:00:00.000Z' })
    await writeHistory(tempHome, [
      JSON.stringify(first),
      JSON.stringify(second),
      JSON.stringify(third),
    ])
    const outPath = path.join(tempHome, 'export.yaml')
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    const err = jest.spyOn(console, 'error').mockImplementation(() => undefined)
    await runExportCommand(['--from-history', '2', '--format', 'yaml', '--out', outPath, '--quiet'])
    expect(log).not.toHaveBeenCalled()
    expect(err).not.toHaveBeenCalled()
    const written = await fs.readFile(outPath, 'utf8')
    const parsed = yaml.load(written)
    expect(parsed as unknown).toEqual(second)
  })
  it('fails with a clear message for invalid selectors', async () => {
    const tempHome = await fs.mkdtemp(path.join(os.tmpdir(), 'pmc-export-home-'))
    tempHomes.push(tempHome)
    process.env.HOME = tempHome
    const first = createPayload({ intent: 'first', timestamp: '2025-01-01T00:00:00.000Z' })
    await writeHistory(tempHome, [JSON.stringify(first)])
    const outPath = path.join(tempHome, 'export.json')
    const err = jest.spyOn(console, 'error').mockImplementation(() => undefined)
    await runExportCommand([
      '--from-history',
      'last:0',
      '--format',
      'json',
      '--out',
      outPath,
      '--quiet',
    ])
    expect(process.exitCode).toBe(1)
    expect(err).toHaveBeenCalledWith(expect.stringContaining('Invalid --from-history selector'))
    await expect(fs.stat(outPath)).rejects.toThrow()
  })
  it('fails when the history file is missing', async () => {
    const tempHome = await fs.mkdtemp(path.join(os.tmpdir(), 'pmc-export-home-'))
    tempHomes.push(tempHome)
    process.env.HOME = tempHome
    const outPath = path.join(tempHome, 'export.json')
    const err = jest.spyOn(console, 'error').mockImplementation(() => undefined)
    await runExportCommand(['--format', 'json', '--out', outPath, '--quiet'])
    expect(process.exitCode).toBe(1)
    expect(err).toHaveBeenCalledWith(expect.stringContaining('History file not found'))
  })
})
</file>
<file path="src/__tests__/generate-stream-dispatcher.test.ts">
import { createStreamDispatcher } from '../generate/stream'
import type { StreamEventInput } from '../generate/types'
describe('createStreamDispatcher', () => {
  it('serializes context.overflow events with timestamps', () => {
    jest.useFakeTimers().setSystemTime(new Date('2024-01-01T00:00:00.000Z'))
    try {
      const chunks: string[] = []
      const dispatcher = createStreamDispatcher('jsonl', {
        writer: (chunk) => {
          chunks.push(chunk)
        },
      })
      const event: StreamEventInput = {
        event: 'context.overflow',
        strategy: 'drop-largest',
        before: {
          files: [],
          intentTokens: 200,
          fileTokens: 300,
          systemTokens: 700,
          totalTokens: 1200,
        },
        after: {
          files: [],
          intentTokens: 200,
          fileTokens: 100,
          systemTokens: 700,
          totalTokens: 1000,
        },
        droppedPaths: [{ path: 'docs/too-big.md', source: 'file' }],
      }
      dispatcher.emit(event)
      expect(chunks).toHaveLength(1)
      const serialized = chunks[0]?.trim()
      expect(serialized).toBeTruthy()
      const parsed = JSON.parse(serialized ?? '{}') as StreamEventInput & { timestamp: string }
      expect(parsed).toEqual(
        expect.objectContaining({
          event: 'context.overflow',
          strategy: 'drop-largest',
          timestamp: '2024-01-01T00:00:00.000Z',
          droppedPaths: [{ path: 'docs/too-big.md', source: 'file' }],
        }),
      )
    } finally {
      jest.useRealTimers()
    }
  })
})
</file>
<file path="src/__tests__/help-config.test.ts">
import { COMMAND_DESCRIPTORS } from '../tui/config'
import { createHelpSections } from '../tui/help-config'
describe('createHelpSections', () => {
  it('includes global shortcuts and help toggle', () => {
    const sections = createHelpSections({ commandDescriptors: COMMAND_DESCRIPTORS })
    const global = sections.find((section) => section.title === 'Global')
    expect(global).toBeDefined()
    expect(global?.lines.join(' ')).toContain('Ctrl+G')
    expect(global?.lines.join(' ')).toContain('Ctrl+T')
    expect(global?.lines.join(' ')).toContain('/help')
  })
  it('mentions every configured command', () => {
    const sections = createHelpSections({ commandDescriptors: COMMAND_DESCRIPTORS })
    const generate = sections.find((section) => section.title === 'Generate')
    expect(generate).toBeDefined()
    const combined = generate?.lines.join(' ') ?? ''
    for (const descriptor of COMMAND_DESCRIPTORS) {
      expect(combined).toContain(`/${descriptor.id}`)
    }
  })
})
</file>
<file path="src/__tests__/index.test.ts">
jest.mock('../compose-command', () => ({ runComposeCommand: jest.fn() }))
jest.mock('../generate-command', () => ({ runGenerateCommand: jest.fn() }))
jest.mock('../test-command', () => ({ runTestCommand: jest.fn() }))
jest.mock('../tui', () => ({ runTuiCommand: jest.fn() }))
const getComposeMock = () =>
  (jest.requireMock('../compose-command') as { runComposeCommand: jest.Mock }).runComposeCommand
const getGenerateMock = () =>
  (jest.requireMock('../generate-command') as { runGenerateCommand: jest.Mock }).runGenerateCommand
const getTestMock = () =>
  (jest.requireMock('../test-command') as { runTestCommand: jest.Mock }).runTestCommand
const getTuiMock = () => (jest.requireMock('../tui') as { runTuiCommand: jest.Mock }).runTuiCommand
describe('CLI entrypoint command routing', () => {
  const originalArgv = [...process.argv]
  afterAll(() => {
    process.argv = originalArgv
  })
  const importCli = async (): Promise<void> => {
    await jest.isolateModulesAsync(async () => {
      await import('../index')
    })
  }
  it('invokes ui when no args are provided', async () => {
    const runTuiCommand = getTuiMock()
    const runGenerateCommand = getGenerateMock()
    runTuiCommand.mockClear()
    runGenerateCommand.mockClear()
    process.argv = ['node', 'cli']
    await importCli()
    expect(runTuiCommand).toHaveBeenCalledWith([])
    expect(runGenerateCommand).not.toHaveBeenCalled()
  })
  it('routes to explicit ui subcommand', async () => {
    const runTuiCommand = getTuiMock()
    runTuiCommand.mockClear()
    process.argv = ['node', 'cli', 'ui', '--verbose']
    await importCli()
    expect(runTuiCommand).toHaveBeenCalledWith(['--verbose'])
  })
  it('routes to test subcommand', async () => {
    const runTestCommand = getTestMock()
    runTestCommand.mockClear()
    process.argv = ['node', 'cli', 'test', '--watch']
    await importCli()
    expect(runTestCommand).toHaveBeenCalledWith(['--watch'])
  })
  it('routes to compose subcommand', async () => {
    const runComposeCommand = getComposeMock()
    runComposeCommand.mockClear()
    process.argv = ['node', 'cli', 'compose', '--recipe', 'recipe.yaml', '--input', 'hello']
    await importCli()
    expect(runComposeCommand).toHaveBeenCalledWith(['--recipe', 'recipe.yaml', '--input', 'hello'])
  })
  it('treats generate alias as generate command', async () => {
    const runGenerateCommand = getGenerateMock()
    runGenerateCommand.mockClear()
    process.argv = ['node', 'cli', 'generate', 'foo']
    await importCli()
    expect(runGenerateCommand).toHaveBeenCalledWith(['foo'])
  })
  it('treats expand alias as generate command', async () => {
    const runGenerateCommand = getGenerateMock()
    runGenerateCommand.mockClear()
    process.argv = ['node', 'cli', 'expand', 'bar']
    await importCli()
    expect(runGenerateCommand).toHaveBeenCalledWith(['bar'])
  })
  it('falls back to generate when first arg is a flag', async () => {
    const runGenerateCommand = getGenerateMock()
    const runTuiCommand = getTuiMock()
    runGenerateCommand.mockClear()
    runTuiCommand.mockClear()
    process.argv = ['node', 'cli', '--json']
    await importCli()
    expect(runGenerateCommand).toHaveBeenCalledWith(['--json'])
    expect(runTuiCommand).not.toHaveBeenCalled()
  })
})
</file>
<file path="src/__tests__/list-window.test.ts">
import { resolveListPopupHeights } from '../tui/components/popups/list-popup-layout'
import { resolveCursorWindow, resolveWindowedList } from '../tui/components/popups/list-window'
describe('resolveCursorWindow', () => {
  it('returns empty range for invalid sizes', () => {
    expect(resolveCursorWindow(10, 5, 0)).toEqual({ startIndex: 0, endIndexExclusive: 0 })
    expect(resolveCursorWindow(0, 0, 5)).toEqual({ startIndex: 0, endIndexExclusive: 0 })
  })
  it('windows near the start', () => {
    expect(resolveCursorWindow(10, 0, 5)).toEqual({ startIndex: 0, endIndexExclusive: 5 })
    expect(resolveCursorWindow(10, 1, 5)).toEqual({ startIndex: 0, endIndexExclusive: 5 })
  })
  it('windows in the middle with a 2-row lead', () => {
    // With lead=2, cursorIndex=5 should put the window start at 3.
    expect(resolveCursorWindow(10, 5, 5)).toEqual({ startIndex: 3, endIndexExclusive: 8 })
  })
  it('windows near the end', () => {
    expect(resolveCursorWindow(10, 9, 5)).toEqual({ startIndex: 5, endIndexExclusive: 10 })
  })
  it('shows all items when list is smaller than window', () => {
    expect(resolveCursorWindow(3, 1, 10)).toEqual({ startIndex: 0, endIndexExclusive: 3 })
  })
})
describe('resolveWindowedList', () => {
  it('shows all items when they fit', () => {
    expect(
      resolveWindowedList({
        itemCount: 2,
        selectedIndex: 1,
        maxVisibleRows: 5,
      }),
    ).toEqual({ start: 0, end: 2, showBefore: false, showAfter: false })
  })
  it('windows a long list with indicators', () => {
    const result = resolveWindowedList({
      itemCount: 10,
      selectedIndex: 9,
      maxVisibleRows: 5,
      lead: 2,
    })
    expect(result.showBefore).toBe(true)
    expect(result.showAfter).toBe(false)
    expect(result.end).toBe(10)
    expect(result.start).toBeGreaterThanOrEqual(0)
    expect(result.start).toBeLessThan(result.end)
  })
  it('returns empty window for invalid sizes', () => {
    expect(resolveWindowedList({ itemCount: 5, selectedIndex: 2, maxVisibleRows: 0 })).toEqual({
      start: 0,
      end: 0,
      showBefore: false,
      showAfter: false,
    })
  })
})
describe('resolveListPopupHeights', () => {
  it('allocates rows for file popup height 16', () => {
    expect(
      resolveListPopupHeights({ maxHeight: 16, hasSuggestions: true, instructionRows: 1 }),
    ).toEqual({
      selectedRows: 3,
      suggestionRows: 4,
    })
  })
  it('prefers selected rows on small heights', () => {
    expect(
      resolveListPopupHeights({ maxHeight: 10, hasSuggestions: true, instructionRows: 1 }),
    ).toEqual({
      selectedRows: 1,
      suggestionRows: 0,
    })
  })
  it('reduces rows when instructions wrap', () => {
    expect(
      resolveListPopupHeights({ maxHeight: 16, hasSuggestions: true, instructionRows: 2 }),
    ).toEqual({
      selectedRows: 3,
      suggestionRows: 3,
    })
  })
  it('keeps defaults when suggestions are absent', () => {
    expect(
      resolveListPopupHeights({ maxHeight: undefined, hasSuggestions: false, instructionRows: 1 }),
    ).toEqual({
      selectedRows: 6,
      suggestionRows: 0,
    })
  })
})
</file>
<file path="src/__tests__/message-builders.test.ts">
import type { MessageContent } from '@prompt-maker/core'
import { buildInitialUserMessage } from '../prompt-generator/message-builders'
jest.mock('../image-loader', () => ({
  resolveImageParts: jest.fn(),
}))
jest.mock('../prompt-generator/video-parts', () => ({
  resolveVideoParts: jest.fn(),
}))
jest.mock('../prompt-generator/pdf-parts', () => ({
  resolvePdfParts: jest.fn(),
}))
const { resolveImageParts } = jest.requireMock('../image-loader') as {
  resolveImageParts: jest.Mock
}
const { resolveVideoParts } = jest.requireMock('../prompt-generator/video-parts') as {
  resolveVideoParts: jest.Mock
}
const { resolvePdfParts } = jest.requireMock('../prompt-generator/pdf-parts') as {
  resolvePdfParts: jest.Mock
}
describe('message-builders', () => {
  beforeEach(() => {
    jest.clearAllMocks()
  })
  it('merges parts in deterministic order: images â videos â PDFs â text', async () => {
    resolveImageParts.mockResolvedValue([{ type: 'image', mimeType: 'image/png', data: 'img' }])
    resolveVideoParts.mockResolvedValue([
      { type: 'video_uri', mimeType: 'video/mp4', fileUri: 'gs://v' },
    ])
    resolvePdfParts.mockResolvedValue([
      { type: 'pdf', mimeType: 'application/pdf', filePath: 'doc.pdf', fileUri: 'gs://p' },
    ])
    const content = await buildInitialUserMessage(
      'Do a thing',
      [],
      ['img.png'],
      ['clip.mp4'],
      ['doc.pdf'],
    )
    expect(Array.isArray(content)).toBe(true)
    const parts = content as Exclude<MessageContent, string>
    expect(parts.map((part) => part.type)).toEqual(['image', 'video_uri', 'pdf', 'text'])
  })
  it('returns plain text when no media parts exist', async () => {
    resolveImageParts.mockResolvedValue([])
    resolveVideoParts.mockResolvedValue([])
    resolvePdfParts.mockResolvedValue([])
    const content = await buildInitialUserMessage('Do a thing', [], [], [], [])
    expect(typeof content).toBe('string')
  })
})
</file>
<file path="src/__tests__/model-manager.test.ts">
const mockReadFile = jest.fn()
const mockMkdir = jest.fn()
const mockWriteFile = jest.fn()
jest.mock('node:fs/promises', () => ({
  readFile: (...args: unknown[]) => mockReadFile(...args),
  mkdir: (...args: unknown[]) => mockMkdir(...args),
  writeFile: (...args: unknown[]) => mockWriteFile(...args),
}))
describe('model-manager', () => {
  beforeEach(() => {
    jest.resetModules()
    mockReadFile.mockReset()
    mockMkdir.mockReset()
    mockWriteFile.mockReset()
    jest.spyOn(console, 'error').mockImplementation(() => undefined)
  })
  it('returns cached models when cache is fresh', async () => {
    const now = 1_700_000_000_000
    mockReadFile.mockResolvedValueOnce(
      JSON.stringify({
        timestamp: now - 1_000,
        models: { openai: ['gpt-4o'], gemini: ['gemini-1.5-pro'] },
      }),
    )
    const mockFetch = jest.fn()
    const { getAvailableModels } = await import('../utils/model-manager')
    const result = await getAvailableModels('openai-key', 'gemini-key', {
      cacheFilePath: '/tmp/models-cache.json',
      now,
      fetchImpl: mockFetch,
    })
    expect(result).toEqual({ openai: ['gpt-4o'], gemini: ['gemini-1.5-pro'] })
    expect(mockFetch).not.toHaveBeenCalled()
    expect(mockWriteFile).not.toHaveBeenCalled()
  })
  it('refreshes a stale cache and writes the updated file', async () => {
    const now = 1_700_000_000_000
    mockReadFile.mockResolvedValueOnce(
      JSON.stringify({
        timestamp: 0,
        models: { openai: ['gpt-legacy'], gemini: ['gemini-legacy'] },
      }),
    )
    const mockFetch = jest.fn(async (url: RequestInfo | URL) => {
      const href = typeof url === 'string' ? url : url.toString()
      if (href.includes('openai.com')) {
        return {
          ok: true,
          status: 200,
          statusText: 'OK',
          json: async () => ({ data: [{ id: 'gpt-4o' }, { id: 'whisper-1' }, { id: 'o1-mini' }] }),
        } as unknown as Response
      }
      return {
        ok: true,
        status: 200,
        statusText: 'OK',
        json: async () => ({
          models: [
            {
              name: 'models/gemini-1.5-pro',
              supportedGenerationMethods: ['generateContent'],
            },
            {
              name: 'models/embedding-001',
              supportedGenerationMethods: ['embedContent'],
            },
          ],
        }),
      } as unknown as Response
    })
    const { getAvailableModels } = await import('../utils/model-manager')
    const result = await getAvailableModels('openai-key', 'gemini-key', {
      cacheFilePath: '/tmp/models-cache.json',
      now,
      fetchImpl: mockFetch,
    })
    expect(result.openai).toEqual(['gpt-4o', 'o1-mini'])
    expect(result.gemini).toEqual(['gemini-1.5-pro'])
    expect(mockMkdir).toHaveBeenCalled()
    expect(mockWriteFile).toHaveBeenCalled()
  })
  it('avoids writes and network calls without keys', async () => {
    mockReadFile.mockRejectedValueOnce(new Error('missing'))
    const mockFetch = jest.fn()
    const { FALLBACK_MODELS, getAvailableModels } = await import('../utils/model-manager')
    const result = await getAvailableModels(null, undefined, {
      cacheFilePath: '/tmp/models-cache.json',
      now: 1_700_000_000_000,
      fetchImpl: mockFetch,
    })
    expect(result).toEqual(FALLBACK_MODELS)
    expect(mockFetch).not.toHaveBeenCalled()
    expect(mockWriteFile).not.toHaveBeenCalled()
  })
  it('filters video-capable Gemini models by token limit and fileData support', async () => {
    const mockFetch = jest.fn(async (url: RequestInfo | URL, init?: RequestInit) => {
      const href = typeof url === 'string' ? url : url.toString()
      if (href.includes('/v1beta/models?')) {
        return {
          ok: true,
          status: 200,
          statusText: 'OK',
          json: async () => ({
            models: [
              {
                name: 'models/gemini-3-pro-preview',
                supportedGenerationMethods: ['generateContent'],
                inputTokenLimit: 1_000_000,
              },
              {
                name: 'models/gemini-2.5-pro',
                supportedGenerationMethods: ['generateContent'],
                inputTokenLimit: 1_000_000,
              },
              {
                name: 'models/gemini-2.0-flash',
                supportedGenerationMethods: ['generateContent'],
                inputTokenLimit: 32_000,
              },
            ],
          }),
        } as unknown as Response
      }
      if (href.includes('gemini-3-pro-preview:generateContent')) {
        return {
          ok: false,
          status: 404,
          statusText: 'Not Found',
          text: async () =>
            JSON.stringify({
              error: { code: 404, message: 'Requested entity was not found.', status: 'NOT_FOUND' },
            }),
        } as unknown as Response
      }
      if (href.includes('gemini-2.5-pro:generateContent')) {
        const parsed = init?.body ? (JSON.parse(String(init.body)) as any) : null
        const fileUri = parsed?.contents?.[0]?.parts?.[0]?.fileData?.fileUri
        if (fileUri !== 'INVALID_FILE_URI') {
          throw new Error('Expected probe to include INVALID_FILE_URI.')
        }
        return {
          ok: false,
          status: 400,
          statusText: 'Bad Request',
          text: async () =>
            JSON.stringify({
              error: { code: 400, message: 'Invalid file uri.', status: 'INVALID_ARGUMENT' },
            }),
        } as unknown as Response
      }
      throw new Error(`Unexpected fetch url: ${href}`)
    })
    const { getVideoCapableGeminiModels } = await import('../utils/model-manager')
    const result = await getVideoCapableGeminiModels('gemini-key', {
      fetchImpl: mockFetch as unknown as typeof fetch,
    })
    expect(result).toEqual(['gemini-2.5-pro'])
  })
})
</file>
<file path="src/__tests__/pdf-parts.test.ts">
import { resolvePdfParts } from '../prompt-generator/pdf-parts'
jest.mock('node:fs/promises', () => ({
  access: jest.fn(),
}))
jest.mock('../media-loader', () => ({
  inferPdfMimeType: jest.fn(),
  uploadFileForGeminiWithMimeType: jest.fn(),
}))
const fs = jest.requireMock('node:fs/promises') as { access: jest.Mock }
const mediaLoader = jest.requireMock('../media-loader') as {
  inferPdfMimeType: jest.Mock
  uploadFileForGeminiWithMimeType: jest.Mock
}
describe('pdf-parts', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    fs.access.mockResolvedValue(undefined)
    mediaLoader.inferPdfMimeType.mockReturnValue('application/pdf')
    mediaLoader.uploadFileForGeminiWithMimeType.mockResolvedValue('gs://pdf/123')
  })
  it('creates local PDF parts when no apiKey provided', async () => {
    const onUpload = jest.fn()
    const parts = await resolvePdfParts(['doc.pdf'], onUpload)
    expect(mediaLoader.uploadFileForGeminiWithMimeType).not.toHaveBeenCalled()
    expect(parts).toEqual([{ type: 'pdf', mimeType: 'application/pdf', filePath: 'doc.pdf' }])
    expect(onUpload).toHaveBeenNthCalledWith(1, 'start', { kind: 'pdf', filePath: 'doc.pdf' })
    expect(onUpload).toHaveBeenNthCalledWith(2, 'finish', { kind: 'pdf', filePath: 'doc.pdf' })
  })
  it('uploads PDFs for Gemini when apiKey is provided', async () => {
    const parts = await resolvePdfParts(['doc.pdf'], undefined, 'gem-key')
    expect(mediaLoader.uploadFileForGeminiWithMimeType).toHaveBeenCalledWith(
      'doc.pdf',
      'application/pdf',
      'gem-key',
    )
    expect(parts).toEqual([
      { type: 'pdf', mimeType: 'application/pdf', filePath: 'doc.pdf', fileUri: 'gs://pdf/123' },
    ])
  })
  it('throws actionable errors for unreadable PDFs', async () => {
    fs.access.mockRejectedValue(new Error('denied'))
    const onUpload = jest.fn()
    await expect(resolvePdfParts(['doc.pdf'], onUpload)).rejects.toThrow('doc.pdf')
    await expect(resolvePdfParts(['doc.pdf'], onUpload)).rejects.toThrow('not readable')
    expect(onUpload).toHaveBeenNthCalledWith(1, 'start', { kind: 'pdf', filePath: 'doc.pdf' })
    expect(onUpload).toHaveBeenNthCalledWith(2, 'finish', { kind: 'pdf', filePath: 'doc.pdf' })
  })
  it('throws actionable errors for unsupported paths', async () => {
    mediaLoader.inferPdfMimeType.mockImplementation(() => {
      throw new Error('Unsupported PDF type')
    })
    await expect(resolvePdfParts(['doc.txt'])).rejects.toThrow('doc.txt')
    await expect(resolvePdfParts(['doc.txt'])).rejects.toThrow('Unsupported PDF type')
  })
})
</file>
<file path="src/__tests__/prompt-generator-service.test.ts">
import { callLLM } from '@prompt-maker/core'
import {
  PromptGeneratorService,
  resolveDefaultGenerateModel,
  ensureModelCredentials,
  isGemini,
} from '../prompt-generator-service'
jest.mock('@prompt-maker/core', () => ({ callLLM: jest.fn() }))
jest.mock('../config', () => ({
  loadCliConfig: jest.fn().mockResolvedValue({
    promptGenerator: { defaultModel: 'gpt-4o-mini', defaultGeminiModel: 'gemini-1.5-pro' },
  }),
  resolveOpenAiCredentials: jest
    .fn()
    .mockResolvedValue({ apiKey: 'OPENAI', baseUrl: 'https://openai' }),
  resolveGeminiCredentials: jest
    .fn()
    .mockResolvedValue({ apiKey: 'GEM', baseUrl: 'https://gemini' }),
}))
jest.mock('../image-loader', () => ({
  resolveImageParts: jest
    .fn()
    .mockResolvedValue([{ type: 'image', mimeType: 'image/png', data: 'aaa' }]),
}))
jest.mock('../prompt-generator/pdf-parts', () => ({
  resolvePdfParts: jest
    .fn()
    .mockResolvedValue([{ type: 'pdf', mimeType: 'application/pdf', filePath: 'doc.pdf' }]),
}))
const { resolvePdfParts } = jest.requireMock('../prompt-generator/pdf-parts') as {
  resolvePdfParts: jest.Mock
}
jest.mock('../media-loader', () => ({
  uploadFileForGemini: jest.fn().mockResolvedValue('gs://video'),
  uploadFileForGeminiWithMimeType: jest.fn().mockResolvedValue('gs://pdf'),
  inferVideoMimeType: jest.fn().mockReturnValue('video/mp4'),
  inferPdfMimeType: jest.fn().mockReturnValue('application/pdf'),
}))
const { resolveImageParts } = jest.requireMock('../image-loader') as {
  resolveImageParts: jest.Mock
}
const mediaLoader = jest.requireMock('../media-loader') as {
  uploadFileForGemini: jest.Mock
  uploadFileForGeminiWithMimeType: jest.Mock
  inferVideoMimeType: jest.Mock
  inferPdfMimeType: jest.Mock
}
const configModule = jest.requireMock('../config') as {
  loadCliConfig: jest.Mock
  resolveOpenAiCredentials: jest.Mock
  resolveGeminiCredentials: jest.Mock
}
const callLLMMock = callLLM as jest.MockedFunction<typeof callLLM>
describe('prompt-generator-service helpers', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    configModule.loadCliConfig.mockResolvedValue({
      promptGenerator: { defaultModel: 'gpt-4o-mini', defaultGeminiModel: 'gemini-1.5-pro' },
    })
    configModule.resolveOpenAiCredentials.mockResolvedValue({
      apiKey: 'OPENAI',
      baseUrl: 'https://openai',
    })
    configModule.resolveGeminiCredentials.mockResolvedValue({
      apiKey: 'GEM',
      baseUrl: 'https://gemini',
    })
    resolveImageParts.mockResolvedValue([{ type: 'image', mimeType: 'image/png', data: 'aaa' }])
  })
  it('detects gemini models via isGemini', () => {
    expect(isGemini('gemini-1.5-pro')).toBe(true)
    expect(isGemini('gemma-2b')).toBe(true)
    expect(isGemini('gpt-4o-mini')).toBe(false)
  })
  it('resolveDefaultGenerateModel prefers config before env', async () => {
    process.env.PROMPT_MAKER_GENERATE_MODEL = 'env-model'
    const model = await resolveDefaultGenerateModel()
    expect(model).toBe('gpt-4o-mini')
  })
  it('ensureModelCredentials sets OpenAI env vars when missing', async () => {
    delete process.env.OPENAI_API_KEY
    await ensureModelCredentials('gpt-4o-mini')
    expect(process.env.OPENAI_API_KEY).toBe('OPENAI')
    expect(process.env.OPENAI_BASE_URL).toBe('https://openai')
  })
  it('ensureModelCredentials sets Gemini env vars when needed', async () => {
    delete process.env.GEMINI_API_KEY
    await ensureModelCredentials('gemini-1.5-pro')
    expect(process.env.GEMINI_API_KEY).toBe('GEM')
    expect(process.env.GEMINI_BASE_URL).toBe('https://gemini')
  })
})
describe('PromptGeneratorService.generatePrompt', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    callLLMMock.mockReset()
    configModule.loadCliConfig.mockResolvedValue({
      promptGenerator: { defaultModel: 'gpt-4o-mini', defaultGeminiModel: 'gemini-1.5-pro' },
    })
    callLLMMock.mockResolvedValue('{"prompt":"Result","reasoning":"ok"}')
    resolveImageParts.mockResolvedValue([{ type: 'image', mimeType: 'image/png', data: 'aaa' }])
    mediaLoader.uploadFileForGemini.mockResolvedValue('gs://video')
    mediaLoader.inferVideoMimeType.mockReturnValue('video/mp4')
  })
  const buildService = async () => new PromptGeneratorService()
  it('constructs initial generation request with context and media', async () => {
    const service = await buildService()
    const prompt = await service.generatePrompt({
      intent: 'Do a thing',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [{ path: 'ctx.md', content: 'context' }],
      images: ['image.png'],
      videos: ['clip.mp4'],
    })
    expect(resolveImageParts).toHaveBeenCalledWith(['image.png'], undefined)
    expect(mediaLoader.uploadFileForGemini).toHaveBeenCalledWith('clip.mp4')
    expect(callLLM).toHaveBeenCalledWith(
      expect.arrayContaining([
        expect.objectContaining({ role: 'system' }),
        expect.objectContaining({ role: 'user' }),
      ]),
      'gpt-4o-mini',
    )
    expect(prompt).toContain('Result')
  })
  it('includes meta instructions and keeps target guidance internal', async () => {
    const service = await buildService()
    await service.generatePrompt({
      intent: 'Do a thing',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [],
      images: [],
      videos: [],
      metaInstructions: 'Be concise',
    })
    const messagePayload = callLLMMock.mock.calls[0]?.[0] as Array<{
      role: string
      content: unknown
    }>
    const systemMessages = messagePayload.filter((msg) => msg.role === 'system')
    const userMessage = messagePayload.find((msg) => msg.role === 'user')
    const userPayloadText = JSON.stringify(userMessage?.content)
    expect(userPayloadText).toContain('Meta-Instructions:\\nBe concise')
    expect(userPayloadText).not.toMatch(/target runtime model/i)
    expect(userPayloadText).not.toContain('gpt-4o-mini')
    const systemPayloadText = JSON.stringify(systemMessages.map((msg) => msg.content))
    expect(systemPayloadText).toContain('targetRuntimeModel: gpt-4o-mini')
    expect(systemPayloadText).toMatch(/do not include phrases like/i)
    expect(systemPayloadText).toMatch(/only include the target model/i)
  })
  it('sanitizes target model leakage from model output', async () => {
    callLLMMock.mockResolvedValueOnce(
      JSON.stringify({
        reasoning: 'x',
        prompt: 'Line 1\nTarget runtime model for executing: **GPT-5.2**\nUse gpt-5.2.',
      }),
    )
    const service = await buildService()
    const prompt = await service.generatePrompt({
      intent: 'Write a prompt about keyboard shortcuts',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-5.2',
      fileContext: [],
      images: [],
      videos: [],
    })
    expect(prompt.toLowerCase()).not.toContain('target runtime model')
    expect(prompt.toLowerCase()).not.toContain('gpt-5.2')
  })
  it('keeps target model mentions when user intent includes it', async () => {
    callLLMMock.mockResolvedValueOnce(
      JSON.stringify({
        reasoning: 'x',
        prompt: 'This prompt must mention gpt-5.2 explicitly.',
      }),
    )
    const service = await buildService()
    const prompt = await service.generatePrompt({
      intent: 'Write a prompt and explicitly mention gpt-5.2.',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-5.2',
      fileContext: [],
      images: [],
      videos: [],
    })
    expect(prompt).toContain('gpt-5.2')
  })
  it('handles refinement flows with previous prompt', async () => {
    const service = await buildService()
    await service.generatePrompt({
      intent: 'Original',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [],
      images: [],
      videos: [],
      previousPrompt: 'draft',
      refinementInstruction: 'shorter',
    })
    const call = callLLMMock.mock.calls[0]?.[0]
    const userMessage = call?.find((msg: { role: string }) => msg.role === 'user')
    expect(userMessage?.content).toEqual(
      expect.arrayContaining([
        expect.objectContaining({
          type: 'text',
          text: expect.stringContaining('Current Prompt Draft'),
        }),
      ]),
    )
  })
  it('auto-refines generic PDF prompt contracts to be grounded', async () => {
    const pdfPath =
      '/Users/eroberts/Downloads/BP-Adopt AI-based Browser Automation-080126-222458.pdf'
    callLLMMock
      .mockResolvedValueOnce(
        JSON.stringify({
          reasoning: 'x',
          prompt:
            '# Title\n\nMake document concise\n\n## Inputs\n- Primary document: already provided in context\n',
        }),
      )
      .mockResolvedValueOnce(
        JSON.stringify({
          reasoning: 'y',
          prompt:
            '# Title\n\nRewrite attached PDF\n\nDocument Snapshot\n- Topic: Browser automation\n\n"This is a verbatim quote from the PDF content."\n\nInputs\n- Attached PDF: BP-Adopt AI-based Browser Automation-080126-222458.pdf\n',
        }),
      )
    const service = await buildService()
    const prompt = await service.generatePrompt({
      intent: 'Please make this document more succinct',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-5.2',
      fileContext: [],
      images: [],
      videos: [],
      pdfs: [pdfPath],
    })
    expect(callLLM).toHaveBeenCalledTimes(2)
    expect(resolvePdfParts).toHaveBeenCalledTimes(1)
    const secondCallMessages = callLLMMock.mock.calls[1]?.[0] as Array<{
      role: string
      content: unknown
    }>
    const secondUser = secondCallMessages.find((msg) => msg.role === 'user')
    expect(JSON.stringify(secondUser?.content)).toContain('Refinement Instruction')
    expect(JSON.stringify(secondUser?.content)).toContain('Document Snapshot')
    expect(prompt).toContain('BP-Adopt AI-based Browser Automation-080126-222458.pdf')
    expect(prompt).toContain('Document Snapshot')
  })
  it('does not refine when PDF contract is already grounded', async () => {
    const pdfPath = '/Users/eroberts/Downloads/doc.pdf'
    callLLMMock.mockResolvedValueOnce(
      JSON.stringify({
        reasoning: 'x',
        prompt:
          '# Title\n\nRewrite PDF\n\nDocument Snapshot\n- A\n\n"Quote one from document"\n\nInputs\n- Attached PDF: doc.pdf',
      }),
    )
    const service = await buildService()
    const prompt = await service.generatePrompt({
      intent: 'Please make this document more succinct',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-5.2',
      fileContext: [],
      images: [],
      videos: [],
      pdfs: [pdfPath],
    })
    expect(callLLM).toHaveBeenCalledTimes(1)
    expect(prompt).toContain('Document Snapshot')
    expect(prompt).toContain('doc.pdf')
  })
  it('returns raw response when LLM output is not JSON', async () => {
    callLLMMock.mockResolvedValue('plain text response')
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    const service = await buildService()
    const prompt = await service.generatePrompt({
      intent: 'Intent',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [],
      images: [],
      videos: [],
    })
    expect(prompt).toBe('plain text response')
    expect(warn).toHaveBeenCalledWith(
      'Failed to parse LLM JSON response. Falling back to raw text.',
    )
    warn.mockRestore()
  })
  it('logs reasoning when DEBUG env var is set', async () => {
    process.env.DEBUG = '1'
    const err = jest.spyOn(console, 'error').mockImplementation(() => undefined)
    const service = await buildService()
    await service.generatePrompt({
      intent: 'Intent',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [],
      images: [],
      videos: [],
    })
    expect(err).toHaveBeenCalledWith(expect.stringContaining('--- AI Reasoning ---'))
    err.mockRestore()
    delete process.env.DEBUG
  })
})
describe('PromptGeneratorService.generatePromptSeries', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    configModule.loadCliConfig.mockResolvedValue({
      promptGenerator: { defaultModel: 'gpt-4o-mini', defaultGeminiModel: 'gemini-1.5-pro' },
    })
    resolveImageParts.mockResolvedValue([{ type: 'image', mimeType: 'image/png', data: 'aaa' }])
    mediaLoader.uploadFileForGemini.mockResolvedValue('gs://video')
    mediaLoader.inferVideoMimeType.mockReturnValue('video/mp4')
  })
  const buildService = async () => new PromptGeneratorService()
  const validAtomicPromptContent = `# Title
Do a thing
Role
You are a coding agent.
Context
This is standalone.
Goals & Tasks
- Make one small change
Inputs
- None
Constraints
- Keep it small
Execution Plan
1. Do the thing
Output Format
- Updated file(s)
Validation
- Run: npx jest apps/prompt-maker-cli/src/__tests__/prompt-generator-service.test.ts --runInBand
`
  const seriesPayload = {
    reasoning: 'analysis',
    overviewPrompt: '# Overview',
    atomicPrompts: [{ title: 'Step', content: validAtomicPromptContent }],
  }
  it('parses valid JSON into a SeriesResponse and uploads media', async () => {
    callLLMMock.mockResolvedValue(JSON.stringify(seriesPayload))
    const service = await buildService()
    const result = await service.generatePromptSeries({
      intent: 'Plan something',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [{ path: 'ctx.md', content: 'context' }],
      images: ['diagram.png'],
      videos: ['clip.mp4'],
    })
    expect(resolveImageParts).toHaveBeenCalledWith(['diagram.png'], undefined)
    expect(mediaLoader.uploadFileForGemini).toHaveBeenCalledWith('clip.mp4')
    expect(result).toEqual(seriesPayload)
  })
  it('throws when the LLM response is not valid JSON', async () => {
    callLLMMock.mockResolvedValue('not json')
    const service = await buildService()
    await expect(
      service.generatePromptSeries({
        intent: 'Plan',
        model: 'gpt-4o-mini',
        targetModel: 'gpt-4o-mini',
        fileContext: [],
        images: [],
        videos: [],
      }),
    ).rejects.toThrow('LLM did not return valid SeriesResponse JSON.')
  })
  it('throws when the JSON is missing atomic prompts', async () => {
    callLLMMock.mockResolvedValue(
      JSON.stringify({ reasoning: 'r', overviewPrompt: '# Overview', atomicPrompts: [] }),
    )
    const service = await buildService()
    await expect(
      service.generatePromptSeries({
        intent: 'Plan',
        model: 'gpt-4o-mini',
        targetModel: 'gpt-4o-mini',
        fileContext: [],
        images: [],
        videos: [],
      }),
    ).rejects.toThrow('Series atomicPrompts must include at least one entry.')
  })
  it('repairs atomic prompts that are missing required sections', async () => {
    callLLMMock
      .mockResolvedValueOnce(
        JSON.stringify({
          reasoning: 'r',
          overviewPrompt: '# Overview',
          atomicPrompts: [
            { title: 'Step', content: '# Title\nMissing most sections\n\nValidation\n- ok' },
          ],
        }),
      )
      .mockResolvedValueOnce(JSON.stringify(seriesPayload))
    const onSeriesRepairAttempt = jest.fn()
    const service = await buildService()
    const result = await service.generatePromptSeries({
      intent: 'Plan',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [],
      images: [],
      videos: [],
      onSeriesRepairAttempt,
    })
    expect(callLLMMock).toHaveBeenCalledTimes(2)
    expect(onSeriesRepairAttempt).toHaveBeenCalledWith(
      expect.objectContaining({
        attempt: 1,
        maxAttempts: 2,
        validationError: expect.stringContaining('missing required section(s)'),
      }),
    )
    expect(result).toEqual(seriesPayload)
  })
  it('repairs atomic prompts that contain cross-references', async () => {
    callLLMMock
      .mockResolvedValueOnce(
        JSON.stringify({
          reasoning: 'r',
          overviewPrompt: '# Overview',
          atomicPrompts: [
            {
              title: 'Step',
              content: `# Title
Do a thing
Role
You are a coding agent.
Context
Continue from step 2.
Goals & Tasks
- Make one small change
Inputs
- None
Constraints
- Keep it small
Execution Plan
1. Do the thing
Output Format
- Updated file(s)
Validation
- Run: npx jest --runInBand
`,
            },
          ],
        }),
      )
      .mockResolvedValueOnce(JSON.stringify(seriesPayload))
    const onSeriesRepairAttempt = jest.fn()
    const service = await buildService()
    const result = await service.generatePromptSeries({
      intent: 'Plan',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [],
      images: [],
      videos: [],
      onSeriesRepairAttempt,
    })
    expect(callLLMMock).toHaveBeenCalledTimes(2)
    expect(onSeriesRepairAttempt).toHaveBeenCalledWith(
      expect.objectContaining({
        attempt: 1,
        maxAttempts: 2,
        validationError: expect.stringContaining('contains forbidden cross-reference phrase'),
      }),
    )
    expect(result).toEqual(seriesPayload)
  })
  it('logs reasoning when DEBUG env var is set', async () => {
    process.env.DEBUG = '1'
    callLLMMock.mockResolvedValue(JSON.stringify(seriesPayload))
    const err = jest.spyOn(console, 'error').mockImplementation(() => undefined)
    const service = await buildService()
    await service.generatePromptSeries({
      intent: 'Plan something',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [],
      images: [],
      videos: [],
    })
    expect(err).toHaveBeenCalledWith(expect.stringContaining('--- Series Reasoning ---'))
    err.mockRestore()
    delete process.env.DEBUG
  })
})
</file>
<file path="src/__tests__/soft-wrap.test.ts">
import {
  expandTokenizedLines,
  getTokenizedCursorCoordinates,
  type TokenLabelLookup,
} from '../tui/components/core/tokenized-text'
import { getSoftWrappedCursorOffset, softWrapLine } from '../tui/components/core/soft-wrap'
describe('soft-wrap', () => {
  it('wraps by character width (constant width)', () => {
    expect(softWrapLine('abcdef', { first: 3, rest: 3 }).segments).toEqual(['abc', 'def'])
  })
  it('wraps with distinct first/rest widths', () => {
    expect(softWrapLine('abcdefg', { first: 2, rest: 3 }).segments).toEqual(['ab', 'cde', 'fg'])
  })
  it('prefers breaking on whitespace when available', () => {
    expect(softWrapLine('hello world', { first: 8, rest: 8 }).segments).toEqual(['hello ', 'world'])
    expect(softWrapLine('hello  world', { first: 8, rest: 8 }).segments).toEqual([
      'hello  ',
      'world',
    ])
  })
  it('treats non-positive widths as 1', () => {
    expect(softWrapLine('abc', { first: 0, rest: -1 }).segments).toEqual(['a', 'b', 'c'])
  })
  it('always returns at least one segment', () => {
    expect(softWrapLine('', { first: 3, rest: 3 }).segments).toEqual([''])
  })
  it('maps cursor positions within wrapped segments', () => {
    const wrapped = softWrapLine('abcdef', { first: 3, rest: 3 })
    expect(getSoftWrappedCursorOffset(wrapped, 0)).toEqual({
      rowOffset: 0,
      column: 0,
      needsTrailingEmptyLine: false,
    })
    expect(getSoftWrappedCursorOffset(wrapped, 2)).toEqual({
      rowOffset: 0,
      column: 2,
      needsTrailingEmptyLine: false,
    })
    expect(getSoftWrappedCursorOffset(wrapped, 3)).toEqual({
      rowOffset: 1,
      column: 0,
      needsTrailingEmptyLine: false,
    })
    expect(getSoftWrappedCursorOffset(wrapped, 5)).toEqual({
      rowOffset: 1,
      column: 2,
      needsTrailingEmptyLine: false,
    })
  })
  it('maps end-of-line to a trailing empty line when the final segment is full', () => {
    const wrapped = softWrapLine('abcdef', { first: 3, rest: 3 })
    expect(getSoftWrappedCursorOffset(wrapped, 6)).toEqual({
      rowOffset: 2,
      column: 0,
      needsTrailingEmptyLine: true,
    })
  })
  it('maps end-of-line within the final segment when there is remaining width', () => {
    const wrapped = softWrapLine('abcde', { first: 3, rest: 3 })
    expect(getSoftWrappedCursorOffset(wrapped, 5)).toEqual({
      rowOffset: 1,
      column: 2,
      needsTrailingEmptyLine: false,
    })
  })
  it('supports cursor mapping with tokenized display columns', () => {
    const tokenLabel: TokenLabelLookup = (character) => (character === '$' ? 'XYZ' : null)
    const displayLine = expandTokenizedLines('a$', tokenLabel)[0] ?? ''
    expect(displayLine).toBe('aXYZ')
    const coordinates = getTokenizedCursorCoordinates('a$', 2, tokenLabel)
    expect(coordinates).toEqual({ row: 0, column: 4 })
    const wrapped = softWrapLine(displayLine, { first: 2, rest: 2 })
    expect(wrapped.segments).toEqual(['aX', 'YZ'])
    expect(getSoftWrappedCursorOffset(wrapped, coordinates.column)).toEqual({
      rowOffset: 2,
      column: 0,
      needsTrailingEmptyLine: true,
    })
  })
})
</file>
<file path="src/__tests__/status-indicators-layout.test.ts">
import {
  formatIndicatorLines,
  formatIndicatorSegmentPlain,
  resolveIndicatorSegments,
} from '../tui/components/core/status-indicators-layout'
describe('status indicator layout', () => {
  const chips = [
    '[status:Idle]',
    '[gpt-4o-mini]',
    '[target:gpt-4o]',
    '[openai:ok]',
    '[tokens:1.2k]',
    '[polish:gpt-4o-mini]',
    '[copy:off]',
    '[chatgpt:off]',
    '[json:on]',
    '[files:2]',
    '[urls:0]',
    '[intent:file]',
    '[file:demo.md]',
    '[instr:on]',
    '[tests:idle]',
  ] as const
  const flattenLine = (line: ReturnType<typeof formatIndicatorLines>[number]): string =>
    line.segments.map(formatIndicatorSegmentPlain).join(' Âˇ ')
  it('packs indicators into one line when wide', () => {
    const lines = formatIndicatorLines({ chips, maxWidth: 240 })
    expect(lines).toHaveLength(1)
    const firstLine = lines[0]
    if (!firstLine) {
      throw new Error('Expected at least one indicator line')
    }
    const text = flattenLine(firstLine)
    expect(text).toContain('Status: Idle')
    expect(text).toContain('Model: gpt-4o-mini')
    expect(text).toContain('Polish: gpt-4o-mini')
    expect(text).toContain('Target: gpt-4o')
    expect(text.indexOf('Model: gpt-4o-mini')).toBeLessThan(text.indexOf('Polish: gpt-4o-mini'))
    expect(text.indexOf('Polish: gpt-4o-mini')).toBeLessThan(text.indexOf('Target: gpt-4o'))
    expect(text).toContain('OpenAI: ok')
    expect(text).toContain('Tokens: 1.2k')
  })
  it('wraps indicators across multiple lines when narrow', () => {
    const maxWidth = 44
    const lines = formatIndicatorLines({ chips, maxWidth })
    expect(lines.length).toBeGreaterThan(1)
    for (const line of lines) {
      expect(flattenLine(line).length).toBeLessThanOrEqual(maxWidth)
    }
  })
  it('marks toggles and model selections with correct styles', () => {
    const segments = resolveIndicatorSegments(chips)
    const polish = segments.find((segment) => segment.label === 'Polish')
    const copy = segments.find((segment) => segment.label === 'Copy')
    const json = segments.find((segment) => segment.label === 'JSON')
    expect(polish?.style).toBe('primary')
    expect(copy?.style).toBe('muted')
    expect(json?.style).toBe('primary')
  })
})
</file>
<file path="src/__tests__/tui-command-descriptors.test.ts">
import { COMMAND_DESCRIPTORS } from '../tui/config'
describe('tui command descriptors', () => {
  const getDescriptor = (id: (typeof COMMAND_DESCRIPTORS)[number]['id']) =>
    COMMAND_DESCRIPTORS.find((entry) => entry.id === id)
  it('surfaces /exit first in the palette list', () => {
    expect(COMMAND_DESCRIPTORS[0]?.id).toBe('exit')
  })
  it('surfaces series intent prefill guidance', () => {
    const descriptor = getDescriptor('series')
    expect(descriptor).toBeDefined()
    expect(descriptor?.description).toMatch(/prefill/i)
    expect(descriptor?.description).toMatch(/intent file/i)
  })
  it('includes a settings command descriptor', () => {
    const descriptor = getDescriptor('settings')
    expect(descriptor).toBeDefined()
    expect(descriptor?.description).toContain('/settings')
  })
  it('includes concrete examples for /test, /json, /history, /intent, and /meta', () => {
    const testDescriptor = getDescriptor('test')
    expect(testDescriptor).toBeDefined()
    expect(testDescriptor?.description).toContain('/test prompt-tests.yaml')
    const jsonDescriptor = getDescriptor('json')
    expect(jsonDescriptor).toBeDefined()
    expect(jsonDescriptor?.description).toContain('/json on|off')
    const historyDescriptor = getDescriptor('history')
    expect(historyDescriptor).toBeDefined()
    expect(historyDescriptor?.description).toContain('/history')
    const intentDescriptor = getDescriptor('intent')
    expect(intentDescriptor).toBeDefined()
    expect(intentDescriptor?.description).toContain('/intent')
    const metaDescriptor = getDescriptor('instructions')
    expect(metaDescriptor).toBeDefined()
    expect(metaDescriptor?.description).toContain('/meta <text>')
  })
})
</file>
<file path="src/export/export-generate-payload.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import type { GenerateJsonPayload } from '../generate/types'
import { serializeGeneratePayload, type PayloadFormat } from '../generate/payload-io'
export const writeGeneratePayloadExport = async (options: {
  payload: GenerateJsonPayload
  format: PayloadFormat
  outPath: string
  cwd?: string | undefined
}): Promise<{ absolutePath: string }> => {
  const cwd = options.cwd ?? process.cwd()
  const absolutePath = path.resolve(cwd, options.outPath)
  await fs.mkdir(path.dirname(absolutePath), { recursive: true })
  const serialized = serializeGeneratePayload(options.payload, options.format)
  await fs.writeFile(absolutePath, serialized, 'utf8')
  return { absolutePath }
}
</file>
<file path="src/generate/context-budget.ts">
import type { FileContext } from '../file-context'
import type { ContextOverflowStrategy, ContextPathMetadata, TokenTelemetry } from './types'
export type ContextEntrySource = Exclude<ContextPathMetadata['source'], 'intent'>
export type ContextEntry = FileContext & {
  source: ContextEntrySource
}
export type BuildTelemetry = (
  intentText: string,
  files: FileContext[],
  metaInstructions: string,
) => TokenTelemetry
export type EvaluateContextBudgetParams = {
  intentText: string
  metaInstructions: string
  contextEntries: ContextEntry[]
  maxInputTokens?: number
  maxContextTokens?: number
  strategy?: ContextOverflowStrategy
  buildTelemetry: BuildTelemetry
}
export type ContextBudgetEvaluation = {
  keptEntries: ContextEntry[]
  droppedEntries: ContextEntry[]
  droppedPaths: ContextPathMetadata[]
  before: TokenTelemetry
  after: TokenTelemetry
  strategy?: ContextOverflowStrategy
}
const INFINITY = Number.POSITIVE_INFINITY
const min = (a: number, b: number): number => (a < b ? a : b)
const formatBudget = (budget: number | undefined): string =>
  typeof budget === 'number' ? String(budget) : 'unset'
// Token budgets apply only to text context entries (file/url/smart).
// Images/videos are not included in the token budget model, so they are never trimmed here.
export const evaluateContextBudget = (
  params: EvaluateContextBudgetParams,
): ContextBudgetEvaluation => {
  const {
    intentText,
    metaInstructions,
    contextEntries,
    maxInputTokens,
    maxContextTokens,
    strategy,
    buildTelemetry,
  } = params
  const before = buildTelemetry(intentText, contextEntries, metaInstructions)
  const budgetsEnabled = maxInputTokens !== undefined || maxContextTokens !== undefined
  if (!budgetsEnabled) {
    return {
      keptEntries: contextEntries,
      droppedEntries: [],
      droppedPaths: [],
      before,
      after: before,
      ...(strategy ? { strategy } : {}),
    }
  }
  const effectiveStrategy: ContextOverflowStrategy = strategy ?? 'fail'
  const maxFileTokensFromInputBudget =
    maxInputTokens !== undefined
      ? maxInputTokens - before.intentTokens - before.systemTokens
      : INFINITY
  const allowedFileTokens = min(maxContextTokens ?? INFINITY, maxFileTokensFromInputBudget)
  const isOverflow = before.fileTokens > allowedFileTokens
  if (!isOverflow) {
    return {
      keptEntries: contextEntries,
      droppedEntries: [],
      droppedPaths: [],
      before,
      after: before,
      strategy: effectiveStrategy,
    }
  }
  if (effectiveStrategy === 'fail') {
    throw new Error(
      `Context token budget exceeded (strategy=fail). ` +
        `totalTokens=${before.totalTokens} (maxInputTokens=${formatBudget(maxInputTokens)}), ` +
        `contextTokens=${before.fileTokens} (maxContextTokens=${formatBudget(maxContextTokens)}).`,
    )
  }
  const fileSummaries = before.files
  if (fileSummaries.length !== contextEntries.length) {
    throw new Error(
      `Invariant violation: telemetry entry count (${fileSummaries.length}) did not match context entry count (${contextEntries.length}).`,
    )
  }
  const entryTokens = fileSummaries.map((file) => file.tokens)
  let remainingFileTokens = before.fileTokens
  const dropOrder = buildDropOrder(effectiveStrategy, contextEntries, entryTokens)
  const droppedIndexSet = new Set<number>()
  const droppedEntries: ContextEntry[] = []
  for (const index of dropOrder) {
    if (remainingFileTokens <= allowedFileTokens) {
      break
    }
    if (droppedIndexSet.has(index)) {
      continue
    }
    droppedIndexSet.add(index)
    remainingFileTokens -= entryTokens[index] ?? 0
    const dropped = contextEntries[index]
    if (dropped) {
      droppedEntries.push(dropped)
    }
  }
  const keptEntries = contextEntries.filter((_, index) => !droppedIndexSet.has(index))
  const after = buildTelemetry(intentText, keptEntries, metaInstructions)
  const satisfiesContextBudget = after.fileTokens <= allowedFileTokens
  const satisfiesInputBudget =
    maxInputTokens !== undefined ? after.totalTokens <= maxInputTokens : true
  if (!satisfiesContextBudget || !satisfiesInputBudget) {
    throw new Error(
      `Unable to satisfy token budgets after trimming context. ` +
        `totalTokens=${after.totalTokens} (maxInputTokens=${formatBudget(maxInputTokens)}), ` +
        `contextTokens=${after.fileTokens} (maxContextTokens=${formatBudget(maxContextTokens)}).`,
    )
  }
  const droppedPaths: ContextPathMetadata[] = droppedEntries.map((entry) => ({
    path: entry.path,
    source: entry.source,
  }))
  return {
    keptEntries,
    droppedEntries,
    droppedPaths,
    before,
    after,
    strategy: effectiveStrategy,
  }
}
const buildDropOrder = (
  strategy: Exclude<ContextOverflowStrategy, 'fail'>,
  entries: ContextEntry[],
  entryTokens: number[],
): number[] => {
  const indices = entries.map((_, index) => index)
  switch (strategy) {
    case 'drop-oldest':
      return indices
    case 'drop-largest':
      return [...indices].sort((a, b) => {
        const tokenDelta = (entryTokens[b] ?? 0) - (entryTokens[a] ?? 0)
        if (tokenDelta !== 0) {
          return tokenDelta
        }
        return a - b
      })
    case 'drop-smart':
      return stableSourceFirst(indices, entries, 'smart')
    case 'drop-url':
      return stableSourceFirst(indices, entries, 'url')
    default: {
      const exhaustive: never = strategy
      return exhaustive
    }
  }
}
const stableSourceFirst = (
  indices: number[],
  entries: ContextEntry[],
  source: ContextEntrySource,
): number[] => {
  const matches: number[] = []
  const rest: number[] = []
  indices.forEach((index) => {
    const entry = entries[index]
    if (entry?.source === source) {
      matches.push(index)
    } else {
      rest.push(index)
    }
  })
  return [...matches, ...rest]
}
</file>
<file path="src/generate/models.ts">
import { resolveGeminiCredentials } from '../config'
import { loadModelOptions } from '../tui/model-options'
import { getVideoCapableGeminiModels } from '../utils/model-manager'
const DEFAULT_GEMINI_VIDEO_MODEL = 'gemini-2.5-pro'
export const resolveGeminiVideoModel = async (): Promise<string> => {
  try {
    const credentials = await resolveGeminiCredentials()
    const candidates = await getVideoCapableGeminiModels(credentials.apiKey, {
      ...(credentials.baseUrl ? { baseUrl: credentials.baseUrl } : {}),
    })
    const preferredOrder = [DEFAULT_GEMINI_VIDEO_MODEL, 'gemini-2.5-flash', 'gemini-2.0-flash']
    for (const preferred of preferredOrder) {
      if (candidates.includes(preferred)) {
        return preferred
      }
    }
    return candidates[0] ?? DEFAULT_GEMINI_VIDEO_MODEL
  } catch {
    return DEFAULT_GEMINI_VIDEO_MODEL
  }
}
type ResolveTargetModelOptions = {
  explicitTarget?: string
  defaultTargetModel: string
}
export const resolveTargetModel = async ({
  explicitTarget,
  defaultTargetModel,
}: ResolveTargetModelOptions): Promise<string> => {
  if (explicitTarget === undefined) {
    return defaultTargetModel
  }
  const normalized = explicitTarget.trim()
  if (!normalized) {
    throw new Error('--target requires a non-empty model id.')
  }
  const { options } = await loadModelOptions()
  const match = options.find((option) => option.id === normalized)
  if (!match) {
    const known = options
      .slice(0, 12)
      .map((option) => option.id)
      .join(', ')
    throw new Error(
      [
        `Unknown --target model: ${normalized}`,
        known ? `Known models include: ${known}` : 'No known models are configured.',
        'Add custom entries under promptGenerator.models in ~/.config/prompt-maker-cli/config.json.',
      ].join('\n'),
    )
  }
  return match.id
}
</file>
<file path="src/history/generate-history.ts">
import fs from 'node:fs/promises'
import { validateGeneratePayloadObject } from '../generate/payload-io'
import { GENERATE_JSON_PAYLOAD_SCHEMA_VERSION, type GenerateJsonPayload } from '../generate/types'
import { resolveHistoryFilePath } from '../history-logger'
type JsonRecord = Record<string, unknown>
type GenerateHistoryEntry = {
  raw: JsonRecord
  schemaVersion: string
}
export type FromHistorySelector = {
  fromEnd: number
  label: string
}
export type GenerateHistoryPickerItem = {
  selector: string
  title: string
  detail: string
  schemaVersion: string
  supported: boolean
}
const isRecord = (value: unknown): value is JsonRecord =>
  typeof value === 'object' && value !== null && !Array.isArray(value)
const isMissingFileError = (error: unknown): error is { code: string } =>
  typeof error === 'object' &&
  error !== null &&
  'code' in error &&
  typeof (error as { code?: unknown }).code === 'string' &&
  (error as { code: string }).code === 'ENOENT'
const formatTimestamp = (raw: string): string => {
  const date = new Date(raw)
  if (!Number.isFinite(date.getTime())) {
    return raw
  }
  return date.toISOString().replace('T', ' ').replace(/\..*$/, '')
}
const summarizeIntent = (intent: string, limit: number): string => {
  const normalized = intent.replace(/\s+/g, ' ').trim()
  if (!normalized) {
    return '(empty intent)'
  }
  if (normalized.length <= limit) {
    return normalized
  }
  return `${normalized.slice(0, Math.max(0, limit - 1))}âŚ`
}
const readGenerateHistoryEntries = async (filePath: string): Promise<GenerateHistoryEntry[]> => {
  let raw: string
  try {
    raw = await fs.readFile(filePath, 'utf8')
  } catch (error) {
    if (isMissingFileError(error)) {
      throw new Error(
        `History file not found at ${filePath}. Run a generate command first to create it.`,
      )
    }
    const message = error instanceof Error ? error.message : 'Unknown file error.'
    throw new Error(`Failed to read history file ${filePath}: ${message}`)
  }
  const lines = raw
    .split('\n')
    .map((line) => line.trim())
    .filter((line) => line.length > 0)
  if (lines.length === 0) {
    throw new Error(`History file ${filePath} is empty.`)
  }
  const entries: GenerateHistoryEntry[] = []
  for (const line of lines) {
    try {
      const parsed = JSON.parse(line) as unknown
      if (!isRecord(parsed) || typeof parsed.schemaVersion !== 'string') {
        continue
      }
      entries.push({ raw: parsed, schemaVersion: parsed.schemaVersion })
    } catch {
      // ignore invalid json lines
    }
  }
  if (entries.length === 0) {
    throw new Error(`No generate payload entries found in history file ${filePath}.`)
  }
  return entries
}
export const parseFromHistorySelector = (raw: string | undefined): FromHistorySelector => {
  const selector = raw?.trim() ?? 'last'
  const parseOffset = (rawOffset: string | undefined): number => {
    if (!rawOffset) {
      throw new Error(
        `Invalid --from-history selector "${selector}". Offset must be a positive integer.`,
      )
    }
    const value = Number(rawOffset)
    if (!Number.isInteger(value) || value <= 0) {
      throw new Error(
        `Invalid --from-history selector "${selector}". Offset must be a positive integer.`,
      )
    }
    return value
  }
  if (selector === 'last') {
    return { fromEnd: 1, label: 'last' }
  }
  const lastMatch = selector.match(/^last:(\d+)$/)
  if (lastMatch) {
    return { fromEnd: parseOffset(lastMatch[1]), label: selector }
  }
  const numericMatch = selector.match(/^(\d+)$/)
  if (numericMatch) {
    return { fromEnd: parseOffset(numericMatch[1]), label: selector }
  }
  throw new Error(
    `Invalid --from-history selector "${selector}". Use "last", "last:N", or "N" (N-th from end).`,
  )
}
const selectFromEnd = <T>(entries: readonly T[], fromEnd: number): T => {
  const index = entries.length - fromEnd
  if (index < 0 || index >= entries.length) {
    const noun = entries.length === 1 ? 'entry' : 'entries'
    throw new Error(
      `History selector is out of range. Requested ${fromEnd} from end but only ${entries.length} ${noun} available.`,
    )
  }
  const selected = entries[index]
  if (!selected) {
    throw new Error('Invariant violation: selected history entry is missing.')
  }
  return selected
}
export const loadGeneratePayloadFromHistory = async (options?: {
  selector?: string | undefined
  historyPath?: string | undefined
}): Promise<GenerateJsonPayload> => {
  const historyPath = options?.historyPath ?? resolveHistoryFilePath()
  const selector = parseFromHistorySelector(options?.selector)
  const entries = await readGenerateHistoryEntries(historyPath)
  const selected = selectFromEnd(entries, selector.fromEnd)
  if (selected.schemaVersion !== GENERATE_JSON_PAYLOAD_SCHEMA_VERSION) {
    throw new Error(
      `Unsupported history payload schemaVersion=${selected.schemaVersion} for selector "${selector.label}". ` +
        `This prompt-maker-cli supports schemaVersion=${GENERATE_JSON_PAYLOAD_SCHEMA_VERSION}; ` +
        'upgrade/downgrade prompt-maker-cli or regenerate the prompt to create a compatible history entry.',
    )
  }
  return validateGeneratePayloadObject(selected.raw, `history selector "${selector.label}"`)
}
export type LoadGenerateHistoryPickerItemsResult =
  | { ok: true; items: GenerateHistoryPickerItem[] }
  | { ok: false; errorMessage: string }
export const loadGenerateHistoryPickerItems = async (options?: {
  limit?: number
  historyPath?: string | undefined
}): Promise<LoadGenerateHistoryPickerItemsResult> => {
  const historyPath = options?.historyPath ?? resolveHistoryFilePath()
  const limit = options?.limit ?? 30
  let entries: GenerateHistoryEntry[]
  try {
    entries = await readGenerateHistoryEntries(historyPath)
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown history error.'
    return { ok: false, errorMessage: message }
  }
  const newestFirst = entries.slice(-limit).reverse()
  const items: GenerateHistoryPickerItem[] = newestFirst.map((entry, index) => {
    const selector = index === 0 ? 'last' : `last:${index + 1}`
    const timestamp = typeof entry.raw.timestamp === 'string' ? entry.raw.timestamp : ''
    const model = typeof entry.raw.model === 'string' ? entry.raw.model : 'unknown-model'
    const iterations =
      typeof entry.raw.iterations === 'number' && Number.isFinite(entry.raw.iterations)
        ? entry.raw.iterations
        : null
    const intent = typeof entry.raw.intent === 'string' ? entry.raw.intent : ''
    const supported = entry.schemaVersion === GENERATE_JSON_PAYLOAD_SCHEMA_VERSION
    const contextPaths = Array.isArray(entry.raw.contextPaths) ? entry.raw.contextPaths : []
    const fileCount = contextPaths.filter((candidate) => {
      if (!isRecord(candidate)) {
        return false
      }
      return candidate.source === 'file'
    }).length
    const titleParts = [timestamp ? formatTimestamp(timestamp) : 'unknown-time', model]
    const detailParts = [
      supported ? null : `schema:${entry.schemaVersion} (unsupported)`,
      `files:${fileCount}`,
      iterations === null ? null : `iters:${iterations}`,
      intent ? summarizeIntent(intent, 60) : '(missing intent)',
    ].filter((part): part is string => Boolean(part))
    return {
      selector,
      title: titleParts.join(' Âˇ '),
      detail: detailParts.join(' Âˇ '),
      schemaVersion: entry.schemaVersion,
      supported,
    }
  })
  return { ok: true, items }
}
</file>
<file path="src/prompt-generator/pdf-parts.ts">
import fs from 'node:fs/promises'
import type { PdfPart } from '@prompt-maker/core'
import { inferPdfMimeType, uploadFileForGeminiWithMimeType } from '../media-loader'
import type { UploadStateChange } from './types'
const assertReadablePdf = async (filePath: string): Promise<void> => {
  try {
    await fs.access(filePath)
  } catch (error) {
    const message = error instanceof Error ? error.message : String(error)
    throw new Error(`PDF file ${filePath} is not readable: ${message}`)
  }
}
export const resolvePdfParts = async (
  pdfPaths: string[],
  onUploadStateChange?: UploadStateChange,
  apiKey?: string,
): Promise<PdfPart[]> => {
  const parts: PdfPart[] = []
  for (const pdfPath of pdfPaths) {
    onUploadStateChange?.('start', { kind: 'pdf', filePath: pdfPath })
    try {
      const mimeType = inferPdfMimeType(pdfPath)
      if (apiKey) {
        const fileUri = await uploadFileForGeminiWithMimeType(pdfPath, mimeType, apiKey)
        parts.push({ type: 'pdf', mimeType, filePath: pdfPath, fileUri })
      } else {
        await assertReadablePdf(pdfPath)
        parts.push({ type: 'pdf', mimeType, filePath: pdfPath })
      }
    } catch (error) {
      const message = error instanceof Error ? error.message : String(error)
      throw new Error(`Failed to attach PDF ${pdfPath}: ${message}`)
    } finally {
      onUploadStateChange?.('finish', { kind: 'pdf', filePath: pdfPath })
    }
  }
  return parts
}
</file>
<file path="src/prompt-generator/prompts.ts">
/**
 * Large system prompts used by `PromptGeneratorService`.
 *
 * Kept separate to avoid mixing orchestration logic with long string constants.
 */
const PROMPT_CONTRACT_REQUIREMENTS = `
 Prompt Contract Requirements:
 1. Start with a concise "# Title" summarizing the requested deliverable.
 2. Include the following sections in order, each with actionable markdown content:
    "Role", "Context", "Goals & Tasks", "Inputs", "Constraints", "Execution Plan",
    "Output Format", "Quality Checks".
 3. Reference any provided context files or inputs explicitly when relevant (e.g., file paths).
 4. If the user provided attachments (images/videos/PDFs), assume the assistant executing this contract receives them as context.
    - Do NOT ask the user to re-upload, paste, or re-provide the attached document.
    - In "Inputs", explicitly state the attachment(s) are already provided and should be used directly.
    - Only ask for OCR/text *if* extraction is clearly impossible (e.g., scanned image-only PDF).
 5. Use bullet lists or short paragraphs; keep instructions concrete and testable.
 6. Do NOT execute the task or provide the final deliverableâonly craft instructions for another assistant.
 `
const META_PROMPT = `
You are an expert Prompt Engineer. Your goal is to convert the user's intent into an optimized prompt contract that another assistant will later execute.
${PROMPT_CONTRACT_REQUIREMENTS}
Response Format:
You must output a valid JSON object with exactly two keys:
1. "reasoning": A string containing your step-by-step analysis of the user's intent, missing details, and strategy.
2. "prompt": The final, polished prompt text (including all markdown formatting).
Do not output any text outside of this JSON object.
`
export const GEN_SYSTEM_PROMPT = META_PROMPT
export const REFINE_SYSTEM_PROMPT = `
You are an expert Prompt Engineer refining an existing prompt based on user feedback. The result must remain a prompt contract for another assistant, never the finished work.
${PROMPT_CONTRACT_REQUIREMENTS}
Response Format:
You must output a valid JSON object with exactly two keys:
1. "reasoning": A string explaining how you interpreted the refinement instructions and intent.
2. "prompt": The fully updated prompt text, preserving useful structure from the prior draft.
Do not output any text outside of this JSON object.
`
export const SERIES_SYSTEM_PROMPT = `
You are a Lead Architect Agent. Decompose the user's intent into a cohesive plan consisting of:
- One overview prompt that frames the entire effort.
- A sequence of atomic prompts that can be executed and tested independently.
Atomic Prompt Standards (non-negotiable):
- Standalone rule (critical): Every atomic prompt must be fully self-contained. Do NOT reference any other prompt, step number, or earlier/later content.
  - Forbidden examples include: "as above", "previous step", "prior step", "earlier step", "from step 2", "in step 3", "see step 1", "continue from step".
  - If a prompt depends on earlier work, express the dependency as "Expected Repo State" / "Prerequisites" using concrete artifacts (file paths, exported functions/types, UI elements), never by referencing another prompt.
  - Include a short re-entry check: "If this is already implemented, verify and skip to Validation".
- Single outcome: Each atomic prompt must target exactly one verifiable state change.
- Completeness: Each atomic prompt must include all context, assumptions, file paths, commands, and acceptance criteria needed to execute the step in a fresh session.
- Validation required: Each atomic prompt must end with a "Validation" section describing concrete commands + expected outcomes.
Required Atomic Prompt Structure (must appear in EACH atomic prompt content, in this order):
- # Title
- Role
- Context
- Goals & Tasks
- Inputs
- Constraints
- Execution Plan
- Output Format
- Validation
Return strict JSON matching this schema (do not wrap in markdown fences):
{
  "reasoning": string,
  "overviewPrompt": string,
  "atomicPrompts": [
    { "title": string, "content": string },
    { "title": string, "content": string }
  ]
}
Do not perform the work yourself. Only return the JSON payload described above.
`
export const SERIES_REPAIR_SYSTEM_PROMPT = `
You are a Prompt Repair Agent.
You will be given:
- The user's intent
- A previously generated SeriesResponse JSON payload
- A validation error describing what is non-compliant
Your task:
- Return a corrected SeriesResponse JSON payload (same schema) that passes validation.
- Preserve the overall plan and keep the number/order of atomicPrompts the same unless the validation error explicitly indicates the shape is invalid.
- Fix any missing required sections in atomic prompt content.
- Remove ALL cross-references between prompts. Do NOT mention any other step/prompt number.
  - If a prompt depends on earlier work, restate the dependency as "Expected Repo State" / "Prerequisites" using concrete artifacts (file paths, exported functions/types, UI elements), not step references.
  - Add a re-entry instruction (e.g., "If already implemented, verify and skip to Validation") inside the prompt content.
- Ensure each atomic prompt ends with a "Validation" section containing concrete checks.
Return strict JSON only. Do not wrap in markdown fences. Do not perform the work yourself.
`
</file>
<file path="src/prompt-generator/types.ts">
import type { FileContext } from '../file-context'
export type UploadState = 'start' | 'finish'
export type UploadDetail = { kind: 'image' | 'video' | 'pdf'; filePath: string }
export type UploadStateChange = (state: UploadState, detail: UploadDetail) => void
export type SeriesRepairAttemptDetail = {
  attempt: number
  maxAttempts: number
  validationError: string
}
export type PromptAutoRepairKind = 'pdf-grounding'
export type PromptAutoRepairAttemptDetail = {
  kind: PromptAutoRepairKind
  reason: 'missing-pdf-filename' | 'asked-for-document' | 'missing-document-snapshot'
  attempt: number
  maxAttempts: number
  pdfs: string[]
}
export type PromptGenerationRequest = {
  intent: string
  model: string
  targetModel: string
  fileContext: FileContext[]
  images: string[]
  videos: string[]
  pdfs?: string[]
  metaInstructions?: string
  previousPrompt?: string
  refinementInstruction?: string
  onUploadStateChange?: UploadStateChange
  onSeriesRepairAttempt?: (detail: SeriesRepairAttemptDetail) => void
  onPromptAutoRepairAttempt?: (detail: PromptAutoRepairAttemptDetail) => void
}
export type SeriesResponse = {
  reasoning: string
  overviewPrompt: string
  atomicPrompts: Array<{ title: string; content: string }>
}
export type PromptGenerationResult = {
  prompt: string
  reasoning?: string
}
</file>
<file path="src/prompt-generator/video-parts.ts">
import type { VideoPart } from '@prompt-maker/core'
import { inferVideoMimeType, uploadFileForGemini } from '../media-loader'
import type { UploadStateChange } from './types'
export const resolveVideoParts = async (
  videoPaths: string[],
  onUploadStateChange?: UploadStateChange,
  apiKey?: string,
): Promise<VideoPart[]> => {
  const parts: VideoPart[] = []
  for (const videoPath of videoPaths) {
    onUploadStateChange?.('start', { kind: 'video', filePath: videoPath })
    try {
      const fileUri =
        apiKey === undefined
          ? await uploadFileForGemini(videoPath)
          : await uploadFileForGemini(videoPath, apiKey)
      const mimeType = inferVideoMimeType(videoPath)
      parts.push({ type: 'video_uri', fileUri, mimeType })
    } catch (error) {
      const message = error instanceof Error ? error.message : 'Unknown video upload error.'
      console.warn(`Failed to upload video ${videoPath}: ${message}`)
    } finally {
      onUploadStateChange?.('finish', { kind: 'video', filePath: videoPath })
    }
  }
  return parts
}
</file>
<file path="src/tui/components/core/command-menu-keymap.ts">
import type { Key } from 'ink'
export type CommandMenuKeyAction =
  | { type: 'none' }
  | { type: 'close' }
  | { type: 'change-selection'; nextIndex: number }
export type ResolveCommandMenuKeyActionOptions = {
  key: Key
  selectedIndex: number
  itemCount: number
}
export const resolveCommandMenuKeyAction = ({
  key,
  selectedIndex,
  itemCount,
}: ResolveCommandMenuKeyActionOptions): CommandMenuKeyAction => {
  if (itemCount <= 0) {
    return { type: 'none' }
  }
  if (key.escape) {
    return { type: 'close' }
  }
  const clampedIndex = Math.min(Math.max(selectedIndex, 0), itemCount - 1)
  if (key.upArrow) {
    const nextIndex = clampedIndex === 0 ? itemCount - 1 : clampedIndex - 1
    return { type: 'change-selection', nextIndex }
  }
  if (key.downArrow) {
    const nextIndex = clampedIndex === itemCount - 1 ? 0 : clampedIndex + 1
    return { type: 'change-selection', nextIndex }
  }
  return { type: 'none' }
}
</file>
<file path="src/tui/components/core/MultilineTextInput.tsx">
import React, { useEffect, useMemo, useRef, useState } from 'react'
import { Box, Text, useInput, useStdout, type Key } from 'ink'
import {
  backspace,
  clampCursor,
  deleteForward,
  insertText,
  moveCursorLeft,
  moveCursorRight,
  type MultilineTextBufferState,
} from './multiline-text-buffer'
import { softWrapLine, getSoftWrappedCursorOffset } from './soft-wrap'
import { isBackspaceKey } from './text-input-keys'
import {
  expandTokenizedLines,
  getTokenizedCursorCoordinates,
  type TokenLabelLookup,
} from './tokenized-text'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps, type InkColorValue } from '../../theme/theme-types'
export type DebugKeyEvent = {
  input: string
  key: Key
}
export type MultilineTextInputGutter = {
  glyph: string
  color: InkColorValue
  spacer?: number | undefined
}
export type MultilineTextInputProps = {
  value: string
  onChange: (next: string) => void
  onSubmit: (value: string) => void
  placeholder?: string | undefined
  focus?: boolean
  isDisabled?: boolean
  isPasteActive?: boolean
  tokenLabel?: TokenLabelLookup | undefined
  onDebugKeyEvent?: ((event: DebugKeyEvent) => void) | undefined
  gutter?: MultilineTextInputGutter | undefined
  // Optional rendering constraints (useful for input bars that must paint their full width).
  width?: number | undefined
  backgroundColor?: InkColorValue
}
const PROMPT = 'âş '
const PROMPT_SPACER = '  '
type RenderLine = {
  id: string
  content: string
  isPlaceholder: boolean
}
type WrappedLayout = {
  readonly lines: readonly RenderLine[]
  readonly cursorRow: number
  readonly cursorColumn: number
}
const toHardLines = (
  value: string,
  placeholder: string | undefined,
  tokenLabel: TokenLabelLookup,
): readonly RenderLine[] => {
  if (!value) {
    return [{ id: 'placeholder', content: placeholder ?? '', isPlaceholder: true }]
  }
  const lines = expandTokenizedLines(value, tokenLabel)
  return lines.map((line, index) => ({ id: `line-${index}`, content: line, isPlaceholder: false }))
}
const normalizeColumns = (columns: number): number => {
  if (!Number.isFinite(columns)) {
    return 0
  }
  return Math.max(0, Math.floor(columns))
}
export const MultilineTextInput: React.FC<MultilineTextInputProps> = ({
  value,
  onChange,
  onSubmit,
  placeholder,
  focus = false,
  isDisabled = false,
  isPasteActive = false,
  tokenLabel,
  onDebugKeyEvent,
  gutter,
  width,
  backgroundColor,
}) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const [cursor, setCursor] = useState<number>(value.length)
  const internalUpdateRef = useRef(false)
  const state: MultilineTextBufferState = useMemo(
    () => ({ value, cursor: clampCursor(cursor, value) }),
    [cursor, value],
  )
  const applyNextState = (nextState: MultilineTextBufferState): void => {
    internalUpdateRef.current = true
    setCursor(nextState.cursor)
    onChange(nextState.value)
  }
  useEffect(() => {
    if (internalUpdateRef.current) {
      internalUpdateRef.current = false
      return
    }
    setCursor(value.length)
  }, [value])
  useInput(
    (input, key) => {
      if (!focus || isDisabled || isPasteActive) {
        return
      }
      if (onDebugKeyEvent) {
        onDebugKeyEvent({ input, key })
      }
      const isCtrlJ = key.ctrl && input.toLowerCase() === 'j'
      const isAltEnter =
        key.meta && (key.return || input === '\r' || input === '\n' || input === '')
      const isEscapedAltEnter = input === '\u001b\r' || input === '\u001b\n'
      if (isCtrlJ || isAltEnter || isEscapedAltEnter) {
        applyNextState(insertText(state, '\n'))
        return
      }
      if (key.return) {
        onSubmit(value)
        return
      }
      if (isBackspaceKey(input, key)) {
        applyNextState(backspace(state))
        return
      }
      if (key.delete) {
        applyNextState(deleteForward(state))
        return
      }
      if (key.leftArrow) {
        setCursor(moveCursorLeft(state).cursor)
        return
      }
      if (key.rightArrow) {
        setCursor(moveCursorRight(state).cursor)
        return
      }
      if (!input) {
        return
      }
      if (key.ctrl || key.meta) {
        return
      }
      applyNextState(insertText(state, input))
    },
    { isActive: focus && !isDisabled },
  )
  const resolvedTokenLabel = useMemo<TokenLabelLookup>(
    () => tokenLabel ?? (() => null),
    [tokenLabel],
  )
  const hardLines = useMemo(
    () => toHardLines(value, placeholder, resolvedTokenLabel),
    [placeholder, resolvedTokenLabel, value],
  )
  const tokenizedCursor = useMemo(
    () => getTokenizedCursorCoordinates(value, cursor, resolvedTokenLabel),
    [cursor, resolvedTokenLabel, value],
  )
  const gutterSpacer = gutter?.spacer ?? 0
  const safeSpacer = Number.isFinite(gutterSpacer) ? Math.max(0, Math.floor(gutterSpacer)) : 0
  const gutterColumns = gutter ? gutter.glyph.length + safeSpacer : 0
  const totalColumns = useMemo(() => {
    if (typeof width === 'number') {
      return normalizeColumns(width)
    }
    return normalizeColumns(stdout?.columns ?? 80)
  }, [stdout?.columns, width])
  const wrapped = useMemo<WrappedLayout>(() => {
    const lines: RenderLine[] = []
    let cursorRow = 0
    let cursorColumn = 0
    for (let hardIndex = 0; hardIndex < hardLines.length; hardIndex += 1) {
      const hardLine = hardLines[hardIndex]
      if (!hardLine) {
        continue
      }
      const hardLineStart = lines.length
      const isFirstHardLine = hardIndex === 0
      const firstPrefixColumns = isFirstHardLine ? PROMPT.length : PROMPT_SPACER.length
      const firstWrapWidth = Math.max(1, totalColumns - gutterColumns - firstPrefixColumns)
      const restWrapWidth = Math.max(1, totalColumns - gutterColumns - PROMPT_SPACER.length)
      const wrappedHardLine = softWrapLine(hardLine.content, {
        first: firstWrapWidth,
        rest: restWrapWidth,
      })
      const isCursorHardLine = hardIndex === tokenizedCursor.row
      const cursorOffset = isCursorHardLine
        ? getSoftWrappedCursorOffset(wrappedHardLine, tokenizedCursor.column)
        : null
      if (isCursorHardLine && cursorOffset) {
        cursorRow = hardLineStart + cursorOffset.rowOffset
        cursorColumn = cursorOffset.column
      }
      for (
        let segmentIndex = 0;
        segmentIndex < wrappedHardLine.segments.length;
        segmentIndex += 1
      ) {
        const segment = wrappedHardLine.segments[segmentIndex]
        if (segment === undefined) {
          continue
        }
        lines.push({
          id: `${hardLine.id}-seg-${segmentIndex}`,
          content: segment,
          isPlaceholder: hardLine.isPlaceholder,
        })
      }
      if (isCursorHardLine && cursorOffset?.needsTrailingEmptyLine) {
        lines.push({
          id: `${hardLine.id}-seg-${wrappedHardLine.segments.length}`,
          content: '',
          isPlaceholder: hardLine.isPlaceholder,
        })
      }
    }
    if (lines.length === 0) {
      return {
        lines: [{ id: 'empty', content: '', isPlaceholder: true }],
        cursorRow: 0,
        cursorColumn: 0,
      }
    }
    const safeCursorRow = Math.max(0, Math.min(cursorRow, lines.length - 1))
    const cursorLine = lines[safeCursorRow]
    const safeCursorColumn = cursorLine
      ? Math.max(0, Math.min(cursorColumn, cursorLine.content.length))
      : 0
    return {
      lines,
      cursorRow: safeCursorRow,
      cursorColumn: safeCursorColumn,
    }
  }, [gutterColumns, hardLines, tokenizedCursor.column, tokenizedCursor.row, totalColumns])
  const backgroundProps = inkBackgroundColorProps(backgroundColor)
  return (
    <Box flexDirection="column" height={wrapped.lines.length}>
      {wrapped.lines.map((line, lineIndex) => {
        const isCursorLine = lineIndex === wrapped.cursorRow
        const safeColumn = isCursorLine ? Math.min(wrapped.cursorColumn, line.content.length) : 0
        const before = isCursorLine ? line.content.slice(0, safeColumn) : line.content
        const cursorCharacter = isCursorLine
          ? safeColumn < line.content.length
            ? line.content.charAt(safeColumn)
            : ' '
          : ''
        const after =
          isCursorLine && safeColumn < line.content.length ? line.content.slice(safeColumn + 1) : ''
        const prefix = lineIndex === 0 ? PROMPT : PROMPT_SPACER
        const lineColorProps = line.isPlaceholder ? inkColorProps(theme.mutedText) : {}
        const spacerText = safeSpacer > 0 ? ' '.repeat(safeSpacer) : ''
        const renderedColumns = isCursorLine
          ? before.length + cursorCharacter.length + after.length
          : before.length
        const usedColumns = gutterColumns + prefix.length + renderedColumns
        const fillerColumns =
          typeof width === 'number' && width > usedColumns ? width - usedColumns : 0
        const filler = fillerColumns > 0 ? ' '.repeat(fillerColumns) : ''
        return (
          <Box key={line.id}>
            {gutter ? (
              <>
                <Text {...backgroundProps} {...inkColorProps(gutter.color)}>
                  {gutter.glyph}
                </Text>
                {spacerText ? <Text {...backgroundProps}>{spacerText}</Text> : null}
              </>
            ) : null}
            <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
              {prefix}
            </Text>
            {isCursorLine ? (
              <>
                <Text {...backgroundProps} {...lineColorProps}>
                  {before}
                </Text>
                <Text inverse {...backgroundProps} {...lineColorProps}>
                  {cursorCharacter}
                </Text>
                <Text {...backgroundProps} {...lineColorProps}>
                  {after}
                </Text>
              </>
            ) : (
              <Text {...backgroundProps} {...lineColorProps}>
                {before}
              </Text>
            )}
            {filler ? <Text {...backgroundProps}>{filler}</Text> : null}
          </Box>
        )
      })}
    </Box>
  )
}
</file>
<file path="src/tui/components/core/ScrollableOutput.tsx">
import { memo, useMemo } from 'react'
import { Box, Text } from 'ink'
import type { HistoryEntry } from '../../types'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps, type InkColorValue } from '../../theme/theme-types'
import {
  DEFAULT_MARKDOWN_STATE,
  resolveMarkdownSlotColor,
  tokenizeMarkdownLine,
} from '../../markdown/markdown-highlight'
const padRight = (value: string, width: number | undefined): string => {
  if (typeof width !== 'number' || width <= 0) {
    return value
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type ScrollableOutputProps = {
  lines: readonly HistoryEntry[]
  visibleRows: number
  scrollOffset: number
  contentWidth?: number | undefined
  backgroundColor?: InkColorValue
}
export const ScrollableOutput = memo(
  ({ lines, visibleRows, scrollOffset, contentWidth, backgroundColor }: ScrollableOutputProps) => {
    const { theme } = useTheme()
    const startIndex = Math.max(0, Math.min(scrollOffset, Math.max(0, lines.length - visibleRows)))
    const endIndex = Math.min(lines.length, startIndex + visibleRows)
    const visibleLines = useMemo(
      () => lines.slice(startIndex, endIndex),
      [lines, startIndex, endIndex],
    )
    const backgroundProps = inkBackgroundColorProps(backgroundColor)
    const padded = useMemo(() => {
      const next: Array<HistoryEntry | null> = [...visibleLines]
      while (next.length < visibleRows) {
        next.push(null)
      }
      return next
    }, [visibleLines, visibleRows])
    const markdownStartState = useMemo(() => {
      let state = DEFAULT_MARKDOWN_STATE
      for (let i = 0; i < startIndex; i += 1) {
        const entry = lines[i]
        if (!entry || entry.format !== 'markdown') {
          state = DEFAULT_MARKDOWN_STATE
          continue
        }
        if (/^\s*```/.test(entry.content)) {
          state = { inCodeBlock: !state.inCodeBlock }
        }
      }
      return state
    }, [lines, startIndex])
    const decorated = useMemo(() => {
      let state = markdownStartState
      return padded.map((entry) => {
        if (!entry) {
          state = DEFAULT_MARKDOWN_STATE
          return { entry, content: '', spans: null }
        }
        const content =
          typeof contentWidth === 'number' && contentWidth > 0
            ? entry.content.slice(0, contentWidth)
            : entry.content
        if (entry.format === 'markdown') {
          const tokenized = tokenizeMarkdownLine(content, state)
          state = tokenized.nextState
          return { entry, content, spans: tokenized.spans }
        }
        state = DEFAULT_MARKDOWN_STATE
        return { entry, content, spans: null }
      })
    }, [contentWidth, markdownStartState, padded])
    return (
      <Box flexDirection="column" height={visibleRows} overflow="hidden">
        {decorated.map((row, index) => {
          if (!row.entry) {
            return (
              <Text key={`blank-${startIndex + index}`} {...backgroundProps}>
                {padRight('', contentWidth)}
              </Text>
            )
          }
          const key = `${row.entry.id}-${startIndex + index}`
          if (row.entry.format === 'markdown' && row.spans) {
            const paddingLength =
              typeof contentWidth === 'number' && contentWidth > 0
                ? Math.max(0, contentWidth - row.content.length)
                : 0
            return (
              <Text key={key} {...backgroundProps}>
                {row.spans.map((span, spanIndex) => (
                  <Text
                    key={`${key}-span-${spanIndex}`}
                    {...inkColorProps(resolveMarkdownSlotColor(theme, span.slot))}
                    {...(span.bold ? { bold: true } : {})}
                    {...(span.italic ? { italic: true } : {})}
                    {...(span.underline ? { underline: true } : {})}
                  >
                    {span.text}
                  </Text>
                ))}
                {paddingLength > 0 ? ' '.repeat(paddingLength) : null}
              </Text>
            )
          }
          const color =
            row.entry.kind === 'user'
              ? theme.accent
              : row.entry.kind === 'progress'
                ? theme.warning
                : theme.text
          return (
            <Text key={key} {...backgroundProps} {...inkColorProps(color)}>
              {padRight(row.entry.content, contentWidth)}
            </Text>
          )
        })}
      </Box>
    )
  },
)
ScrollableOutput.displayName = 'ScrollableOutput'
</file>
<file path="src/tui/components/core/soft-wrap.ts">
export type SoftWrapWidths = {
  readonly first: number
  readonly rest: number
}
export type SoftWrappedLine = {
  readonly segments: readonly string[]
  readonly segmentStarts: readonly number[]
  readonly segmentWidths: readonly number[]
}
export type SoftWrapCursorOffset = {
  readonly rowOffset: number
  readonly column: number
  readonly needsTrailingEmptyLine: boolean
}
const normalizeWrapWidth = (width: number): number => {
  if (!Number.isFinite(width)) {
    return 1
  }
  return Math.max(1, Math.floor(width))
}
export const softWrapLine = (displayLine: string, widths: SoftWrapWidths): SoftWrappedLine => {
  const firstWidth = normalizeWrapWidth(widths.first)
  const restWidth = normalizeWrapWidth(widths.rest)
  const segments: string[] = []
  const segmentStarts: number[] = []
  const segmentWidths: number[] = []
  if (!displayLine) {
    segments.push('')
    segmentStarts.push(0)
    segmentWidths.push(firstWidth)
    return { segments, segmentStarts, segmentWidths }
  }
  const whitespacePattern = /\s/
  let offset = 0
  let segmentIndex = 0
  while (offset < displayLine.length) {
    const segmentWidth = segmentIndex === 0 ? firstWidth : restWidth
    const window = displayLine.slice(offset, offset + segmentWidth)
    let breakIndex = window.length
    // Prefer breaking after the last whitespace that fits in the window.
    // Keep the whitespace on the previous segment to avoid trimming and to
    // preserve cursor-to-display mappings.
    const hasMoreContent = offset + window.length < displayLine.length
    if (hasMoreContent && window.length === segmentWidth) {
      for (let index = window.length - 1; index >= 0; index -= 1) {
        const character = window.charAt(index)
        if (whitespacePattern.test(character)) {
          breakIndex = index + 1
          break
        }
      }
    }
    const next = window.slice(0, breakIndex)
    segments.push(next)
    segmentStarts.push(offset)
    segmentWidths.push(segmentWidth)
    offset += next.length
    segmentIndex += 1
  }
  return { segments, segmentStarts, segmentWidths }
}
export const getSoftWrappedCursorOffset = (
  wrapped: SoftWrappedLine,
  displayColumn: number,
): SoftWrapCursorOffset => {
  const safeColumn = Math.max(0, Math.floor(displayColumn))
  const totalLength = wrapped.segments.reduce((sum, segment) => sum + segment.length, 0)
  const clampedColumn = Math.min(safeColumn, totalLength)
  if (totalLength === 0) {
    return { rowOffset: 0, column: 0, needsTrailingEmptyLine: false }
  }
  // Cursor is inside the rendered characters.
  if (clampedColumn < totalLength) {
    for (let index = 0; index < wrapped.segments.length; index += 1) {
      const segment = wrapped.segments[index]
      const segmentStart = wrapped.segmentStarts[index]
      if (segment === undefined || segmentStart === undefined) {
        continue
      }
      const segmentEnd = segmentStart + segment.length
      if (clampedColumn < segmentEnd) {
        return {
          rowOffset: index,
          column: clampedColumn - segmentStart,
          needsTrailingEmptyLine: false,
        }
      }
    }
    // Fallback to last segment if something unexpected happens.
    const lastIndex = Math.max(0, wrapped.segments.length - 1)
    const lastStart = wrapped.segmentStarts[lastIndex] ?? 0
    return {
      rowOffset: lastIndex,
      column: Math.max(0, clampedColumn - lastStart),
      needsTrailingEmptyLine: false,
    }
  }
  // Cursor is at end-of-line: place it after the last character.
  const lastIndex = Math.max(0, wrapped.segments.length - 1)
  const lastSegment = wrapped.segments[lastIndex] ?? ''
  const lastWidth = wrapped.segmentWidths[lastIndex] ?? 1
  if (lastSegment.length >= lastWidth) {
    // The last segment has no remaining cells to render the cursor.
    return { rowOffset: wrapped.segments.length, column: 0, needsTrailingEmptyLine: true }
  }
  return { rowOffset: lastIndex, column: lastSegment.length, needsTrailingEmptyLine: false }
}
</file>
<file path="src/tui/components/core/status-indicators-layout.ts">
import { TOGGLE_LABELS } from '../../config'
import type { ModelProvider } from '../../../model-providers'
export type IndicatorStyle = 'primary' | 'muted' | 'success' | 'warning' | 'danger'
export type IndicatorSegment = {
  id: string
  label: string
  value: string
  style: IndicatorStyle
  raw: string
}
export type IndicatorLine = {
  segments: readonly IndicatorSegment[]
}
const PROVIDER_KEYS: ReadonlySet<ModelProvider> = new Set(['openai', 'gemini', 'other'])
const normalizeChipBody = (raw: string): string | null => {
  const trimmed = raw.trim()
  if (!trimmed.startsWith('[') || !trimmed.endsWith(']')) {
    return null
  }
  return trimmed.slice(1, -1)
}
type ParsedChip = {
  key: string
  value: string
  raw: string
}
const parseIndicatorChip = (raw: string): ParsedChip | null => {
  const body = normalizeChipBody(raw)
  if (!body) {
    return null
  }
  const separatorIndex = body.indexOf(':')
  if (separatorIndex === -1) {
    return { key: 'model', value: body, raw }
  }
  const key = body.slice(0, separatorIndex).trim()
  const value = body.slice(separatorIndex + 1).trim()
  if (!key) {
    return null
  }
  return { key, value, raw }
}
const toTitleCase = (value: string): string => {
  if (!value) {
    return value
  }
  return `${value[0]?.toUpperCase() ?? ''}${value.slice(1)}`
}
const resolveToggleLabel = (key: string): string | null => {
  if (key === 'smart') {
    return 'Smart'
  }
  const label = (TOGGLE_LABELS as Record<string, string>)[key]
  return label ?? null
}
const resolveSegmentLabel = (chip: ParsedChip): string => {
  if (chip.key === 'model') {
    return 'Model'
  }
  if (chip.key === 'polish') {
    return 'Polish'
  }
  if (chip.key === 'target') {
    return 'Target'
  }
  if (chip.key === 'instr') {
    return 'Meta'
  }
  const toggleLabel = resolveToggleLabel(chip.key)
  if (toggleLabel) {
    return toggleLabel
  }
  if (chip.key === 'openai') {
    return 'OpenAI'
  }
  if (chip.key === 'urls') {
    return 'URLs'
  }
  if (PROVIDER_KEYS.has(chip.key as ModelProvider)) {
    return toTitleCase(chip.key)
  }
  return toTitleCase(chip.key)
}
const resolveSegmentStyle = (chip: ParsedChip): IndicatorStyle => {
  if (
    chip.key === 'status' ||
    chip.key === 'model' ||
    chip.key === 'polish' ||
    chip.key === 'target'
  ) {
    return 'primary'
  }
  if (chip.key === 'tokens') {
    return 'primary'
  }
  if (PROVIDER_KEYS.has(chip.key as ModelProvider)) {
    const suffix = chip.value.toLowerCase()
    if (suffix === 'ok') {
      return 'success'
    }
    if (suffix === 'missing-key' || suffix === 'missing') {
      return 'warning'
    }
    if (suffix === 'error') {
      return 'danger'
    }
    return 'muted'
  }
  if (chip.key === 'files' || chip.key === 'urls') {
    const count = Number.parseInt(chip.value, 10)
    return Number.isFinite(count) && count > 0 ? 'primary' : 'muted'
  }
  const toggleLabel = resolveToggleLabel(chip.key)
  if (toggleLabel) {
    return chip.value.toLowerCase() === 'on' ? 'primary' : 'muted'
  }
  if (chip.key === 'intent') {
    return chip.value.toLowerCase() === 'file' ? 'primary' : 'muted'
  }
  if (chip.key === 'tests') {
    return chip.value.toLowerCase() === 'running' ? 'warning' : 'muted'
  }
  return 'muted'
}
const resolveSortWeight = (segment: IndicatorSegment): number => {
  switch (segment.label) {
    case 'Status':
      return 0
    case 'Model':
      return 1
    case 'Polish':
      return 1
    case 'Target':
      return 1
    case 'OpenAI':
    case 'Gemini':
    case 'Other':
      return 2
    case 'Tokens':
      return 3
    case 'Intent':
      return 4
    case 'File':
      return 5
    case 'Meta':
      return 6
    case 'Root':
      return 7
    case 'Files':
      return 8
    case 'URLs':
      return 9
    default:
      return 10
  }
}
export const resolveIndicatorSegments = (chips: readonly string[]): IndicatorSegment[] => {
  const segments: IndicatorSegment[] = []
  for (const raw of chips) {
    const parsed = parseIndicatorChip(raw)
    if (!parsed) {
      continue
    }
    const label = resolveSegmentLabel(parsed)
    segments.push({
      id: `${parsed.key}-${segments.length}`,
      label,
      value: parsed.value,
      style: resolveSegmentStyle(parsed),
      raw,
    })
  }
  return segments.sort((left, right) => {
    const weightDiff = resolveSortWeight(left) - resolveSortWeight(right)
    return weightDiff !== 0 ? weightDiff : left.label.localeCompare(right.label)
  })
}
export const formatIndicatorSegmentPlain = (segment: IndicatorSegment): string =>
  `${segment.label}: ${segment.value}`
export const formatIndicatorLines = (params: {
  chips: readonly string[]
  maxWidth: number
}): IndicatorLine[] => {
  const maxWidth = Math.max(16, Math.floor(params.maxWidth))
  const segments = resolveIndicatorSegments(params.chips)
  const lines: IndicatorSegment[][] = []
  let current: IndicatorSegment[] = []
  let currentLength = 0
  const separator = ' Âˇ '
  const pushLine = (): void => {
    if (current.length === 0) {
      return
    }
    lines.push(current)
    current = []
    currentLength = 0
  }
  for (const segment of segments) {
    const token = formatIndicatorSegmentPlain(segment)
    const tokenLength = token.length
    if (current.length === 0) {
      current = [segment]
      currentLength = tokenLength
      continue
    }
    const nextLength = currentLength + separator.length + tokenLength
    if (nextLength <= maxWidth) {
      current.push(segment)
      currentLength = nextLength
      continue
    }
    pushLine()
    current = [segment]
    currentLength = tokenLength
  }
  pushLine()
  return lines.map((segmentsForLine) => ({ segments: segmentsForLine }))
}
</file>
<file path="src/tui/components/core/Toast.tsx">
import { Box, Text, useStdout } from 'ink'
import type { ToastKind } from '../../notifier'
import { TOAST_HEIGHT } from '../../toast-constants'
import { useTheme } from '../../theme/theme-provider'
import {
  inkBackgroundColorProps,
  inkBorderColorProps,
  inkColorProps,
} from '../../theme/theme-types'
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type ToastProps = {
  message: string
  kind: ToastKind
}
type ToastChromeTone = 'default' | 'warning' | 'error'
type ToastChrome = {
  borderTone: ToastChromeTone
  titleTone: ToastChromeTone
  title: string
}
const toastChrome = (kind: ToastKind): ToastChrome => {
  switch (kind) {
    case 'info':
      return { borderTone: 'default', titleTone: 'default', title: 'Notice' }
    case 'progress':
      return { borderTone: 'warning', titleTone: 'warning', title: 'Working' }
    case 'warning':
      return { borderTone: 'warning', titleTone: 'warning', title: 'Warning' }
    case 'error':
      return { borderTone: 'error', titleTone: 'error', title: 'Error' }
    default: {
      const exhaustive: never = kind
      return exhaustive
    }
  }
}
export { TOAST_HEIGHT }
export const Toast = ({ message, kind }: ToastProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const chrome = toastChrome(kind)
  // Ink doesn't paint "empty" cells when rendering overlapping/absolute layers.
  // To keep the toast opaque, we explicitly pad each content line to the
  // available inner width so it prints background-colored spaces.
  const terminalColumns = stdout?.columns ?? 80
  const toastWidth = terminalColumns
  const borderColumns = 2
  const paddingColumns = 2
  const contentWidth = Math.max(0, toastWidth - borderColumns - paddingColumns)
  const borderColor =
    chrome.borderTone === 'warning'
      ? theme.warning
      : chrome.borderTone === 'error'
        ? theme.error
        : theme.border
  const titleColor =
    chrome.titleTone === 'warning'
      ? theme.warning
      : chrome.titleTone === 'error'
        ? theme.error
        : theme.mutedText
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  return (
    <Box
      flexDirection="column"
      borderStyle="round"
      paddingX={1}
      paddingY={0}
      height={TOAST_HEIGHT}
      width={toastWidth}
      overflow="hidden"
      {...inkBorderColorProps(borderColor)}
      {...backgroundProps}
    >
      <Text {...backgroundProps} {...inkColorProps(titleColor)}>
        {padRight(chrome.title, contentWidth)}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.text)}>
        {padRight(message, contentWidth)}
      </Text>
    </Box>
  )
}
</file>
<file path="src/tui/components/core/ToastOverlay.tsx">
import { Box } from 'ink'
import React, { memo, useEffect } from 'react'
import {
  TOAST_ANIMATION_TICK_MS,
  TOAST_HEIGHT,
  TOAST_HORIZONTAL_INSET_COLUMNS,
  TOAST_TOP_OFFSET_ROWS,
} from '../../toast-constants'
import type { ToastId, ToastItem } from '../../notifier'
import { useToastContext } from '../../notifier'
import { Toast } from './Toast'
const useAnimatedInt = (targetValue: number, initialValue: number) => {
  const [value, setValue] = React.useState(initialValue)
  useEffect(() => {
    if (value === targetValue) {
      return
    }
    const timer = setTimeout(() => {
      setValue((prev) => {
        if (prev === targetValue) {
          return prev
        }
        const direction = prev < targetValue ? 1 : -1
        return prev + direction
      })
    }, TOAST_ANIMATION_TICK_MS)
    return () => {
      clearTimeout(timer)
    }
  }, [targetValue, value])
  return {
    value,
    isComplete: value === targetValue,
  }
}
type ToastOverlayItemProps = {
  toast: ToastItem
  onExitComplete: (id: ToastId) => void
}
const ToastOverlayItem = ({ toast, onExitComplete }: ToastOverlayItemProps) => {
  const targetHeight = toast.isExiting ? 0 : TOAST_HEIGHT
  const { value: height, isComplete } = useAnimatedInt(targetHeight, 0)
  useEffect(() => {
    if (!toast.isExiting) {
      return
    }
    if (!isComplete) {
      return
    }
    onExitComplete(toast.id)
  }, [isComplete, onExitComplete, toast.id, toast.isExiting])
  if (height === 0 && toast.isExiting) {
    return null
  }
  return (
    <Box height={height} overflow="hidden">
      <Toast message={toast.message} kind={toast.kind} />
    </Box>
  )
}
export const ToastOverlay = memo(() => {
  const { toasts, removeToast } = useToastContext()
  return (
    <Box
      position="absolute"
      width="100%"
      height="100%"
      flexDirection="column"
      justifyContent="flex-start"
      alignItems="flex-start"
      paddingX={TOAST_HORIZONTAL_INSET_COLUMNS}
      paddingTop={TOAST_TOP_OFFSET_ROWS}
    >
      {toasts.map((toast) => (
        <ToastOverlayItem key={toast.id} toast={toast} onExitComplete={removeToast} />
      ))}
    </Box>
  )
})
ToastOverlay.displayName = 'ToastOverlay'
</file>
<file path="src/tui/components/popups/BudgetsPopup.tsx">
import { Box, Text, useStdout } from 'ink'
import type { ContextOverflowStrategy } from '../../../config'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
const STRATEGY_OPTIONS = [
  '',
  'fail',
  'drop-smart',
  'drop-url',
  'drop-largest',
  'drop-oldest',
] as const satisfies ReadonlyArray<ContextOverflowStrategy | ''>
const formatStrategy = (strategy: ContextOverflowStrategy | ''): string =>
  strategy ? strategy : '(unset)'
export type BudgetsPopupProps = {
  selectionIndex: number
  maxContextTokensDraft: string
  maxInputTokensDraft: string
  contextOverflowStrategyDraft: ContextOverflowStrategy | ''
  errorMessage: string | null
  onMaxContextTokensChange: (next: string) => void
  onMaxInputTokensChange: (next: string) => void
  onSubmit: () => void
}
export const BudgetsPopup = ({
  selectionIndex,
  maxContextTokensDraft,
  maxInputTokensDraft,
  contextOverflowStrategyDraft,
  errorMessage,
  onMaxContextTokensChange,
  onMaxInputTokensChange,
  onSubmit,
}: BudgetsPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 44, 80)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const popupHeight = 16 + (errorMessage ? 1 : 0)
  const activeRowProps = {
    ...inkColorProps(theme.selectionText),
    ...inkBackgroundColorProps(theme.selectionBackground),
  }
  const inactiveRowProps = { ...backgroundProps, ...inkColorProps(theme.text) }
  const normalizedSelection = clamp(selectionIndex, 0, 2)
  const budgetsEnabled = Boolean(maxContextTokensDraft.trim() || maxInputTokensDraft.trim())
  const effectiveStrategy = budgetsEnabled
    ? contextOverflowStrategyDraft || 'fail'
    : contextOverflowStrategyDraft
  const effectiveLine = budgetsEnabled
    ? `Effective: input=${maxInputTokensDraft.trim() || 'unset'} Âˇ context=${maxContextTokensDraft.trim() || 'unset'} Âˇ overflow=${formatStrategy(effectiveStrategy)}`
    : `Effective: budgets disabled Âˇ overflow=${formatStrategy(effectiveStrategy)}`
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Token Budgets', contentWidth)}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('â/â select Âˇ Enter apply Âˇ â/â change strategy Âˇ Esc close', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Box flexDirection="row">
          <Text {...(normalizedSelection === 0 ? activeRowProps : inactiveRowProps)}>
            {padRight('Max context tokens:', 18)}
          </Text>
          <SingleLineTextInput
            value={maxContextTokensDraft}
            onChange={onMaxContextTokensChange}
            onSubmit={() => onSubmit()}
            placeholder="unset"
            focus={normalizedSelection === 0}
            width={Math.max(1, contentWidth - 18)}
            backgroundColor={theme.popupBackground}
          />
        </Box>
        <Box flexDirection="row">
          <Text {...(normalizedSelection === 1 ? activeRowProps : inactiveRowProps)}>
            {padRight('Max input tokens:', 18)}
          </Text>
          <SingleLineTextInput
            value={maxInputTokensDraft}
            onChange={onMaxInputTokensChange}
            onSubmit={() => onSubmit()}
            placeholder="unset"
            focus={normalizedSelection === 1}
            width={Math.max(1, contentWidth - 18)}
            backgroundColor={theme.popupBackground}
          />
        </Box>
        <Box flexDirection="row">
          <Text {...(normalizedSelection === 2 ? activeRowProps : inactiveRowProps)}>
            {padRight('Overflow strategy:', 18)}
          </Text>
          <Text {...(normalizedSelection === 2 ? activeRowProps : inactiveRowProps)}>
            {padRight(formatStrategy(contextOverflowStrategyDraft), contentWidth - 18)}
          </Text>
        </Box>
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight(effectiveLine, contentWidth)}
      </Text>
      {errorMessage ? (
        <Text {...backgroundProps} {...inkColorProps(theme.error)}>
          {padRight(errorMessage, contentWidth)}
        </Text>
      ) : null}
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Tip: clearing both token fields disables budgets.', contentWidth)}
      </Text>
    </PopupSheet>
  )
}
export const BUDGET_STRATEGY_OPTIONS = STRATEGY_OPTIONS
</file>
<file path="src/tui/components/popups/ExportPopup.tsx">
import { Box, Text, useStdout } from 'ink'
import type { ExportHistoryItem } from '../../types'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, Math.max(0, width - 1)).concat('âŚ') : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
const joinItemLine = (item: ExportHistoryItem): string => `${item.title} Âˇ ${item.detail}`
export type ExportPopupProps = {
  selectionIndex: number
  format: 'json' | 'yaml'
  outPathDraft: string
  historyItems: ExportHistoryItem[]
  historySelectionIndex: number
  historyErrorMessage: string | null
  onOutPathChange: (next: string) => void
  onSubmit: () => void
}
export const ExportPopup = ({
  selectionIndex,
  format,
  outPathDraft,
  historyItems,
  historySelectionIndex,
  historyErrorMessage,
  onOutPathChange,
  onSubmit,
}: ExportPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 54, 92)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const activeRowProps = {
    ...inkColorProps(theme.selectionText),
    ...inkBackgroundColorProps(theme.selectionBackground),
  }
  const inactiveRowProps = { ...backgroundProps, ...inkColorProps(theme.text) }
  const normalizedSelection = clamp(selectionIndex, 0, 2)
  const visibleRows = 7
  const historyStartIndex = clamp(
    historySelectionIndex - Math.floor(visibleRows / 2),
    0,
    Math.max(0, historyItems.length - visibleRows),
  )
  const historySlice = historyItems.slice(historyStartIndex, historyStartIndex + visibleRows)
  return (
    <PopupSheet
      width={popupWidth}
      height={18}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Export History Payload', contentWidth)}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('â/â select Âˇ â/â change Âˇ Enter export Âˇ Esc close', contentWidth)}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Tip: use an absolute path, or relative to cwd.', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...(normalizedSelection === 0 ? activeRowProps : inactiveRowProps)}>
          {padRight(`Format: ${format}`, contentWidth)}
        </Text>
        <Box flexDirection="row">
          <Text {...(normalizedSelection === 1 ? activeRowProps : inactiveRowProps)}>
            {padRight('Out:', 6)}
          </Text>
          <SingleLineTextInput
            value={outPathDraft}
            onChange={onOutPathChange}
            onSubmit={() => onSubmit()}
            placeholder={`prompt-export.${format}`}
            focus={normalizedSelection === 1}
            width={Math.max(1, contentWidth - 6)}
            backgroundColor={theme.popupBackground}
          />
        </Box>
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        {historyItems.length === 0 ? (
          <Text
            {...backgroundProps}
            {...inkColorProps(historyErrorMessage ? theme.error : theme.mutedText)}
          >
            {padRight(historyErrorMessage ?? 'Loading historyâŚ', contentWidth)}
          </Text>
        ) : (
          historySlice.map((item, offset) => {
            const absoluteIndex = historyStartIndex + offset
            const isSelected = absoluteIndex === historySelectionIndex
            const rowProps =
              normalizedSelection === 2 && isSelected
                ? activeRowProps
                : { ...backgroundProps, ...inkColorProps(theme.text) }
            return (
              <Text key={item.selector} {...rowProps}>
                {padRight(`${isSelected ? 'âş' : ' '} ${joinItemLine(item)}`, contentWidth)}
              </Text>
            )
          })
        )}
      </Box>
    </PopupSheet>
  )
}
</file>
<file path="src/tui/components/popups/IntentFilePopup.tsx">
import { useMemo, type ComponentProps } from 'react'
import { Box, Text, useStdout } from 'ink'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { clampSelectionIndex, resolveWindowedValues } from './list-windowing'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type IntentFilePopupProps = {
  draft: string
  suggestions: readonly string[]
  suggestedSelectionIndex: number
  suggestedFocused: boolean
  maxHeight?: number
  onDraftChange: (value: string) => void
  onSubmitDraft: (value: string) => void
}
export const IntentFilePopup = ({
  draft,
  suggestions,
  suggestedSelectionIndex,
  suggestedFocused,
  maxHeight,
  onDraftChange,
  onSubmitDraft,
}: IntentFilePopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const popupHeight = Math.max(9, Math.floor(maxHeight ?? 9))
  const suggestionRows = useMemo(() => {
    const paddingRows = 2 * POPUP_PADDING_Y
    const contentRows = Math.max(1, popupHeight - paddingRows)
    const fixedRows = 3
    return Math.max(0, contentRows - fixedRows)
  }, [popupHeight])
  const hasSuggestions = suggestions.length > 0
  const safeSuggestedSelection = clampSelectionIndex(suggestions.length, suggestedSelectionIndex)
  const effectiveSuggestedFocused = hasSuggestions && suggestedFocused
  const visibleSuggestions = useMemo(
    () => resolveWindowedValues(suggestions, safeSuggestedSelection, suggestionRows, { lead: 1 }),
    [safeSuggestedSelection, suggestions, suggestionRows],
  )
  const focusedSelectionProps = {
    ...inkColorProps(theme.selectionText),
    ...inkBackgroundColorProps(theme.selectionBackground),
  }
  const unfocusedSelectionProps = {
    ...inkColorProps(theme.chipText),
    ...inkBackgroundColorProps(theme.chipBackground),
  }
  const suggestionLines = useMemo(() => {
    const lines: Array<{ key: string; label: string; props: ComponentProps<typeof Text> }> = []
    if (!hasSuggestions) {
      lines.push({
        key: 'empty',
        label: '(type to search)',
        props: { ...backgroundProps, ...inkColorProps(theme.mutedText) },
      })
    } else {
      if (visibleSuggestions.showBefore) {
        lines.push({
          key: 'before',
          label: 'âŚ earlier âŚ',
          props: { ...backgroundProps, ...inkColorProps(theme.mutedText) },
        })
      }
      visibleSuggestions.values.forEach((value, index) => {
        const actualIndex = visibleSuggestions.start + index
        const isSelected = actualIndex === safeSuggestedSelection
        const textProps = isSelected
          ? effectiveSuggestedFocused
            ? focusedSelectionProps
            : unfocusedSelectionProps
          : { ...backgroundProps, ...inkColorProps(theme.text) }
        lines.push({ key: `${value}-${actualIndex}`, label: value, props: textProps })
      })
      if (visibleSuggestions.showAfter) {
        lines.push({
          key: 'after',
          label: 'âŚ later âŚ',
          props: { ...backgroundProps, ...inkColorProps(theme.mutedText) },
        })
      }
    }
    while (lines.length < suggestionRows) {
      lines.push({ key: `pad-${lines.length}`, label: '', props: backgroundProps })
    }
    return lines
  }, [
    backgroundProps,
    effectiveSuggestedFocused,
    focusedSelectionProps,
    hasSuggestions,
    safeSuggestedSelection,
    suggestionRows,
    theme.mutedText,
    theme.text,
    unfocusedSelectionProps,
    visibleSuggestions.showAfter,
    visibleSuggestions.showBefore,
    visibleSuggestions.start,
    visibleSuggestions.values,
  ])
  const pathLabel = 'Path: '
  const inputWidth = Math.max(1, contentWidth - pathLabel.length)
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Intent File', contentWidth)}
      </Text>
      <Box flexDirection="row">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {pathLabel}
        </Text>
        <SingleLineTextInput
          value={draft}
          onChange={onDraftChange}
          onSubmit={() => onSubmitDraft(draft)}
          placeholder="prompts/intent.md"
          focus={!effectiveSuggestedFocused}
          width={inputWidth}
          backgroundColor={theme.popupBackground}
        />
      </Box>
      {suggestionRows > 0 ? (
        <Box flexDirection="column" height={suggestionRows} flexShrink={0} overflow="hidden">
          {suggestionLines.map((line) => (
            <Text key={line.key} {...line.props}>
              {padRight(line.label, contentWidth)}
            </Text>
          ))}
        </Box>
      ) : null}
      <Box flexShrink={0}>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Tab suggestions Âˇ â/â select Âˇ Enter apply Âˇ Esc close', contentWidth)}
        </Text>
      </Box>
    </PopupSheet>
  )
}
</file>
<file path="src/tui/components/popups/list-popup-layout.ts">
export const DEFAULT_MAX_VISIBLE_LIST_ITEMS = 6
export const DEFAULT_MAX_VISIBLE_SUGGESTIONS = 4
export type ListPopupHeights = {
  selectedRows: number
  suggestionRows: number
}
type ResolveListPopupHeightsOptions = {
  maxHeight: number | undefined
  hasSuggestions: boolean
  instructionRows?: number
}
export const resolveListPopupHeights = ({
  maxHeight,
  hasSuggestions,
  instructionRows = 1,
}: ResolveListPopupHeightsOptions): ListPopupHeights => {
  if (!hasSuggestions) {
    return { selectedRows: DEFAULT_MAX_VISIBLE_LIST_ITEMS, suggestionRows: 0 }
  }
  const fallbackHeight = 16
  const resolvedHeight = maxHeight ?? fallbackHeight
  const paddingRows = 4
  const contentHeight = Math.max(1, resolvedHeight - paddingRows)
  const safeInstructionRows = Math.max(1, Math.floor(instructionRows))
  const fixedRows = 4 + safeInstructionRows
  const availableRows = Math.max(contentHeight - fixedRows, 1)
  const selectedMin = Math.min(3, availableRows)
  const suggestionRows = Math.min(
    DEFAULT_MAX_VISIBLE_SUGGESTIONS,
    Math.max(0, availableRows - selectedMin),
  )
  const selectedRows = Math.max(1, availableRows - suggestionRows)
  return { selectedRows, suggestionRows }
}
</file>
<file path="src/tui/components/popups/list-popup-model.ts">
import { DEFAULT_MAX_VISIBLE_LIST_ITEMS, resolveListPopupHeights } from './list-popup-layout'
import { resolveCursorWindow } from './list-window'
import { clampSelectionIndex, resolveWindowedValues } from './list-windowing'
export type ListPopupLayout = 'input-first' | 'selected-first'
export type ListPopupRowTone = 'text' | 'muted'
export type ListPopupRowSelection = 'none' | 'focused' | 'unfocused'
export type ListPopupRowModel = {
  key: string
  label: string
  tone: ListPopupRowTone
  selection: ListPopupRowSelection
}
export type ListPopupSectionId = 'selected' | 'suggestions'
export type ListPopupSectionModel = {
  id: ListPopupSectionId
  header: string
  rows: readonly ListPopupRowModel[]
  fixedRowCount?: number
}
export type ListPopupInputModel =
  | {
      variant: 'inline'
      label: string
      focus: boolean
    }
  | {
      variant: 'titled'
      title: string
      focus: boolean
    }
export type ListPopupInstructionsModel = {
  normalizedLines: readonly string[]
  renderLines: readonly string[]
  rowCount: number
}
export type ListPopupBlockModel =
  | {
      type: 'spacer'
      key: string
    }
  | {
      type: 'input'
      input: ListPopupInputModel
    }
  | {
      type: 'section'
      section: ListPopupSectionModel
    }
  | {
      type: 'instructions'
      lines: readonly string[]
    }
export type ListPopupModel = {
  hasSuggestions: boolean
  safeSuggestedSelection: number
  effectiveSuggestedFocused: boolean
  effectiveSelectedFocused: boolean
  shouldHighlightSelectedAsFocused: boolean
  input: ListPopupInputModel
  selectedSection: ListPopupSectionModel
  suggestionsSection?: ListPopupSectionModel
  instructions: ListPopupInstructionsModel
  blocks: readonly ListPopupBlockModel[]
}
export type BuildListPopupModelOptions = {
  items: readonly string[]
  selectedIndex: number
  emptyLabel: string
  instructions: string
  layout: ListPopupLayout
  popupHeight: number
  suggestedItems?: readonly string[]
  suggestedSelectionIndex?: number
  suggestedFocused?: boolean
  selectedFocused?: boolean
}
export const parseListPopupInstructions = (instructions: string): ListPopupInstructionsModel => {
  const normalized = instructions.replaceAll('\\n', '\n')
  const normalizedLines = normalized
    .split('\n')
    .map((line) => line.trim())
    .filter((line) => line.length > 0)
  return {
    normalizedLines,
    renderLines: normalizedLines.length > 0 ? normalizedLines : [instructions],
    rowCount: Math.max(1, normalizedLines.length),
  }
}
const buildSelectedRowsFixedHeight = ({
  items,
  selectedIndex,
  emptyLabel,
  maxRows,
  shouldHighlightSelectedAsFocused,
}: {
  items: readonly string[]
  selectedIndex: number
  emptyLabel: string
  maxRows: number
  shouldHighlightSelectedAsFocused: boolean
}): readonly ListPopupRowModel[] => {
  const rows: Array<ListPopupRowModel> = []
  if (items.length === 0) {
    rows.push({ key: 'empty', label: emptyLabel, tone: 'muted', selection: 'none' })
  } else {
    const window = resolveWindowedValues(items, selectedIndex, maxRows)
    if (window.showBefore) {
      rows.push({
        key: 'before',
        label: 'âŚ earlier entries âŚ',
        tone: 'muted',
        selection: 'none',
      })
    }
    window.values.forEach((value, index) => {
      const actualIndex = window.start + index
      const isSelected = actualIndex === selectedIndex
      rows.push({
        key: `${value}-${actualIndex}`,
        label: `${actualIndex + 1}. ${value}`,
        tone: 'text',
        selection: isSelected
          ? shouldHighlightSelectedAsFocused
            ? 'focused'
            : 'unfocused'
          : 'none',
      })
    })
    if (window.showAfter) {
      rows.push({
        key: 'after',
        label: 'âŚ later entries âŚ',
        tone: 'muted',
        selection: 'none',
      })
    }
  }
  while (rows.length < maxRows) {
    rows.push({ key: `pad-${rows.length}`, label: '', tone: 'text', selection: 'none' })
  }
  return rows
}
const buildSelectedRowsFreeHeight = ({
  items,
  selectedIndex,
  emptyLabel,
  shouldHighlightSelectedAsFocused,
}: {
  items: readonly string[]
  selectedIndex: number
  emptyLabel: string
  shouldHighlightSelectedAsFocused: boolean
}): readonly ListPopupRowModel[] => {
  const rows: Array<ListPopupRowModel> = []
  if (items.length === 0) {
    rows.push({ key: 'empty', label: emptyLabel, tone: 'muted', selection: 'none' })
    return rows
  }
  const range = resolveCursorWindow(items.length, selectedIndex, DEFAULT_MAX_VISIBLE_LIST_ITEMS)
  const start = range.startIndex
  const end = range.endIndexExclusive
  if (start > 0) {
    rows.push({ key: 'before', label: 'âŚ earlier entries âŚ', tone: 'muted', selection: 'none' })
  }
  items.slice(start, end).forEach((value, index) => {
    const actualIndex = start + index
    const isSelected = actualIndex === selectedIndex
    rows.push({
      key: `${value}-${actualIndex}`,
      label: `${actualIndex + 1}. ${value}`,
      tone: 'text',
      selection: isSelected ? (shouldHighlightSelectedAsFocused ? 'focused' : 'unfocused') : 'none',
    })
  })
  if (end < items.length) {
    rows.push({ key: 'after', label: 'âŚ later entries âŚ', tone: 'muted', selection: 'none' })
  }
  return rows
}
const buildSuggestionRows = ({
  suggestedItems,
  safeSuggestedSelection,
  maxRows,
  effectiveSuggestedFocused,
}: {
  suggestedItems: readonly string[]
  safeSuggestedSelection: number
  maxRows: number
  effectiveSuggestedFocused: boolean
}): readonly ListPopupRowModel[] => {
  const rows: Array<ListPopupRowModel> = []
  if (maxRows <= 0) {
    return rows
  }
  const window = resolveWindowedValues(suggestedItems, safeSuggestedSelection, maxRows, { lead: 1 })
  if (window.showBefore) {
    rows.push({
      key: 'before',
      label: 'âŚ earlier suggestions âŚ',
      tone: 'muted',
      selection: 'none',
    })
  }
  window.values.forEach((value, index) => {
    const actualIndex = window.start + index
    const isSelected = actualIndex === safeSuggestedSelection
    rows.push({
      key: `${value}-${actualIndex}`,
      label: value,
      tone: 'text',
      selection: isSelected ? (effectiveSuggestedFocused ? 'focused' : 'unfocused') : 'none',
    })
  })
  if (window.showAfter) {
    rows.push({
      key: 'after',
      label: 'âŚ later suggestions âŚ',
      tone: 'muted',
      selection: 'none',
    })
  }
  while (rows.length < maxRows) {
    rows.push({ key: `pad-${rows.length}`, label: '', tone: 'text', selection: 'none' })
  }
  return rows
}
export const buildListPopupModel = (options: BuildListPopupModelOptions): ListPopupModel => {
  const {
    items,
    selectedIndex,
    emptyLabel,
    instructions,
    suggestedItems,
    suggestedSelectionIndex,
    suggestedFocused,
    selectedFocused,
    layout,
    popupHeight,
  } = options
  const safeSuggestedItems = suggestedItems ?? []
  const hasSuggestions = safeSuggestedItems.length > 0
  const safeSuggestedSelection = clampSelectionIndex(
    safeSuggestedItems.length,
    suggestedSelectionIndex ?? 0,
  )
  const effectiveSuggestedFocused = Boolean(hasSuggestions && suggestedFocused)
  const effectiveSelectedFocused = Boolean(selectedFocused)
  const shouldHighlightSelectedAsFocused = selectedFocused ?? true
  const instructionsModel = parseListPopupInstructions(instructions)
  const heights = resolveListPopupHeights({
    maxHeight: popupHeight,
    hasSuggestions,
    instructionRows: instructionsModel.rowCount,
  })
  const selectedRows = hasSuggestions
    ? buildSelectedRowsFixedHeight({
        items,
        selectedIndex,
        emptyLabel,
        maxRows: heights.selectedRows,
        shouldHighlightSelectedAsFocused,
      })
    : buildSelectedRowsFreeHeight({
        items,
        selectedIndex,
        emptyLabel,
        shouldHighlightSelectedAsFocused,
      })
  const selectedSection: ListPopupSectionModel = {
    id: 'selected',
    header: 'Selected',
    rows: selectedRows,
    ...(hasSuggestions ? { fixedRowCount: heights.selectedRows } : {}),
  }
  const suggestionsSection: ListPopupSectionModel | undefined =
    hasSuggestions && heights.suggestionRows > 0
      ? {
          id: 'suggestions',
          header: 'Suggestions',
          rows: buildSuggestionRows({
            suggestedItems: safeSuggestedItems,
            safeSuggestedSelection,
            maxRows: heights.suggestionRows,
            effectiveSuggestedFocused,
          }),
          fixedRowCount: heights.suggestionRows,
        }
      : undefined
  const input: ListPopupInputModel = hasSuggestions
    ? {
        variant: 'inline',
        label: 'Add:',
        focus: !effectiveSuggestedFocused && !effectiveSelectedFocused,
      }
    : {
        variant: 'titled',
        title: 'Add new',
        focus: !effectiveSelectedFocused,
      }
  const blocks: Array<ListPopupBlockModel> = []
  if (!hasSuggestions) {
    blocks.push({ type: 'spacer', key: 'after-title' })
  }
  const pushInputAndSelected = (): void => {
    if (layout === 'selected-first') {
      blocks.push({ type: 'section', section: selectedSection })
      if (!hasSuggestions) {
        blocks.push({ type: 'spacer', key: 'between-selected-and-input' })
      }
      blocks.push({ type: 'input', input })
      return
    }
    blocks.push({ type: 'input', input })
    if (!hasSuggestions) {
      blocks.push({ type: 'spacer', key: 'between-input-and-selected' })
    }
    blocks.push({ type: 'section', section: selectedSection })
  }
  pushInputAndSelected()
  if (suggestionsSection) {
    blocks.push({ type: 'section', section: suggestionsSection })
  }
  if (!hasSuggestions && instructionsModel.rowCount <= 1) {
    blocks.push({ type: 'spacer', key: 'before-instructions' })
  }
  blocks.push({ type: 'instructions', lines: instructionsModel.renderLines })
  return {
    hasSuggestions,
    safeSuggestedSelection,
    effectiveSuggestedFocused,
    effectiveSelectedFocused,
    shouldHighlightSelectedAsFocused,
    input,
    selectedSection,
    ...(suggestionsSection ? { suggestionsSection } : {}),
    instructions: instructionsModel,
    blocks,
  }
}
</file>
<file path="src/tui/components/popups/list-windowing.ts">
import { resolveWindowedList } from './list-window'
export type WindowedValues<T> = {
  start: number
  end: number
  values: readonly T[]
  showBefore: boolean
  showAfter: boolean
}
const clamp = (value: number, min: number, max: number): number => {
  if (value < min) {
    return min
  }
  if (value > max) {
    return max
  }
  return value
}
export const clampSelectionIndex = (itemCount: number, selectedIndex: number): number => {
  if (itemCount <= 0) {
    return 0
  }
  return clamp(selectedIndex, 0, itemCount - 1)
}
export type ResolveWindowedValuesOptions = {
  lead?: number
}
export const resolveWindowedValues = <T>(
  items: readonly T[],
  selectedIndex: number,
  maxVisibleRows: number,
  options: ResolveWindowedValuesOptions = {},
): WindowedValues<T> => {
  if (items.length === 0 || maxVisibleRows <= 0) {
    return {
      start: 0,
      end: 0,
      values: [],
      showBefore: false,
      showAfter: false,
    }
  }
  const window = resolveWindowedList({
    itemCount: items.length,
    selectedIndex,
    maxVisibleRows,
    ...(options.lead === undefined ? {} : { lead: options.lead }),
  })
  return {
    start: window.start,
    end: window.end,
    values: items.slice(window.start, window.end),
    showBefore: window.showBefore,
    showAfter: window.showAfter,
  }
}
export type WindowBounds = {
  start: number
  end: number
}
export const ensureLeadingHeaderVisible = <Row extends { type: string }>(
  rows: readonly Row[],
  bounds: WindowBounds,
  maxRows: number,
  headerType: Row['type'],
  itemType: Row['type'],
): WindowBounds => {
  const { start, end } = bounds
  if (start <= 0 || end - start >= maxRows) {
    return bounds
  }
  const first = rows[start]
  const previous = rows[start - 1]
  if (first?.type === itemType && previous?.type === headerType) {
    const nextStart = start - 1
    const nextEnd = Math.min(rows.length, nextStart + maxRows)
    return { start: nextStart, end: nextEnd }
  }
  return bounds
}
</file>
<file path="src/tui/components/popups/ResumePopup.tsx">
import { Box, Text, useStdout } from 'ink'
import type { ResumeHistoryItem, ResumeMode, ResumeSourceKind } from '../../types'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, Math.max(0, width - 1)).concat('âŚ') : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
const formatMode = (mode: ResumeMode): string => (mode === 'strict' ? 'strict' : 'best-effort')
const formatSource = (source: ResumeSourceKind): string => (source === 'file' ? 'file' : 'history')
const joinItemLine = (item: ResumeHistoryItem): string => `${item.title} Âˇ ${item.detail}`
export type ResumePopupProps = {
  selectionIndex: number
  sourceKind: ResumeSourceKind
  mode: ResumeMode
  historyItems: ResumeHistoryItem[]
  historySelectionIndex: number
  historyErrorMessage: string | null
  payloadPathDraft: string
  suggestedItems: string[]
  suggestedSelectionIndex: number
  suggestedFocused: boolean
  onPayloadPathChange: (next: string) => void
  onSubmit: () => void
}
export const ResumePopup = ({
  selectionIndex,
  sourceKind,
  mode,
  historyItems,
  historySelectionIndex,
  historyErrorMessage,
  payloadPathDraft,
  suggestedItems,
  suggestedSelectionIndex,
  suggestedFocused,
  onPayloadPathChange,
  onSubmit,
}: ResumePopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 54, 92)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const activeRowProps = {
    ...inkColorProps(theme.selectionText),
    ...inkBackgroundColorProps(theme.selectionBackground),
  }
  const inactiveRowProps = { ...backgroundProps, ...inkColorProps(theme.text) }
  const normalizedSelection = clamp(selectionIndex, 0, 2)
  const visibleRows = 7
  const historyStartIndex = clamp(
    historySelectionIndex - Math.floor(visibleRows / 2),
    0,
    Math.max(0, historyItems.length - visibleRows),
  )
  const historySlice = historyItems.slice(historyStartIndex, historyStartIndex + visibleRows)
  return (
    <PopupSheet
      width={popupWidth}
      height={18}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Resume Generation', contentWidth)}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight(
          'â/â select Âˇ â/â change Âˇ Enter resume Âˇ Tab suggestions Âˇ Esc close',
          contentWidth,
        )}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Note: resumes file-path context only; URL/smart treated missing.', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...(normalizedSelection === 0 ? activeRowProps : inactiveRowProps)}>
          {padRight(`Source: ${formatSource(sourceKind)}`, contentWidth)}
        </Text>
        <Text {...(normalizedSelection === 1 ? activeRowProps : inactiveRowProps)}>
          {padRight(`Mode: ${formatMode(mode)}`, contentWidth)}
        </Text>
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      {sourceKind === 'history' ? (
        <Box flexDirection="column">
          {historyItems.length === 0 ? (
            <Text
              {...backgroundProps}
              {...inkColorProps(historyErrorMessage ? theme.error : theme.mutedText)}
            >
              {padRight(historyErrorMessage ?? 'Loading historyâŚ', contentWidth)}
            </Text>
          ) : (
            historySlice.map((item, offset) => {
              const absoluteIndex = historyStartIndex + offset
              const isSelected = absoluteIndex === historySelectionIndex
              const rowProps =
                normalizedSelection === 2 && isSelected
                  ? activeRowProps
                  : { ...backgroundProps, ...inkColorProps(theme.text) }
              return (
                <Text key={item.selector} {...rowProps}>
                  {padRight(`${isSelected ? 'âş' : ' '} ${joinItemLine(item)}`, contentWidth)}
                </Text>
              )
            })
          )}
        </Box>
      ) : (
        <Box flexDirection="column">
          <Box flexDirection="row">
            <Text
              {...(normalizedSelection === 2 && !suggestedFocused
                ? activeRowProps
                : inactiveRowProps)}
            >
              {padRight('Payload:', 9)}
            </Text>
            <SingleLineTextInput
              value={payloadPathDraft}
              onChange={onPayloadPathChange}
              onSubmit={() => onSubmit()}
              placeholder="path/to/payload.json"
              focus={normalizedSelection === 2 && !suggestedFocused}
              width={Math.max(1, contentWidth - 9)}
              backgroundColor={theme.popupBackground}
            />
          </Box>
          {suggestedFocused && suggestedItems.length > 0 ? (
            <Box flexDirection="column" marginTop={1}>
              {suggestedItems.slice(0, 6).map((suggestion, index) => {
                const isSelected = index === suggestedSelectionIndex
                const rowProps = isSelected ? activeRowProps : inactiveRowProps
                return (
                  <Text key={suggestion} {...rowProps}>
                    {padRight(`${isSelected ? 'âş' : ' '} ${suggestion}`, contentWidth)}
                  </Text>
                )
              })}
            </Box>
          ) : null}
        </Box>
      )}
    </PopupSheet>
  )
}
</file>
<file path="src/tui/components/popups/SmartPopup.tsx">
import { useMemo, type ComponentProps } from 'react'
import { Box, Text, useStdout } from 'ink'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { clampSelectionIndex, resolveWindowedValues } from './list-windowing'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type SmartPopupProps = {
  savedRoot: string | null
  draft: string
  suggestedItems: readonly string[]
  suggestedSelectionIndex: number
  suggestedFocused: boolean
  maxHeight?: number
  onDraftChange: (value: string) => void
  onSubmitRoot: (value: string) => void
}
export const SmartPopup = ({
  savedRoot,
  draft,
  suggestedItems,
  suggestedSelectionIndex,
  suggestedFocused,
  maxHeight,
  onDraftChange,
  onSubmitRoot,
}: SmartPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const hasSuggestions = suggestedItems.length > 0
  const safeSuggestedSelection = clampSelectionIndex(suggestedItems.length, suggestedSelectionIndex)
  const effectiveSuggestedFocused = hasSuggestions && suggestedFocused
  const popupHeight = Math.max(9, Math.floor(maxHeight ?? 9))
  const suggestionRows = useMemo(() => {
    const paddingRows = 2 * POPUP_PADDING_Y
    const contentRows = Math.max(1, popupHeight - paddingRows)
    const fixedRows = 5
    return Math.max(0, contentRows - fixedRows)
  }, [popupHeight])
  const visibleSuggestions = useMemo(
    () =>
      resolveWindowedValues(suggestedItems, safeSuggestedSelection, suggestionRows, { lead: 1 }),
    [safeSuggestedSelection, suggestedItems, suggestionRows],
  )
  const savedLabel = savedRoot ? savedRoot : '(none)'
  const focusedSelectionProps = {
    ...inkColorProps(theme.selectionText),
    ...inkBackgroundColorProps(theme.selectionBackground),
  }
  const unfocusedSelectionProps = {
    ...inkColorProps(theme.chipText),
    ...inkBackgroundColorProps(theme.chipBackground),
  }
  const suggestionLines = useMemo(() => {
    const lines: Array<{ key: string; label: string; props: ComponentProps<typeof Text> }> = []
    if (!hasSuggestions) {
      lines.push({
        key: 'empty',
        label: '(type to filter)',
        props: { ...backgroundProps, ...inkColorProps(theme.mutedText) },
      })
    } else {
      if (visibleSuggestions.showBefore) {
        lines.push({
          key: 'before',
          label: 'âŚ earlier âŚ',
          props: { ...backgroundProps, ...inkColorProps(theme.mutedText) },
        })
      }
      visibleSuggestions.values.forEach((value, index) => {
        const actualIndex = visibleSuggestions.start + index
        const isSelected = actualIndex === safeSuggestedSelection
        const textProps = isSelected
          ? effectiveSuggestedFocused
            ? focusedSelectionProps
            : unfocusedSelectionProps
          : { ...backgroundProps, ...inkColorProps(theme.text) }
        lines.push({ key: `${value}-${actualIndex}`, label: value, props: textProps })
      })
      if (visibleSuggestions.showAfter) {
        lines.push({
          key: 'after',
          label: 'âŚ later âŚ',
          props: { ...backgroundProps, ...inkColorProps(theme.mutedText) },
        })
      }
    }
    while (lines.length < suggestionRows) {
      lines.push({ key: `pad-${lines.length}`, label: '', props: backgroundProps })
    }
    return lines
  }, [
    backgroundProps,
    effectiveSuggestedFocused,
    focusedSelectionProps,
    hasSuggestions,
    safeSuggestedSelection,
    suggestionRows,
    theme.mutedText,
    theme.text,
    unfocusedSelectionProps,
    visibleSuggestions.showAfter,
    visibleSuggestions.showBefore,
    visibleSuggestions.start,
    visibleSuggestions.values,
  ])
  const rootLabel = 'Root: '
  const inputWidth = Math.max(1, contentWidth - rootLabel.length)
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Smart Context Root', contentWidth)}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.text)}>
        {padRight('Enter to save Âˇ Tab suggestions Âˇ Esc close', contentWidth)}
      </Text>
      <Box flexDirection="row">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {rootLabel}
        </Text>
        <SingleLineTextInput
          value={draft}
          onChange={onDraftChange}
          onSubmit={onSubmitRoot}
          placeholder="relative/dir"
          focus={!effectiveSuggestedFocused}
          width={inputWidth}
          backgroundColor={theme.popupBackground}
        />
      </Box>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight(`Saved root: ${savedLabel}`, contentWidth)}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Suggestions', contentWidth)}
      </Text>
      {suggestionRows > 0 ? (
        <Box flexDirection="column" height={suggestionRows} flexShrink={0} overflow="hidden">
          {suggestionLines.map((line) => (
            <Text key={line.key} {...line.props}>
              {padRight(line.label, contentWidth)}
            </Text>
          ))}
        </Box>
      ) : null}
    </PopupSheet>
  )
}
</file>
<file path="src/tui/components/popups/ThemePickerPopup.tsx">
import { useMemo } from 'react'
import { Box, Text, useStdout } from 'ink'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { resolveWindowedValues } from './list-windowing'
import { PopupSheet } from './PopupSheet'
export type ThemePickerPopupProps = {
  selectionIndex: number
  initialThemeName: string
  maxHeight?: number
}
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const resolveListRows = (popupHeight: number, hasError: boolean): number => {
  const paddingRows = 2 * POPUP_PADDING_Y
  const contentHeight = Math.max(1, popupHeight - paddingRows)
  const fixedRows = 4 + (hasError ? 1 : 0)
  return Math.max(1, contentHeight - fixedRows)
}
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  if (trimmed.length === width) {
    return trimmed
  }
  return `${trimmed}${' '.repeat(width - trimmed.length)}`
}
export const ThemePickerPopup = ({
  selectionIndex,
  initialThemeName,
  maxHeight,
}: ThemePickerPopupProps) => {
  const { theme, themes, activeThemeName, error } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  // Keep the popup reasonably sized and deterministic.
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(10, popupWidth - paddingColumns)
  const fallbackHeight = 16
  const popupHeight = Math.max(10, Math.floor(maxHeight ?? fallbackHeight))
  const listRows = useMemo(() => resolveListRows(popupHeight, Boolean(error)), [error, popupHeight])
  const names = useMemo(() => themes.map((descriptor) => descriptor.name), [themes])
  const labelsByName = useMemo(() => {
    const entries = themes.map((descriptor) => [descriptor.name, descriptor.label] as const)
    return new Map(entries)
  }, [themes])
  const initialLabel = labelsByName.get(initialThemeName) ?? initialThemeName
  const clampedSelection = Math.min(selectionIndex, Math.max(names.length - 1, 0))
  const window = useMemo(
    () => resolveWindowedValues(names, clampedSelection, listRows),
    [clampedSelection, listRows, names],
  )
  const selectedTextProps = {
    ...inkColorProps(theme.selectionText),
    ...inkBackgroundColorProps(theme.selectionBackground),
  }
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const renderFill = (width: number): string => (width > 0 ? ' '.repeat(width) : '')
  const headerLeft = 'Theme'
  const headerRight = 'esc'
  const headerGap = Math.max(0, contentWidth - headerLeft.length - headerRight.length)
  const currentLabel = `Current: ${initialLabel}`
  const listLines = useMemo((): Array<{ key: string; label: string; isSelected: boolean }> => {
    const lines: Array<{ key: string; label: string; isSelected: boolean }> = []
    if (names.length === 0) {
      lines.push({
        key: 'empty',
        label: padRight('No themes loaded.', contentWidth),
        isSelected: false,
      })
      while (lines.length < listRows) {
        lines.push({
          key: `pad-${lines.length}`,
          label: padRight('', contentWidth),
          isSelected: false,
        })
      }
      return lines
    }
    if (window.showBefore) {
      lines.push({ key: 'before', label: 'âŚ earlier âŚ', isSelected: false })
    }
    window.values.forEach((name, offset) => {
      const label = labelsByName.get(name) ?? name
      const isActive = name === activeThemeName
      const line = `${isActive ? 'â' : ' '} ${label}`
      const isSelected = window.start + offset === clampedSelection
      lines.push({ key: name, label: line, isSelected })
    })
    if (window.showAfter) {
      lines.push({ key: 'after', label: 'âŚ later âŚ', isSelected: false })
    }
    while (lines.length < listRows) {
      lines.push({ key: `pad-${lines.length}`, label: '', isSelected: false })
    }
    return lines.map((line) => ({ ...line, label: padRight(line.label, contentWidth) }))
  }, [activeThemeName, clampedSelection, contentWidth, labelsByName, listRows, names, window])
  const footer = 'â/â preview Âˇ Enter confirm Âˇ Esc cancel'
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Box flexDirection="row">
        <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
          {headerLeft}
        </Text>
        <Text {...backgroundProps}>{renderFill(headerGap)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {headerRight}
        </Text>
      </Box>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight(currentLabel, contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        {listLines.map((line) => (
          <Text
            key={line.key}
            {...(line.isSelected
              ? selectedTextProps
              : { ...backgroundProps, ...inkColorProps(theme.text) })}
          >
            {line.label}
          </Text>
        ))}
      </Box>
      {error ? (
        <Text {...backgroundProps} {...inkColorProps(theme.error)}>
          {padRight(error.message, contentWidth)}
        </Text>
      ) : null}
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight(footer, contentWidth)}
      </Text>
    </PopupSheet>
  )
}
</file>
<file path="src/tui/components/popups/TokenUsagePopup.tsx">
import { Box, Text, useStdout } from 'ink'
import type { TokenUsageBreakdown, TokenUsageRun } from '../../token-usage-store'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const formatNumber = (value: number): string => value.toLocaleString('en-US')
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
const formatUsd = (value: number | null): string => {
  if (value === null) {
    return 'n/a'
  }
  if (value === 0) {
    return '$0.00'
  }
  if (value < 0.01) {
    return `$${value.toFixed(4)}`
  }
  return `$${value.toFixed(2)}`
}
const padCell = (value: string, width: number, align: 'left' | 'right'): string => {
  if (value.length >= width) {
    return value
  }
  const padding = ' '.repeat(width - value.length)
  return align === 'right' ? `${padding}${value}` : `${value}${padding}`
}
type Row = {
  label: string
  tokens: number
}
const renderTable = (rows: readonly Row[]): string[] => {
  const labelWidth = Math.max(12, ...rows.map((row) => row.label.length))
  const tokenWidth = Math.max(8, ...rows.map((row) => formatNumber(row.tokens).length))
  return rows.map((row) => {
    const label = padCell(row.label, labelWidth, 'left')
    const tokens = padCell(formatNumber(row.tokens), tokenWidth, 'right')
    return `${label}  ${tokens}`
  })
}
export type TokenUsagePopupProps = {
  run: TokenUsageRun | null
  breakdown: TokenUsageBreakdown | null
  budgets: import('../../budget-settings').BudgetSettings
  latestContextOverflow: import('../../generation-pipeline-reducer').ContextOverflowDetails | null
}
export const TokenUsagePopup = ({
  run,
  breakdown,
  budgets,
  latestContextOverflow,
}: TokenUsagePopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const terminalRows = stdout?.rows ?? 24
  const popupHeight = clamp(terminalRows - 6, 18, 34)
  const budgetsEnabled = budgets.maxContextTokens !== null || budgets.maxInputTokens !== null
  const effectiveOverflow = budgets.contextOverflowStrategy ?? (budgetsEnabled ? 'fail' : null)
  const overflowSummary = latestContextOverflow
    ? `Last overflow: ${latestContextOverflow.strategy} Âˇ dropped ${latestContextOverflow.droppedPaths.length}`
    : 'Last overflow: none'
  const overflowPreview = latestContextOverflow
    ? (() => {
        const previewLimit = 5
        const preview = latestContextOverflow.droppedPaths
          .slice(0, previewLimit)
          .map((entry) => entry.path)
        const remaining = latestContextOverflow.droppedPaths.length - preview.length
        const suffix = remaining > 0 ? ` âŚ(+${remaining} more)` : ''
        return preview.length > 0 ? `Dropped: ${preview.join(', ')}${suffix}` : 'Dropped: (none)'
      })()
    : null
  if (!run || !breakdown) {
    return (
      <PopupSheet
        width={popupWidth}
        height={popupHeight}
        paddingX={POPUP_PADDING_X}
        paddingY={POPUP_PADDING_Y}
        background={theme.popupBackground}
      >
        <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
          {padRight('Token Usage', contentWidth)}
        </Text>
        <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('No token usage recorded yet. Run generation first.', contentWidth)}
        </Text>
        <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight(
            `Budgets: input=${budgets.maxInputTokens ?? 'unset'} Âˇ context=${budgets.maxContextTokens ?? 'unset'} Âˇ overflow=${effectiveOverflow ?? 'unset'}`,
            contentWidth,
          )}
        </Text>
        <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight(overflowSummary, contentWidth)}
        </Text>
        {overflowPreview ? (
          <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
            {padRight(overflowPreview, contentWidth)}
          </Text>
        ) : null}
        <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Esc to close', contentWidth)}
        </Text>
      </PopupSheet>
    )
  }
  const inputRows = renderTable([
    { label: 'Intent', tokens: breakdown.input.intent },
    { label: 'Files', tokens: breakdown.input.files },
    { label: 'System', tokens: breakdown.input.system },
    { label: 'Input total', tokens: breakdown.input.total },
  ])
  const outputRows = renderTable([
    { label: 'Reasoning', tokens: breakdown.output.reasoning },
    { label: 'Final prompt', tokens: breakdown.output.prompt },
    { label: 'Output total', tokens: breakdown.output.total },
  ])
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Token Usage', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.text)}>
          {padRight(`Model: ${run.model}`, contentWidth)}
        </Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight(`Started: ${run.startedAt}`, contentWidth)}
        </Text>
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Input', contentWidth)}
        </Text>
        {inputRows.map((line) => (
          <Text key={`input-${line}`} {...backgroundProps} {...inkColorProps(theme.text)}>
            {padRight(line, contentWidth)}
          </Text>
        ))}
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Output', contentWidth)}
        </Text>
        {outputRows.map((line) => (
          <Text key={`output-${line}`} {...backgroundProps} {...inkColorProps(theme.text)}>
            {padRight(line, contentWidth)}
          </Text>
        ))}
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Totals', contentWidth)}
        </Text>
        <Text {...backgroundProps} {...inkColorProps(theme.text)}>
          {padRight(`Total tokens ${formatNumber(breakdown.totals.tokens)}`, contentWidth)}
        </Text>
        <Text {...backgroundProps} {...inkColorProps(theme.text)}>
          {padRight(`Estimated cost ${formatUsd(breakdown.totals.estimatedCostUsd)}`, contentWidth)}
        </Text>
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Budgets', contentWidth)}
        </Text>
        <Text {...backgroundProps} {...inkColorProps(theme.text)}>
          {padRight(
            `input=${budgets.maxInputTokens ?? 'unset'} Âˇ context=${budgets.maxContextTokens ?? 'unset'} Âˇ overflow=${effectiveOverflow ?? 'unset'}`,
            contentWidth,
          )}
        </Text>
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Context Overflow', contentWidth)}
        </Text>
        <Text {...backgroundProps} {...inkColorProps(theme.text)}>
          {padRight(overflowSummary, contentWidth)}
        </Text>
        {overflowPreview ? (
          <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
            {padRight(overflowPreview, contentWidth)}
          </Text>
        ) : null}
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Esc to close', contentWidth)}
      </Text>
    </PopupSheet>
  )
}
</file>
<file path="src/tui/hooks/popup-manager/scan-orchestrator.ts">
import type { PopupAction, PopupScanKind } from '../../popup-reducer'
import type { HistoryEntry } from '../../types'
type ScanIdRef = {
  current: number
}
type PopupDispatch = (action: PopupAction) => void
type PushHistory = (content: string, kind?: HistoryEntry['kind']) => void
export type RunSuggestionScanOptions = {
  kind: PopupScanKind
  open: (scanId: number) => PopupAction
  scan: () => Promise<string[]>
}
export type PopupScanOrchestrator = {
  runSuggestionScan: (options: RunSuggestionScanOptions) => void
}
export const createPopupScanOrchestrator = ({
  scanIdRef,
  dispatch,
  pushHistory,
}: {
  scanIdRef: ScanIdRef
  dispatch: PopupDispatch
  pushHistory: PushHistory
}): PopupScanOrchestrator => {
  const nextScanId = (): number => {
    scanIdRef.current += 1
    return scanIdRef.current
  }
  const runSuggestionScan = ({ kind, open, scan }: RunSuggestionScanOptions): void => {
    const scanId = nextScanId()
    dispatch(open(scanId))
    const run = async (): Promise<void> => {
      try {
        const suggestions = await scan()
        dispatch({
          type: 'scan-suggestions-success',
          kind,
          scanId,
          suggestions,
        })
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown workspace scan error.'
        pushHistory(`[${kind}] Failed to scan workspace: ${message}`, 'system')
      }
    }
    void run()
  }
  return { runSuggestionScan }
}
</file>
<file path="src/tui/hooks/popup-scans.ts">
import path from 'node:path'
import {
  discoverDirectorySuggestions,
  discoverFileSuggestions,
  discoverIntentFileSuggestions,
  type DiscoverDirectorySuggestionsOptions,
  type DiscoverFileSuggestionsOptions,
  type DiscoverIntentFileSuggestionsOptions,
} from '../file-suggestions'
const IMAGE_EXTENSIONS = new Set(['.png', '.jpg', '.jpeg', '.webp', '.gif'])
const VIDEO_EXTENSIONS = new Set(['.mp4', '.mov', '.m4v', '.webm', '.avi', '.mpeg', '.mpg', '.gif'])
const PDF_EXTENSIONS = new Set(['.pdf'])
export const scanFileSuggestions = async (
  options: DiscoverFileSuggestionsOptions = {},
): Promise<string[]> => discoverFileSuggestions(options)
export const scanImageSuggestions = async (
  options: DiscoverFileSuggestionsOptions = {},
): Promise<string[]> => {
  const suggestions = await discoverFileSuggestions(options)
  return suggestions.filter((candidate) => {
    const ext = path.extname(candidate).toLowerCase()
    return IMAGE_EXTENSIONS.has(ext)
  })
}
export const scanVideoSuggestions = async (
  options: DiscoverFileSuggestionsOptions = {},
): Promise<string[]> => {
  const suggestions = await discoverFileSuggestions(options)
  return suggestions.filter((candidate) => {
    const ext = path.extname(candidate).toLowerCase()
    return VIDEO_EXTENSIONS.has(ext)
  })
}
export const scanPdfSuggestions = async (
  options: DiscoverFileSuggestionsOptions = {},
): Promise<string[]> => {
  const suggestions = await discoverFileSuggestions(options)
  return suggestions.filter((candidate) => {
    const ext = path.extname(candidate).toLowerCase()
    return PDF_EXTENSIONS.has(ext)
  })
}
export const scanSmartSuggestions = async (
  options: DiscoverDirectorySuggestionsOptions = {},
): Promise<string[]> => discoverDirectorySuggestions(options)
export const scanIntentSuggestions = async (
  options: DiscoverIntentFileSuggestionsOptions = {},
): Promise<string[]> => discoverIntentFileSuggestions(options)
</file>
<file path="src/tui/hooks/series-artifacts-io.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import type { SeriesResponse } from '../../prompt-generator-service'
import { buildSeriesOutputDirName, sanitizeForPathSegment } from '../../utils/series-path'
export type PrepareSeriesOutputDirResult = {
  seriesDir: string
  canWriteFiles: boolean
  errorMessage?: string
}
export const prepareSeriesOutputDir = async (
  intent: string,
  cwd: string = process.cwd(),
): Promise<PrepareSeriesOutputDirResult> => {
  const seriesDir = path.join(
    path.resolve(cwd, 'generated', 'series'),
    buildSeriesOutputDirName(intent),
  )
  try {
    await fs.mkdir(seriesDir, { recursive: true })
    return { seriesDir, canWriteFiles: true }
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown filesystem error.'
    return { seriesDir, canWriteFiles: false, errorMessage: message }
  }
}
export type WriteSeriesArtifactsResult = {
  writtenCount: number
  errors: Array<{ fileName: string; message: string }>
}
export const writeSeriesArtifacts = async (
  seriesDir: string,
  series: SeriesResponse,
): Promise<WriteSeriesArtifactsResult> => {
  const tasks: Array<{ fileName: string; content: string }> = []
  tasks.push({ fileName: '00-overview.md', content: series.overviewPrompt })
  series.atomicPrompts.forEach((step, index) => {
    const stepNumber = index + 1
    const stepPrefix = stepNumber.toString().padStart(2, '0')
    const titleSlug = sanitizeForPathSegment(step.title, 'step', 60)
    tasks.push({ fileName: `${stepPrefix}-${titleSlug}.md`, content: step.content })
  })
  const results = await Promise.allSettled(
    tasks.map(async (task) => {
      await fs.writeFile(path.join(seriesDir, task.fileName), task.content, 'utf8')
      return task.fileName
    }),
  )
  const errors: Array<{ fileName: string; message: string }> = []
  let writtenCount = 0
  results.forEach((result, index) => {
    const fileName = tasks[index]?.fileName ?? 'unknown'
    if (result.status === 'fulfilled') {
      writtenCount += 1
      return
    }
    const message = result.reason instanceof Error ? result.reason.message : String(result.reason)
    errors.push({ fileName, message })
  })
  return { writtenCount, errors }
}
</file>
<file path="src/tui/hooks/useCommandHistory.ts">
import { useCallback, useEffect, useMemo, useRef, useState } from 'react'
import { useLatestRef } from './useLatestRef'
import type { HistoryEntry } from '../types'
export type UseCommandHistoryOptions = {
  initialEntries: HistoryEntry[]
  visibleRows: number
}
export const useCommandHistory = ({
  initialEntries,
  visibleRows,
}: UseCommandHistoryOptions): {
  history: HistoryEntry[]
  pushHistory: (
    content: string,
    kind?: HistoryEntry['kind'],
    format?: HistoryEntry['format'],
  ) => void
  resetHistory: () => void
  clearHistory: () => void
  scroll: {
    offset: number
    scrollTo: (next: number) => void
    scrollBy: (delta: number) => void
  }
} => {
  const [history, setHistory] = useState<HistoryEntry[]>(() => [...initialEntries])
  const initialEntriesSnapshot = useMemo(() => [...initialEntries], [initialEntries])
  const initialEntriesRef = useLatestRef<HistoryEntry[]>(initialEntriesSnapshot)
  const historyIdRef = useRef(initialEntries.length)
  const [scrollOffset, setScrollOffset] = useState(0)
  const [isPinnedToBottom, setIsPinnedToBottom] = useState(true)
  const pushHistory = useCallback(
    (content: string, kind: HistoryEntry['kind'] = 'system', format?: HistoryEntry['format']) => {
      setHistory((prev) => [
        ...prev,
        {
          id: `entry-${historyIdRef.current++}`,
          content,
          kind,
          ...(format ? { format } : {}),
        },
      ])
      setIsPinnedToBottom(true)
    },
    [],
  )
  useEffect(() => {
    setScrollOffset((prev) => {
      const nextMax = Math.max(0, history.length - visibleRows)
      if (isPinnedToBottom) {
        return nextMax
      }
      return Math.min(prev, nextMax)
    })
  }, [history, visibleRows, isPinnedToBottom])
  const scrollTo = useCallback(
    (next: number) => {
      const nextMax = Math.max(0, history.length - visibleRows)
      const clamped = Math.max(0, Math.min(next, nextMax))
      setScrollOffset(clamped)
      setIsPinnedToBottom(clamped >= nextMax)
    },
    [history.length, visibleRows],
  )
  const scrollBy = useCallback(
    (delta: number) => {
      scrollTo(scrollOffset + delta)
    },
    [scrollOffset, scrollTo],
  )
  const resetHistory = useCallback(() => {
    const seed = [...initialEntriesRef.current]
    historyIdRef.current = seed.length
    setHistory(seed)
    setScrollOffset(Math.max(0, seed.length - visibleRows))
    setIsPinnedToBottom(true)
  }, [initialEntriesRef, visibleRows])
  const clearHistory = useCallback(() => {
    historyIdRef.current = 0
    setHistory([])
    setScrollOffset(0)
    setIsPinnedToBottom(true)
  }, [])
  return {
    history,
    pushHistory,
    resetHistory,
    clearHistory,
    scroll: {
      offset: scrollOffset,
      scrollTo,
      scrollBy,
    },
  }
}
</file>
<file path="src/tui/markdown/markdown-highlight.ts">
import type {
  InkColorValue,
  MarkdownThemeSlot,
  RequiredThemeSlot,
  ResolvedTheme,
} from '../theme/theme-types'
/**
 * Minimal markdown token styling for the history pane.
 *
 * This is intentionally *not* a full markdown renderer; itâs a lightweight
 * tokenizer that applies semantic theme slots to common markdown constructs so
 * generated prompts are easier to review in the TUI.
 */
export type MarkdownSpan = {
  text: string
  slot: MarkdownThemeSlot
  bold?: boolean
  italic?: boolean
  underline?: boolean
}
export type MarkdownLineTokenization = {
  spans: MarkdownSpan[]
  nextState: MarkdownTokenizationState
}
export type MarkdownTokenizationState = {
  inCodeBlock: boolean
}
export const DEFAULT_MARKDOWN_STATE: MarkdownTokenizationState = { inCodeBlock: false }
const SLOT_FALLBACKS: Record<MarkdownThemeSlot, RequiredThemeSlot> = {
  markdownText: 'text',
  markdownHeading: 'accent',
  markdownLink: 'accent',
  markdownLinkText: 'accent',
  markdownCode: 'warning',
  markdownBlockQuote: 'mutedText',
  markdownEmph: 'text',
  markdownStrong: 'text',
  markdownHorizontalRule: 'border',
  markdownListItem: 'mutedText',
  markdownListEnumeration: 'mutedText',
  markdownImage: 'accent',
  markdownImageText: 'text',
  markdownCodeBlock: 'mutedText',
}
export const resolveMarkdownSlotColor = (
  theme: ResolvedTheme,
  slot: MarkdownThemeSlot,
): InkColorValue => {
  const raw = theme[slot]
  if (raw !== undefined || Object.prototype.hasOwnProperty.call(theme, slot)) {
    return raw
  }
  return theme[SLOT_FALLBACKS[slot]]
}
const HR_RE = /^(?:\*\s*\*\s*\*\s*|-{3,}\s*|_{3,}\s*)$/
const HEADING_RE = /^(\s{0,3})(#{1,6})\s+(.*)$/
const BLOCKQUOTE_RE = /^(\s{0,3})>\s?(.*)$/
const BULLET_RE = /^(\s*)([-+*])\s+(.*)$/
const ENUM_RE = /^(\s*)(\d+)([.)])\s+(.*)$/
const startsFence = (line: string): boolean => /^\s*```/.test(line)
const pushSpan = (spans: MarkdownSpan[], span: MarkdownSpan) => {
  if (!span.text) {
    return
  }
  const last = spans[spans.length - 1]
  if (
    last &&
    last.slot === span.slot &&
    last.bold === span.bold &&
    last.italic === span.italic &&
    last.underline === span.underline
  ) {
    last.text += span.text
    return
  }
  spans.push(span)
}
const tokenizeInline = (text: string, baseSlot: MarkdownThemeSlot): MarkdownSpan[] => {
  const spans: MarkdownSpan[] = []
  let index = 0
  const flushPlain = (endExclusive: number) => {
    if (endExclusive <= index) {
      return
    }
    pushSpan(spans, { text: text.slice(index, endExclusive), slot: baseSlot })
    index = endExclusive
  }
  while (index < text.length) {
    const char = text[index]
    if (char === '`') {
      const end = text.indexOf('`', index + 1)
      if (end > index + 1) {
        flushPlain(index)
        pushSpan(spans, { text: text.slice(index, end + 1), slot: 'markdownCode' })
        index = end + 1
        continue
      }
    }
    if (text.startsWith('![', index)) {
      const closeBracket = text.indexOf(']', index + 2)
      const openParen = closeBracket >= 0 ? text.indexOf('(', closeBracket + 1) : -1
      const closeParen = openParen >= 0 ? text.indexOf(')', openParen + 1) : -1
      if (closeBracket >= 0 && openParen === closeBracket + 1 && closeParen > openParen) {
        flushPlain(index)
        pushSpan(spans, { text: '![', slot: 'markdownImage' })
        pushSpan(spans, {
          text: text.slice(index + 2, closeBracket),
          slot: 'markdownImageText',
        })
        pushSpan(spans, {
          text: text.slice(closeBracket, closeParen + 1),
          slot: 'markdownImage',
        })
        index = closeParen + 1
        continue
      }
    }
    if (char === '[') {
      const closeBracket = text.indexOf(']', index + 1)
      const openParen = closeBracket >= 0 ? text.indexOf('(', closeBracket + 1) : -1
      const closeParen = openParen >= 0 ? text.indexOf(')', openParen + 1) : -1
      if (closeBracket >= 0 && openParen === closeBracket + 1 && closeParen > openParen) {
        flushPlain(index)
        pushSpan(spans, { text: '[', slot: 'markdownLink' })
        pushSpan(spans, {
          text: text.slice(index + 1, closeBracket),
          slot: 'markdownLinkText',
          underline: true,
        })
        pushSpan(spans, {
          text: text.slice(closeBracket, closeParen + 1),
          slot: 'markdownLink',
        })
        index = closeParen + 1
        continue
      }
    }
    if (text.startsWith('**', index)) {
      const end = text.indexOf('**', index + 2)
      if (end > index + 2) {
        flushPlain(index)
        pushSpan(spans, { text: text.slice(index, end + 2), slot: 'markdownStrong', bold: true })
        index = end + 2
        continue
      }
    }
    if (char === '*') {
      const end = text.indexOf('*', index + 1)
      if (end > index + 1) {
        flushPlain(index)
        pushSpan(spans, { text: text.slice(index, end + 1), slot: 'markdownEmph', italic: true })
        index = end + 1
        continue
      }
    }
    const nextSpecialCandidates = [
      text.indexOf('`', index + 1),
      text.indexOf('![', index + 1),
      text.indexOf('[', index + 1),
      text.indexOf('*', index + 1),
    ].filter((value) => value >= 0)
    const nextSpecial = nextSpecialCandidates.length > 0 ? Math.min(...nextSpecialCandidates) : -1
    if (nextSpecial === -1) {
      flushPlain(text.length)
      break
    }
    flushPlain(nextSpecial)
  }
  return spans
}
const withLineStyle = (
  spans: MarkdownSpan[],
  style: Pick<MarkdownSpan, 'bold' | 'italic' | 'underline'>,
): MarkdownSpan[] => {
  if (!style.bold && !style.italic && !style.underline) {
    return spans
  }
  return spans.map((span) => ({
    ...span,
    ...(style.bold ? { bold: true } : {}),
    ...(style.italic ? { italic: true } : {}),
    ...(style.underline ? { underline: true } : {}),
  }))
}
export const tokenizeMarkdownLine = (
  line: string,
  state: MarkdownTokenizationState,
): MarkdownLineTokenization => {
  const trimmed = line.trimStart()
  if (startsFence(trimmed)) {
    return {
      spans: [{ text: line, slot: 'markdownCodeBlock' }],
      nextState: { inCodeBlock: !state.inCodeBlock },
    }
  }
  if (state.inCodeBlock) {
    return {
      spans: [{ text: line, slot: 'markdownCodeBlock' }],
      nextState: state,
    }
  }
  if (HR_RE.test(trimmed)) {
    return {
      spans: [{ text: line, slot: 'markdownHorizontalRule' }],
      nextState: state,
    }
  }
  const heading = HEADING_RE.exec(line)
  if (heading) {
    const indent = heading[1] ?? ''
    const hashes = heading[2] ?? ''
    const rest = heading[3] ?? ''
    const spans: MarkdownSpan[] = []
    if (indent) {
      pushSpan(spans, { text: indent, slot: 'markdownHeading' })
    }
    pushSpan(spans, { text: `${hashes} `, slot: 'markdownHeading' })
    tokenizeInline(rest, 'markdownHeading').forEach((span) => {
      pushSpan(spans, span)
    })
    return {
      spans: withLineStyle(spans, { bold: true }),
      nextState: state,
    }
  }
  const quote = BLOCKQUOTE_RE.exec(line)
  if (quote) {
    const indent = quote[1] ?? ''
    const rest = quote[2] ?? ''
    const spans: MarkdownSpan[] = []
    if (indent) {
      pushSpan(spans, { text: indent, slot: 'markdownBlockQuote' })
    }
    pushSpan(spans, { text: '> ', slot: 'markdownBlockQuote' })
    tokenizeInline(rest, 'markdownBlockQuote').forEach((span) => pushSpan(spans, span))
    return { spans, nextState: state }
  }
  const enumeration = ENUM_RE.exec(line)
  if (enumeration) {
    const indent = enumeration[1] ?? ''
    const number = enumeration[2] ?? ''
    const punctuation = enumeration[3] ?? '.'
    const rest = enumeration[4] ?? ''
    const spans: MarkdownSpan[] = []
    if (indent) {
      pushSpan(spans, { text: indent, slot: 'markdownText' })
    }
    pushSpan(spans, { text: `${number}${punctuation} `, slot: 'markdownListEnumeration' })
    tokenizeInline(rest, 'markdownText').forEach((span) => pushSpan(spans, span))
    return { spans, nextState: state }
  }
  const bullet = BULLET_RE.exec(line)
  if (bullet) {
    const indent = bullet[1] ?? ''
    const marker = bullet[2] ?? '-'
    const rest = bullet[3] ?? ''
    const spans: MarkdownSpan[] = []
    if (indent) {
      pushSpan(spans, { text: indent, slot: 'markdownText' })
    }
    pushSpan(spans, { text: `${marker} `, slot: 'markdownListItem' })
    tokenizeInline(rest, 'markdownText').forEach((span) => pushSpan(spans, span))
    return { spans, nextState: state }
  }
  return {
    spans: tokenizeInline(line, 'markdownText'),
    nextState: state,
  }
}
export const tokenizeMarkdownLines = (lines: readonly string[]): MarkdownSpan[][] => {
  let state = DEFAULT_MARKDOWN_STATE
  return lines.map((line) => {
    const tokenized = tokenizeMarkdownLine(line, state)
    state = tokenized.nextState
    return tokenized.spans
  })
}
</file>
<file path="src/tui/screens/command/hooks/context-popup-glue/popup-state-mutations.ts">
import { parseAbsolutePathFromInput } from '../../../../drag-drop-path'
import type { PopupState } from '../../../../types'
export const clampSelectionIndex = (selectionIndex: number, itemsLength: number): number =>
  Math.min(selectionIndex, Math.max(itemsLength - 1, 0))
type DeriveSuggestionsViewModelOptions = {
  draft: string
  suggestedItems: string[]
  suggestedSelectionIndex: number
  suggestedFocused: boolean
  filterSuggestions: (query: string, suggestions: string[]) => string[]
}
export type SuggestionsViewModel = {
  suggestions: string[]
  selectionIndex: number
  focused: boolean
  shouldDefocus: boolean
}
export const deriveSuggestionsViewModel = ({
  draft,
  suggestedItems,
  suggestedSelectionIndex,
  suggestedFocused,
  filterSuggestions,
}: DeriveSuggestionsViewModelOptions): SuggestionsViewModel => {
  if (suggestedItems.length === 0) {
    return {
      suggestions: [],
      selectionIndex: 0,
      focused: false,
      shouldDefocus: suggestedFocused,
    }
  }
  if (!draft.trim()) {
    return {
      suggestions: [],
      selectionIndex: 0,
      focused: false,
      shouldDefocus: suggestedFocused,
    }
  }
  const suggestions = filterSuggestions(draft, suggestedItems)
  const selectionIndex = clampSelectionIndex(suggestedSelectionIndex, suggestions.length)
  return {
    suggestions,
    selectionIndex,
    focused: suggestedFocused && suggestions.length > 0,
    shouldDefocus: suggestedFocused && suggestions.length === 0,
  }
}
type SuggestionFocusState = {
  suggestedFocused: boolean
  suggestedSelectionIndex: number
}
export const defocusSuggestionsIfEmpty = <T extends SuggestionFocusState>(
  prev: T,
  suggestionsLength: number,
): T => {
  if (!prev.suggestedFocused) {
    return prev
  }
  if (suggestionsLength > 0) {
    return prev
  }
  return { ...prev, suggestedFocused: false, suggestedSelectionIndex: 0 }
}
type SuggestedInputPopupState = {
  draft: string
  suggestedSelectionIndex: number
  suggestedFocused: boolean
}
export const updateSuggestedInputDraft = <T extends SuggestedInputPopupState>(
  prev: T,
  nextDraft: string,
): T => ({ ...prev, draft: nextDraft, suggestedSelectionIndex: 0, suggestedFocused: false })
type SuggestedSelectedListPopupState = {
  draft: string
  selectionIndex: number
  selectedFocused: boolean
  suggestedSelectionIndex: number
  suggestedFocused: boolean
}
export const updateSuggestedSelectedListDraft = <T extends SuggestedSelectedListPopupState>(
  prev: T,
  nextDraft: string,
): T => ({
  ...prev,
  draft: nextDraft,
  selectedFocused: false,
  suggestedSelectionIndex: 0,
  suggestedFocused: false,
})
export const resetSuggestedSelectedListAfterAdd = <T extends SuggestedSelectedListPopupState>(
  prev: T,
  nextSelectionIndex: number,
): T => ({
  ...prev,
  draft: '',
  selectionIndex: nextSelectionIndex,
  selectedFocused: false,
  suggestedFocused: false,
  suggestedSelectionIndex: 0,
})
export const getAutoAddAbsolutePathCandidate = (
  draft: string,
  isFilePath: (candidate: string) => boolean,
): string | null => {
  const candidate = parseAbsolutePathFromInput(draft)
  if (!candidate) {
    return null
  }
  return isFilePath(candidate) ? candidate : null
}
export const guardPopupUpdater = <T extends NonNullable<PopupState>['type']>(
  popupType: T,
  updater: (prev: Extract<NonNullable<PopupState>, { type: T }>) => PopupState,
): ((prev: PopupState) => PopupState) => {
  return (prev) => {
    if (!prev || prev.type !== popupType) {
      return prev
    }
    return updater(prev as Extract<NonNullable<PopupState>, { type: T }>)
  }
}
</file>
<file path="src/tui/screens/command/hooks/context-popup-glue/useFilePopupGlue.ts">
import { useCallback, useEffect, useMemo } from 'react'
import { stripTerminalPasteArtifacts } from '../../../../components/core/bracketed-paste'
import { filterFileSuggestions } from '../../../../file-suggestions'
import type { PopupState } from '../../../../types'
import {
  defocusSuggestionsIfEmpty,
  deriveSuggestionsViewModel,
  getAutoAddAbsolutePathCandidate,
  guardPopupUpdater,
  resetSuggestedSelectedListAfterAdd,
  updateSuggestedSelectedListDraft,
} from './popup-state-mutations'
const EMPTY_SUGGESTIONS: string[] = []
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
export type UseFilePopupGlueOptions = {
  popupState: PopupState
  files: string[]
  setPopupState: SetPopupState
  pushHistory: (content: string, kind?: 'system' | 'user' | 'progress') => void
  addFile: (value: string) => void
  removeFile: (index: number) => void
  consumeSuppressedTextInputChange: () => boolean
  isFilePath: (candidate: string) => boolean
}
export type UseFilePopupGlueResult = {
  filePopupSuggestions: string[]
  filePopupSuggestionSelectionIndex: number
  filePopupSuggestionsFocused: boolean
  onFilePopupDraftChange: (next: string) => void
  onAddFile: (value: string) => void
  onRemoveFile: (index: number) => void
}
export const useFilePopupGlue = ({
  popupState,
  files,
  setPopupState,
  pushHistory,
  addFile,
  removeFile,
  consumeSuppressedTextInputChange,
  isFilePath,
}: UseFilePopupGlueOptions): UseFilePopupGlueResult => {
  const addFileToContext = useCallback(
    (value: string): void => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      if (files.includes(trimmed)) {
        pushHistory(`Context file already added: ${trimmed}`)
        return
      }
      addFile(trimmed)
      pushHistory(`Context file added: ${trimmed}`)
    },
    [addFile, files, pushHistory],
  )
  const onAddFile = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      addFileToContext(trimmed)
      setPopupState(
        guardPopupUpdater('file', (prev) =>
          resetSuggestedSelectedListAfterAdd(prev, Math.max(files.length, 0)),
        ),
      )
    },
    [addFileToContext, files.length, setPopupState],
  )
  useEffect(() => {
    if (popupState?.type !== 'file') {
      return
    }
    const candidate = getAutoAddAbsolutePathCandidate(popupState.draft, isFilePath)
    if (!candidate) {
      return
    }
    onAddFile(candidate)
  }, [isFilePath, onAddFile, popupState])
  const onRemoveFile = useCallback(
    (index: number) => {
      if (index < 0 || index >= files.length) {
        return
      }
      const target = files[index]
      removeFile(index)
      pushHistory(`Context file removed: ${target}`)
    },
    [files, pushHistory, removeFile],
  )
  const filePopupDraft = popupState?.type === 'file' ? popupState.draft : ''
  const filePopupSuggestedItems =
    popupState?.type === 'file' ? popupState.suggestedItems : EMPTY_SUGGESTIONS
  const filePopupSuggestedFocused =
    popupState?.type === 'file' ? popupState.suggestedFocused : false
  const filePopupSuggestedSelectionIndex =
    popupState?.type === 'file' ? popupState.suggestedSelectionIndex : 0
  const suggestionsModel = useMemo(
    () =>
      deriveSuggestionsViewModel({
        draft: filePopupDraft,
        suggestedItems: filePopupSuggestedItems,
        suggestedSelectionIndex: filePopupSuggestedSelectionIndex,
        suggestedFocused: filePopupSuggestedFocused,
        filterSuggestions: (query, suggestions) =>
          filterFileSuggestions({ suggestions, query, exclude: files }),
      }),
    [
      filePopupDraft,
      filePopupSuggestedFocused,
      filePopupSuggestedItems,
      filePopupSuggestedSelectionIndex,
      files,
    ],
  )
  useEffect(() => {
    if (popupState?.type !== 'file') {
      return
    }
    if (!suggestionsModel.shouldDefocus) {
      return
    }
    setPopupState(
      guardPopupUpdater('file', (prev) =>
        defocusSuggestionsIfEmpty(prev, suggestionsModel.suggestions.length),
      ),
    )
  }, [
    popupState?.type,
    setPopupState,
    suggestionsModel.shouldDefocus,
    suggestionsModel.suggestions.length,
  ])
  const onFilePopupDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      const sanitized = stripTerminalPasteArtifacts(next)
      setPopupState(
        guardPopupUpdater('file', (prev) => updateSuggestedSelectedListDraft(prev, sanitized)),
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  return {
    filePopupSuggestions: suggestionsModel.suggestions,
    filePopupSuggestionSelectionIndex: suggestionsModel.selectionIndex,
    filePopupSuggestionsFocused: suggestionsModel.focused,
    onFilePopupDraftChange,
    onAddFile,
    onRemoveFile,
  }
}
</file>
<file path="src/tui/screens/command/hooks/context-popup-glue/useImagePopupGlue.ts">
import { useCallback, useEffect, useMemo } from 'react'
import { stripTerminalPasteArtifacts } from '../../../../components/core/bracketed-paste'
import { filterFileSuggestions } from '../../../../file-suggestions'
import type { PopupState } from '../../../../types'
import {
  defocusSuggestionsIfEmpty,
  deriveSuggestionsViewModel,
  getAutoAddAbsolutePathCandidate,
  guardPopupUpdater,
  resetSuggestedSelectedListAfterAdd,
  updateSuggestedSelectedListDraft,
} from './popup-state-mutations'
const EMPTY_SUGGESTIONS: string[] = []
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
export type UseImagePopupGlueOptions = {
  popupState: PopupState
  images: string[]
  setPopupState: SetPopupState
  pushHistory: (content: string, kind?: 'system' | 'user' | 'progress') => void
  addImage: (value: string) => void
  removeImage: (index: number) => void
  consumeSuppressedTextInputChange: () => boolean
  isFilePath: (candidate: string) => boolean
}
export type UseImagePopupGlueResult = {
  imagePopupSuggestions: string[]
  imagePopupSuggestionSelectionIndex: number
  imagePopupSuggestionsFocused: boolean
  onImagePopupDraftChange: (next: string) => void
  onAddImage: (value: string) => void
  onRemoveImage: (index: number) => void
}
export const useImagePopupGlue = ({
  popupState,
  images,
  setPopupState,
  pushHistory,
  addImage,
  removeImage,
  consumeSuppressedTextInputChange,
  isFilePath,
}: UseImagePopupGlueOptions): UseImagePopupGlueResult => {
  const addImageToContext = useCallback(
    (value: string): void => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      if (images.includes(trimmed)) {
        pushHistory(`[image] Already attached: ${trimmed}`)
        return
      }
      addImage(trimmed)
      pushHistory(`[image] Attached: ${trimmed}`)
    },
    [addImage, images, pushHistory],
  )
  const onAddImage = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      addImageToContext(trimmed)
      setPopupState(
        guardPopupUpdater('image', (prev) =>
          resetSuggestedSelectedListAfterAdd(prev, Math.max(images.length, 0)),
        ),
      )
    },
    [addImageToContext, images.length, setPopupState],
  )
  useEffect(() => {
    if (popupState?.type !== 'image') {
      return
    }
    const candidate = getAutoAddAbsolutePathCandidate(popupState.draft, isFilePath)
    if (!candidate) {
      return
    }
    onAddImage(candidate)
  }, [isFilePath, onAddImage, popupState])
  const onRemoveImage = useCallback(
    (index: number) => {
      if (index < 0 || index >= images.length) {
        return
      }
      const target = images[index]
      removeImage(index)
      pushHistory(`[image] Removed: ${target}`)
    },
    [images, pushHistory, removeImage],
  )
  const imagePopupDraft = popupState?.type === 'image' ? popupState.draft : ''
  const imagePopupSuggestedItems =
    popupState?.type === 'image' ? popupState.suggestedItems : EMPTY_SUGGESTIONS
  const imagePopupSuggestedFocused =
    popupState?.type === 'image' ? popupState.suggestedFocused : false
  const imagePopupSuggestedSelectionIndex =
    popupState?.type === 'image' ? popupState.suggestedSelectionIndex : 0
  const suggestionsModel = useMemo(
    () =>
      deriveSuggestionsViewModel({
        draft: imagePopupDraft,
        suggestedItems: imagePopupSuggestedItems,
        suggestedSelectionIndex: imagePopupSuggestedSelectionIndex,
        suggestedFocused: imagePopupSuggestedFocused,
        filterSuggestions: (query, suggestions) =>
          filterFileSuggestions({ suggestions, query, exclude: images }),
      }),
    [
      imagePopupDraft,
      imagePopupSuggestedFocused,
      imagePopupSuggestedItems,
      imagePopupSuggestedSelectionIndex,
      images,
    ],
  )
  useEffect(() => {
    if (popupState?.type !== 'image') {
      return
    }
    if (!suggestionsModel.shouldDefocus) {
      return
    }
    setPopupState(
      guardPopupUpdater('image', (prev) =>
        defocusSuggestionsIfEmpty(prev, suggestionsModel.suggestions.length),
      ),
    )
  }, [
    popupState?.type,
    setPopupState,
    suggestionsModel.shouldDefocus,
    suggestionsModel.suggestions.length,
  ])
  const onImagePopupDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      const sanitized = stripTerminalPasteArtifacts(next)
      setPopupState(
        guardPopupUpdater('image', (prev) => updateSuggestedSelectedListDraft(prev, sanitized)),
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  return {
    imagePopupSuggestions: suggestionsModel.suggestions,
    imagePopupSuggestionSelectionIndex: suggestionsModel.selectionIndex,
    imagePopupSuggestionsFocused: suggestionsModel.focused,
    onImagePopupDraftChange,
    onAddImage,
    onRemoveImage,
  }
}
</file>
<file path="src/tui/screens/command/hooks/context-popup-glue/usePdfPopupGlue.ts">
import { useCallback, useEffect, useMemo } from 'react'
import { stripTerminalPasteArtifacts } from '../../../../components/core/bracketed-paste'
import { filterFileSuggestions } from '../../../../file-suggestions'
import type { PopupState } from '../../../../types'
import {
  defocusSuggestionsIfEmpty,
  deriveSuggestionsViewModel,
  getAutoAddAbsolutePathCandidate,
  guardPopupUpdater,
  resetSuggestedSelectedListAfterAdd,
  updateSuggestedSelectedListDraft,
} from './popup-state-mutations'
const EMPTY_SUGGESTIONS: string[] = []
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
export type UsePdfPopupGlueOptions = {
  popupState: PopupState
  pdfs: string[]
  setPopupState: SetPopupState
  pushHistory: (content: string, kind?: 'system' | 'user' | 'progress') => void
  addPdf: (value: string) => void
  removePdf: (index: number) => void
  consumeSuppressedTextInputChange: () => boolean
  isFilePath: (candidate: string) => boolean
}
export type UsePdfPopupGlueResult = {
  pdfPopupSuggestions: string[]
  pdfPopupSuggestionSelectionIndex: number
  pdfPopupSuggestionsFocused: boolean
  onPdfPopupDraftChange: (next: string) => void
  onAddPdf: (value: string) => void
  onRemovePdf: (index: number) => void
}
export const usePdfPopupGlue = ({
  popupState,
  pdfs,
  setPopupState,
  pushHistory,
  addPdf,
  removePdf,
  consumeSuppressedTextInputChange,
  isFilePath,
}: UsePdfPopupGlueOptions): UsePdfPopupGlueResult => {
  const addPdfToContext = useCallback(
    (value: string): void => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      if (pdfs.includes(trimmed)) {
        pushHistory(`[pdf] Already attached: ${trimmed}`)
        return
      }
      addPdf(trimmed)
      pushHistory(`[pdf] Attached: ${trimmed}`)
    },
    [addPdf, pdfs, pushHistory],
  )
  const onAddPdf = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      addPdfToContext(trimmed)
      setPopupState(
        guardPopupUpdater('pdf', (prev) =>
          resetSuggestedSelectedListAfterAdd(prev, Math.max(pdfs.length, 0)),
        ),
      )
    },
    [addPdfToContext, pdfs.length, setPopupState],
  )
  useEffect(() => {
    if (popupState?.type !== 'pdf') {
      return
    }
    const candidate = getAutoAddAbsolutePathCandidate(popupState.draft, isFilePath)
    if (!candidate) {
      return
    }
    onAddPdf(candidate)
  }, [isFilePath, onAddPdf, popupState])
  const onRemovePdf = useCallback(
    (index: number) => {
      if (index < 0 || index >= pdfs.length) {
        return
      }
      const target = pdfs[index]
      removePdf(index)
      pushHistory(`[pdf] Removed: ${target}`)
    },
    [pdfs, pushHistory, removePdf],
  )
  const pdfPopupDraft = popupState?.type === 'pdf' ? popupState.draft : ''
  const pdfPopupSuggestedItems =
    popupState?.type === 'pdf' ? popupState.suggestedItems : EMPTY_SUGGESTIONS
  const pdfPopupSuggestedFocused = popupState?.type === 'pdf' ? popupState.suggestedFocused : false
  const pdfPopupSuggestedSelectionIndex =
    popupState?.type === 'pdf' ? popupState.suggestedSelectionIndex : 0
  const suggestionsModel = useMemo(
    () =>
      deriveSuggestionsViewModel({
        draft: pdfPopupDraft,
        suggestedItems: pdfPopupSuggestedItems,
        suggestedSelectionIndex: pdfPopupSuggestedSelectionIndex,
        suggestedFocused: pdfPopupSuggestedFocused,
        filterSuggestions: (query, suggestions) =>
          filterFileSuggestions({ suggestions, query, exclude: pdfs }),
      }),
    [
      pdfPopupDraft,
      pdfPopupSuggestedFocused,
      pdfPopupSuggestedItems,
      pdfPopupSuggestedSelectionIndex,
      pdfs,
    ],
  )
  useEffect(() => {
    if (popupState?.type !== 'pdf') {
      return
    }
    if (!suggestionsModel.shouldDefocus) {
      return
    }
    setPopupState(
      guardPopupUpdater('pdf', (prev) =>
        defocusSuggestionsIfEmpty(prev, suggestionsModel.suggestions.length),
      ),
    )
  }, [
    popupState?.type,
    setPopupState,
    suggestionsModel.shouldDefocus,
    suggestionsModel.suggestions.length,
  ])
  const onPdfPopupDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      const sanitized = stripTerminalPasteArtifacts(next)
      setPopupState(
        guardPopupUpdater('pdf', (prev) => updateSuggestedSelectedListDraft(prev, sanitized)),
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  return {
    pdfPopupSuggestions: suggestionsModel.suggestions,
    pdfPopupSuggestionSelectionIndex: suggestionsModel.selectionIndex,
    pdfPopupSuggestionsFocused: suggestionsModel.focused,
    onPdfPopupDraftChange,
    onAddPdf,
    onRemovePdf,
  }
}
</file>
<file path="src/tui/screens/command/hooks/context-popup-glue/useSmartPopupGlue.ts">
import { useCallback, useEffect, useMemo } from 'react'
import { stripTerminalPasteArtifacts } from '../../../../components/core/bracketed-paste'
import { filterDirectorySuggestions } from '../../../../file-suggestions'
import type { PopupState } from '../../../../types'
import {
  defocusSuggestionsIfEmpty,
  deriveSuggestionsViewModel,
  guardPopupUpdater,
  updateSuggestedInputDraft,
} from './popup-state-mutations'
const EMPTY_SUGGESTIONS: string[] = []
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
export type UseSmartPopupGlueOptions = {
  popupState: PopupState
  smartContextEnabled: boolean
  smartContextRoot: string | null
  setPopupState: SetPopupState
  notify: (message: string) => void
  toggleSmartContext: () => void
  setSmartRoot: (value: string) => void
  consumeSuppressedTextInputChange: () => boolean
}
export type UseSmartPopupGlueResult = {
  smartPopupSuggestions: string[]
  smartPopupSuggestionSelectionIndex: number
  smartPopupSuggestionsFocused: boolean
  onSmartPopupDraftChange: (next: string) => void
  onSmartToggle: (nextEnabled: boolean) => void
  onSmartRootSubmit: (value: string) => void
}
export const useSmartPopupGlue = ({
  popupState,
  smartContextEnabled,
  smartContextRoot,
  setPopupState,
  notify,
  toggleSmartContext,
  setSmartRoot,
  consumeSuppressedTextInputChange,
}: UseSmartPopupGlueOptions): UseSmartPopupGlueResult => {
  const onSmartToggle = useCallback(
    (nextEnabled: boolean) => {
      if (smartContextEnabled === nextEnabled) {
        return
      }
      const shouldClearRoot = !nextEnabled && Boolean(smartContextRoot)
      if (shouldClearRoot) {
        setSmartRoot('')
        setPopupState((prev) =>
          prev?.type === 'smart' && prev.draft === smartContextRoot
            ? updateSuggestedInputDraft(prev, '')
            : prev,
        )
      }
      toggleSmartContext()
      notify(
        nextEnabled
          ? 'Smart context enabled'
          : shouldClearRoot
            ? 'Smart context disabled; root cleared'
            : 'Smart context disabled',
      )
    },
    [
      notify,
      setPopupState,
      setSmartRoot,
      smartContextEnabled,
      smartContextRoot,
      toggleSmartContext,
    ],
  )
  const onSmartRootSubmit = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      const shouldEnable = Boolean(trimmed) && !smartContextEnabled
      setSmartRoot(trimmed)
      if (shouldEnable) {
        toggleSmartContext()
      }
      notify(
        trimmed
          ? shouldEnable
            ? `Smart context enabled; root set to ${trimmed}`
            : `Smart context root set to ${trimmed}`
          : 'Smart context root cleared',
      )
      if (trimmed) {
        setPopupState((prev) => (prev?.type === 'smart' ? null : prev))
        return
      }
      setPopupState(guardPopupUpdater('smart', (prev) => updateSuggestedInputDraft(prev, trimmed)))
    },
    [notify, setPopupState, setSmartRoot, smartContextEnabled, toggleSmartContext],
  )
  const smartPopupDraft = popupState?.type === 'smart' ? popupState.draft : ''
  const smartPopupSuggestedItems =
    popupState?.type === 'smart' ? popupState.suggestedItems : EMPTY_SUGGESTIONS
  const smartPopupSuggestedFocused =
    popupState?.type === 'smart' ? popupState.suggestedFocused : false
  const smartPopupSuggestedSelectionIndex =
    popupState?.type === 'smart' ? popupState.suggestedSelectionIndex : 0
  const suggestionsModel = useMemo(
    () =>
      deriveSuggestionsViewModel({
        draft: smartPopupDraft,
        suggestedItems: smartPopupSuggestedItems,
        suggestedSelectionIndex: smartPopupSuggestedSelectionIndex,
        suggestedFocused: smartPopupSuggestedFocused,
        filterSuggestions: (query, suggestions) => {
          const excluded = smartContextRoot ? [smartContextRoot] : []
          return filterDirectorySuggestions({ suggestions, query, exclude: excluded })
        },
      }),
    [
      smartContextRoot,
      smartPopupDraft,
      smartPopupSuggestedFocused,
      smartPopupSuggestedItems,
      smartPopupSuggestedSelectionIndex,
    ],
  )
  useEffect(() => {
    if (popupState?.type !== 'smart') {
      return
    }
    if (!suggestionsModel.shouldDefocus) {
      return
    }
    setPopupState(
      guardPopupUpdater('smart', (prev) =>
        defocusSuggestionsIfEmpty(prev, suggestionsModel.suggestions.length),
      ),
    )
  }, [
    popupState?.type,
    setPopupState,
    suggestionsModel.shouldDefocus,
    suggestionsModel.suggestions.length,
  ])
  const onSmartPopupDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      const sanitized = stripTerminalPasteArtifacts(next)
      setPopupState(
        guardPopupUpdater('smart', (prev) => updateSuggestedInputDraft(prev, sanitized)),
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  return {
    smartPopupSuggestions: suggestionsModel.suggestions,
    smartPopupSuggestionSelectionIndex: suggestionsModel.selectionIndex,
    smartPopupSuggestionsFocused: suggestionsModel.focused,
    onSmartPopupDraftChange,
    onSmartToggle,
    onSmartRootSubmit,
  }
}
</file>
<file path="src/tui/screens/command/hooks/context-popup-glue/useUrlPopupGlue.ts">
import { useCallback } from 'react'
import { parseUrlArgs, validateHttpUrlCandidate } from '../../utils/url-args'
import type { PopupState } from '../../../../types'
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
export type UseUrlPopupGlueOptions = {
  popupState: PopupState
  urls: string[]
  setPopupState: SetPopupState
  pushHistory: (content: string, kind?: 'system' | 'user' | 'progress') => void
  addUrl: (value: string) => void
  removeUrl: (index: number) => void
  updateUrl: (index: number, value: string) => void
  consumeSuppressedTextInputChange: () => boolean
}
export type UseUrlPopupGlueResult = {
  onUrlPopupDraftChange: (next: string) => void
  onAddUrl: (value: string) => void
  onRemoveUrl: (index: number) => void
}
export const useUrlPopupGlue = ({
  popupState,
  urls,
  setPopupState,
  pushHistory,
  addUrl,
  removeUrl,
  updateUrl,
  consumeSuppressedTextInputChange,
}: UseUrlPopupGlueOptions): UseUrlPopupGlueResult => {
  const onAddUrl = useCallback(
    (value: string) => {
      const currentPopup = popupState?.type === 'url' ? popupState : null
      const editingIndex = currentPopup?.editingIndex ?? null
      if (editingIndex !== null) {
        if (editingIndex < 0 || editingIndex >= urls.length) {
          setPopupState((prev) =>
            prev?.type === 'url'
              ? { ...prev, draft: '', editingIndex: null, selectedFocused: false }
              : prev,
          )
          return
        }
        const trimmed = value.trim()
        if (!trimmed) {
          pushHistory('Warning: URL cannot be empty.', 'system')
          return
        }
        const validation = validateHttpUrlCandidate(trimmed)
        if (!validation.ok) {
          pushHistory(`Warning: ${validation.message}`, 'system')
          return
        }
        const duplicateIndex = urls.findIndex(
          (existing, idx) => idx !== editingIndex && existing === trimmed,
        )
        if (duplicateIndex !== -1) {
          pushHistory(`Context URL already added: ${trimmed}`, 'system')
          return
        }
        const previous = urls[editingIndex]
        if (!previous) {
          return
        }
        if (previous === trimmed) {
          setPopupState((prev) =>
            prev?.type === 'url'
              ? { ...prev, draft: '', editingIndex: null, selectedFocused: false }
              : prev,
          )
          return
        }
        updateUrl(editingIndex, trimmed)
        pushHistory(`Context URL updated: ${previous} â ${trimmed}`)
        setPopupState((prev) =>
          prev?.type === 'url'
            ? { ...prev, draft: '', editingIndex: null, selectedFocused: false }
            : prev,
        )
        return
      }
      const candidates = parseUrlArgs(value)
      if (candidates.length === 0) {
        return
      }
      const seen = new Set<string>()
      const baseIndex = urls.length
      let addedCount = 0
      for (const candidate of candidates) {
        if (seen.has(candidate)) {
          continue
        }
        seen.add(candidate)
        const validation = validateHttpUrlCandidate(candidate)
        if (!validation.ok) {
          pushHistory(`Warning: ${validation.message}`, 'system')
          continue
        }
        if (urls.includes(candidate)) {
          pushHistory(`Context URL already added: ${candidate}`, 'system')
          continue
        }
        addUrl(candidate)
        addedCount += 1
        pushHistory(`Context URL added: ${candidate}`)
      }
      if (addedCount === 0) {
        return
      }
      setPopupState((prev) =>
        prev?.type === 'url'
          ? {
              ...prev,
              draft: '',
              selectionIndex: Math.max(baseIndex + addedCount - 1, 0),
              selectedFocused: false,
              editingIndex: null,
            }
          : prev,
      )
    },
    [addUrl, popupState, pushHistory, setPopupState, updateUrl, urls],
  )
  const onRemoveUrl = useCallback(
    (index: number) => {
      if (index < 0 || index >= urls.length) {
        return
      }
      const target = urls[index]
      if (!target) {
        return
      }
      removeUrl(index)
      pushHistory(`Context URL removed: ${target}`)
      setPopupState((prev) => {
        if (prev?.type !== 'url') {
          return prev
        }
        const nextMaxIndex = Math.max(urls.length - 2, 0)
        const selectionIndexBefore = prev.selectionIndex
        const nextSelectionIndexUnclamped =
          selectionIndexBefore > index
            ? selectionIndexBefore - 1
            : selectionIndexBefore === index
              ? Math.min(index, nextMaxIndex)
              : selectionIndexBefore
        const nextEditingIndex =
          prev.editingIndex === null
            ? null
            : index === prev.editingIndex
              ? null
              : index < prev.editingIndex
                ? prev.editingIndex - 1
                : prev.editingIndex
        const nextDraft = index === prev.editingIndex ? '' : prev.draft
        return {
          ...prev,
          selectionIndex: Math.min(nextSelectionIndexUnclamped, nextMaxIndex),
          selectedFocused: urls.length > 1 ? prev.selectedFocused : false,
          editingIndex: nextEditingIndex,
          draft: nextDraft,
        }
      })
    },
    [pushHistory, removeUrl, setPopupState, urls],
  )
  const onUrlPopupDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) => (prev?.type === 'url' ? { ...prev, draft: next } : prev))
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  return {
    onUrlPopupDraftChange,
    onAddUrl,
    onRemoveUrl,
  }
}
</file>
<file path="src/tui/screens/command/hooks/context-popup-glue/useVideoPopupGlue.ts">
import { useCallback, useEffect, useMemo } from 'react'
import { stripTerminalPasteArtifacts } from '../../../../components/core/bracketed-paste'
import { filterFileSuggestions } from '../../../../file-suggestions'
import type { PopupState } from '../../../../types'
import {
  defocusSuggestionsIfEmpty,
  deriveSuggestionsViewModel,
  getAutoAddAbsolutePathCandidate,
  guardPopupUpdater,
  resetSuggestedSelectedListAfterAdd,
  updateSuggestedSelectedListDraft,
} from './popup-state-mutations'
const EMPTY_SUGGESTIONS: string[] = []
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
export type UseVideoPopupGlueOptions = {
  popupState: PopupState
  videos: string[]
  setPopupState: SetPopupState
  pushHistory: (content: string, kind?: 'system' | 'user' | 'progress') => void
  addVideo: (value: string) => void
  removeVideo: (index: number) => void
  consumeSuppressedTextInputChange: () => boolean
  isFilePath: (candidate: string) => boolean
}
export type UseVideoPopupGlueResult = {
  videoPopupSuggestions: string[]
  videoPopupSuggestionSelectionIndex: number
  videoPopupSuggestionsFocused: boolean
  onVideoPopupDraftChange: (next: string) => void
  onAddVideo: (value: string) => void
  onRemoveVideo: (index: number) => void
}
export const useVideoPopupGlue = ({
  popupState,
  videos,
  setPopupState,
  pushHistory,
  addVideo,
  removeVideo,
  consumeSuppressedTextInputChange,
  isFilePath,
}: UseVideoPopupGlueOptions): UseVideoPopupGlueResult => {
  const addVideoToContext = useCallback(
    (value: string): void => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      if (videos.includes(trimmed)) {
        pushHistory(`[video] Already attached: ${trimmed}`)
        return
      }
      addVideo(trimmed)
      pushHistory(`[video] Attached: ${trimmed}`)
    },
    [addVideo, pushHistory, videos],
  )
  const onAddVideo = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      addVideoToContext(trimmed)
      setPopupState(
        guardPopupUpdater('video', (prev) =>
          resetSuggestedSelectedListAfterAdd(prev, Math.max(videos.length, 0)),
        ),
      )
    },
    [addVideoToContext, setPopupState, videos.length],
  )
  useEffect(() => {
    if (popupState?.type !== 'video') {
      return
    }
    const candidate = getAutoAddAbsolutePathCandidate(popupState.draft, isFilePath)
    if (!candidate) {
      return
    }
    onAddVideo(candidate)
  }, [isFilePath, onAddVideo, popupState])
  const onRemoveVideo = useCallback(
    (index: number) => {
      if (index < 0 || index >= videos.length) {
        return
      }
      const target = videos[index]
      removeVideo(index)
      pushHistory(`[video] Removed: ${target}`)
    },
    [pushHistory, removeVideo, videos],
  )
  const videoPopupDraft = popupState?.type === 'video' ? popupState.draft : ''
  const videoPopupSuggestedItems =
    popupState?.type === 'video' ? popupState.suggestedItems : EMPTY_SUGGESTIONS
  const videoPopupSuggestedFocused =
    popupState?.type === 'video' ? popupState.suggestedFocused : false
  const videoPopupSuggestedSelectionIndex =
    popupState?.type === 'video' ? popupState.suggestedSelectionIndex : 0
  const suggestionsModel = useMemo(
    () =>
      deriveSuggestionsViewModel({
        draft: videoPopupDraft,
        suggestedItems: videoPopupSuggestedItems,
        suggestedSelectionIndex: videoPopupSuggestedSelectionIndex,
        suggestedFocused: videoPopupSuggestedFocused,
        filterSuggestions: (query, suggestions) =>
          filterFileSuggestions({ suggestions, query, exclude: videos }),
      }),
    [
      videoPopupDraft,
      videoPopupSuggestedFocused,
      videoPopupSuggestedItems,
      videoPopupSuggestedSelectionIndex,
      videos,
    ],
  )
  useEffect(() => {
    if (popupState?.type !== 'video') {
      return
    }
    if (!suggestionsModel.shouldDefocus) {
      return
    }
    setPopupState(
      guardPopupUpdater('video', (prev) =>
        defocusSuggestionsIfEmpty(prev, suggestionsModel.suggestions.length),
      ),
    )
  }, [
    popupState?.type,
    setPopupState,
    suggestionsModel.shouldDefocus,
    suggestionsModel.suggestions.length,
  ])
  const onVideoPopupDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      const sanitized = stripTerminalPasteArtifacts(next)
      setPopupState(
        guardPopupUpdater('video', (prev) => updateSuggestedSelectedListDraft(prev, sanitized)),
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  return {
    videoPopupSuggestions: suggestionsModel.suggestions,
    videoPopupSuggestionSelectionIndex: suggestionsModel.selectionIndex,
    videoPopupSuggestionsFocused: suggestionsModel.focused,
    onVideoPopupDraftChange,
    onAddVideo,
    onRemoveVideo,
  }
}
</file>
<file path="src/tui/screens/command/hooks/popup-shortcuts/budgets-popup-shortcuts.ts">
import type { Key } from 'ink'
import type { ContextOverflowStrategy } from '../../../../../config'
import type { PopupState } from '../../../../types'
import { wrapIndex } from './selection'
import { guardPopupUpdater } from './types'
const FIELD_COUNT = 3
const STRATEGY_OPTIONS = [
  '',
  'fail',
  'drop-smart',
  'drop-url',
  'drop-largest',
  'drop-oldest',
] as const satisfies ReadonlyArray<ContextOverflowStrategy | ''>
export type HandleBudgetsPopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'budgets' }>
  key: Key
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  onBudgetsSubmit: () => void
}
const cycleStrategy = (
  current: ContextOverflowStrategy | '',
  delta: number,
): ContextOverflowStrategy | '' => {
  const index = STRATEGY_OPTIONS.indexOf(current)
  const safeIndex = index >= 0 ? index : 0
  return STRATEGY_OPTIONS[wrapIndex(safeIndex + delta, STRATEGY_OPTIONS.length)] ?? ''
}
export const handleBudgetsPopupShortcuts = ({
  popupState,
  key,
  setPopupState,
  closePopup,
  onBudgetsSubmit,
}: HandleBudgetsPopupShortcutsOptions): void => {
  if (key.escape) {
    closePopup()
    return
  }
  if (key.upArrow) {
    setPopupState(
      guardPopupUpdater('budgets', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex - 1, FIELD_COUNT),
      })),
    )
    return
  }
  if (key.downArrow) {
    setPopupState(
      guardPopupUpdater('budgets', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex + 1, FIELD_COUNT),
      })),
    )
    return
  }
  if (popupState.selectionIndex === 2 && (key.leftArrow || key.rightArrow)) {
    const delta = key.rightArrow ? 1 : -1
    setPopupState(
      guardPopupUpdater('budgets', (prev) => ({
        ...prev,
        contextOverflowStrategyDraft: cycleStrategy(prev.contextOverflowStrategyDraft, delta),
        errorMessage: null,
      })),
    )
    return
  }
  if (popupState.selectionIndex === 2 && key.return) {
    onBudgetsSubmit()
  }
}
</file>
<file path="src/tui/screens/command/hooks/popup-shortcuts/escape-only-popup-shortcuts.ts">
import type { Key } from 'ink'
export const handleEscapeOnlyPopupShortcuts = (key: Key, closePopup: () => void): void => {
  if (key.escape) {
    closePopup()
  }
}
</file>
<file path="src/tui/screens/command/hooks/popup-shortcuts/export-popup-shortcuts.ts">
import path from 'node:path'
import type { Key } from 'ink'
import type { PopupState } from '../../../../types'
import { guardPopupUpdater } from './types'
export type HandleExportPopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'export' }>
  key: Key
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  onExportSubmit: () => void
}
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const replaceExtension = (filePath: string, nextExt: string): string => {
  const ext = path.extname(filePath)
  if (!ext) {
    return filePath
  }
  const lowered = ext.toLowerCase()
  if (lowered === '.json' || lowered === '.yaml' || lowered === '.yml') {
    return filePath.slice(0, Math.max(0, filePath.length - ext.length)).concat(nextExt)
  }
  return filePath
}
export const handleExportPopupShortcuts = ({
  popupState,
  key,
  setPopupState,
  closePopup,
  onExportSubmit,
}: HandleExportPopupShortcutsOptions): void => {
  if (key.escape) {
    closePopup()
    return
  }
  const historyCount = popupState.historyItems.length
  if (popupState.selectionIndex === 2) {
    if (key.upArrow && historyCount > 0) {
      if (popupState.historySelectionIndex === 0) {
        setPopupState(guardPopupUpdater('export', (prev) => ({ ...prev, selectionIndex: 1 })))
        return
      }
      setPopupState(
        guardPopupUpdater('export', (prev) => ({
          ...prev,
          historySelectionIndex: clamp(prev.historySelectionIndex - 1, 0, historyCount - 1),
        })),
      )
      return
    }
    if (key.downArrow && historyCount > 0) {
      setPopupState(
        guardPopupUpdater('export', (prev) => ({
          ...prev,
          historySelectionIndex: clamp(prev.historySelectionIndex + 1, 0, historyCount - 1),
        })),
      )
      return
    }
    if (key.return) {
      onExportSubmit()
      return
    }
  }
  if (key.upArrow) {
    setPopupState(
      guardPopupUpdater('export', (prev) => ({
        ...prev,
        selectionIndex: clamp(prev.selectionIndex - 1, 0, 2),
      })),
    )
    return
  }
  if (key.downArrow) {
    setPopupState(
      guardPopupUpdater('export', (prev) => ({
        ...prev,
        selectionIndex: clamp(prev.selectionIndex + 1, 0, 2),
      })),
    )
    return
  }
  if (popupState.selectionIndex === 0 && (key.leftArrow || key.rightArrow)) {
    setPopupState(
      guardPopupUpdater('export', (prev) => {
        const format = prev.format === 'json' ? 'yaml' : 'json'
        const nextExt = format === 'json' ? '.json' : '.yaml'
        return {
          ...prev,
          format,
          outPathDraft: replaceExtension(prev.outPathDraft, nextExt),
        }
      }),
    )
    return
  }
  if (key.return && popupState.selectionIndex < 2) {
    setPopupState(
      guardPopupUpdater('export', (prev) => ({
        ...prev,
        selectionIndex: clamp(prev.selectionIndex + 1, 0, 2),
      })),
    )
  }
}
</file>
<file path="src/tui/screens/command/hooks/popup-shortcuts/history-popup-shortcuts.ts">
import type { Key } from 'ink'
import type { PopupState } from '../../../../types'
import { guardPopupUpdater } from './types'
export type HandleHistoryPopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'history' }>
  key: Key
  itemCount: number
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
}
export const handleHistoryPopupShortcuts = ({
  key,
  itemCount,
  setPopupState,
  closePopup,
}: HandleHistoryPopupShortcutsOptions): void => {
  if (key.upArrow && itemCount > 0) {
    setPopupState(
      guardPopupUpdater('history', (prev) => ({
        ...prev,
        selectionIndex: Math.max(prev.selectionIndex - 1, 0),
      })),
    )
    return
  }
  if (key.downArrow && itemCount > 0) {
    setPopupState(
      guardPopupUpdater('history', (prev) => ({
        ...prev,
        selectionIndex: Math.min(prev.selectionIndex + 1, itemCount - 1),
      })),
    )
    return
  }
  if (key.escape) {
    closePopup()
  }
}
</file>
<file path="src/tui/screens/command/hooks/popup-shortcuts/intent-popup-shortcuts.ts">
import type { Key } from 'ink'
import type { PopupState } from '../../../../types'
import { getSuggestedInputPopupEffect } from './suggested-input'
export type HandleIntentPopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'intent' }>
  key: Key
  suggestions: readonly string[]
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  onIntentFileSubmit: (value: string) => void
}
export const handleIntentPopupShortcuts = ({
  popupState,
  key,
  suggestions,
  setPopupState,
  closePopup,
  onIntentFileSubmit,
}: HandleIntentPopupShortcutsOptions): void => {
  const effect = getSuggestedInputPopupEffect({
    popupType: 'intent',
    popupState,
    suggestions,
    key,
  })
  switch (effect.type) {
    case 'close':
      closePopup()
      return
    case 'set':
      setPopupState(effect.updater)
      return
    case 'selectSuggestion':
      onIntentFileSubmit(effect.value)
      return
    case 'remove':
    case 'none':
      return
    default: {
      const exhaustive: never = effect
      return exhaustive
    }
  }
}
</file>
<file path="src/tui/screens/command/hooks/popup-shortcuts/model-popup-shortcuts.ts">
import type { Key } from 'ink'
import { isBackspaceKey } from '../../../../components/core/text-input-keys'
import type { ModelOption, PopupState } from '../../../../types'
import { clampIndex, wrapIndex } from './selection'
import { guardPopupUpdater } from './types'
export type HandleModelPopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'model' }>
  input: string
  key: Key
  options: readonly ModelOption[]
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  onModelPopupSubmit: (option: ModelOption | null | undefined) => void
}
export const handleModelPopupShortcuts = ({
  popupState,
  input,
  key,
  options,
  setPopupState,
  closePopup,
  onModelPopupSubmit,
}: HandleModelPopupShortcutsOptions): void => {
  const modelSelectionIndex = clampIndex(popupState.selectionIndex, options.length)
  if (key.upArrow && options.length > 0) {
    setPopupState(
      guardPopupUpdater('model', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex - 1, options.length),
      })),
    )
    return
  }
  if (key.downArrow && options.length > 0) {
    setPopupState(
      guardPopupUpdater('model', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex + 1, options.length),
      })),
    )
    return
  }
  const draftIsEmpty = popupState.query.trim().length === 0
  if (
    popupState.kind === 'polish' &&
    (key.delete || (draftIsEmpty && isBackspaceKey(input, key)))
  ) {
    onModelPopupSubmit(null)
    return
  }
  if (key.escape) {
    closePopup()
    return
  }
  if (key.return) {
    onModelPopupSubmit(options[modelSelectionIndex])
  }
}
</file>
<file path="src/tui/screens/command/hooks/popup-shortcuts/reasoning-popup-shortcuts.ts">
import type { Key } from 'ink'
import type { PopupState } from '../../../../types'
import { clamp } from './selection'
import { guardPopupUpdater } from './types'
export type HandleReasoningPopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'reasoning' }>
  key: Key
  lineCount: number
  visibleRows: number
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
}
export const handleReasoningPopupShortcuts = ({
  key,
  lineCount,
  visibleRows,
  setPopupState,
  closePopup,
}: HandleReasoningPopupShortcutsOptions): void => {
  const maxOffset = Math.max(0, lineCount - visibleRows)
  if (key.upArrow) {
    setPopupState(
      guardPopupUpdater('reasoning', (prev) => ({
        ...prev,
        scrollOffset: clamp(prev.scrollOffset - 1, 0, maxOffset),
      })),
    )
    return
  }
  if (key.downArrow) {
    setPopupState(
      guardPopupUpdater('reasoning', (prev) => ({
        ...prev,
        scrollOffset: clamp(prev.scrollOffset + 1, 0, maxOffset),
      })),
    )
    return
  }
  if (key.pageUp) {
    setPopupState(
      guardPopupUpdater('reasoning', (prev) => ({
        ...prev,
        scrollOffset: clamp(prev.scrollOffset - visibleRows, 0, maxOffset),
      })),
    )
    return
  }
  if (key.pageDown) {
    setPopupState(
      guardPopupUpdater('reasoning', (prev) => ({
        ...prev,
        scrollOffset: clamp(prev.scrollOffset + visibleRows, 0, maxOffset),
      })),
    )
    return
  }
  if (key.escape) {
    closePopup()
  }
}
</file>
<file path="src/tui/screens/command/hooks/popup-shortcuts/resume-popup-shortcuts.ts">
import type { Key } from 'ink'
import type { PopupState } from '../../../../types'
import { guardPopupUpdater } from './types'
export type HandleResumePopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'resume' }>
  key: Key
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  onResumeSubmit: () => void
}
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
export const handleResumePopupShortcuts = ({
  popupState,
  key,
  setPopupState,
  closePopup,
  onResumeSubmit,
}: HandleResumePopupShortcutsOptions): void => {
  if (key.escape) {
    closePopup()
    return
  }
  const historyCount = popupState.historyItems.length
  const suggestionCount = popupState.suggestedItems.length
  if (
    key.tab &&
    popupState.selectionIndex === 2 &&
    popupState.sourceKind === 'file' &&
    suggestionCount
  ) {
    setPopupState(
      guardPopupUpdater('resume', (prev) => ({
        ...prev,
        suggestedFocused: !prev.suggestedFocused,
        suggestedSelectionIndex: 0,
      })),
    )
    return
  }
  if (popupState.selectionIndex === 2) {
    if (popupState.sourceKind === 'history') {
      if (key.upArrow && historyCount > 0) {
        if (popupState.historySelectionIndex === 0) {
          setPopupState(guardPopupUpdater('resume', (prev) => ({ ...prev, selectionIndex: 1 })))
          return
        }
        setPopupState(
          guardPopupUpdater('resume', (prev) => ({
            ...prev,
            historySelectionIndex: clamp(prev.historySelectionIndex - 1, 0, historyCount - 1),
          })),
        )
        return
      }
      if (key.downArrow && historyCount > 0) {
        setPopupState(
          guardPopupUpdater('resume', (prev) => ({
            ...prev,
            historySelectionIndex: clamp(prev.historySelectionIndex + 1, 0, historyCount - 1),
          })),
        )
        return
      }
      if (key.return) {
        onResumeSubmit()
        return
      }
    } else {
      if (popupState.suggestedFocused && suggestionCount > 0) {
        if (key.upArrow) {
          if (popupState.suggestedSelectionIndex === 0) {
            setPopupState(
              guardPopupUpdater('resume', (prev) => ({
                ...prev,
                suggestedFocused: false,
                suggestedSelectionIndex: 0,
              })),
            )
            return
          }
          setPopupState(
            guardPopupUpdater('resume', (prev) => ({
              ...prev,
              suggestedSelectionIndex: clamp(
                prev.suggestedSelectionIndex - 1,
                0,
                suggestionCount - 1,
              ),
            })),
          )
          return
        }
        if (key.downArrow) {
          setPopupState(
            guardPopupUpdater('resume', (prev) => ({
              ...prev,
              suggestedSelectionIndex: clamp(
                prev.suggestedSelectionIndex + 1,
                0,
                suggestionCount - 1,
              ),
            })),
          )
          return
        }
        if (key.return) {
          const selected = popupState.suggestedItems[popupState.suggestedSelectionIndex]
          if (!selected) {
            return
          }
          setPopupState(
            guardPopupUpdater('resume', (prev) => ({
              ...prev,
              payloadPathDraft: selected,
              suggestedFocused: false,
              suggestedSelectionIndex: 0,
            })),
          )
          return
        }
      }
      if (!popupState.suggestedFocused && key.downArrow && suggestionCount > 0) {
        setPopupState(
          guardPopupUpdater('resume', (prev) => ({
            ...prev,
            suggestedFocused: true,
            suggestedSelectionIndex: 0,
          })),
        )
        return
      }
      if (!popupState.suggestedFocused && key.upArrow) {
        setPopupState(guardPopupUpdater('resume', (prev) => ({ ...prev, selectionIndex: 1 })))
        return
      }
      if (!popupState.suggestedFocused && key.return) {
        onResumeSubmit()
        return
      }
    }
  }
  if (key.upArrow) {
    setPopupState(
      guardPopupUpdater('resume', (prev) => ({
        ...prev,
        selectionIndex: clamp(prev.selectionIndex - 1, 0, 2),
      })),
    )
    return
  }
  if (key.downArrow) {
    setPopupState(
      guardPopupUpdater('resume', (prev) => ({
        ...prev,
        selectionIndex: clamp(prev.selectionIndex + 1, 0, 2),
      })),
    )
    return
  }
  if (popupState.selectionIndex === 0 && (key.leftArrow || key.rightArrow)) {
    setPopupState(
      guardPopupUpdater('resume', (prev) => ({
        ...prev,
        sourceKind: prev.sourceKind === 'history' ? 'file' : 'history',
        suggestedFocused: false,
        suggestedSelectionIndex: 0,
      })),
    )
    return
  }
  if (popupState.selectionIndex === 1 && (key.leftArrow || key.rightArrow)) {
    setPopupState(
      guardPopupUpdater('resume', (prev) => ({
        ...prev,
        mode: prev.mode === 'strict' ? 'best-effort' : 'strict',
      })),
    )
    return
  }
  if (key.return && popupState.selectionIndex < 2) {
    setPopupState(guardPopupUpdater('resume', (prev) => ({ ...prev, selectionIndex: 2 })))
  }
}
</file>
<file path="src/tui/screens/command/hooks/popup-shortcuts/selection.ts">
export const clamp = (value: number, min: number, max: number): number => {
  return Math.max(min, Math.min(value, max))
}
export const clampIndex = (index: number, length: number): number => {
  if (length <= 0) {
    return 0
  }
  return clamp(index, 0, length - 1)
}
export const wrapIndex = (index: number, length: number): number => {
  if (length <= 0) {
    return 0
  }
  return ((index % length) + length) % length
}
</file>
<file path="src/tui/screens/command/hooks/popup-shortcuts/smart-popup-shortcuts.ts">
import type { Key } from 'ink'
import { isBackspaceKey } from '../../../../components/core/text-input-keys'
import type { PopupState } from '../../../../types'
import { guardPopupUpdater, type PopupShortcutEffect } from './types'
import { getSuggestedInputPopupEffect } from './suggested-input'
export type HandleSmartPopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'smart' }>
  input: string
  key: Key
  suggestions: readonly string[]
  smartContextRoot: string | null
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  onSmartRootSubmit: (value: string) => void
}
const applySmartPopupEffect = (
  effect: PopupShortcutEffect,
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void,
  closePopup: () => void,
): void => {
  if (effect.type === 'close') {
    closePopup()
    return
  }
  if (effect.type === 'set') {
    setPopupState(effect.updater)
  }
}
export const handleSmartPopupShortcuts = ({
  popupState,
  input,
  key,
  suggestions,
  smartContextRoot,
  setPopupState,
  closePopup,
  onSmartRootSubmit,
}: HandleSmartPopupShortcutsOptions): void => {
  const hasSuggestions = suggestions.length > 0
  if (popupState.suggestedFocused && hasSuggestions) {
    const effect = getSuggestedInputPopupEffect({
      popupType: 'smart',
      popupState,
      suggestions,
      key,
    })
    if (effect.type === 'selectSuggestion') {
      setPopupState(
        guardPopupUpdater('smart', (prev) => ({
          ...prev,
          draft: effect.value,
          suggestedFocused: false,
        })),
      )
      return
    }
    applySmartPopupEffect(effect, setPopupState, closePopup)
    return
  }
  const effect = getSuggestedInputPopupEffect({
    popupType: 'smart',
    popupState,
    suggestions,
    key,
  })
  if (effect.type === 'selectSuggestion') {
    setPopupState(
      guardPopupUpdater('smart', (prev) => ({
        ...prev,
        draft: effect.value,
        suggestedFocused: false,
      })),
    )
    return
  }
  applySmartPopupEffect(effect, setPopupState, closePopup)
  if (effect.type !== 'none') {
    return
  }
  const draftIsEmpty = popupState.draft.trim().length === 0
  if ((key.delete || (draftIsEmpty && isBackspaceKey(input, key))) && smartContextRoot) {
    onSmartRootSubmit('')
  }
}
</file>
<file path="src/tui/screens/command/hooks/popup-shortcuts/suggested-input.ts">
import type { PopupState } from '../../../../types'
import { clampIndex } from './selection'
import {
  NO_EFFECT,
  guardPopupUpdater,
  type PopupKey,
  type PopupShortcutEffect,
  type PopupStateFor,
  type PopupType,
} from './types'
export type SuggestedInputPopupType = Extract<PopupType, 'smart' | 'intent'>
export type SuggestedInputPopupState = PopupStateFor<'smart'> | PopupStateFor<'intent'>
export type GetSuggestedInputPopupEffectOptions = {
  popupType: SuggestedInputPopupType
  popupState: SuggestedInputPopupState
  suggestions: readonly string[]
  key: PopupKey
}
const guardSuggestedInputUpdater = (
  popupType: SuggestedInputPopupType,
  updater: (prev: SuggestedInputPopupState) => SuggestedInputPopupState,
): ((prev: PopupState) => PopupState) => {
  return guardPopupUpdater(
    popupType,
    updater as (prev: PopupStateFor<typeof popupType>) => PopupStateFor<typeof popupType>,
  )
}
export const getSuggestedInputPopupEffect = ({
  popupType,
  popupState,
  suggestions,
  key,
}: GetSuggestedInputPopupEffectOptions): PopupShortcutEffect => {
  const hasSuggestions = suggestions.length > 0
  const maxSuggestedIndex = Math.max(suggestions.length - 1, 0)
  if (key.escape) {
    return { type: 'close' }
  }
  if (popupState.suggestedFocused && hasSuggestions) {
    if (key.tab) {
      return {
        type: 'set',
        updater: guardSuggestedInputUpdater(popupType, (prev) => ({
          ...prev,
          suggestedFocused: false,
        })),
      }
    }
    if (key.upArrow) {
      return {
        type: 'set',
        updater: guardSuggestedInputUpdater(popupType, (prev) => {
          const effectiveIndex = clampIndex(prev.suggestedSelectionIndex, suggestions.length)
          if (effectiveIndex === 0) {
            return { ...prev, suggestedFocused: false }
          }
          return {
            ...prev,
            suggestedSelectionIndex: Math.max(prev.suggestedSelectionIndex - 1, 0),
          }
        }),
      }
    }
    if (key.downArrow) {
      return {
        type: 'set',
        updater: guardSuggestedInputUpdater(popupType, (prev) => ({
          ...prev,
          suggestedSelectionIndex: Math.min(prev.suggestedSelectionIndex + 1, maxSuggestedIndex),
        })),
      }
    }
    if (key.return) {
      const effectiveIndex = clampIndex(popupState.suggestedSelectionIndex, suggestions.length)
      const selection = suggestions[effectiveIndex]
      return selection ? { type: 'selectSuggestion', value: selection } : NO_EFFECT
    }
    return NO_EFFECT
  }
  if (key.tab && !key.shift && hasSuggestions) {
    return {
      type: 'set',
      updater: guardSuggestedInputUpdater(popupType, (prev) => ({
        ...prev,
        suggestedFocused: true,
        suggestedSelectionIndex: 0,
      })),
    }
  }
  if (key.downArrow && hasSuggestions) {
    return {
      type: 'set',
      updater: guardSuggestedInputUpdater(popupType, (prev) => ({
        ...prev,
        suggestedFocused: true,
        suggestedSelectionIndex: 0,
      })),
    }
  }
  return NO_EFFECT
}
</file>
<file path="src/tui/screens/command/hooks/popup-shortcuts/suggested-selected-list-popup-shortcuts.ts">
import type { Key } from 'ink'
import { isBackspaceKey } from '../../../../components/core/text-input-keys'
import type { PopupState } from '../../../../types'
import {
  applySuggestedSelectedListEffect,
  getSuggestedSelectedListPopupEffect,
  type SuggestedSelectedListPopupState,
  type SuggestedSelectedListPopupType,
} from './suggested-selected-list'
export type HandleSuggestedSelectedListPopupShortcutsOptions = {
  popupType: SuggestedSelectedListPopupType
  popupState: SuggestedSelectedListPopupState
  input: string
  key: Key
  itemsLength: number
  suggestions: readonly string[]
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  onRemove: (index: number) => void
  onSelectSuggestion: (value: string) => void
}
export const handleSuggestedSelectedListPopupShortcuts = ({
  popupType,
  popupState,
  input,
  key,
  itemsLength,
  suggestions,
  setPopupState,
  closePopup,
  onRemove,
  onSelectSuggestion,
}: HandleSuggestedSelectedListPopupShortcutsOptions): void => {
  const effect = getSuggestedSelectedListPopupEffect({
    popupType,
    popupState,
    itemsLength,
    suggestions,
    key,
    isBackspace: isBackspaceKey(input, key),
  })
  applySuggestedSelectedListEffect(effect, setPopupState, closePopup, onRemove, onSelectSuggestion)
}
</file>
<file path="src/tui/screens/command/hooks/popup-shortcuts/theme-popup-shortcuts.ts">
import type { Key } from 'ink'
import type { PopupState } from '../../../../types'
import { wrapIndex } from './selection'
import { guardPopupUpdater } from './types'
export type HandleThemePopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'theme' }>
  key: Key
  themeCount: number
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  onThemeConfirm: () => void
  onThemeCancel: () => void
}
export const handleThemePopupShortcuts = ({
  key,
  themeCount,
  setPopupState,
  onThemeConfirm,
  onThemeCancel,
}: HandleThemePopupShortcutsOptions): void => {
  if (key.upArrow && themeCount > 0) {
    setPopupState(
      guardPopupUpdater('theme', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex - 1, themeCount),
      })),
    )
    return
  }
  if (key.downArrow && themeCount > 0) {
    setPopupState(
      guardPopupUpdater('theme', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex + 1, themeCount),
      })),
    )
    return
  }
  if (key.escape) {
    onThemeCancel()
    return
  }
  if (key.return) {
    onThemeConfirm()
  }
}
export type HandleThemeModePopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'themeMode' }>
  key: Key
  optionCount: number
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  onThemeModeConfirm: () => void
  onThemeModeCancel: () => void
}
export const handleThemeModePopupShortcuts = ({
  key,
  optionCount,
  setPopupState,
  onThemeModeConfirm,
  onThemeModeCancel,
}: HandleThemeModePopupShortcutsOptions): void => {
  if ((key.leftArrow || key.upArrow) && optionCount > 0) {
    setPopupState(
      guardPopupUpdater('themeMode', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex - 1, optionCount),
      })),
    )
    return
  }
  if ((key.rightArrow || key.downArrow) && optionCount > 0) {
    setPopupState(
      guardPopupUpdater('themeMode', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex + 1, optionCount),
      })),
    )
    return
  }
  if (key.escape) {
    onThemeModeCancel()
    return
  }
  if (key.return) {
    onThemeModeConfirm()
  }
}
</file>
<file path="src/tui/screens/command/hooks/popup-shortcuts/toggle-popup-shortcuts.ts">
import type { Key } from 'ink'
import type { PopupState, ToggleField } from '../../../../types'
import { wrapIndex } from './selection'
import { guardPopupUpdater } from './types'
const TOGGLE_OPTION_COUNT = 2
export type HandleTogglePopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'toggle' }>
  key: Key
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  applyToggleSelection: (field: ToggleField, value: boolean) => void
}
export const handleTogglePopupShortcuts = ({
  popupState,
  key,
  setPopupState,
  closePopup,
  applyToggleSelection,
}: HandleTogglePopupShortcutsOptions): void => {
  if (key.leftArrow || key.upArrow) {
    setPopupState(
      guardPopupUpdater('toggle', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex - 1, TOGGLE_OPTION_COUNT),
      })),
    )
    return
  }
  if (key.rightArrow || key.downArrow) {
    setPopupState(
      guardPopupUpdater('toggle', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex + 1, TOGGLE_OPTION_COUNT),
      })),
    )
    return
  }
  if (key.escape) {
    closePopup()
    return
  }
  if (key.return) {
    applyToggleSelection(popupState.field, popupState.selectionIndex === 0)
  }
}
</file>
<file path="src/tui/screens/command/hooks/popup-shortcuts/types.ts">
import type { PopupState } from '../../../../types'
export type NonNullPopupState = Exclude<PopupState, null>
export type PopupType = NonNullPopupState['type']
export type PopupStateFor<T extends PopupType> = Extract<NonNullPopupState, { type: T }>
export type PopupKey = {
  upArrow?: boolean
  downArrow?: boolean
  leftArrow?: boolean
  rightArrow?: boolean
  tab?: boolean
  shift?: boolean
  return?: boolean
  escape?: boolean
  delete?: boolean
  pageUp?: boolean
  pageDown?: boolean
  ctrl?: boolean
}
export type PopupShortcutEffect =
  | { type: 'none' }
  | { type: 'close' }
  | { type: 'set'; updater: (prev: PopupState) => PopupState }
  | { type: 'remove'; index: number }
  | { type: 'selectSuggestion'; value: string }
export const NO_EFFECT: PopupShortcutEffect = { type: 'none' }
export const guardPopupUpdater = <T extends PopupType>(
  popupType: T,
  updater: (prev: PopupStateFor<T>) => PopupStateFor<T>,
): ((prev: PopupState) => PopupState) => {
  return (prev) => {
    if (!prev || prev.type !== popupType) {
      return prev
    }
    return updater(prev as PopupStateFor<T>)
  }
}
</file>
<file path="src/tui/screens/command/hooks/popup-shortcuts/url-popup-shortcuts.ts">
import type { Key } from 'ink'
import { isBackspaceKey } from '../../../../components/core/text-input-keys'
import type { PopupState } from '../../../../types'
import { isControlKey } from '../../utils/control-key'
import { clampIndex } from './selection'
import { guardPopupUpdater } from './types'
export type HandleUrlPopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'url' }>
  input: string
  key: Key
  urls: readonly string[]
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  onRemoveUrl: (index: number) => void
}
export const handleUrlPopupShortcuts = ({
  popupState,
  input,
  key,
  urls,
  setPopupState,
  closePopup,
  onRemoveUrl,
}: HandleUrlPopupShortcutsOptions): void => {
  const draftIsEmpty = popupState.draft.trim().length === 0
  if (popupState.editingIndex !== null) {
    if (key.escape) {
      setPopupState(
        guardPopupUpdater('url', (prev) => ({
          ...prev,
          draft: '',
          editingIndex: null,
          selectedFocused: false,
        })),
      )
      return
    }
    if (draftIsEmpty && (key.delete || isBackspaceKey(input, key))) {
      if (urls.length > 0) {
        onRemoveUrl(popupState.selectionIndex)
      }
      return
    }
    return
  }
  if (key.escape) {
    closePopup()
    return
  }
  if (!popupState.selectedFocused && (key.upArrow || key.downArrow) && urls.length > 0) {
    setPopupState(
      guardPopupUpdater('url', (prev) => ({
        ...prev,
        selectedFocused: true,
        selectionIndex: clampIndex(prev.selectionIndex, urls.length),
      })),
    )
    return
  }
  if (popupState.selectedFocused) {
    if (key.upArrow) {
      setPopupState(
        guardPopupUpdater('url', (prev) => {
          if (prev.selectionIndex === 0) {
            return { ...prev, selectedFocused: false }
          }
          return { ...prev, selectionIndex: Math.max(prev.selectionIndex - 1, 0) }
        }),
      )
      return
    }
    if (key.downArrow) {
      if (popupState.selectionIndex >= urls.length - 1) {
        return
      }
      setPopupState(
        guardPopupUpdater('url', (prev) => ({
          ...prev,
          selectionIndex: Math.min(prev.selectionIndex + 1, urls.length - 1),
        })),
      )
      return
    }
    if (key.delete || isBackspaceKey(input, key)) {
      if (urls.length > 0) {
        onRemoveUrl(popupState.selectionIndex)
      }
      return
    }
    if (isControlKey(input, key, 'e')) {
      return
    }
    if (input.toLowerCase() === 'e' && urls.length > 0) {
      const selected = urls[popupState.selectionIndex]
      if (!selected) {
        return
      }
      setPopupState(
        guardPopupUpdater('url', (prev) => ({
          ...prev,
          draft: selected,
          editingIndex: prev.selectionIndex,
          selectedFocused: false,
        })),
      )
      return
    }
    return
  }
  if (draftIsEmpty && isBackspaceKey(input, key) && urls.length > 0) {
    onRemoveUrl(popupState.selectionIndex)
  }
}
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenHistoryAndTests.ts">
import type { MutableRefObject } from 'react'
import { useMemo, useRef } from 'react'
import { usePersistentCommandHistory } from '../../../hooks/usePersistentCommandHistory'
import { useStableCallback } from '../../../hooks/useStableCallback'
import type { HistoryEntry } from '../../../types'
import { usePromptTestRunner } from './usePromptTestRunner'
const DEFAULT_TEST_FILE = 'prompt-tests.yaml'
type PushHistory = (
  content: string,
  kind?: HistoryEntry['kind'],
  format?: HistoryEntry['format'],
) => void
export type UseCommandScreenHistoryAndTestsResult = {
  pushHistoryRef: MutableRefObject<PushHistory>
  pushHistoryProxy: PushHistory
  clearHistoryRef: MutableRefObject<() => void>
  clearHistoryProxy: () => void
  scrollToRef: MutableRefObject<(row: number) => void>
  scrollToProxy: (row: number) => void
  closeTestPopupRef: MutableRefObject<() => void>
  closeTestPopupProxy: () => void
  commandHistoryValues: string[]
  addCommandHistoryEntry: (value: string) => void
  isTestCommandRunning: boolean
  lastTestFile: string | null
  runTestsFromCommandProxy: (value: string) => void
  onTestPopupSubmit: (value: string) => void
}
export const useCommandScreenHistoryAndTests = (): UseCommandScreenHistoryAndTestsResult => {
  const pushHistoryRef = useRef<PushHistory>((_content, _kind) => {
    throw new Error('pushHistoryRef.current has not been initialized yet.')
  })
  const pushHistoryProxy: PushHistory = useStableCallback(
    (content: string, kind: HistoryEntry['kind'] = 'system', format?: HistoryEntry['format']) => {
      pushHistoryRef.current(content, kind, format)
    },
  )
  const clearHistoryRef = useRef<() => void>(() => {
    throw new Error('clearHistoryRef.current has not been initialized yet.')
  })
  const clearHistoryProxy = useStableCallback(() => {
    clearHistoryRef.current()
  })
  const scrollToRef = useRef<(row: number) => void>(() => {
    throw new Error('scrollToRef.current has not been initialized yet.')
  })
  const scrollToProxy = useStableCallback((row: number) => {
    scrollToRef.current(row)
  })
  const closeTestPopupRef = useRef<() => void>(() => {
    throw new Error('closeTestPopupRef.current has not been initialized yet.')
  })
  const closeTestPopupProxy = useStableCallback(() => {
    closeTestPopupRef.current()
  })
  const { entries: commandHistoryEntries, addEntry: addCommandHistoryEntry } =
    usePersistentCommandHistory({
      onError: (message) => {
        pushHistoryProxy(`[history] ${message}`, 'system')
      },
    })
  const commandHistoryValues = useMemo(
    () => commandHistoryEntries.map((entry) => entry.value),
    [commandHistoryEntries],
  )
  const { isTestCommandRunning, lastTestFile, runTestsFromCommand, onTestPopupSubmit } =
    usePromptTestRunner({
      defaultTestFile: DEFAULT_TEST_FILE,
      pushHistory: pushHistoryProxy,
      clearHistory: clearHistoryProxy,
      closeTestPopup: closeTestPopupProxy,
      addCommandHistoryEntry,
    })
  return {
    pushHistoryRef,
    pushHistoryProxy,
    clearHistoryRef,
    clearHistoryProxy,
    scrollToRef,
    scrollToProxy,
    closeTestPopupRef,
    closeTestPopupProxy,
    commandHistoryValues,
    addCommandHistoryEntry,
    isTestCommandRunning,
    lastTestFile,
    runTestsFromCommandProxy: runTestsFromCommand,
    onTestPopupSubmit,
  }
}
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenShell.ts">
import type { WriteStream } from 'node:tty'
import { useEffect } from 'react'
import { COMMAND_DESCRIPTORS, POPUP_HEIGHTS } from '../../../config'
import { useCommandHistory } from '../../../hooks/useCommandHistory'
import type { HistoryEntry, PopupState } from '../../../types'
import { useCommandMenuManager } from './useCommandMenuManager'
import { useCommandScreenLayout } from './useCommandScreenLayout'
import { useDroppedFileDetection } from './useDroppedFileDetection'
import { useHistoryScrollKeys } from './useHistoryScrollKeys'
import { usePopupSelectionClamp } from './usePopupSelectionClamp'
import { useSessionCommands } from './useSessionCommands'
import { useTerminalEffects } from './useTerminalEffects'
const APP_STATIC_ROWS = 7
const COMMAND_SCREEN_OVERHEAD_ROWS = 3
const COMMAND_MENU_HEIGHT = COMMAND_DESCRIPTORS.length + 2
const EMPTY_HISTORY: HistoryEntry[] = []
type SetPopupState = import('react').Dispatch<import('react').SetStateAction<PopupState>>
type UseCommandScreenShellOptions = {
  stdout: WriteStream | undefined
  setTerminalSize: (rows: number, columns: number) => void
  interactiveTransportPath?: string | undefined
  // screen state
  terminalRows: number
  inputValue: string
  debugKeyLine: string | null
  debugKeysEnabled: boolean
  helpOpen: boolean
  reservedRows: number
  popupState: PopupState
  isPopupOpen: boolean
  setPopupState: SetPopupState
  commandMenuSignal?: number | undefined
  commandSelectionIndex: number
  setCommandSelectionIndex: (next: number | ((prev: number) => number)) => void
  // generation state
  isGenerating: boolean
  awaitingInteractiveMode:
    | import('../../../generation-pipeline-reducer').InteractiveAwaitingMode
    | null
  // context
  files: string[]
  urls: string[]
  lastGeneratedPrompt: string | null
  resetContext: () => void
  // mutable refs
  lastUserIntentRef: import('react').MutableRefObject<string | null>
  lastTypedIntentRef: import('react').MutableRefObject<string>
  // setters
  setInputValue: (value: string | ((prev: string) => string)) => void
  setIntentFilePath: (value: string) => void
  setMetaInstructions: (value: string) => void
  scrollToRef: import('react').MutableRefObject<(row: number) => void>
  clearHistoryRef: import('react').MutableRefObject<() => void>
  pushHistoryRef: import('react').MutableRefObject<
    (content: string, kind?: HistoryEntry['kind']) => void
  >
  scrollToProxy: (row: number) => void
}
export type UseCommandScreenShellResult = {
  // menu
  isCommandMode: boolean
  commandMenuArgsRaw: string
  visibleCommands: readonly import('../../../types').CommandDescriptor[]
  isCommandMenuActive: boolean
  menuHeight: number
  selectedCommand: import('../../../types').CommandDescriptor | undefined
  // layout
  overlayHeight: number
  inputBarHint: string | undefined
  inputBarDebugLine: string | undefined
  isAwaitingTransportInput: boolean
  historyRows: number
  // history output
  history: HistoryEntry[]
  scrollOffset: number
  scrollBy: (delta: number) => void
  // history
  scrollTo: (row: number) => void
  clearHistory: () => void
  // session
  handleNewCommand: (argsRaw: string) => void
  handleReuseCommand: () => void
}
export const useCommandScreenShell = ({
  stdout,
  setTerminalSize,
  interactiveTransportPath,
  terminalRows,
  inputValue,
  debugKeyLine,
  debugKeysEnabled,
  helpOpen,
  reservedRows,
  popupState,
  isPopupOpen,
  setPopupState,
  commandMenuSignal,
  commandSelectionIndex,
  setCommandSelectionIndex,
  isGenerating,
  awaitingInteractiveMode,
  files,
  urls,
  lastGeneratedPrompt,
  resetContext,
  lastUserIntentRef,
  lastTypedIntentRef,
  setInputValue,
  setIntentFilePath,
  setMetaInstructions,
  scrollToRef,
  clearHistoryRef,
  pushHistoryRef,
  scrollToProxy,
}: UseCommandScreenShellOptions): UseCommandScreenShellResult => {
  const { droppedFilePath, existsSync } = useDroppedFileDetection(inputValue)
  const {
    isCommandMode,
    commandMenuArgsRaw,
    visibleCommands,
    isCommandMenuActive,
    menuHeight,
    selectedCommand,
  } = useCommandMenuManager({
    inputValue,
    existsSync,
    popupState,
    helpOpen,
    ...(commandMenuSignal !== undefined ? { commandMenuSignal } : {}),
    commands: COMMAND_DESCRIPTORS,
    commandMenuHeight: COMMAND_MENU_HEIGHT,
    commandSelectionIndex,
    setCommandSelectionIndex,
    setInputValue,
    setPopupState,
    scrollTo: scrollToProxy,
  })
  const { overlayHeight, inputBarHint, inputBarDebugLine, isAwaitingTransportInput, historyRows } =
    useCommandScreenLayout({
      terminalRows,
      reservedRows,
      helpOpen,
      isPopupOpen,
      popupState,
      menuHeight,
      popupHeights: POPUP_HEIGHTS,
      inputValue,
      droppedFilePath,
      debugKeysEnabled,
      debugKeyLine,
      interactiveTransportPath,
      isGenerating,
      awaitingInteractiveMode,
      isCommandMenuActive,
      appStaticRows: APP_STATIC_ROWS,
      commandScreenOverheadRows: COMMAND_SCREEN_OVERHEAD_ROWS,
    })
  const {
    history,
    resetHistory: resetOutputHistory,
    clearHistory,
    pushHistory: pushOutputHistory,
    scroll,
  } = useCommandHistory({
    initialEntries: EMPTY_HISTORY,
    visibleRows: historyRows,
  })
  // Keep upstream refs in sync.
  useEffect(() => {
    pushHistoryRef.current = pushOutputHistory
    clearHistoryRef.current = clearHistory
    scrollToRef.current = scroll.scrollTo
  }, [
    clearHistory,
    pushHistoryRef,
    pushOutputHistory,
    scroll.scrollTo,
    scrollToRef,
    clearHistoryRef,
  ])
  useTerminalEffects({
    stdout,
    setTerminalSize,
    interactiveTransportPath,
    history,
    pushHistory: pushOutputHistory,
  })
  usePopupSelectionClamp({
    setPopupState,
    filesLength: files.length,
    urlsLength: urls.length,
  })
  useHistoryScrollKeys({
    isCommandMenuActive,
    isPopupOpen,
    helpOpen,
    historyRows,
    scrollBy: scroll.scrollBy,
  })
  const { handleNewCommand, handleReuseCommand } = useSessionCommands({
    isGenerating,
    lastGeneratedPrompt,
    resetContext,
    resetHistory: resetOutputHistory,
    scrollTo: scroll.scrollTo,
    setInputValue,
    setPopupState,
    setIntentFilePath,
    setMetaInstructions,
    lastUserIntentRef,
    lastTypedIntentRef,
    pushHistory: pushOutputHistory,
  })
  return {
    isCommandMode,
    commandMenuArgsRaw,
    visibleCommands,
    isCommandMenuActive,
    menuHeight,
    selectedCommand,
    overlayHeight,
    inputBarHint,
    inputBarDebugLine,
    isAwaitingTransportInput,
    historyRows,
    history,
    scrollOffset: scroll.offset,
    scrollBy: scroll.scrollBy,
    scrollTo: scroll.scrollTo,
    clearHistory,
    handleNewCommand,
    handleReuseCommand,
  }
}
</file>
<file path="src/tui/screens/command/hooks/useDroppedFileDetection.ts">
import fs from 'node:fs'
import { useEffect, useMemo, useState } from 'react'
import { parseAbsolutePathFromInput } from '../../../drag-drop-path'
type StatFn = (candidate: string) => Promise<fs.Stats>
export type UseDroppedFileDetectionFs = {
  stat: StatFn
}
export type UseDroppedFileDetectionResult = {
  droppedFilePath: string | null
  existsSync: (candidate: string) => boolean
  isFilePath: (candidate: string) => boolean
}
type PathProbe = {
  exists: boolean
  isFile: boolean
}
const getErrnoCode = (error: unknown): string | null => {
  if (!(error instanceof Error)) {
    return null
  }
  const maybeWithCode = error as unknown as { code?: unknown }
  return typeof maybeWithCode.code === 'string' ? maybeWithCode.code : null
}
const DEFAULT_FS_IMPL: UseDroppedFileDetectionFs = {
  stat: (candidate: string) => fs.promises.stat(candidate),
}
export const useDroppedFileDetection = (
  inputValue: string,
  fsImpl: UseDroppedFileDetectionFs = DEFAULT_FS_IMPL,
): UseDroppedFileDetectionResult => {
  const [cacheVersion, setCacheVersion] = useState(0)
  const cache = useMemo(() => new Map<string, PathProbe>(), [])
  const inFlight = useMemo(() => new Set<string>(), [])
  const candidate = useMemo(() => parseAbsolutePathFromInput(inputValue), [inputValue])
  useEffect(() => {
    if (!candidate) {
      return
    }
    if (cache.has(candidate) || inFlight.has(candidate)) {
      return
    }
    inFlight.add(candidate)
    let cancelled = false
    void (async () => {
      try {
        const stats = await fsImpl.stat(candidate)
        if (cancelled) {
          return
        }
        cache.set(candidate, { exists: true, isFile: stats.isFile() })
      } catch (error) {
        if (cancelled) {
          return
        }
        const code = getErrnoCode(error)
        if (code === 'ENOENT' || code === 'ENOTDIR') {
          cache.set(candidate, { exists: false, isFile: false })
        } else {
          cache.set(candidate, { exists: false, isFile: false })
        }
      } finally {
        if (!cancelled) {
          inFlight.delete(candidate)
          setCacheVersion((prev) => prev + 1)
        }
      }
    })()
    return () => {
      cancelled = true
      inFlight.delete(candidate)
    }
  }, [candidate, cache, fsImpl, inFlight])
  const droppedFilePath = useMemo(() => {
    if (!candidate) {
      return null
    }
    const entry = cache.get(candidate)
    return entry?.exists && entry.isFile ? candidate : null
  }, [cacheVersion, cache, candidate])
  const existsSync = useMemo(() => {
    return (path: string): boolean => cache.get(path)?.exists ?? false
  }, [cacheVersion, cache])
  const isFilePath = useMemo(() => {
    return (path: string): boolean => {
      const entry = cache.get(path)
      return entry?.exists === true && entry.isFile
    }
  }, [cacheVersion, cache])
  return {
    droppedFilePath,
    existsSync,
    isFilePath,
  }
}
</file>
<file path="src/tui/screens/command/hooks/useDroppedFilePath.ts">
import { useDroppedFileDetection } from './useDroppedFileDetection'
export type UseDroppedFilePathResult = string | null
export const useDroppedFilePath = (inputValue: string): UseDroppedFilePathResult => {
  const { droppedFilePath } = useDroppedFileDetection(inputValue)
  return droppedFilePath
}
</file>
<file path="src/tui/screens/command/hooks/useIntentPopupGlue.ts">
import { useCallback, useEffect, useMemo } from 'react'
import { stripTerminalPasteArtifacts } from '../../../components/core/bracketed-paste'
import { filterIntentFileSuggestions } from '../../../file-suggestions'
import type { PopupState } from '../../../types'
const EMPTY_SUGGESTIONS: string[] = []
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
export type UseIntentPopupGlueOptions = {
  popupState: PopupState
  setPopupState: SetPopupState
}
export type UseIntentPopupGlueResult = {
  intentPopupSuggestions: string[]
  intentPopupSuggestionSelectionIndex: number
  intentPopupSuggestionsFocused: boolean
  onIntentPopupDraftChange: (next: string) => void
}
export const useIntentPopupGlue = ({
  popupState,
  setPopupState,
}: UseIntentPopupGlueOptions): UseIntentPopupGlueResult => {
  const intentPopupDraft = popupState?.type === 'intent' ? popupState.draft : ''
  const intentPopupSuggestedItems =
    popupState?.type === 'intent' ? popupState.suggestedItems : EMPTY_SUGGESTIONS
  const intentPopupSuggestedFocused =
    popupState?.type === 'intent' ? popupState.suggestedFocused : false
  const intentPopupSuggestedSelectionIndex =
    popupState?.type === 'intent' ? popupState.suggestedSelectionIndex : 0
  const intentPopupSuggestions = useMemo(() => {
    if (!intentPopupSuggestedItems.length) {
      return []
    }
    if (!intentPopupDraft.trim()) {
      return []
    }
    return filterIntentFileSuggestions({
      suggestions: intentPopupSuggestedItems,
      query: intentPopupDraft,
      limit: 200,
    })
  }, [intentPopupDraft, intentPopupSuggestedItems])
  const intentPopupSuggestionSelectionIndex = Math.min(
    intentPopupSuggestedSelectionIndex,
    Math.max(intentPopupSuggestions.length - 1, 0),
  )
  const intentPopupSuggestionsFocused =
    intentPopupSuggestedFocused && intentPopupSuggestions.length > 0
  useEffect(() => {
    if (popupState?.type !== 'intent') {
      return
    }
    if (!intentPopupSuggestedFocused) {
      return
    }
    if (intentPopupSuggestions.length > 0) {
      return
    }
    setPopupState((prev) =>
      prev?.type === 'intent'
        ? { ...prev, suggestedFocused: false, suggestedSelectionIndex: 0 }
        : prev,
    )
  }, [intentPopupSuggestedFocused, intentPopupSuggestions.length, popupState?.type, setPopupState])
  const onIntentPopupDraftChange = useCallback(
    (next: string) => {
      const sanitized = stripTerminalPasteArtifacts(next)
      setPopupState((prev) =>
        prev?.type === 'intent'
          ? {
              ...prev,
              draft: sanitized,
              suggestedSelectionIndex: 0,
              suggestedFocused: false,
            }
          : prev,
      )
    },
    [setPopupState],
  )
  return {
    intentPopupSuggestions,
    intentPopupSuggestionSelectionIndex,
    intentPopupSuggestionsFocused,
    onIntentPopupDraftChange,
  }
}
</file>
<file path="src/tui/screens/command/hooks/usePopupSelectionClamp.ts">
import { useEffect } from 'react'
import type { PopupState } from '../../../types'
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
export type UsePopupSelectionClampOptions = {
  setPopupState: SetPopupState
  filesLength: number
  urlsLength: number
}
export const usePopupSelectionClamp = ({
  setPopupState,
  filesLength,
  urlsLength,
}: UsePopupSelectionClampOptions): void => {
  useEffect(() => {
    setPopupState((prev) => {
      if (!prev) {
        return prev
      }
      if (prev.type === 'file') {
        const maxIndex = Math.max(filesLength - 1, 0)
        const nextIndex = Math.min(prev.selectionIndex, maxIndex)
        return prev.selectionIndex === nextIndex ? prev : { ...prev, selectionIndex: nextIndex }
      }
      if (prev.type === 'url') {
        const maxIndex = Math.max(urlsLength - 1, 0)
        const nextIndex = Math.min(prev.selectionIndex, maxIndex)
        const shouldResetFocus =
          urlsLength === 0 && (prev.selectedFocused || prev.editingIndex !== null)
        if (!shouldResetFocus && prev.selectionIndex === nextIndex) {
          return prev
        }
        return {
          ...prev,
          selectionIndex: nextIndex,
          selectedFocused: urlsLength === 0 ? false : prev.selectedFocused,
          editingIndex: urlsLength === 0 ? null : prev.editingIndex,
          draft: urlsLength === 0 && prev.editingIndex !== null ? '' : prev.draft,
        }
      }
      return prev
    })
  }, [filesLength, setPopupState, urlsLength])
}
</file>
<file path="src/tui/screens/command/hooks/usePromptTestRunner.ts">
import path from 'node:path'
import { useState } from 'react'
import { useStableCallback } from '../../../hooks/useStableCallback'
import { runPromptTestSuite, type PromptTestRunReporter } from '../../../../test-command'
import type { HistoryEntry } from '../../../types'
type PushHistory = (
  content: string,
  kind?: HistoryEntry['kind'],
  format?: HistoryEntry['format'],
) => void
type ClearHistory = () => void
type CloseTestPopup = () => void
type AddCommandHistoryEntry = (value: string) => void
export type UsePromptTestRunnerOptions = {
  defaultTestFile: string
  pushHistory: PushHistory
  clearHistory: ClearHistory
  closeTestPopup: CloseTestPopup
  addCommandHistoryEntry: AddCommandHistoryEntry
}
export type UsePromptTestRunnerResult = {
  isTestCommandRunning: boolean
  lastTestFile: string | null
  runTestsFromCommand: (value: string) => void
  onTestPopupSubmit: (value: string) => void
}
export const usePromptTestRunner = ({
  defaultTestFile,
  pushHistory,
  clearHistory,
  closeTestPopup,
  addCommandHistoryEntry,
}: UsePromptTestRunnerOptions): UsePromptTestRunnerResult => {
  const [isTestCommandRunning, setIsTestCommandRunning] = useState(false)
  const [lastTestFile, setLastTestFile] = useState<string | null>(null)
  const runTestsFromCommand = useStableCallback((value: string) => {
    void (async () => {
      const normalized = value.trim()
      const targetFile = normalized || lastTestFile || defaultTestFile
      if (!targetFile) {
        pushHistory('No test file specified. Use /test <file>.', 'system')
        return
      }
      if (isTestCommandRunning) {
        pushHistory('Test run already in progress. Please wait.', 'system')
        return
      }
      const resolvedPath = path.resolve(process.cwd(), targetFile)
      clearHistory()
      setIsTestCommandRunning(true)
      setLastTestFile(targetFile)
      closeTestPopup()
      pushHistory(`[tests] Running ${resolvedPath}`, 'progress')
      try {
        const reporter: PromptTestRunReporter = {
          onSuiteLoaded: (suite, loadedPath) => {
            pushHistory(
              `[tests] Loaded ${suite.tests.length} test(s) from ${loadedPath}`,
              'progress',
            )
          },
          onTestStart: (ordinal, test) => {
            pushHistory(`[tests] (${ordinal}) ${test.name}`, 'progress')
          },
          onTestComplete: (_ordinal, result) => {
            const status = result.pass ? 'PASS' : 'FAIL'
            const reason = result.reason ? ` Âˇ ${result.reason}` : ''
            pushHistory(
              `[tests] ${status} ${result.name}${reason}`,
              result.pass ? 'system' : 'progress',
            )
          },
          onComplete: (results) => {
            const passed = results.filter((result) => result.pass).length
            const failed = results.length - passed
            const kind: HistoryEntry['kind'] = failed > 0 ? 'progress' : 'system'
            pushHistory(`[tests] Summary Âˇ passed ${passed} Âˇ failed ${failed}`, kind)
          },
        }
        await runPromptTestSuite(resolvedPath, { reporter })
        pushHistory('[tests] Complete.', 'progress')
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown test execution error.'
        pushHistory(`[tests] Failed: ${message}`, 'progress')
      } finally {
        setIsTestCommandRunning(false)
      }
    })()
  })
  const onTestPopupSubmit = useStableCallback((value: string) => {
    const trimmed = value.trim()
    addCommandHistoryEntry(`/test${trimmed ? ` ${trimmed}` : ''}`)
    runTestsFromCommand(value)
  })
  return {
    isTestCommandRunning,
    lastTestFile,
    runTestsFromCommand,
    onTestPopupSubmit,
  }
}
</file>
<file path="src/tui/screens/command/hooks/useTerminalEffects.ts">
import type { WriteStream } from 'node:tty'
import { useEffect } from 'react'
import type { HistoryEntry } from '../../../types'
type PushHistory = (
  content: string,
  kind?: HistoryEntry['kind'],
  format?: HistoryEntry['format'],
) => void
type SetTerminalSize = (rows: number, columns: number) => void
export type UseTerminalEffectsOptions = {
  stdout: WriteStream | undefined
  setTerminalSize: SetTerminalSize
  interactiveTransportPath?: string | undefined
  history: HistoryEntry[]
  pushHistory: PushHistory
}
export const useTerminalEffects = ({
  stdout,
  setTerminalSize,
  interactiveTransportPath,
  history,
  pushHistory,
}: UseTerminalEffectsOptions): void => {
  useEffect(() => {
    if (!stdout) {
      return undefined
    }
    stdout.write('\x1bc')
    stdout.write('\x1b[?2004h')
    return () => {
      stdout.write('\x1b[?2004l')
    }
  }, [stdout])
  useEffect(() => {
    if (!interactiveTransportPath) {
      return
    }
    const transportLine = `Interactive transport listening on ${interactiveTransportPath}`
    if (history.some((entry) => entry.content === transportLine)) {
      return
    }
    pushHistory(transportLine, 'system')
  }, [history, interactiveTransportPath, pushHistory])
  useEffect(() => {
    if (!stdout) {
      return undefined
    }
    const handleResize = (): void => {
      setTerminalSize(stdout.rows, stdout.columns)
    }
    stdout.on('resize', handleResize)
    return () => {
      stdout.off('resize', handleResize)
    }
  }, [setTerminalSize, stdout])
}
</file>
<file path="src/tui/screens/command/utils/submit-plan.ts">
import { resolveIntentSource } from '../../../intent-source'
import type { IntentSourceSelection } from '../../../intent-source'
import type { CommandDescriptor } from '../../../types'
export type SubmitPlanHistoryKind = 'system' | 'user' | 'progress'
export type SubmitPlanAction =
  | { type: 'set-input'; value: string }
  | { type: 'push-history'; kind: SubmitPlanHistoryKind; content: string }
  | { type: 'add-command-history'; value: string }
  | { type: 'set-last-user-intent'; value: string }
  | { type: 'submit-refinement'; value: string }
  | {
      type: 'run-generation'
      intent?: string
      intentFile?: string
      resume?:
        | { kind: 'history'; selector: string; mode: import('../../../types').ResumeMode }
        | { kind: 'file'; payloadPath: string; mode: import('../../../types').ResumeMode }
    }
  | { type: 'run-new'; argsRaw: string }
  | { type: 'run-reuse' }
  | { type: 'run-command'; commandId: CommandDescriptor['id']; argsRaw: string }
export type ResolveSubmitPlanInput = {
  expandedValue: string
  isAwaitingRefinement: boolean
  popupOpen: boolean
  isCommandMenuActive: boolean
  selectedCommandId: CommandDescriptor['id'] | null
  commandMenuArgsRaw: string
  isCommandMode: boolean
  intentFilePath: string
  isGenerating: boolean
}
const buildCommandHistoryEntry = (commandId: CommandDescriptor['id'], argsRaw: string): string => {
  const trimmedArgs = argsRaw.trim()
  return `/${commandId}${trimmedArgs ? ` ${trimmedArgs}` : ''}`
}
export const resolveSubmitPlan = (input: ResolveSubmitPlanInput): SubmitPlanAction[] => {
  const actions: SubmitPlanAction[] = []
  if (input.isAwaitingRefinement) {
    actions.push({ type: 'submit-refinement', value: input.expandedValue })
    actions.push({ type: 'set-input', value: '' })
    return actions
  }
  if (input.popupOpen) {
    return actions
  }
  if (input.isCommandMenuActive) {
    if (input.selectedCommandId) {
      actions.push({
        type: 'add-command-history',
        value: buildCommandHistoryEntry(input.selectedCommandId, input.commandMenuArgsRaw),
      })
      if (input.selectedCommandId === 'new') {
        actions.push({ type: 'run-new', argsRaw: input.commandMenuArgsRaw })
      } else if (input.selectedCommandId === 'reuse') {
        actions.push({ type: 'run-reuse' })
      } else {
        actions.push({
          type: 'run-command',
          commandId: input.selectedCommandId,
          argsRaw: input.commandMenuArgsRaw,
        })
      }
    }
    actions.push({ type: 'set-input', value: '' })
    return actions
  }
  if (input.isCommandMode) {
    actions.push({ type: 'set-input', value: '' })
    return actions
  }
  const trimmed = input.expandedValue.trim()
  const intentSource: IntentSourceSelection = resolveIntentSource(trimmed, input.intentFilePath)
  if (intentSource.kind === 'empty') {
    actions.push({ type: 'set-input', value: '' })
    return actions
  }
  if (input.isGenerating) {
    actions.push({
      type: 'push-history',
      kind: 'system',
      content: 'Generation already running. Please wait.',
    })
    actions.push({ type: 'set-input', value: '' })
    return actions
  }
  if (intentSource.kind === 'file') {
    actions.push({
      type: 'push-history',
      kind: 'user',
      content: `> [intent file] ${intentSource.intentFile}`,
    })
    if (trimmed.length > 0) {
      actions.push({
        type: 'push-history',
        kind: 'system',
        content: 'Typed intent ignored because an intent file is active.',
      })
    }
    actions.push({ type: 'set-input', value: '' })
    actions.push({ type: 'run-generation', intentFile: intentSource.intentFile })
    return actions
  }
  actions.push({ type: 'add-command-history', value: intentSource.intent })
  actions.push({ type: 'push-history', kind: 'user', content: `> ${intentSource.intent}` })
  actions.push({ type: 'set-last-user-intent', value: intentSource.intent })
  actions.push({ type: 'set-input', value: '' })
  actions.push({ type: 'run-generation', intent: intentSource.intent })
  return actions
}
</file>
<file path="src/tui/screens/command/utils/url-args.ts">
export const parseUrlArgs = (raw: string): string[] => {
  return raw
    .split(/[\s,]+/g)
    .map((value) => value.trim())
    .filter((value) => value.length > 0)
}
export type UrlValidationResult =
  | { ok: true }
  | {
      ok: false
      message: string
    }
export const validateHttpUrlCandidate = (candidate: string): UrlValidationResult => {
  const trimmed = candidate.trim()
  if (!trimmed) {
    return { ok: false, message: 'URL is empty.' }
  }
  let parsed: URL
  try {
    parsed = new URL(trimmed)
  } catch {
    return { ok: false, message: `Invalid URL: ${trimmed}` }
  }
  if (!/^https?:$/.test(parsed.protocol)) {
    return {
      ok: false,
      message: `Unsupported URL protocol for ${trimmed}. Only HTTP(S) URLs are allowed.`,
    }
  }
  return { ok: true }
}
</file>
<file path="src/tui/screens/command/command-screen-reducer.ts">
/*
 * Command screen reducer (pure state transitions).
 *
 * This is the âscreen modelâ for the main Ink TUI screen.
 *
 * Reducers in plain terms:
 * - Instead of calling many `setState(...)` functions from all over the file,
 *   we send small âactionsâ describing what happened.
 * - The reducer is a pure function that turns (previousState + action) into
 *   nextState.
 *
 * Why this helps:
 * - Keeps state transitions explicit and testable.
 * - Lets us update multiple related fields in one render.
 * - Reduces accidental bugs where different `setState` calls race.
 *
 * This file is intentionally pure: no React/Ink imports.
 */
// Lightweight replacement for React's SetStateAction.
export type SetStateAction<State> = State | ((prev: State) => State)
export type CommandScreenState = {
  terminalRows: number
  terminalColumns: number
  inputValue: string
  isPasteActive: boolean
  commandSelectionIndex: number
  debugKeyLine: string | null
  // Command screen UI options (single source of truth).
  intentFilePath: string
  copyEnabled: boolean
  chatGptEnabled: boolean
  jsonOutputEnabled: boolean
}
export type CommandScreenAction =
  | { type: 'set-terminal-size'; rows: number; columns: number }
  | { type: 'set-input'; next: SetStateAction<string> }
  | { type: 'set-paste-active'; isPasteActive: boolean }
  | { type: 'set-command-selection'; next: SetStateAction<number> }
  | { type: 'set-debug-line'; line: string | null }
  | { type: 'set-intent-file-path'; next: SetStateAction<string> }
  | { type: 'set-copy-enabled'; next: SetStateAction<boolean> }
  | { type: 'set-chatgpt-enabled'; next: SetStateAction<boolean> }
  | { type: 'set-json-output-enabled'; next: SetStateAction<boolean> }
export const createInitialCommandScreenState = (options: {
  terminalRows: number
  terminalColumns: number
}): CommandScreenState => ({
  terminalRows: options.terminalRows,
  terminalColumns: options.terminalColumns,
  inputValue: '',
  isPasteActive: false,
  commandSelectionIndex: 0,
  debugKeyLine: null,
  intentFilePath: '',
  copyEnabled: true,
  chatGptEnabled: false,
  jsonOutputEnabled: false,
})
export const commandScreenReducer = (
  state: CommandScreenState,
  action: CommandScreenAction,
): CommandScreenState => {
  switch (action.type) {
    case 'set-terminal-size':
      // Terminal resize events can be noisy; avoid needless rerenders.
      if (state.terminalRows === action.rows && state.terminalColumns === action.columns) {
        return state
      }
      return { ...state, terminalRows: action.rows, terminalColumns: action.columns }
    case 'set-input': {
      const nextValue =
        typeof action.next === 'function' ? action.next(state.inputValue) : action.next
      return nextValue === state.inputValue ? state : { ...state, inputValue: nextValue }
    }
    case 'set-paste-active':
      return action.isPasteActive === state.isPasteActive
        ? state
        : { ...state, isPasteActive: action.isPasteActive }
    case 'set-command-selection': {
      const nextIndex =
        typeof action.next === 'function' ? action.next(state.commandSelectionIndex) : action.next
      return nextIndex === state.commandSelectionIndex
        ? state
        : { ...state, commandSelectionIndex: nextIndex }
    }
    case 'set-debug-line':
      return action.line === state.debugKeyLine ? state : { ...state, debugKeyLine: action.line }
    case 'set-intent-file-path': {
      const nextValue =
        typeof action.next === 'function' ? action.next(state.intentFilePath) : action.next
      return nextValue === state.intentFilePath ? state : { ...state, intentFilePath: nextValue }
    }
    case 'set-copy-enabled': {
      const nextValue =
        typeof action.next === 'function' ? action.next(state.copyEnabled) : action.next
      return nextValue === state.copyEnabled ? state : { ...state, copyEnabled: nextValue }
    }
    case 'set-chatgpt-enabled': {
      const nextValue =
        typeof action.next === 'function' ? action.next(state.chatGptEnabled) : action.next
      return nextValue === state.chatGptEnabled ? state : { ...state, chatGptEnabled: nextValue }
    }
    case 'set-json-output-enabled': {
      const nextValue =
        typeof action.next === 'function' ? action.next(state.jsonOutputEnabled) : action.next
      return nextValue === state.jsonOutputEnabled
        ? state
        : { ...state, jsonOutputEnabled: nextValue }
    }
    default:
      return state
  }
}
</file>
<file path="src/tui/screens/command/useCommandScreen.ts">
/*
 * useCommandScreen
 *
 * This hook owns the CommandScreen "screen model" state.
 *
 * Incremental refactor note:
 * - The legacy `CommandScreen.tsx` had a lot of scattered `useState` calls.
 * - Weâre starting by moving the highest-churn UI state into a reducer.
 * - The rest of the orchestration logic can migrate here over time.
 */
import { useCallback, useMemo, useReducer } from 'react'
import { useStdout } from 'ink'
import {
  commandScreenReducer,
  createInitialCommandScreenState,
  type CommandScreenState,
  type SetStateAction,
} from './command-screen-reducer'
export type UseCommandScreenResult = {
  state: CommandScreenState
  setTerminalSize: (rows: number, columns: number) => void
  setInputValue: (next: SetStateAction<string>) => void
  setPasteActive: (isPasteActive: boolean) => void
  setCommandSelectionIndex: (next: SetStateAction<number>) => void
  setDebugKeyLine: (line: string | null) => void
  setIntentFilePath: (next: SetStateAction<string>) => void
  setCopyEnabled: (next: SetStateAction<boolean>) => void
  setChatGptEnabled: (next: SetStateAction<boolean>) => void
  setJsonOutputEnabled: (next: SetStateAction<boolean>) => void
}
export const useCommandScreen = (): UseCommandScreenResult => {
  const { stdout } = useStdout()
  const initialState = useMemo(
    () =>
      createInitialCommandScreenState({
        terminalRows: stdout?.rows ?? 24,
        terminalColumns: stdout?.columns ?? 80,
      }),
    [stdout?.columns, stdout?.rows],
  )
  const [state, dispatch] = useReducer(commandScreenReducer, initialState)
  const setTerminalSize = useCallback((rows: number, columns: number) => {
    dispatch({ type: 'set-terminal-size', rows, columns })
  }, [])
  const setInputValue = useCallback((next: SetStateAction<string>) => {
    dispatch({ type: 'set-input', next })
  }, [])
  const setPasteActive = useCallback((isPasteActive: boolean) => {
    dispatch({ type: 'set-paste-active', isPasteActive })
  }, [])
  const setCommandSelectionIndex = useCallback((next: SetStateAction<number>) => {
    dispatch({ type: 'set-command-selection', next })
  }, [])
  const setDebugKeyLine = useCallback((line: string | null) => {
    dispatch({ type: 'set-debug-line', line })
  }, [])
  const setIntentFilePath = useCallback((next: SetStateAction<string>) => {
    dispatch({ type: 'set-intent-file-path', next })
  }, [])
  const setCopyEnabled = useCallback((next: SetStateAction<boolean>) => {
    dispatch({ type: 'set-copy-enabled', next })
  }, [])
  const setChatGptEnabled = useCallback((next: SetStateAction<boolean>) => {
    dispatch({ type: 'set-chatgpt-enabled', next })
  }, [])
  const setJsonOutputEnabled = useCallback((next: SetStateAction<boolean>) => {
    dispatch({ type: 'set-json-output-enabled', next })
  }, [])
  return {
    state,
    setTerminalSize,
    setInputValue,
    setPasteActive,
    setCommandSelectionIndex,
    setDebugKeyLine,
    setIntentFilePath,
    setCopyEnabled,
    setChatGptEnabled,
    setJsonOutputEnabled,
  }
}
</file>
<file path="src/tui/theme/theme-adapter.ts">
const isRecord = (value: unknown): value is Record<string, unknown> =>
  typeof value === 'object' && value !== null
export const adaptOpencodeThemeJson = (value: unknown): unknown | null => {
  if (!isRecord(value)) {
    return null
  }
  const themeRaw = value.theme
  if (!isRecord(themeRaw)) {
    return null
  }
  const looksLikeOpencode =
    'textMuted' in themeRaw ||
    'backgroundPanel' in themeRaw ||
    'backgroundElement' in themeRaw ||
    'primary' in themeRaw
  if (!looksLikeOpencode) {
    return null
  }
  const theme: Record<string, unknown> = { ...themeRaw }
  if (theme.mutedText === undefined && themeRaw.textMuted !== undefined) {
    theme.mutedText = 'textMuted'
  }
  if (theme.panelBackground === undefined && themeRaw.backgroundPanel !== undefined) {
    theme.panelBackground = 'backgroundPanel'
  }
  if (theme.popupBackground === undefined && theme.panelBackground !== undefined) {
    theme.popupBackground = 'panelBackground'
  }
  if (theme.accent === undefined && themeRaw.primary !== undefined) {
    theme.accent = 'primary'
  }
  if (theme.accentText === undefined && themeRaw.background !== undefined) {
    theme.accentText = 'background'
  }
  if (theme.selectionBackground === undefined) {
    if (themeRaw.backgroundElement !== undefined) {
      theme.selectionBackground = 'backgroundElement'
    } else if (themeRaw.backgroundPanel !== undefined) {
      theme.selectionBackground = 'backgroundPanel'
    }
  }
  if (theme.selectionText === undefined && themeRaw.text !== undefined) {
    theme.selectionText = 'text'
  }
  if (theme.chipBackground === undefined) {
    if (themeRaw.backgroundElement !== undefined) {
      theme.chipBackground = 'backgroundElement'
    } else if (themeRaw.backgroundPanel !== undefined) {
      theme.chipBackground = 'backgroundPanel'
    }
  }
  if (theme.chipText === undefined && themeRaw.text !== undefined) {
    theme.chipText = 'text'
  }
  if (theme.chipMutedText === undefined) {
    if (themeRaw.textMuted !== undefined) {
      theme.chipMutedText = 'textMuted'
    } else if (themeRaw.text !== undefined) {
      theme.chipMutedText = 'text'
    }
  }
  const adapted: Record<string, unknown> = { theme }
  if ('defs' in value) {
    adapted.defs = value.defs
  }
  return adapted
}
</file>
<file path="src/tui/theme/theme-discovery.ts">
import fs from 'node:fs/promises'
import type { Dirent } from 'node:fs'
import path from 'node:path'
export type ThemeDiscoveryError = {
  kind: 'read'
  filePath: string
  message: string
}
const pathExists = async (targetPath: string): Promise<boolean> => {
  try {
    await fs.stat(targetPath)
    return true
  } catch {
    return false
  }
}
export const defaultGlobalThemesDir = (homedir: string): string =>
  path.join(homedir, '.config', 'prompt-maker-cli', 'themes')
export const listThemeJsonFiles = async (
  themesDir: string,
): Promise<{ files: string[]; errors: ThemeDiscoveryError[] }> => {
  const errors: ThemeDiscoveryError[] = []
  if (!(await pathExists(themesDir))) {
    return { files: [], errors }
  }
  let entries: Dirent[]
  try {
    entries = await fs.readdir(themesDir, { withFileTypes: true })
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown directory read error.'
    errors.push({ kind: 'read', filePath: themesDir, message })
    return { files: [], errors }
  }
  const files = entries
    .filter((entry) => entry.isFile() && entry.name.endsWith('.json'))
    .map((entry) => path.join(themesDir, entry.name))
    .sort((a, b) => path.basename(a).localeCompare(path.basename(b)))
  return { files, errors }
}
export const discoverProjectThemesDirs = async (
  cwd: string,
  stopAt?: string | undefined,
): Promise<string[]> => {
  const discovered: string[] = []
  const resolvedStopAt = stopAt ? path.resolve(stopAt) : null
  let current: string | null = path.resolve(cwd)
  while (current !== null) {
    const candidate = path.join(current, '.prompt-maker-cli', 'themes')
    if (await pathExists(candidate)) {
      discovered.push(candidate)
    }
    if (resolvedStopAt && current === resolvedStopAt) {
      current = null
      continue
    }
    const parent = path.dirname(current)
    if (parent === current) {
      current = null
      continue
    }
    current = parent
  }
  return discovered
}
</file>
<file path="src/tui/theme/theme-loader.ts">
import os from 'node:os'
import path from 'node:path'
import { adaptOpencodeThemeJson } from './theme-adapter'
import {
  defaultGlobalThemesDir,
  discoverProjectThemesDirs,
  listThemeJsonFiles,
} from './theme-discovery'
import { readJsonFile } from './theme-parse'
import { listThemes, type ThemeRegistryEntry } from './theme-registry'
import type { ThemeJson } from './theme-types'
import { validateThemeJson } from './theme-validate'
export type ThemeSource = 'builtin' | 'global' | 'project'
export type ThemeDescriptor = {
  name: string
  label: string
  source: ThemeSource
  theme: ThemeJson
  filePath?: string
}
export type ThemeLoadErrorKind = 'read' | 'parse' | 'validate'
export type ThemeLoadError = {
  kind: ThemeLoadErrorKind
  filePath: string
  message: string
}
export type LoadThemesOptions = {
  cwd?: string
  stopAt?: string | undefined
  homedir?: string
  globalThemesDir?: string
}
type ThemeCandidate = {
  name: string
  theme: ThemeJson
}
const loadThemeCandidateFromFile = async (
  filePath: string,
): Promise<{ ok: true; candidate: ThemeCandidate } | { ok: false; error: ThemeLoadError }> => {
  const name = path.basename(filePath, '.json')
  let parsed: unknown
  try {
    parsed = await readJsonFile(filePath)
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown theme file read error.'
    const kind: ThemeLoadErrorKind = error instanceof SyntaxError ? 'parse' : 'read'
    return { ok: false, error: { kind, filePath, message } }
  }
  const validated = validateThemeJson(parsed)
  if (validated.ok) {
    return { ok: true, candidate: { name, theme: validated.theme } }
  }
  const adapted = adaptOpencodeThemeJson(parsed)
  if (adapted) {
    const adaptedValidated = validateThemeJson(adapted)
    if (adaptedValidated.ok) {
      return { ok: true, candidate: { name, theme: adaptedValidated.theme } }
    }
    return {
      ok: false,
      error: {
        kind: 'validate',
        filePath,
        message: `Theme JSON invalid (after adapting opencode schema): ${adaptedValidated.message}`,
      },
    }
  }
  return { ok: false, error: { kind: 'validate', filePath, message: validated.message } }
}
const toBuiltinDescriptor = (entry: ThemeRegistryEntry): ThemeDescriptor => ({
  name: entry.name,
  label: entry.label,
  source: 'builtin',
  theme: entry.theme,
})
const toCustomDescriptor = (
  candidate: ThemeCandidate,
  source: Exclude<ThemeSource, 'builtin'>,
  filePath: string,
  existing?: ThemeDescriptor,
): ThemeDescriptor => ({
  name: candidate.name,
  label: existing?.label ?? candidate.name,
  source,
  theme: candidate.theme,
  filePath,
})
export const loadThemes = async (
  options: LoadThemesOptions = {},
): Promise<{ themes: ThemeDescriptor[]; errors: ThemeLoadError[] }> => {
  const cwd = options.cwd ?? process.cwd()
  const homedir = options.homedir ?? os.homedir()
  const globalThemesDir = options.globalThemesDir ?? defaultGlobalThemesDir(homedir)
  const builtins = listThemes()
  const order: string[] = builtins.map((theme) => theme.name)
  const extras = new Set<string>()
  const themesByName = new Map<string, ThemeDescriptor>()
  for (const entry of builtins) {
    themesByName.set(entry.name, toBuiltinDescriptor(entry))
  }
  const errors: ThemeLoadError[] = []
  const applyThemeFiles = async (
    source: Exclude<ThemeSource, 'builtin'>,
    themesDir: string,
  ): Promise<void> => {
    const listing = await listThemeJsonFiles(themesDir)
    errors.push(...listing.errors)
    for (const filePath of listing.files) {
      const loaded = await loadThemeCandidateFromFile(filePath)
      if (!loaded.ok) {
        errors.push(loaded.error)
        continue
      }
      const existing = themesByName.get(loaded.candidate.name)
      themesByName.set(
        loaded.candidate.name,
        toCustomDescriptor(loaded.candidate, source, filePath, existing),
      )
      if (!order.includes(loaded.candidate.name)) {
        extras.add(loaded.candidate.name)
      }
    }
  }
  await applyThemeFiles('global', globalThemesDir)
  const projectDirs = await discoverProjectThemesDirs(cwd, options.stopAt)
  const orderedProjectDirs = [...projectDirs].reverse()
  for (const dir of orderedProjectDirs) {
    await applyThemeFiles('project', dir)
  }
  const extrasSorted = Array.from(extras).sort((a, b) => a.localeCompare(b))
  const finalNames = [...order, ...extrasSorted]
  return {
    themes: finalNames
      .map((name) => themesByName.get(name))
      .filter((theme): theme is ThemeDescriptor => Boolean(theme)),
    errors,
  }
}
</file>
<file path="src/tui/theme/theme-parse.ts">
import fs from 'node:fs/promises'
export const readJsonFile = async (filePath: string): Promise<unknown> => {
  const raw = await fs.readFile(filePath, 'utf8')
  return JSON.parse(raw) as unknown
}
</file>
<file path="src/tui/theme/theme-resolver.ts">
import {
  MARKDOWN_THEME_SLOTS,
  REQUIRED_THEME_SLOTS,
  type InkColorValue,
  type RequiredThemeSlot,
  type ResolvedTheme,
  type ThemeAppearanceMode,
  type ThemeColorValue,
  type ThemeJson,
  type ThemeSlot,
} from './theme-types'
export type ResolveColorContext = {
  mode: ThemeAppearanceMode
  defs: Record<string, ThemeColorValue>
  theme: Record<string, ThemeColorValue>
}
type ResolutionState = {
  stack: string[]
  cache: Map<string, InkColorValue>
}
const HEX_6_RE = /^#[0-9a-fA-F]{6}$/
const HEX_8_RE = /^#[0-9a-fA-F]{8}$/
const ANSI_16_HEX: readonly string[] = [
  '#000000',
  '#800000',
  '#008000',
  '#808000',
  '#000080',
  '#800080',
  '#008080',
  '#c0c0c0',
  '#808080',
  '#ff0000',
  '#00ff00',
  '#ffff00',
  '#0000ff',
  '#ff00ff',
  '#00ffff',
  '#ffffff',
]
const ANSI_CUBE_LEVELS: readonly number[] = [0, 95, 135, 175, 215, 255]
const toHexByte = (value: number): string => value.toString(16).padStart(2, '0')
const rgbToHex = (r: number, g: number, b: number): string =>
  `#${toHexByte(r)}${toHexByte(g)}${toHexByte(b)}`
export const ansiToHex = (code: number): string => {
  if (!Number.isInteger(code) || code < 0 || code > 255) {
    throw new Error(`ANSI color must be an integer 0..255 (received: ${String(code)})`)
  }
  if (code < 16) {
    const value = ANSI_16_HEX[code]
    if (!value) {
      throw new Error(`Internal ANSI mapping missing for code ${code}`)
    }
    return value
  }
  if (code < 232) {
    const index = code - 16
    const rIndex = Math.floor(index / 36)
    const gIndex = Math.floor((index % 36) / 6)
    const bIndex = index % 6
    const r = ANSI_CUBE_LEVELS[rIndex]
    const g = ANSI_CUBE_LEVELS[gIndex]
    const b = ANSI_CUBE_LEVELS[bIndex]
    if (r === undefined || g === undefined || b === undefined) {
      throw new Error(`Internal ANSI color cube mapping failed for code ${code}`)
    }
    return rgbToHex(r, g, b)
  }
  const gray = 8 + 10 * (code - 232)
  return rgbToHex(gray, gray, gray)
}
const normalizeSpecial = (value: string): InkColorValue | null => {
  const lowered = value.toLowerCase()
  if (lowered === 'none' || lowered === 'transparent') {
    return undefined
  }
  return null
}
const normalizeHex = (value: string): InkColorValue | null => {
  if (HEX_6_RE.test(value)) {
    return value.toLowerCase()
  }
  if (HEX_8_RE.test(value)) {
    const rgb = value.slice(0, 7).toLowerCase()
    const alpha = value.slice(7, 9).toLowerCase()
    return alpha === '00' ? undefined : rgb
  }
  return null
}
const isVariant = (
  value: ThemeColorValue,
): value is { dark: ThemeColorValue; light: ThemeColorValue } =>
  typeof value === 'object' && value !== null && 'dark' in value && 'light' in value
const resolveNamed = (
  key: string,
  raw: ThemeColorValue,
  ctx: ResolveColorContext,
  state: ResolutionState,
) => {
  const cached = state.cache.get(key)
  if (cached !== undefined || state.cache.has(key)) {
    return cached
  }
  const cycleStart = state.stack.indexOf(key)
  if (cycleStart >= 0) {
    const cyclePath = [...state.stack.slice(cycleStart), key].join(' -> ')
    throw new Error(`Theme reference cycle detected: ${cyclePath}`)
  }
  state.stack.push(key)
  const resolved = resolveColorInternal(raw, ctx, state)
  state.stack.pop()
  state.cache.set(key, resolved)
  return resolved
}
const resolveReference = (
  name: string,
  ctx: ResolveColorContext,
  state: ResolutionState,
): InkColorValue => {
  const def = ctx.defs[name]
  if (def !== undefined) {
    return resolveNamed(`defs.${name}`, def, ctx, state)
  }
  const themeValue = ctx.theme[name]
  if (themeValue !== undefined) {
    return resolveNamed(`theme.${name}`, themeValue, ctx, state)
  }
  throw new Error(`Unknown theme color reference: ${name}`)
}
const resolveColorInternal = (
  value: ThemeColorValue,
  ctx: ResolveColorContext,
  state: ResolutionState,
): InkColorValue => {
  if (typeof value === 'number') {
    return ansiToHex(value)
  }
  if (typeof value === 'string') {
    const special = normalizeSpecial(value)
    if (special !== null) {
      return special
    }
    const hex = normalizeHex(value)
    if (hex !== null) {
      return hex
    }
    return resolveReference(value, ctx, state)
  }
  if (isVariant(value)) {
    const selected = value[ctx.mode]
    return resolveColorInternal(selected, ctx, state)
  }
  const exhaustive: never = value
  return exhaustive
}
export const resolveColor = (value: ThemeColorValue, ctx: ResolveColorContext): InkColorValue => {
  return resolveColorInternal(value, ctx, { stack: [], cache: new Map() })
}
const resolveRequiredSlot = (
  slot: RequiredThemeSlot,
  ctx: ResolveColorContext,
  state: ResolutionState,
): InkColorValue => {
  const raw = ctx.theme[slot]
  if (raw === undefined) {
    throw new Error(`Theme is missing required slot: ${slot}`)
  }
  return resolveNamed(`theme.${slot}`, raw, ctx, state)
}
export const resolveTheme = (themeJson: ThemeJson, mode: ThemeAppearanceMode): ResolvedTheme => {
  const missing = REQUIRED_THEME_SLOTS.filter((slot) => themeJson.theme[slot] === undefined)
  if (missing.length > 0) {
    throw new Error(`Theme is missing required slots: ${missing.join(', ')}`)
  }
  const ctx: ResolveColorContext = {
    mode,
    defs: themeJson.defs ?? {},
    theme: themeJson.theme,
  }
  const state: ResolutionState = { stack: [], cache: new Map() }
  const resolved: Partial<ResolvedTheme> = {}
  for (const slot of REQUIRED_THEME_SLOTS) {
    resolved[slot] = resolveRequiredSlot(slot, ctx, state)
  }
  for (const slot of MARKDOWN_THEME_SLOTS) {
    const raw = ctx.theme[slot]
    if (raw !== undefined) {
      resolved[slot] = resolveNamed(`theme.${slot}`, raw, ctx, state)
    }
  }
  return resolved as ResolvedTheme
}
</file>
<file path="src/tui/theme/theme-types.ts">
// Theme types for the Ink TUI.
//
// This module intentionally has *no* imports from `ink` (or any components).
// It defines the data model only; resolution/loading happens elsewhere.
/**
 * User-facing mode preference.
 *
 * We use `system` (not `auto`) to mirror the upstream OpenCode TUI concept:
 * "system" resolves to the terminal's current background/appearance.
 */
export type ThemeMode = 'light' | 'dark' | 'system'
/**
 * Ink-compatible color value.
 *
 * Ink accepts named colors (e.g. `cyanBright`) and can also accept 0â255 ANSI codes.
 * We keep this loose for now; later we can narrow based on the resolver we add.
 */
export type InkColorValue = string | number | undefined
export const asInkColor = (value: InkColorValue): string | undefined =>
  value === undefined ? undefined : (value as unknown as string)
export const inkColorProps = (value: InkColorValue): { color?: string } => {
  const resolved = asInkColor(value)
  return resolved === undefined ? {} : { color: resolved }
}
export const inkBackgroundColorProps = (value: InkColorValue): { backgroundColor?: string } => {
  const resolved = asInkColor(value)
  return resolved === undefined ? {} : { backgroundColor: resolved }
}
export const inkBorderColorProps = (value: InkColorValue): { borderColor?: string } => {
  const resolved = asInkColor(value)
  return resolved === undefined ? {} : { borderColor: resolved }
}
export type RequiredThemeSlot =
  | 'background'
  | 'text'
  | 'mutedText'
  | 'border'
  | 'accent'
  | 'accentText'
  | 'warning'
  | 'error'
  | 'success'
  | 'panelBackground'
  | 'popupBackground'
  | 'selectionBackground'
  | 'selectionText'
  | 'chipBackground'
  | 'chipText'
  | 'chipMutedText'
export type MarkdownThemeSlot =
  | 'markdownText'
  | 'markdownHeading'
  | 'markdownLink'
  | 'markdownLinkText'
  | 'markdownCode'
  | 'markdownBlockQuote'
  | 'markdownEmph'
  | 'markdownStrong'
  | 'markdownHorizontalRule'
  | 'markdownListItem'
  | 'markdownListEnumeration'
  | 'markdownImage'
  | 'markdownImageText'
  | 'markdownCodeBlock'
export type ThemeSlot = RequiredThemeSlot | MarkdownThemeSlot
export const REQUIRED_THEME_SLOTS: readonly RequiredThemeSlot[] = [
  'background',
  'text',
  'mutedText',
  'border',
  'accent',
  'accentText',
  'warning',
  'error',
  'success',
  'panelBackground',
  'popupBackground',
  'selectionBackground',
  'selectionText',
  'chipBackground',
  'chipText',
  'chipMutedText',
]
export const MARKDOWN_THEME_SLOTS: readonly MarkdownThemeSlot[] = [
  'markdownText',
  'markdownHeading',
  'markdownLink',
  'markdownLinkText',
  'markdownCode',
  'markdownBlockQuote',
  'markdownEmph',
  'markdownStrong',
  'markdownHorizontalRule',
  'markdownListItem',
  'markdownListEnumeration',
  'markdownImage',
  'markdownImageText',
  'markdownCodeBlock',
]
export type ResolvedTheme = Record<RequiredThemeSlot, InkColorValue> &
  Partial<Record<MarkdownThemeSlot, InkColorValue>>
// Concrete appearance mode after resolving `system`.
export type ThemeAppearanceMode = Exclude<ThemeMode, 'system'>
// Theme JSON model (structure only): modeled after OpenCode's TUI themes.
export type ThemeColorVariant = {
  dark: ThemeColorValue
  light: ThemeColorValue
}
/**
 * A single color value in theme JSON.
 *
 * Mirrors OpenCode behavior:
 * - hex strings ("#RRGGBB")
 * - reference strings (defs key or another theme slot key)
 * - ANSI code numbers (0â255)
 * - variants `{ dark, light }`
 * - special strings like "none" / "transparent" (resolver handles meaning)
 */
export type ThemeColorValue = string | number | ThemeColorVariant
export type ThemeJson<TSlots extends string = ThemeSlot> = {
  defs?: Record<string, ThemeColorValue>
  theme: Partial<Record<TSlots, ThemeColorValue>> & Record<string, ThemeColorValue>
}
</file>
<file path="src/tui/app-container-keymap.ts">
import type { Key } from 'ink'
export type AppContainerView = 'generate' | 'tests'
export type AppContainerKeyAction =
  | { type: 'none' }
  | { type: 'exit' }
  | { type: 'toggle-help'; nextIsHelpOpen: boolean }
  | { type: 'open-command-palette' }
  | { type: 'switch-to-tests' }
  | { type: 'switch-to-generate-and-open-command-palette' }
const toControlCharacter = (letter: string): string | null => {
  if (!letter) {
    return null
  }
  const normalized = letter.toLowerCase()
  const code = normalized.charCodeAt(0)
  if (code < 97 || code > 122) {
    return null
  }
  return String.fromCharCode(code - 96)
}
const matchesControlKey = (input: string, key: Key, target: string): boolean => {
  if (!target || !input) {
    return false
  }
  if (key.ctrl && input.toLowerCase() === target.toLowerCase()) {
    return true
  }
  const controlChar = toControlCharacter(target)
  return controlChar ? input === controlChar : false
}
export type ResolveAppContainerKeyActionOptions = {
  input: string
  key: Key
  view: AppContainerView
  isPopupOpen: boolean
  isHelpOpen: boolean
}
export const resolveAppContainerKeyAction = ({
  input,
  key,
  view,
  isPopupOpen,
  isHelpOpen,
}: ResolveAppContainerKeyActionOptions): AppContainerKeyAction => {
  if (isHelpOpen) {
    if (key.escape) {
      return { type: 'toggle-help', nextIsHelpOpen: false }
    }
    return { type: 'none' }
  }
  if (matchesControlKey(input, key, 'c')) {
    return { type: 'exit' }
  }
  if (key.escape) {
    return { type: 'none' }
  }
  if (view === 'generate' && isPopupOpen) {
    if (matchesControlKey(input, key, 'g') || matchesControlKey(input, key, 't')) {
      return { type: 'none' }
    }
  }
  if (matchesControlKey(input, key, 'g')) {
    return view === 'generate'
      ? { type: 'open-command-palette' }
      : { type: 'switch-to-generate-and-open-command-palette' }
  }
  if (matchesControlKey(input, key, 't')) {
    return { type: 'switch-to-tests' }
  }
  return { type: 'none' }
}
</file>
<file path="src/tui/AppContainer.tsx">
import React, { useCallback, useEffect, useMemo, useRef, useState } from 'react'
import { Box, Text, useApp, useInput, useStdout } from 'ink'
import cliCursor from 'cli-cursor'
import { CommandScreen, type CommandScreenHandle } from './CommandScreen'
import { TestRunnerScreen, type TestRunnerScreenHandle } from './TestRunnerScreen'
import { ContextProvider } from './context'
import { ToastProvider, useNotifier } from './notifier'
import { ThemeProvider, useTheme } from './theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from './theme/theme-types'
import { HelpOverlay } from './components/core/HelpOverlay'
import { ToastOverlay } from './components/core/ToastOverlay'
import { COMMAND_DESCRIPTORS } from './config'
import { createHelpSections, estimateHelpOverlayHeight } from './help-config'
import { resolveAppContainerKeyAction } from './app-container-keymap'
export type AppContainerProps = {
  interactiveTransport?: string | undefined
}
const AppContainerInner: React.FC<AppContainerProps> = ({ interactiveTransport }) => {
  const { theme } = useTheme()
  const { exit } = useApp()
  const { stdout } = useStdout()
  const [view, setView] = useState<'generate' | 'tests'>('generate')
  const [isPopupOpen, setIsPopupOpen] = useState(false)
  const [isHelpOpen, setIsHelpOpen] = useState(false)
  const [pendingCommandMenu, setPendingCommandMenu] = useState(false)
  const [commandMenuSignal, setCommandMenuSignal] = useState(0)
  const { notify } = useNotifier({ autoDismissMs: 2200 })
  const commandScreenRef = useRef<CommandScreenHandle | null>(null)
  const testRunnerRef = useRef<TestRunnerScreenHandle | null>(null)
  useEffect(() => {
    if (!process.stdout.isTTY) {
      return
    }
    cliCursor.hide()
    return () => {
      cliCursor.show()
    }
  }, [])
  useEffect(() => {
    if (view !== 'generate') {
      setIsPopupOpen(false)
    }
  }, [view])
  useEffect(() => {
    if (view === 'generate' && pendingCommandMenu) {
      commandScreenRef.current?.suppressNextInput()
      setCommandMenuSignal((prev) => prev + 1)
      setPendingCommandMenu(false)
      return
    }
    if (view !== 'generate' && pendingCommandMenu) {
      setPendingCommandMenu(false)
    }
  }, [pendingCommandMenu, view])
  const openHelp = useCallback(() => {
    if (isHelpOpen) {
      return
    }
    if (view === 'generate') {
      commandScreenRef.current?.suppressNextInput()
    } else {
      testRunnerRef.current?.suppressNextInput()
    }
    setIsHelpOpen(true)
  }, [isHelpOpen, view])
  useInput((input, key) => {
    const action = resolveAppContainerKeyAction({
      input,
      key,
      view,
      isPopupOpen,
      isHelpOpen,
    })
    if (action.type === 'none') {
      return
    }
    if (action.type === 'toggle-help') {
      if (!isHelpOpen && action.nextIsHelpOpen) {
        if (view === 'generate') {
          commandScreenRef.current?.suppressNextInput()
        } else {
          testRunnerRef.current?.suppressNextInput()
        }
      }
      setIsHelpOpen(action.nextIsHelpOpen)
      return
    }
    if (action.type === 'exit') {
      if (view === 'generate') {
        commandScreenRef.current?.suppressNextInput()
      }
      exit()
      return
    }
    if (action.type === 'open-command-palette') {
      commandScreenRef.current?.suppressNextInput()
      setCommandMenuSignal((prev) => prev + 1)
      return
    }
    if (action.type === 'switch-to-generate-and-open-command-palette') {
      setPendingCommandMenu(true)
      setView('generate')
      return
    }
    if (action.type === 'switch-to-tests') {
      if (view === 'generate') {
        commandScreenRef.current?.suppressNextInput()
      }
      setView('tests')
    }
  })
  const terminalRows = stdout?.rows ?? 24
  const helpMaxHeight = Math.max(10, terminalRows - 6)
  const helpSections = useMemo(
    () => createHelpSections({ commandDescriptors: COMMAND_DESCRIPTORS }),
    [],
  )
  const helpIdealHeight = useMemo(() => estimateHelpOverlayHeight(helpSections), [helpSections])
  const helpOverlayHeight = Math.min(helpIdealHeight, helpMaxHeight)
  const reservedRows = isHelpOpen ? helpOverlayHeight + 1 : 0
  return (
    <ContextProvider>
      <Box
        flexDirection="column"
        paddingX={2}
        paddingY={1}
        height="100%"
        width="100%"
        {...inkBackgroundColorProps(theme.background)}
      >
        <Text {...inkColorProps(theme.accent)}>Prompt Maker Âˇ Command Palette Preview</Text>
        <Text {...inkColorProps(theme.mutedText)}>
          Ctrl+G â Command Palette Âˇ Ctrl+T â Test Runner Âˇ Type /help for help Âˇ Ctrl+C or /exit to
          exit.
        </Text>
        <Box flexDirection="column" flexGrow={1} marginTop={1}>
          {view === 'generate' ? (
            <>
              <Text {...inkColorProps(theme.mutedText)}>
                Type intents freely or prefix with /command. Use arrow keys to browse history.
              </Text>
              {interactiveTransport ? (
                <Text {...inkColorProps(theme.mutedText)}>
                  Interactive transport listening on {interactiveTransport}. Remote refinements will
                  appear in history.
                </Text>
              ) : null}
              <Box flexDirection="column" flexGrow={1} marginTop={1}>
                <CommandScreen
                  ref={commandScreenRef}
                  interactiveTransportPath={interactiveTransport}
                  onPopupVisibilityChange={setIsPopupOpen}
                  commandMenuSignal={commandMenuSignal}
                  helpOpen={isHelpOpen}
                  reservedRows={reservedRows}
                  onOpenHelp={openHelp}
                  notify={notify}
                />
              </Box>
            </>
          ) : (
            <>
              <Text {...inkColorProps(theme.mutedText)}>
                Enter a test file and press Enter to run suites.
              </Text>
              <TestRunnerScreen ref={testRunnerRef} helpOpen={isHelpOpen} />
            </>
          )}
        </Box>
        {isHelpOpen ? (
          <Box marginTop={1}>
            <HelpOverlay activeView={view} maxHeight={helpMaxHeight} />
          </Box>
        ) : null}
        <ToastOverlay />
      </Box>
    </ContextProvider>
  )
}
export const AppContainer: React.FC<AppContainerProps> = ({ interactiveTransport }) => (
  <ThemeProvider>
    <ToastProvider>
      <AppContainerInner interactiveTransport={interactiveTransport} />
    </ToastProvider>
  </ThemeProvider>
)
</file>
<file path="src/tui/budget-settings.ts">
import type { ContextOverflowStrategy } from '../config'
export type BudgetSettings = {
  maxContextTokens: number | null
  maxInputTokens: number | null
  contextOverflowStrategy: ContextOverflowStrategy | null
}
export type BudgetSettingsDraft = {
  maxContextTokensDraft: string
  maxInputTokensDraft: string
  contextOverflowStrategyDraft: ContextOverflowStrategy | ''
}
export type BudgetSettingsParseResult =
  | { ok: true; settings: BudgetSettings }
  | { ok: false; errorMessage: string }
const parseOptionalPositiveInteger = (
  raw: string,
  label: string,
): { ok: true; value: number | null } | { ok: false; errorMessage: string } => {
  const trimmed = raw.trim()
  if (!trimmed) {
    return { ok: true, value: null }
  }
  if (!/^[0-9]+$/.test(trimmed)) {
    return { ok: false, errorMessage: `${label} must be a positive integer.` }
  }
  const parsed = Number(trimmed)
  if (!Number.isFinite(parsed) || !Number.isInteger(parsed) || parsed <= 0) {
    return { ok: false, errorMessage: `${label} must be a positive integer.` }
  }
  return { ok: true, value: parsed }
}
export const parseBudgetSettingsDraft = (draft: BudgetSettingsDraft): BudgetSettingsParseResult => {
  const maxContextResult = parseOptionalPositiveInteger(
    draft.maxContextTokensDraft,
    'Max context tokens',
  )
  if (!maxContextResult.ok) {
    return maxContextResult
  }
  const maxInputResult = parseOptionalPositiveInteger(draft.maxInputTokensDraft, 'Max input tokens')
  if (!maxInputResult.ok) {
    return maxInputResult
  }
  const budgetsEnabled = maxContextResult.value !== null || maxInputResult.value !== null
  const effectiveStrategy: ContextOverflowStrategy | null = draft.contextOverflowStrategyDraft
    ? draft.contextOverflowStrategyDraft
    : budgetsEnabled
      ? 'fail'
      : null
  return {
    ok: true,
    settings: {
      maxContextTokens: maxContextResult.value,
      maxInputTokens: maxInputResult.value,
      contextOverflowStrategy: effectiveStrategy,
    },
  }
}
</file>
<file path="src/tui/DEVELOPER_NOTE.md">
# prompt-maker-cli TUI Developer Note
This document explains the _current_ Ink TUI architecture in `src/tui/**` after the refactor series.
Goals of this architecture:
- Keep UI behavior stable while making state flow explicit.
- Reduce avoidable re-renders by stabilizing props/callbacks.
- Make complex state transitions unit-testable (pure reducers).
## Directory structure
### Entry points (thin re-exports)
These files stay intentionally small so âbig screensâ donât live in the root `tui/` folder:
- `src/tui/CommandScreen.tsx`
- `src/tui/TestRunnerScreen.tsx`
### Screens
Each screen owns orchestration + rendering, and delegates state transitions to a reducer.
- Command screen
  - `src/tui/screens/command/CommandScreen.tsx`
  - `src/tui/screens/command/useCommandScreen.ts`
  - `src/tui/screens/command/command-screen-reducer.ts`
  - `src/tui/screens/command/components/*`
- Test runner
  - `src/tui/screens/test-runner/TestRunnerScreen.tsx`
  - `src/tui/screens/test-runner/useTestRunnerScreen.ts`
  - `src/tui/screens/test-runner/test-runner-reducer.ts`
  - `src/tui/screens/test-runner/components/*`
### Cross-screen hooks and reducers
These modules are shared âfeature hooksâ used by screens:
- Popup state machine
  - Pure reducer: `src/tui/popup-reducer.ts`
  - Hook + effects: `src/tui/hooks/usePopupManager.ts`
- Generation pipeline
  - Pure reducer: `src/tui/generation-pipeline-reducer.ts`
  - Hook + effects: `src/tui/hooks/useGenerationPipeline.ts`
### Core UI components
Reusable Ink components live in:
- `src/tui/components/core/*`
- `src/tui/components/popups/*`
Rule of thumb:
- **Core components** should be mostly presentational: props in, render out.
- Side effects (timers, subscriptions, async work) belong in hooks/screens.
## Input routing invariants
Input routing is one of the easiest places for TUIs to regress.
The intended priority order is:
1. **Help overlay**: when help is open, most screen input should be suppressed.
2. **Popup input**: when any popup is open, it should âownâ the keyboard.
3. **Screen input**: otherwise, the active screen handles keys.
4. **AppContainer global keys**: keys like exit remain truly global.
Implementation notes:
- `usePopupManager` exposes `popupState` and popup actions; screens should use `popupState !== null` to decide whether screen-level inputs should be active.
- Avoid âfallthroughâ key handling where the same key can be interpreted by both a popup and the screen.
## Reducers: responsibilities and patterns
Why reducers are used here (plain terms):
- Instead of calling many different `setState(...)` functions from many callbacks, we send a single âactionâ describing what happened.
- The reducer is a pure function that returns the next state.
Benefits:
- Easier to reason about state transitions.
- Easier to test (no Ink/TTY required).
- Often fewer renders because multiple fields can change in one dispatch.
Patterns used in this codebase:
- Pure reducers live in `*-reducer.ts` files with no React/Ink imports.
- Hooks (e.g. `usePopupManager`, `useGenerationPipeline`) wrap reducers and are responsible for effects.
### Stale closure (important React/Ink concept)
A âstale closureâ happens when a callback captures old values.
Example:
- If a stable `handleStreamEvent` callback closes over an old terminal width, it would keep wrapping output at the wrong width after resize.
Fix pattern used here:
- Keep callbacks stable, but read changing values from refs (e.g. `terminalColumnsRef.current`).
## Performance notes
### Windowing and log/history rendering
- Main history rendering is windowed using `ScrollableOutput` which slices to the visible rows.
- List/windowing primitives live in `src/tui/components/popups/list-window.ts`.
- Test runner logs are capped by `useLogBuffer` (default 20 entries) in `src/tui/useLogBuffer.ts`.
### Where re-renders come from
Typical rerender causes in this TUI:
- Passing newly-created arrays/objects as props (e.g. `statusChips`, filtered lists).
- Recreating callbacks each render.
- Doing heavy formatting work in render paths.
Mitigations used:
- Memoize derived arrays when it actually prevents work.
- Use reducer dispatches to group related state updates.
- Use refs to avoid stale closures when callbacks must stay stable.
### Before vs after (high-level)
Before:
- `CommandScreen.tsx` and popup/generation logic mixed together in large, hard-to-test modules.
- More state âfan outâ across many `useState` hooks.
- Async popup suggestion scans relied on implicit âprev.type === âŚâ guards.
After:
- Popups and pipeline have explicit reducers and tests.
- Screens are organized under `screens/*` with presentational subcomponents.
- Async suggestion scans are guarded with scan IDs to avoid stale updates.
Why it matters:
- Less accidental rerender churn during typing.
- Fewer subtle regressions when splitting UI or moving logic.
- Faster onboarding: new contributors can locate âstate vs effects vs renderingâ more easily.
## How to add a new popup safely
1. Add a new popup union member in `src/tui/types.ts`.
2. Add explicit transitions in `src/tui/popup-reducer.ts`.
3. Add the render branch in `src/tui/screens/command/components/PopupArea.tsx`.
4. Add a reducer unit test in `src/__tests__/tui/popup-reducer.test.ts`.
Keep behavior stable:
- Prefer âopen-\*â actions for initial state.
- If the popup loads async suggestions, ensure scan results are only applied when popup type + scan id still match.
## How to add a new screen safely
1. Create `src/tui/screens/<name>/`.
2. Add a pure reducer `*-reducer.ts`.
3. Add a screen hook `use<Name>Screen.ts` to expose a view-model API.
4. Keep the root-level entry file (`src/tui/<Name>Screen.tsx`) as a small re-export.
## Manual verification checklist
These checks are intentionally âhuman-visibleâ and catch most regressions quickly:
- Responsiveness while typing quickly in the command input.
- Large history:
  - generate a long output and scroll; ensure no lag.
- Large model list:
  - open model popup and type queries quickly.
- Popup churn:
  - open/close popups repeatedly; ensure no delayed updates after switching.
- Test runner:
  - tab between fields, run tests, confirm statuses + logs update.
</file>
<file path="src/tui/generation-pipeline-reducer.ts">
/*
 * Generation pipeline reducer (pure state transitions).
 *
 * This reducer manages UI-facing generation state for the Ink TUI:
 * - whether we are generating
 * - current status message
 * - whether we're waiting for interactive input/refinement
 * - latest token telemetry
 *
 * Why a reducer?
 * - Itâs common to update multiple related fields at once (e.g. set generating +
 *   clear interactive waiting + update the status message).
 * - A single reducer action lets us do that in one render.
 *
 * This file is intentionally pure:
 * - no React/Ink imports
 * - easy to unit test
 */
import type { StreamEventInput } from '../generate/types'
import type { GeneratePipelineResult } from '../generate-command'
export type InteractiveAwaitingMode = 'transport' | 'tty'
export type GenerationTelemetry = GeneratePipelineResult['telemetry']
export type ResumeLoadedDetails = Extract<StreamEventInput, { event: 'resume.loaded' }>
export type ContextOverflowDetails = Extract<StreamEventInput, { event: 'context.overflow' }>
export type GenerationPipelineState = {
  isGenerating: boolean
  statusMessage: string
  isAwaitingRefinement: boolean
  awaitingInteractiveMode: InteractiveAwaitingMode | null
  latestTelemetry: GenerationTelemetry | null
  latestResumeLoaded: ResumeLoadedDetails | null
  latestContextOverflow: ContextOverflowDetails | null
}
export type GenerationPipelineAction =
  | { type: 'generation-start'; statusMessage: string }
  | { type: 'generation-stop'; statusMessage?: string }
  | { type: 'set-status'; statusMessage: string }
  | {
      type: 'set-awaiting-interactive'
      awaitingInteractiveMode: InteractiveAwaitingMode | null
      statusMessage?: string
    }
  | { type: 'set-awaiting-refinement'; isAwaitingRefinement: boolean }
  | { type: 'set-telemetry'; telemetry: GenerationTelemetry | null }
  | { type: 'set-resume-loaded'; details: ResumeLoadedDetails | null }
  | { type: 'set-context-overflow'; details: ContextOverflowDetails | null }
export const INITIAL_GENERATION_PIPELINE_STATE: GenerationPipelineState = {
  isGenerating: false,
  statusMessage: 'Idle',
  isAwaitingRefinement: false,
  awaitingInteractiveMode: null,
  latestTelemetry: null,
  latestResumeLoaded: null,
  latestContextOverflow: null,
}
export const generationPipelineReducer = (
  state: GenerationPipelineState,
  action: GenerationPipelineAction,
): GenerationPipelineState => {
  switch (action.type) {
    case 'generation-start':
      return {
        ...state,
        isGenerating: true,
        statusMessage: action.statusMessage,
        isAwaitingRefinement: false,
        awaitingInteractiveMode: null,
        latestTelemetry: null,
        latestResumeLoaded: null,
        latestContextOverflow: null,
      }
    case 'generation-stop':
      return {
        ...state,
        isGenerating: false,
        // Preserve the existing status message unless explicitly overridden.
        statusMessage: action.statusMessage ?? state.statusMessage,
        isAwaitingRefinement: false,
        awaitingInteractiveMode: null,
      }
    case 'set-status':
      return { ...state, statusMessage: action.statusMessage }
    case 'set-awaiting-interactive':
      return {
        ...state,
        awaitingInteractiveMode: action.awaitingInteractiveMode,
        ...(action.statusMessage ? { statusMessage: action.statusMessage } : {}),
      }
    case 'set-awaiting-refinement':
      return { ...state, isAwaitingRefinement: action.isAwaitingRefinement }
    case 'set-telemetry':
      return { ...state, latestTelemetry: action.telemetry }
    case 'set-resume-loaded':
      return { ...state, latestResumeLoaded: action.details }
    case 'set-context-overflow':
      return { ...state, latestContextOverflow: action.details }
    default:
      return state
  }
}
</file>
<file path="src/tui/index.tsx">
import { render } from 'ink'
import { AppContainer } from './AppContainer'
type TuiOptions = {
  interactiveTransport?: string
}
const parseTuiArgs = (argv: string[]): TuiOptions => {
  const options: TuiOptions = {}
  for (let i = 0; i < argv.length; i += 1) {
    const token = argv[i]
    if (!token) {
      continue
    }
    if (token === '--interactive-transport') {
      const next = argv[i + 1]
      if (next && !next.startsWith('-')) {
        options.interactiveTransport = next
        i += 1
      }
      continue
    }
    if (token.startsWith('--interactive-transport=')) {
      options.interactiveTransport = token.split('=').slice(1).join('=')
    }
  }
  return options
}
const EXIT_CLEAR_SEQUENCE = '\u001b[0m\u001b[2J\u001b[H'
const clearTerminalOnExit = (): void => {
  if (process.stdout.isTTY) {
    process.stdout.write(EXIT_CLEAR_SEQUENCE)
  }
}
export const runTuiCommand = async (argv: string[]): Promise<void> => {
  const options = parseTuiArgs(argv)
  const { waitUntilExit } = render(
    <AppContainer interactiveTransport={options.interactiveTransport} />,
  )
  await waitUntilExit()
  clearTerminalOnExit()
}
</file>
<file path="src/tui/model-options.ts">
import { loadCliConfig, resolveGeminiCredentials, resolveOpenAiCredentials } from '../config'
import type { ModelDefinition } from '../model-providers'
import { inferProviderFromModelId } from '../model-providers'
import { getAvailableModels, getVideoCapableGeminiModels } from '../utils/model-manager'
import type { ModelOption } from './types'
const DEFAULT_MODEL_FALLBACK = 'gpt-4o-mini'
const BUILT_IN_MODEL_DEFINITIONS: ModelDefinition[] = [
  {
    id: 'gpt-4o-mini',
    label: 'GPT-4o Mini',
    provider: 'openai',
    description: 'Fast, general-purpose GPT-4o variant.',
    capabilities: ['fast', 'economical', 'general-purpose'],
    notes: 'Great default for day-to-day prompts.',
    default: true,
  },
  {
    id: 'gpt-4o',
    label: 'GPT-4o',
    provider: 'openai',
    description: 'Flagship GPT-4 omni model.',
    capabilities: ['multimodal', 'high-quality'],
    notes: 'Use when quality matters more than speed.',
  },
  {
    id: 'gpt-4.1',
    label: 'GPT-4.1',
    provider: 'openai',
    description: 'Reasoning-optimized GPT-4 family model.',
    capabilities: ['reasoning', 'multimodal'],
    notes: 'Best for planning and structured tasks.',
  },
  {
    id: 'gpt-5.2',
    label: 'GPT-5.2',
    provider: 'openai',
    description: 'Next-gen GPT model with advanced reasoning.',
    capabilities: ['reasoning', 'multimodal', 'long-context'],
    notes: 'Use when you need top-tier quality and depth.',
  },
  {
    id: 'gemini-1.5-pro',
    label: 'Gemini 1.5 Pro',
    provider: 'gemini',
    description: 'Google Gemini multimodal model with long context.',
    capabilities: ['multimodal', 'long-context'],
    notes: 'Required for video uploads or giant contexts.',
  },
  {
    id: 'gemini-1.5-flash',
    label: 'Gemini 1.5 Flash',
    provider: 'gemini',
    description: 'Lower-latency Gemini model.',
    capabilities: ['fast', 'multimodal'],
    notes: 'Use for chatty flows that still need multimodal.',
  },
  {
    id: 'gemini-3-pro-preview',
    label: 'Gemini 3 Pro (Preview)',
    provider: 'gemini',
    description: 'Latest Gemini 3 preview model.',
    capabilities: ['multimodal', 'reasoning', 'preview'],
    notes: 'Preview tierâexpect rapid changes.',
  },
]
const cloneOption = (option: ModelOption): ModelOption => ({
  ...option,
  capabilities: [...option.capabilities],
})
const normalizeModelDefinition = (
  definition: ModelDefinition,
  source: ModelOption['source'],
): ModelOption => {
  const provider = definition.provider ?? inferProviderFromModelId(definition.id)
  const label = definition.label?.trim() || definition.id
  const description = definition.description?.trim() || `${label} (${provider})`
  const capabilities = definition.capabilities?.map((cap) => cap.trim()).filter(Boolean) ?? []
  const normalized: ModelOption = {
    id: definition.id,
    label,
    provider,
    description,
    capabilities,
    source,
  }
  if (definition.default) {
    normalized.default = true
  }
  if (definition.notes?.trim()) {
    normalized.notes = definition.notes.trim()
  }
  return normalized
}
const BUILT_IN_MODEL_OPTIONS = BUILT_IN_MODEL_DEFINITIONS.map((definition) =>
  normalizeModelDefinition(definition, 'builtin'),
)
let cachedModelOptions: ModelOption[] | null = null
let cachedWarning: string | null = null
let cachedModelOptionsNoDiscovery: ModelOption[] | null = null
let cachedWarningNoDiscovery: string | null = null
const mergeModelOptions = (base: ModelOption[], overrides: ModelOption[]): ModelOption[] => {
  const merged = new Map<string, ModelOption>()
  base.forEach((option) => merged.set(option.id, option))
  overrides.forEach((option) => merged.set(option.id, option))
  return Array.from(merged.values())
}
export const getBuiltInModelOptions = (): ModelOption[] => BUILT_IN_MODEL_OPTIONS.map(cloneOption)
export type LoadModelOptionsResult = {
  options: ModelOption[]
  warning?: string
}
export type LoadModelOptionsOptions = {
  includeDiscovered?: boolean
}
export const loadModelOptions = async (
  options: LoadModelOptionsOptions = {},
): Promise<LoadModelOptionsResult> => {
  const includeDiscovered = options.includeDiscovered !== false
  const cached = includeDiscovered ? cachedModelOptions : cachedModelOptionsNoDiscovery
  const warning = includeDiscovered ? cachedWarning : cachedWarningNoDiscovery
  if (cached) {
    const warningResult = warning
      ? { options: cached.map(cloneOption), warning }
      : { options: cached.map(cloneOption) }
    return warningResult
  }
  try {
    const config = await loadCliConfig()
    const extraDefinitions = config?.promptGenerator?.models ?? []
    const normalizedExtras = extraDefinitions.map((definition) =>
      normalizeModelDefinition(definition, 'config'),
    )
    const reservedIds = new Set(
      [...BUILT_IN_MODEL_OPTIONS, ...normalizedExtras].map((option) => option.id),
    )
    let discoveredOptions: ModelOption[] = []
    let videoCapableGeminiIds: Set<string> | null = null
    if (includeDiscovered) {
      try {
        const [openAiCredentials, geminiCredentials] = await Promise.all([
          resolveOpenAiCredentials().catch(() => null),
          resolveGeminiCredentials().catch(() => null),
        ])
        if (geminiCredentials?.apiKey) {
          try {
            const videoIds = await getVideoCapableGeminiModels(geminiCredentials.apiKey, {
              ...(geminiCredentials.baseUrl ? { baseUrl: geminiCredentials.baseUrl } : {}),
            })
            videoCapableGeminiIds = new Set(videoIds)
          } catch {
            videoCapableGeminiIds = null
          }
        }
        const discovered = await getAvailableModels(
          openAiCredentials?.apiKey,
          geminiCredentials?.apiKey,
          {
            ...(openAiCredentials?.baseUrl ? { openAiBaseUrl: openAiCredentials.baseUrl } : {}),
            ...(geminiCredentials?.baseUrl ? { geminiBaseUrl: geminiCredentials.baseUrl } : {}),
            ...(process.env.OPENAI_ORG_ID?.trim()
              ? { openAiOrganizationId: process.env.OPENAI_ORG_ID.trim() }
              : {}),
            ...(process.env.OPENAI_PROJECT_ID?.trim()
              ? { openAiProjectId: process.env.OPENAI_PROJECT_ID.trim() }
              : {}),
          },
        )
        const discoveredIds = [...discovered.openai, ...discovered.gemini].filter(
          (modelId) => !reservedIds.has(modelId),
        )
        discoveredOptions = discoveredIds.map((modelId) =>
          normalizeModelDefinition({ id: modelId }, 'discovered'),
        )
      } catch {
        // Best-effort; dynamic discovery should never block model selection.
      }
    }
    const mergedBase = mergeModelOptions(BUILT_IN_MODEL_OPTIONS, [
      ...normalizedExtras,
      ...discoveredOptions,
    ])
    const merged =
      videoCapableGeminiIds && videoCapableGeminiIds.size > 0
        ? mergedBase.map((option) => {
            if (option.provider !== 'gemini') {
              return option
            }
            if (!videoCapableGeminiIds.has(option.id)) {
              return option
            }
            if (option.capabilities.includes('video')) {
              return option
            }
            return { ...option, capabilities: [...option.capabilities, 'video'] }
          })
        : mergedBase
    if (includeDiscovered) {
      cachedModelOptions = merged
      cachedWarning = null
    } else {
      cachedModelOptionsNoDiscovery = merged
      cachedWarningNoDiscovery = null
    }
    return { options: merged.map(cloneOption) }
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown CLI config error.'
    const fallbackOptions = BUILT_IN_MODEL_OPTIONS
    const fallbackWarning = `Failed to load CLI model entries: ${message}`
    if (includeDiscovered) {
      cachedModelOptions = fallbackOptions
      cachedWarning = fallbackWarning
    } else {
      cachedModelOptionsNoDiscovery = fallbackOptions
      cachedWarningNoDiscovery = fallbackWarning
    }
    return { options: fallbackOptions.map(cloneOption), warning: fallbackWarning }
  }
}
export const getPreferredModelId = (
  options: ModelOption[],
  requestedId?: string | null,
): string => {
  if (requestedId) {
    const requested = options.find((option) => option.id === requestedId)
    if (requested) {
      return requested.id
    }
  }
  const defaultOption = options.find((option) => option.default)
  if (defaultOption) {
    return defaultOption.id
  }
  return options[0]?.id ?? DEFAULT_MODEL_FALLBACK
}
export const DEFAULT_MODEL_ID = DEFAULT_MODEL_FALLBACK
</file>
<file path="src/tui/notifier.ts">
/**
 * TODO: the dedupe logic in showToast could be made more sophisticated.
 * It should not be deduping at all. Need to figure out a better way to handle
 * repeated messages that need to be updated (e.g. progress toasts).
 */
import React, {
  createContext,
  useCallback,
  useContext,
  useEffect,
  useMemo,
  useRef,
  useState,
} from 'react'
import { DEFAULT_MAX_TOASTS, TOAST_ANIMATION_TICK_MS, TOAST_HEIGHT } from './toast-constants'
import {
  addOrReuseToast,
  beginExitToast,
  createToastTrimmedMessage,
  getLatestActiveToast,
  getOldestActiveToastId,
  removeToast as removeToastFromList,
} from './toast-state'
import type { ToastId, ToastItem, ToastKind } from './toast-state'
export type { ToastId, ToastItem, ToastKind } from './toast-state'
export type NotifyOptions = {
  kind?: ToastKind
  autoDismissMs?: number
}
export type UseNotifierOptions = {
  autoDismissMs?: number
}
type ToastContextValue = {
  toasts: ToastItem[]
  showToast: (message: string, options?: NotifyOptions) => ToastId | null
  dismissToast: (id: ToastId) => void
  removeToast: (id: ToastId) => void
  dismissLatest: () => void
  maxToasts: number
  defaultAutoDismissMs: number
  exitAnimationMs: number
}
const DEFAULT_AUTO_DISMISS_MS = 2200
const ToastContext = createContext<ToastContextValue | null>(null)
export type ToastProviderProps = {
  children: React.ReactNode
  maxToasts?: number
  defaultAutoDismissMs?: number
  exitAnimationMs?: number
}
export const ToastProvider = ({
  children,
  maxToasts = DEFAULT_MAX_TOASTS,
  defaultAutoDismissMs = DEFAULT_AUTO_DISMISS_MS,
  exitAnimationMs = TOAST_HEIGHT * TOAST_ANIMATION_TICK_MS,
}: ToastProviderProps) => {
  const [toasts, setToasts] = useState<ToastItem[]>([])
  const nextToastIdRef = useRef(1)
  const dismissTimersRef = useRef(new Map<ToastId, ReturnType<typeof setTimeout>>())
  const removalTimersRef = useRef(new Map<ToastId, ReturnType<typeof setTimeout>>())
  const toastsRef = useRef<ToastItem[]>([])
  useEffect(() => {
    toastsRef.current = toasts
  }, [toasts])
  const clearDismissTimer = useCallback((id: ToastId): void => {
    const timer = dismissTimersRef.current.get(id)
    if (!timer) {
      return
    }
    clearTimeout(timer)
    dismissTimersRef.current.delete(id)
  }, [])
  const clearRemovalTimer = useCallback((id: ToastId): void => {
    const timer = removalTimersRef.current.get(id)
    if (!timer) {
      return
    }
    clearTimeout(timer)
    removalTimersRef.current.delete(id)
  }, [])
  const removeToast = useCallback(
    (id: ToastId): void => {
      clearDismissTimer(id)
      clearRemovalTimer(id)
      setToasts((prev) => removeToastFromList(prev, id))
    },
    [clearDismissTimer, clearRemovalTimer],
  )
  const beginExit = useCallback(
    (id: ToastId): void => {
      clearDismissTimer(id)
      setToasts((prev) => beginExitToast(prev, id))
      if (removalTimersRef.current.has(id)) {
        return
      }
      const timer = setTimeout(() => {
        removeToast(id)
      }, exitAnimationMs)
      removalTimersRef.current.set(id, timer)
    },
    [clearDismissTimer, exitAnimationMs, removeToast],
  )
  const dismissToast = useCallback(
    (id: ToastId): void => {
      beginExit(id)
    },
    [beginExit],
  )
  const dismissLatest = useCallback((): void => {
    const latestActive = getLatestActiveToast(toastsRef.current)
    if (!latestActive) {
      return
    }
    beginExit(latestActive.id)
  }, [beginExit])
  const showToast = useCallback(
    (message: string, options: NotifyOptions = {}): ToastId | null => {
      const trimmed = createToastTrimmedMessage(message)
      if (!trimmed) {
        return null
      }
      const kind = options.kind ?? 'info'
      const autoDismissMs = options.autoDismissMs ?? defaultAutoDismissMs
      const result = addOrReuseToast({
        toasts: toastsRef.current,
        message: trimmed,
        kind,
        autoDismissMs,
        now: Date.now(),
        nextToastId: nextToastIdRef.current,
      })
      nextToastIdRef.current = result.nextToastId
      const toastId = result.toastId
      if (result.action === 'added') {
        toastsRef.current = result.toasts
        setToasts(result.toasts)
      }
      clearDismissTimer(toastId)
      if (autoDismissMs !== null) {
        const timer = setTimeout(() => {
          dismissToast(toastId)
        }, autoDismissMs)
        dismissTimersRef.current.set(toastId, timer)
      }
      return toastId
    },
    [clearDismissTimer, defaultAutoDismissMs, dismissToast],
  )
  useEffect(() => {
    const activeToasts = toasts.filter((toast) => !toast.isExiting)
    if (activeToasts.length <= maxToasts) {
      return
    }
    const oldestActiveId = getOldestActiveToastId(toasts)
    if (oldestActiveId === null) {
      return
    }
    beginExit(oldestActiveId)
  }, [beginExit, maxToasts, toasts])
  useEffect(() => {
    const dismissTimers = dismissTimersRef.current
    const removalTimers = removalTimersRef.current
    return () => {
      for (const timer of dismissTimers.values()) {
        clearTimeout(timer)
      }
      dismissTimers.clear()
      for (const timer of removalTimers.values()) {
        clearTimeout(timer)
      }
      removalTimers.clear()
    }
  }, [])
  const value = useMemo<ToastContextValue>(
    () => ({
      toasts,
      showToast,
      dismissToast,
      removeToast,
      dismissLatest,
      maxToasts,
      defaultAutoDismissMs,
      exitAnimationMs,
    }),
    [
      defaultAutoDismissMs,
      dismissLatest,
      dismissToast,
      exitAnimationMs,
      maxToasts,
      removeToast,
      showToast,
      toasts,
    ],
  )
  return React.createElement(ToastContext.Provider, { value }, children)
}
export const useToastContext = (): ToastContextValue => {
  const value = useContext(ToastContext)
  if (!value) {
    throw new Error('useToastContext must be used within a ToastProvider')
  }
  return value
}
export const useNotifier = (options: UseNotifierOptions = {}) => {
  const { toasts, showToast, dismissToast, dismissLatest, defaultAutoDismissMs } = useToastContext()
  const defaultDismissMs = options.autoDismissMs ?? defaultAutoDismissMs
  const notify = useCallback(
    (message: string, notifyOptions: NotifyOptions = {}): void => {
      void showToast(message, {
        ...notifyOptions,
        autoDismissMs: notifyOptions.autoDismissMs ?? defaultDismissMs,
      })
    },
    [defaultDismissMs, showToast],
  )
  return {
    toasts,
    notify,
    showToast,
    dismiss: dismissLatest,
    dismissToast,
  }
}
</file>
<file path="src/tui/provider-status.ts">
import { resolveGeminiCredentials, resolveOpenAiCredentials } from '../config'
import { inferProviderFromModelId } from '../model-providers'
import type { ModelProvider } from '../model-providers'
import type { ProviderStatus } from './types'
const statusCache = new Map<ModelProvider, ProviderStatus>()
const inflightChecks = new Map<ModelProvider, Promise<ProviderStatus>>()
const buildStatus = (
  provider: ModelProvider,
  status: ProviderStatus['status'],
  message: string,
): ProviderStatus => ({ provider, status, message })
const resolveStatusInternal = async (provider: ModelProvider): Promise<ProviderStatus> => {
  try {
    if (provider === 'openai') {
      await resolveOpenAiCredentials()
      return buildStatus('openai', 'ok', 'Credentials available')
    }
    if (provider === 'gemini') {
      await resolveGeminiCredentials()
      return buildStatus('gemini', 'ok', 'Credentials available')
    }
    return buildStatus(provider, 'ok', 'Custom provider (not validated)')
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown provider error.'
    const status: ProviderStatus['status'] = /missing/i.test(message) ? 'missing' : 'error'
    return buildStatus(provider, status, message)
  }
}
export const checkProviderStatus = async (provider: ModelProvider): Promise<ProviderStatus> => {
  if (statusCache.has(provider)) {
    return statusCache.get(provider) as ProviderStatus
  }
  if (inflightChecks.has(provider)) {
    return inflightChecks.get(provider) as Promise<ProviderStatus>
  }
  const promise = resolveStatusInternal(provider).then((status) => {
    statusCache.set(provider, status)
    return status
  })
  inflightChecks.set(provider, promise)
  try {
    return await promise
  } finally {
    inflightChecks.delete(provider)
  }
}
export const checkModelProviderStatus = async (modelId: string): Promise<ProviderStatus> => {
  const provider = inferProviderFromModelId(modelId)
  return await checkProviderStatus(provider)
}
export const invalidateProviderStatus = (provider?: ModelProvider): void => {
  if (provider) {
    statusCache.delete(provider)
    inflightChecks.delete(provider)
    return
  }
  statusCache.clear()
  inflightChecks.clear()
}
</file>
<file path="src/tui/resume-history.ts">
import fs from 'node:fs/promises'
import { GENERATE_JSON_PAYLOAD_SCHEMA_VERSION, type GenerateJsonPayload } from '../generate/types'
import { resolveHistoryFilePath } from '../history-logger'
import type { ResumeHistoryItem } from './types'
type JsonRecord = Record<string, unknown>
const isRecord = (value: unknown): value is JsonRecord =>
  typeof value === 'object' && value !== null && !Array.isArray(value)
const isStringArray = (value: unknown): value is string[] =>
  Array.isArray(value) && value.every((entry) => typeof entry === 'string')
const isContextPaths = (value: unknown): value is GenerateJsonPayload['contextPaths'] =>
  Array.isArray(value) &&
  value.every(
    (entry) =>
      isRecord(entry) && typeof entry.path === 'string' && typeof entry.source === 'string',
  )
const isSupportedGeneratePayload = (value: unknown): value is GenerateJsonPayload => {
  if (!isRecord(value)) {
    return false
  }
  if (value.schemaVersion !== GENERATE_JSON_PAYLOAD_SCHEMA_VERSION) {
    return false
  }
  return (
    typeof value.intent === 'string' &&
    typeof value.model === 'string' &&
    typeof value.targetModel === 'string' &&
    typeof value.prompt === 'string' &&
    isStringArray(value.refinements) &&
    typeof value.iterations === 'number' &&
    Number.isFinite(value.iterations) &&
    typeof value.interactive === 'boolean' &&
    typeof value.timestamp === 'string' &&
    isContextPaths(value.contextPaths)
  )
}
const isMissingFileError = (error: unknown): error is { code: string } =>
  typeof error === 'object' &&
  error !== null &&
  'code' in error &&
  typeof (error as { code?: unknown }).code === 'string' &&
  (error as { code: string }).code === 'ENOENT'
const formatTimestamp = (raw: string): string => {
  const date = new Date(raw)
  if (!Number.isFinite(date.getTime())) {
    return raw
  }
  return date.toISOString().replace('T', ' ').replace(/\..*$/, '')
}
const summarizeIntent = (intent: string, limit: number): string => {
  const normalized = intent.replace(/\s+/g, ' ').trim()
  if (!normalized) {
    return '(empty intent)'
  }
  if (normalized.length <= limit) {
    return normalized
  }
  return `${normalized.slice(0, Math.max(0, limit - 1))}âŚ`
}
export type ResumeHistoryLoadResult =
  | { ok: true; items: ResumeHistoryItem[] }
  | { ok: false; errorMessage: string }
export const loadResumeHistoryItems = async (options?: {
  limit?: number
}): Promise<ResumeHistoryLoadResult> => {
  const filePath = resolveHistoryFilePath()
  let raw: string
  try {
    raw = await fs.readFile(filePath, 'utf8')
  } catch (error) {
    if (isMissingFileError(error)) {
      return {
        ok: false,
        errorMessage: `No prompt history found at ${filePath}. Run a generation first to create it.`,
      }
    }
    const message = error instanceof Error ? error.message : 'Unknown file error.'
    return { ok: false, errorMessage: `Failed to read history file ${filePath}: ${message}` }
  }
  const lines = raw
    .split('\n')
    .map((line) => line.trim())
    .filter((line) => line.length > 0)
  if (lines.length === 0) {
    return { ok: false, errorMessage: `History file ${filePath} is empty.` }
  }
  const supported: GenerateJsonPayload[] = []
  const schemaVersions = new Set<string>()
  for (const line of lines) {
    try {
      const parsed = JSON.parse(line) as unknown
      if (isRecord(parsed) && typeof parsed.schemaVersion === 'string') {
        schemaVersions.add(parsed.schemaVersion)
      }
      if (isSupportedGeneratePayload(parsed)) {
        supported.push(parsed)
      }
    } catch {
      // ignore invalid json lines
    }
  }
  if (supported.length === 0) {
    const versions = Array.from(schemaVersions).sort().join(', ') || '(unknown)'
    return {
      ok: false,
      errorMessage:
        `No resumable history entries found. ` +
        `This version supports schemaVersion=${GENERATE_JSON_PAYLOAD_SCHEMA_VERSION}, but history contains: ${versions}. ` +
        'Upgrade/downgrade prompt-maker-cli to match, or generate a new prompt to create a compatible history entry.',
    }
  }
  const limit = options?.limit ?? 30
  const newestFirst = supported.slice(-limit).reverse()
  const items: ResumeHistoryItem[] = newestFirst.map((payload, index) => {
    const selector = index === 0 ? 'last' : `last:${index + 1}`
    const fileCount = payload.contextPaths.filter((entry) => entry.source === 'file').length
    return {
      selector,
      title: `${formatTimestamp(payload.timestamp)} Âˇ ${payload.model}`,
      detail: `files:${fileCount} Âˇ iters:${payload.iterations} Âˇ ${summarizeIntent(payload.intent, 60)}`,
    }
  })
  return { ok: true, items }
}
</file>
<file path="src/tui/toast-constants.ts">
export const TOAST_HEIGHT = 5
export const TOAST_ANIMATION_TICK_MS = 40
export const DEFAULT_MAX_TOASTS = 5
// How many terminal rows to offset the toast stack
// from the top edge of the screen.
export const TOAST_TOP_OFFSET_ROWS = 2
// Horizontal inset (in terminal columns) for toast overlay.
export const TOAST_HORIZONTAL_INSET_COLUMNS = 0
</file>
<file path="src/tui/toast-state.ts">
export type ToastKind = 'info' | 'progress' | 'warning' | 'error'
export type ToastId = number
export type ToastItem = {
  id: ToastId
  message: string
  kind: ToastKind
  createdAt: number
  autoDismissMs: number | null
  isExiting: boolean
}
export const createToastTrimmedMessage = (message: string): string | null => {
  const trimmed = message.trim()
  return trimmed ? trimmed : null
}
export const getLatestActiveToast = (toasts: ToastItem[]): ToastItem | null => {
  const latestActive = [...toasts].reverse().find((toast) => !toast.isExiting)
  return latestActive ?? null
}
export const getOldestActiveToastId = (toasts: ToastItem[]): ToastId | null => {
  const oldest = toasts.find((toast) => !toast.isExiting)
  return oldest ? oldest.id : null
}
export type AddOrReuseToastResult =
  | {
      action: 'added'
      toastId: ToastId
      toasts: ToastItem[]
      nextToastId: ToastId
    }
  | {
      action: 'reused'
      toastId: ToastId
      toasts: ToastItem[]
      nextToastId: ToastId
    }
export const addOrReuseToast = (input: {
  toasts: ToastItem[]
  message: string
  kind: ToastKind
  autoDismissMs: number | null
  now: number
  nextToastId: ToastId
}): AddOrReuseToastResult => {
  const latestActive = getLatestActiveToast(input.toasts)
  const shouldReuse =
    latestActive !== null &&
    latestActive.message === input.message &&
    latestActive.kind === input.kind
  if (shouldReuse) {
    return {
      action: 'reused',
      toastId: latestActive.id,
      toasts: input.toasts,
      nextToastId: input.nextToastId,
    }
  }
  const toast: ToastItem = {
    id: input.nextToastId,
    message: input.message,
    kind: input.kind,
    createdAt: input.now,
    autoDismissMs: input.autoDismissMs,
    isExiting: false,
  }
  return {
    action: 'added',
    toastId: toast.id,
    toasts: [...input.toasts, toast],
    nextToastId: toast.id + 1,
  }
}
export const beginExitToast = (toasts: ToastItem[], id: ToastId): ToastItem[] => {
  const toast = toasts.find((candidate) => candidate.id === id)
  if (!toast || toast.isExiting) {
    return toasts
  }
  return toasts.map((candidate) =>
    candidate.id === id ? { ...candidate, isExiting: true } : candidate,
  )
}
export const removeToast = (toasts: ToastItem[], id: ToastId): ToastItem[] =>
  toasts.filter((toast) => toast.id !== id)
</file>
<file path="src/utils/model-manager.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
export type ProviderModelLists = {
  openai: string[]
  gemini: string[]
}
export type ModelCache = {
  timestamp: number
  models: ProviderModelLists
}
export type GetAvailableModelsOptions = {
  cacheFilePath?: string
  now?: number
  fetchImpl?: typeof fetch
  openAiBaseUrl?: string
  geminiBaseUrl?: string
  openAiOrganizationId?: string
  openAiProjectId?: string
}
const ONE_DAY_MS = 24 * 60 * 60 * 1000
export const DEFAULT_MODEL_CACHE_FILE = path.join(
  os.homedir(),
  '.config',
  'prompt-maker-cli',
  'models-cache.json',
)
// Hardcoded fallbacks in case of network failure / first run offline.
export const FALLBACK_MODELS: ProviderModelLists = {
  openai: ['gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-3.5-turbo'],
  gemini: ['gemini-1.5-pro-latest', 'gemini-1.5-flash-latest'],
}
type OpenAiModelListResponse = {
  data: Array<{ id: string }>
}
type GeminiModelListResponse = {
  models: Array<{
    name: string
    supportedGenerationMethods?: string[]
    inputTokenLimit?: number
  }>
}
const normalizeModelId = (value: string): string => value.trim()
const normalizeModelList = (models: readonly string[]): string[] => {
  const unique = new Set(models.map(normalizeModelId).filter(Boolean))
  return Array.from(unique).sort((a, b) => a.localeCompare(b))
}
const isRecord = (value: unknown): value is Record<string, unknown> =>
  typeof value === 'object' && value !== null
const isOpenAiModelListResponse = (value: unknown): value is OpenAiModelListResponse => {
  if (!isRecord(value)) {
    return false
  }
  const data = value.data
  if (!Array.isArray(data)) {
    return false
  }
  return data.every((entry) => isRecord(entry) && typeof entry.id === 'string')
}
const isGeminiModelListResponse = (value: unknown): value is GeminiModelListResponse => {
  if (!isRecord(value)) {
    return false
  }
  const models = value.models
  if (!Array.isArray(models)) {
    return false
  }
  return models.every((entry) => {
    if (!isRecord(entry) || typeof entry.name !== 'string') {
      return false
    }
    if (
      entry.supportedGenerationMethods !== undefined &&
      (!Array.isArray(entry.supportedGenerationMethods) ||
        !entry.supportedGenerationMethods.every((method) => typeof method === 'string'))
    ) {
      return false
    }
    if (entry.inputTokenLimit !== undefined && typeof entry.inputTokenLimit !== 'number') {
      return false
    }
    return true
  })
}
const buildOpenAiModelsUrl = (baseUrl?: string): string => {
  const normalized = baseUrl?.trim().replace(/\/$/, '')
  if (!normalized) {
    return 'https://api.openai.com/v1/models'
  }
  if (normalized.endsWith('/v1')) {
    return `${normalized}/models`
  }
  return `${normalized}/v1/models`
}
const normalizeGeminiBaseUrl = (value?: string): string => {
  const normalized = value?.trim().replace(/\/$/, '')
  if (!normalized) {
    return 'https://generativelanguage.googleapis.com'
  }
  const suffixes = ['/v1beta/models', '/v1/models', '/v1beta', '/v1']
  return suffixes.reduce((current, suffix) => {
    return current.endsWith(suffix) ? current.slice(0, -suffix.length) : current
  }, normalized)
}
const buildGeminiModelsUrl = (baseUrl?: string, apiKey?: string): string => {
  const root = normalizeGeminiBaseUrl(baseUrl)
  const key = apiKey?.trim() || ''
  return `${root}/v1beta/models?key=${encodeURIComponent(key)}`
}
const fetchJson = async (
  fetchImpl: typeof fetch,
  url: string,
  init?: RequestInit,
): Promise<unknown> => {
  const response = await fetchImpl(url, init)
  if (!response.ok) {
    throw new Error(`Fetch failed (${response.status}): ${response.statusText}`)
  }
  return (await response.json()) as unknown
}
const fetchOpenAiModels = async (
  apiKey: string | null | undefined,
  options: {
    fetchImpl: typeof fetch
    baseUrl?: string
    organizationId?: string
    projectId?: string
  },
): Promise<string[]> => {
  const trimmedKey = apiKey?.trim()
  if (!trimmedKey) {
    return []
  }
  const url = buildOpenAiModelsUrl(options.baseUrl)
  const headers: Record<string, string> = { Authorization: `Bearer ${trimmedKey}` }
  if (options.organizationId?.trim()) {
    headers['OpenAI-Organization'] = options.organizationId.trim()
  }
  if (options.projectId?.trim()) {
    headers['OpenAI-Project'] = options.projectId.trim()
  }
  const data = await fetchJson(options.fetchImpl, url, {
    headers,
  })
  if (!isOpenAiModelListResponse(data)) {
    throw new Error('OpenAI models response had unexpected shape.')
  }
  return data.data
    .map((entry) => entry.id)
    .filter((id) => {
      const normalized = id.toLowerCase()
      return (
        normalized.startsWith('gpt') ||
        normalized.startsWith('o') ||
        normalized.startsWith('chatgpt')
      )
    })
}
export type GeminiModelDescriptor = {
  id: string
  supportedGenerationMethods: string[]
  inputTokenLimit?: number
}
export const MIN_VIDEO_INPUT_TOKEN_LIMIT = 128_000
const fetchGeminiModels = async (
  apiKey: string | null | undefined,
  options: {
    fetchImpl: typeof fetch
    baseUrl?: string
  },
): Promise<string[]> => {
  const trimmedKey = apiKey?.trim()
  if (!trimmedKey) {
    return []
  }
  const url = buildGeminiModelsUrl(options.baseUrl, trimmedKey)
  const data = await fetchJson(options.fetchImpl, url)
  if (!isGeminiModelListResponse(data)) {
    throw new Error('Gemini models response had unexpected shape.')
  }
  return data.models
    .filter((entry) => entry.supportedGenerationMethods?.includes('generateContent'))
    .map((entry) => entry.name.replace(/^models\//, ''))
}
const fetchGeminiModelDescriptors = async (
  apiKey: string | null | undefined,
  options: {
    fetchImpl: typeof fetch
    baseUrl?: string
  },
): Promise<GeminiModelDescriptor[]> => {
  const trimmedKey = apiKey?.trim()
  if (!trimmedKey) {
    return []
  }
  const url = buildGeminiModelsUrl(options.baseUrl, trimmedKey)
  const data = await fetchJson(options.fetchImpl, url)
  if (!isGeminiModelListResponse(data)) {
    throw new Error('Gemini models response had unexpected shape.')
  }
  return data.models
    .filter((entry) => entry.supportedGenerationMethods?.includes('generateContent'))
    .map((entry) => ({
      id: entry.name.replace(/^models\//, ''),
      supportedGenerationMethods: entry.supportedGenerationMethods ?? [],
      ...(typeof entry.inputTokenLimit === 'number'
        ? { inputTokenLimit: entry.inputTokenLimit }
        : {}),
    }))
}
const buildGeminiGenerateContentUrl = (
  baseUrl: string,
  apiKey: string,
  modelId: string,
): string => {
  const normalized = normalizeGeminiBaseUrl(baseUrl)
  return `${normalized}/v1beta/models/${encodeURIComponent(modelId)}:generateContent?key=${encodeURIComponent(apiKey)}`
}
type GeminiProbeOutcome =
  | { ok: true }
  | {
      ok: false
      reason: 'model-not-found' | 'filedata-unsupported' | 'unknown'
      details: string
    }
export const probeGeminiModelSupportsFileData = async (
  fetchImpl: typeof fetch,
  modelId: string,
  apiKey: string,
  baseUrl?: string,
): Promise<GeminiProbeOutcome> => {
  const trimmedKey = apiKey.trim()
  if (!trimmedKey) {
    return { ok: false, reason: 'unknown', details: 'Missing Gemini API key.' }
  }
  const url = buildGeminiGenerateContentUrl(
    baseUrl ?? 'https://generativelanguage.googleapis.com',
    trimmedKey,
    modelId,
  )
  // Important: we purposely use a bogus fileUri to test schema support without actually uploading.
  // If the model supports fileData, the request should fail for a different reason (e.g. bad file uri),
  // not "Unknown name fileData".
  const body = {
    contents: [
      {
        role: 'user',
        parts: [
          {
            fileData: {
              mimeType: 'video/mp4',
              // Intentionally invalid to avoid ambiguous 404s.
              fileUri: 'INVALID_FILE_URI',
            },
          },
          { text: 'Describe the video.' },
        ],
      },
    ],
    generationConfig: { temperature: 0.2 },
  }
  const response = await fetchImpl(url, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(body),
  })
  if (response.ok) {
    return { ok: true }
  }
  const details = await response.text()
  if (response.status === 404) {
    return { ok: false, reason: 'model-not-found', details }
  }
  if (response.status === 400 && details.includes('Unknown name') && details.includes('fileData')) {
    return { ok: false, reason: 'filedata-unsupported', details }
  }
  // If it fails for any other reason, we assume the schema was accepted.
  return { ok: true }
}
export const getVideoCapableGeminiModels = async (
  apiKey: string | null | undefined,
  options: {
    fetchImpl?: typeof fetch
    baseUrl?: string
  } = {},
): Promise<string[]> => {
  const fetchImpl = options.fetchImpl ?? globalThis.fetch
  if (!fetchImpl) {
    return []
  }
  const trimmedKey = apiKey?.trim()
  if (!trimmedKey) {
    return []
  }
  const descriptors = await fetchGeminiModelDescriptors(trimmedKey, {
    fetchImpl,
    ...(options.baseUrl ? { baseUrl: options.baseUrl } : {}),
  })
  const candidates = descriptors
    .filter((model) => {
      const normalizedId = model.id.toLowerCase()
      const isGemini = normalizedId.includes('gemini')
      const tokenLimit = model.inputTokenLimit ?? 0
      return isGemini && tokenLimit >= MIN_VIDEO_INPUT_TOKEN_LIMIT
    })
    .sort((a, b) => {
      const aLimit = a.inputTokenLimit ?? 0
      const bLimit = b.inputTokenLimit ?? 0
      if (bLimit !== aLimit) {
        return bLimit - aLimit
      }
      return a.id.localeCompare(b.id)
    })
  const supported: string[] = []
  for (const model of candidates) {
    const probe = await probeGeminiModelSupportsFileData(
      fetchImpl,
      model.id,
      trimmedKey,
      options.baseUrl,
    )
    if (probe.ok) {
      supported.push(model.id)
    }
  }
  return supported
}
const readCacheFile = async (cacheFilePath: string): Promise<ModelCache | null> => {
  try {
    const raw = await fs.readFile(cacheFilePath, 'utf8')
    const parsed = JSON.parse(raw) as unknown
    if (!isRecord(parsed)) {
      return null
    }
    const timestamp = parsed.timestamp
    const models = parsed.models
    if (typeof timestamp !== 'number' || !isRecord(models)) {
      return null
    }
    const openai = models.openai
    const gemini = models.gemini
    if (!Array.isArray(openai) || !Array.isArray(gemini)) {
      return null
    }
    if (
      !openai.every((id) => typeof id === 'string') ||
      !gemini.every((id) => typeof id === 'string')
    ) {
      return null
    }
    return {
      timestamp,
      models: {
        openai: normalizeModelList(openai),
        gemini: normalizeModelList(gemini),
      },
    }
  } catch {
    return null
  }
}
const writeCacheFile = async (
  cacheFilePath: string,
  models: ProviderModelLists,
  now: number,
): Promise<void> => {
  await fs.mkdir(path.dirname(cacheFilePath), { recursive: true })
  const payload: ModelCache = { timestamp: now, models }
  await fs.writeFile(cacheFilePath, JSON.stringify(payload, null, 2), 'utf8')
}
export const getAvailableModels = async (
  openAiKey?: string | null,
  geminiKey?: string | null,
  options: GetAvailableModelsOptions = {},
): Promise<ProviderModelLists> => {
  const cacheFilePath = options.cacheFilePath ?? DEFAULT_MODEL_CACHE_FILE
  const now = options.now ?? Date.now()
  const cached = await readCacheFile(cacheFilePath)
  if (cached) {
    const age = now - cached.timestamp
    if (age >= 0 && age < ONE_DAY_MS) {
      return cached.models
    }
    const canRefresh = Boolean(openAiKey?.trim() || geminiKey?.trim())
    if (!canRefresh) {
      return cached.models
    }
  } else {
    const canRefresh = Boolean(openAiKey?.trim() || geminiKey?.trim())
    if (!canRefresh) {
      return FALLBACK_MODELS
    }
  }
  const fetchImpl = options.fetchImpl ?? globalThis.fetch
  if (!fetchImpl) {
    return cached?.models ?? FALLBACK_MODELS
  }
  let openai: string[] = []
  let gemini: string[] = []
  try {
    const results = await Promise.allSettled([
      fetchOpenAiModels(openAiKey, {
        fetchImpl,
        ...(options.openAiBaseUrl ? { baseUrl: options.openAiBaseUrl } : {}),
        ...(options.openAiOrganizationId ? { organizationId: options.openAiOrganizationId } : {}),
        ...(options.openAiProjectId ? { projectId: options.openAiProjectId } : {}),
      }),
      fetchGeminiModels(geminiKey, {
        fetchImpl,
        ...(options.geminiBaseUrl ? { baseUrl: options.geminiBaseUrl } : {}),
      }),
    ])
    const openAiResult = results[0]
    if (openAiResult.status === 'fulfilled') {
      openai = openAiResult.value
    } else {
      console.error('Failed to fetch OpenAI models', openAiResult.reason)
    }
    const geminiResult = results[1]
    if (geminiResult.status === 'fulfilled') {
      gemini = geminiResult.value
    } else {
      console.error('Failed to fetch Gemini models', geminiResult.reason)
    }
  } catch (error) {
    console.error('Failed to refresh model cache', error)
  }
  const result: ProviderModelLists = {
    openai: normalizeModelList(openai.length > 0 ? openai : FALLBACK_MODELS.openai),
    gemini: normalizeModelList(gemini.length > 0 ? gemini : FALLBACK_MODELS.gemini),
  }
  try {
    await writeCacheFile(cacheFilePath, result, now)
  } catch (error) {
    console.error('Failed to write model cache', error)
  }
  return result
}
</file>
<file path="src/compose-command.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import yargs from 'yargs'
import type { ArgumentsCamelCase } from 'yargs'
type ComposeArgs = {
  recipe: string
  input: string
  help: boolean
}
export const runComposeCommand = async (argv: string[]): Promise<void> => {
  try {
    const { args, showHelp } = parseComposeArgs(argv)
    if (args.help) {
      showHelp()
      return
    }
    const recipePath = path.resolve(process.cwd(), args.recipe)
    const recipeText = await readRecipeFile(recipePath)
    const composed = composeDeterministicPrompt(recipeText, args.input)
    process.stdout.write(composed)
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown compose error.'
    console.error(message)
    process.exitCode = 1
  }
}
const HELP_FLAGS = new Set(['--help', '-h'])
const COMPOSE_HELP_TEXT = `Usage:\n  prompt-maker-cli compose --recipe <path> --input <text>\n\nOptions:\n  --recipe  Path to a recipe file (text for now)\n  --input   Input text to compose into the recipe\n  --help,-h Show help\n`
const parseComposeArgs = (argv: string[]): { args: ComposeArgs; showHelp: () => void } => {
  const { optionArgs, helpRequested } = stripHelpFlags(argv)
  if (helpRequested) {
    return {
      args: {
        recipe: '',
        input: '',
        help: true,
      },
      showHelp: () => {
        process.stdout.write(COMPOSE_HELP_TEXT)
      },
    }
  }
  const parser = yargs(optionArgs)
    .scriptName('prompt-maker-cli compose')
    .usage('Usage:\n  prompt-maker-cli compose --recipe <path> --input <text>')
    .option('recipe', {
      type: 'string',
      describe: 'Path to a recipe file (text for now)',
    })
    .option('input', {
      type: 'string',
      describe: 'Input text to compose into the recipe',
    })
    .check((argv) => {
      const recipe = argv.recipe
      if (typeof recipe !== 'string' || recipe.trim().length === 0) {
        throw new Error('--recipe is required.')
      }
      const input = argv.input
      if (typeof input !== 'string' || input.trim().length === 0) {
        throw new Error('--input is required.')
      }
      return true
    })
    .help('help')
    .alias('help', 'h')
    .exitProcess(false)
    .showHelpOnFail(false)
    .parserConfiguration({ 'halt-at-non-option': true })
    .strict(false)
    .fail((msg, err) => {
      throw err ?? new Error(msg ?? 'Invalid CLI arguments.')
    })
  const parsed = parser.parseSync() as ArgumentsCamelCase<{
    recipe?: string
    input?: string
    help?: boolean
  }>
  const recipe = parsed.recipe?.trim() ?? ''
  const input = parsed.input ?? ''
  return {
    args: {
      recipe,
      input,
      help: Boolean(parsed.help),
    },
    showHelp: () => {
      process.stdout.write(COMPOSE_HELP_TEXT)
    },
  }
}
const readRecipeFile = async (filePath: string): Promise<string> => {
  try {
    return await fs.readFile(filePath, 'utf8')
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown file error.'
    throw new Error(`Failed to read recipe file ${formatDisplayPath(filePath)}: ${message}`)
  }
}
export const composeDeterministicPrompt = (recipeText: string, input: string): string => {
  const normalizedRecipe = normalizeNewlines(recipeText).trimEnd()
  const normalizedInput = normalizeNewlines(input)
  return `${normalizedRecipe}\n---\n${normalizedInput}\n`
}
const normalizeNewlines = (value: string): string => value.replace(/\r\n/g, '\n')
const formatDisplayPath = (absolutePath: string): string => {
  const relative = path.relative(process.cwd(), absolutePath)
  return relative && !relative.startsWith('..') ? relative : absolutePath
}
const stripHelpFlags = (tokens: string[]): { optionArgs: string[]; helpRequested: boolean } => {
  const optionArgs: string[] = []
  let helpRequested = false
  let passthrough = false
  tokens.forEach((token) => {
    if (passthrough) {
      optionArgs.push(token)
      return
    }
    if (token === '--') {
      optionArgs.push(token)
      passthrough = true
      return
    }
    if (HELP_FLAGS.has(token)) {
      helpRequested = true
      return
    }
    optionArgs.push(token)
  })
  return { optionArgs, helpRequested }
}
</file>
<file path="src/history-logger.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
export const resolveHistoryFilePath = (): string => {
  const envHome = process.env.HOME?.trim()
  const homeDir = envHome && envHome.length > 0 ? envHome : os.homedir()
  return path.join(homeDir, '.config', 'prompt-maker-cli', 'history.jsonl')
}
export const appendToHistory = async (payload: object): Promise<void> => {
  const historyFile = resolveHistoryFilePath()
  try {
    await fs.mkdir(path.dirname(historyFile), { recursive: true })
    const line = `${JSON.stringify(payload)}\n`
    await fs.appendFile(historyFile, line, 'utf8')
  } catch (error) {
    console.warn('Failed to write history entry:', error)
  }
}
</file>
<file path="tests/mocks/yargs.ts">
import yargsParser from 'yargs-parser'
type PositionalOptions = {
  default?: unknown
}
type Builder = {
  positional: (name: string, opts: PositionalOptions) => Builder
}
type YargsApi = {
  scriptName: (...args: unknown[]) => YargsApi
  usage: (...args: unknown[]) => YargsApi
  option: (...args: unknown[]) => YargsApi
  alias: (...args: unknown[]) => YargsApi
  help: (...args: unknown[]) => YargsApi
  exitProcess: (...args: unknown[]) => YargsApi
  showHelpOnFail: (...args: unknown[]) => YargsApi
  parserConfiguration: (...args: unknown[]) => YargsApi
  strict: (...args: unknown[]) => YargsApi
  command: (_pattern: string, _desc: string, builder?: (cmd: Builder) => Builder) => YargsApi
  check: (handler: (argv: Record<string, unknown>) => boolean | void) => YargsApi
  fail: (handler: (msg?: string, err?: Error) => void) => YargsApi
  showHelp: () => void
  parseSync: () => Record<string, unknown>
}
const BOOLEAN_OPTIONS = [
  'interactive',
  'copy',
  'open-chatgpt',
  'polish',
  'json',
  'progress',
  'smart-context',
  'help',
]
const ARRAY_OPTIONS = ['context', 'image', 'video']
const NUMBER_OPTIONS = ['max-input-tokens', 'max-context-tokens']
const DEFAULTS: Record<string, unknown> = {
  context: [],
  image: [],
  video: [],
  progress: true,
}
const ALIASES: Record<string, string> = {
  c: 'context',
  f: 'intent-file',
  i: 'interactive',
  h: 'help',
}
const createYargs = (argv: string[]): YargsApi => {
  const positionalDefaults: Record<string, unknown> = {}
  const checkHandlers: Array<(argv: Record<string, unknown>) => boolean | void> = []
  let failHandler: ((msg?: string, err?: Error) => void) | undefined
  const api: YargsApi = {
    scriptName: () => api,
    usage: () => api,
    option: () => api,
    alias: () => api,
    help: () => api,
    exitProcess: () => api,
    showHelpOnFail: () => api,
    parserConfiguration: () => api,
    strict: () => api,
    check(handler: (argv: Record<string, unknown>) => boolean | void) {
      checkHandlers.push(handler)
      return api
    },
    command(_pattern: string, _desc: string, builder?: (cmd: Builder) => Builder) {
      if (builder) {
        builder({
          positional(name, opts) {
            if (opts && Object.prototype.hasOwnProperty.call(opts, 'default')) {
              positionalDefaults[name] = opts.default
            }
            return this
          },
        })
      }
      return api
    },
    fail(handler: (msg?: string, err?: Error) => void) {
      failHandler = handler
      return api
    },
    showHelp: () => undefined,
    parseSync() {
      try {
        const parsed = yargsParser(argv, {
          alias: ALIASES,
          array: ARRAY_OPTIONS,
          boolean: BOOLEAN_OPTIONS,
          number: NUMBER_OPTIONS,
          configuration: {
            'halt-at-non-option': true,
            'camel-case-expansion': true,
          },
          default: DEFAULTS,
        }) as Record<string, unknown>
        for (const [name, value] of Object.entries(positionalDefaults)) {
          if (parsed[name] === undefined || parsed[name] === '') {
            parsed[name] = value
          }
        }
        checkHandlers.forEach((handler) => {
          const result = handler(parsed)
          if (result === false) {
            throw new Error('Invalid CLI arguments.')
          }
        })
        return parsed
      } catch (error) {
        const err = error instanceof Error ? error : new Error(String(error))
        failHandler?.(err.message, err)
        throw err
      }
    },
  }
  return api
}
const yargs = (argv: string[]) => createYargs(argv)
export type ArgumentsCamelCase<T> = T
export default yargs
</file>
<file path="session-ses_4602.md">
# Integrating PDF attachments into Prompt attachments pipeline
**Session ID:** ses_4602ec1f1ffeUNtnac7LLUak22
**Created:** 1/8/2026, 4:54:08 PM
**Updated:** 1/9/2026, 7:18:39 AM
---
## Assistant
_Thinking:_
```
Tool: grep
**Input:**
```json
{
  "pattern": "useCommandScreenContextPopupBindings\\({",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Error:**
```
Error: ripgrep failed: rg: regex parse error:
    (?:useCommandScreenContextPopupBindings\({)
                                              ^
error: repetition quantifier expects a valid decimal
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "oldString": "    images: options.context.images,\n    videos: options.context.videos,\n",
  "newString": "    images: options.context.images,\n    videos: options.context.videos,\n    pdfs: options.context.pdfs,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [251:27] Property 'pdfs' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [263:29] Property 'addPdf' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [264:32] Property 'removePdf' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [368:30] Property 'pdfs' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
</file_diagnostics>
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "oldString": "    addVideo: options.context.addVideo,\n    removeVideo: options.context.removeVideo,\n",
  "newString": "    addVideo: options.context.addVideo,\n    removeVideo: options.context.removeVideo,\n    addPdf: options.context.addPdf,\n    removePdf: options.context.removePdf,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [251:27] Property 'pdfs' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [263:29] Property 'addPdf' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [264:32] Property 'removePdf' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [368:30] Property 'pdfs' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
</file_diagnostics>
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "oldString": "    video: {\n      items: options.context.videos,\n      suggestions: context.videoPopupSuggestions,\n      onAdd: context.onAddVideo,\n      onRemove: context.onRemoveVideo,\n    },\n",
  "newString": "    video: {\n      items: options.context.videos,\n      suggestions: context.videoPopupSuggestions,\n      onAdd: context.onAddVideo,\n      onRemove: context.onRemoveVideo,\n    },\n\n    pdf: {\n      items: options.context.pdfs,\n      suggestions: context.pdfPopupSuggestions,\n      onAdd: context.onAddPdf,\n      onRemove: context.onRemovePdf,\n    },\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [251:27] Property 'pdfs' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [263:29] Property 'addPdf' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [264:32] Property 'removePdf' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [368:30] Property 'pdfs' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupAndView.types.ts",
  "offset": 1,
  "limit": 90
}
```
**Output:**
```
<file>
00002| 
00003| import type { MutableRefObject } from 'react'
00004| 
00005| import type { NotifyOptions } from '../../../notifier'
00006| import type { HistoryEntry, ModelOption, ProviderStatusMap } from '../../../types'
00007| 
00008| export type PushHistory = (
00009|   content: string,
00010|   kind?: HistoryEntry['kind'],
00011|   format?: HistoryEntry['format'],
00012| ) => void
00013| 
00014| export type CommandContextOptions = {
00015|   interactiveTransportPath?: string | undefined
00016| 
00017|   notify: (message: string, options?: NotifyOptions) => void
00018|   stdout: WriteStream | undefined
00019| 
00020|   // context state
00021|   files: string[]
00022|   urls: string[]
00023|   images: string[]
00024|   videos: string[]
00025|   smartContextEnabled: boolean
00026|   smartContextRoot: string | null
00027|   metaInstructions: string
00028|   maxContextTokens: number | null
00029|   maxInputTokens: number | null
00030|   contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
00031|   lastReasoning: string | null
00032|   lastGeneratedPrompt: string | null
00033| 
00034|   // context dispatch
00035|   addFile: (value: string) => void
00036|   removeFile: (index: number) => void
00037|   addUrl: (value: string) => void
00038|   removeUrl: (index: number) => void
00039|   updateUrl: (index: number, value: string) => void
00040|   addImage: (value: string) => void
00041|   removeImage: (index: number) => void
00042|   addVideo: (value: string) => void
00043|   removeVideo: (index: number) => void
00044|   toggleSmartContext: () => void
00045|   setSmartRoot: (value: string) => void
00046|   setMetaInstructions: (value: string) => void
00047|   setBudgets: (value: {
00048|     maxContextTokens: number | null
00049|     maxInputTokens: number | null
00050|     contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
00051|   }) => void
00052|   resetContext: () => void
00053| }
00054| 
00055| export type CommandInputOptions = {
00056|   // screen state
00057|   terminalRows: number
00058|   terminalColumns: number
00059|   inputValue: string
00060|   isPasteActive: boolean
00061|   commandSelectionIndex: number
00062|   debugKeyLine: string | null
00063|   debugKeysEnabled: boolean
00064| 
00065|   setTerminalSize: (rows: number, columns: number) => void
00066|   setInputValue: (value: string | ((prev: string) => string)) => void
00067|   setPasteActive: (active: boolean) => void
00068|   setCommandSelectionIndex: (next: number | ((prev: number) => number)) => void
00069| 
00070|   // input local
00071|   intentFilePath: string
00072|   setIntentFilePath: (value: string) => void
00073|   copyEnabled: boolean
00074|   setCopyEnabled: (value: boolean) => void
00075|   chatGptEnabled: boolean
00076|   setChatGptEnabled: (value: boolean) => void
00077|   jsonOutputEnabled: boolean
00078|   setJsonOutputEnabled: (value: boolean) => void
00079| 
00080|   // refs
00081|   lastUserIntentRef: MutableRefObject<string | null>
00082|   lastTypedIntentRef: MutableRefObject<string>
00083| 
00084|   // suppression
00085|   consumeSuppressedTextInputChange: () => boolean
00086|   suppressNextInput: () => void
00087|   updateLastTypedIntent: (next: string) => void
00088| 
00089|   onDebugKeyEvent: (
00090|     event: import('../../../components/core/MultilineTextInput').DebugKeyEvent,
00091|   ) => void
(File has more lines. Use 'offset' parameter to read beyond line 91)
</file>
```
```
---
## Assistant
_Thinking:_
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupAndView.types.ts",
  "oldString": "  images: string[]\n  videos: string[]\n",
  "newString": "  images: string[]\n  videos: string[]\n  pdfs: string[]\n"
}
```
```
---
## Assistant
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupAndView.types.ts",
  "oldString": "  addVideo: (value: string) => void\n  removeVideo: (index: number) => void\n",
  "newString": "  addVideo: (value: string) => void\n  removeVideo: (index: number) => void\n  addPdf: (value: string) => void\n  removePdf: (index: number) => void\n"
}
```
```
---
## Assistant
_Thinking:_
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "    images: context.images,\n    videos: context.videos,\n    addImage: context.addImage,\n    addVideo: context.addVideo,\n",
  "newString": "    images: context.images,\n    videos: context.videos,\n    pdfs: context.pdfs,\n    addImage: context.addImage,\n    addVideo: context.addVideo,\n    addPdf: context.addPdf,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [42:5] Object literal may only specify known properties, and 'pdfs' does not exist in type 'UseCommandScreenPopupManagerOptions'.
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
**Updating popup manager options**
I need to update the `UseCommandScreenPopupManagerOptions` type, which involves wrapping the `usePopupManager` to include PDFs and the `addPdf` function. First, I'll locate the `useCommandScreenPopupManager.ts` file and then focus on the options type. This task seems straightforward, but I want to ensure I capture all required details for a seamless integration. Let's dig into the code and see how this can be implemented effectively!
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupManager.ts",
  "offset": 1,
  "limit": 120
}
```
**Output:**
```
<file>
00002| import { useCallback, useEffect, useState } from 'react'
00003| 
00004| import { loadCliConfig } from '../../../../config'
00005| 
00006| import { usePopupManager } from '../../../hooks/usePopupManager'
00007| import type { NotifyOptions } from '../../../notifier'
00008| import { useTheme } from '../../../theme/theme-provider'
00009| import type {
00010|   HistoryEntry,
00011|   ModelOption,
00012|   PopupState,
00013|   ResumeMode,
00014|   ResumeSourceKind,
00015| } from '../../../types'
00016| 
00017| const DEFAULT_TEST_FILE = 'prompt-tests.yaml'
00018| 
00019| type PushHistory = (
00020|   content: string,
00021|   kind?: HistoryEntry['kind'],
00022|   format?: HistoryEntry['format'],
00023| ) => void
00024| 
00025| type UseCommandScreenPopupManagerOptions = {
00026|   currentModel: ModelOption['id']
00027|   currentTargetModel: ModelOption['id']
00028|   modelOptions: readonly ModelOption[]
00029|   smartContextEnabled: boolean
00030|   smartContextRoot: string | null
00031|   toggleSmartContext: () => void
00032|   setSmartRoot: (value: string) => void
00033|   urls: string[]
00034|   addUrl: (value: string) => void
00035|   images: string[]
00036|   videos: string[]
00037|   addImage: (value: string) => void
00038|   addVideo: (value: string) => void
00039|   lastTestFile: string | null
00040|   interactiveTransportPath?: string | undefined
00041|   isGenerating: boolean
00042|   lastUserIntentRef: import('react').MutableRefObject<string | null>
00043|   lastTypedIntentRef: import('react').MutableRefObject<string>
00044| 
00045|   pushHistoryProxy: PushHistory
00046|   notify: (message: string, options?: NotifyOptions) => void
00047|   setInputValue: (value: string | ((prev: string) => string)) => void
00048| 
00049|   runGeneration: (payload: {
00050|     intent?: string
00051|     intentFile?: string
00052|     resume?:
00053|       | { kind: 'history'; selector: string; mode: ResumeMode }
00054|       | { kind: 'file'; payloadPath: string; mode: ResumeMode }
00055|   }) => Promise<void>
00056|   runSeriesGeneration: (intent: string) => void
00057|   runTestsFromCommandProxy: (value: string) => void
00058| 
00059|   setCurrentModel: (value: ModelOption['id']) => void
00060|   setCurrentTargetModel: (value: ModelOption['id']) => void
00061|   setPolishModelId: (value: ModelOption['id'] | null) => void
00062|   setCopyEnabled: (value: boolean) => void
00063|   setChatGptEnabled: (value: boolean) => void
00064|   setJsonOutputEnabled: (value: boolean) => void
00065| 
00066|   intentFilePath: string
00067|   setIntentFilePath: (value: string) => void
00068| 
00069|   metaInstructions: string
00070|   setMetaInstructions: (value: string) => void
00071|   budgets: {
00072|     maxContextTokens: number | null
00073|     maxInputTokens: number | null
00074|     contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
00075|   }
00076|   setBudgets: (value: {
00077|     maxContextTokens: number | null
00078|     maxInputTokens: number | null
00079|     contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
00080|   }) => void
00081| 
00082|   polishModelId: ModelOption['id'] | null
00083|   copyEnabled: boolean
00084|   chatGptEnabled: boolean
00085|   jsonOutputEnabled: boolean
00086| }
00087| 
00088| export type UseCommandScreenPopupManagerResult = {
00089|   popupState: PopupState
00090|   setPopupState: import('react').Dispatch<import('react').SetStateAction<PopupState>>
00091|   actions: ReturnType<typeof usePopupManager>['actions']
00092|   isPopupOpen: boolean
00093| }
00094| 
00095| export const useCommandScreenPopupManager = ({
00096|   currentModel,
00097|   currentTargetModel,
00098|   modelOptions,
00099|   smartContextEnabled,
00100|   smartContextRoot,
00101|   toggleSmartContext,
00102|   setSmartRoot,
00103|   urls,
00104|   addUrl,
00105|   images,
00106|   videos,
00107|   addImage,
00108|   addVideo,
00109|   lastTestFile,
00110|   interactiveTransportPath,
00111|   isGenerating,
00112|   lastUserIntentRef,
00113|   lastTypedIntentRef,
00114|   pushHistoryProxy,
00115|   notify,
00116|   setInputValue,
00117|   runGeneration,
00118|   runSeriesGeneration,
00119|   runTestsFromCommandProxy,
00120|   setCurrentModel,
00121|   setCurrentTargetModel,
(File has more lines. Use 'offset' parameter to read beyond line 121)
</file>
```
```
---
## Assistant
_Thinking:_
I'm considering how to extend the options with PDFs and the addPdf function. I need to ensure that I integrate it properly with the use of PopupManager. It seems like I have to make some edits accordingly to get everything working. Alright, let's take it step by step to ensure I get it right! I want to make sure the integration is smooth and everything functions as intended.
```
Tool: grep
**Input:**
```json
{
  "pattern": "videos: string\\[\\]|addVideo",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupManager.ts"
}
```
**Output:**
```
Found 4 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupManager.ts:
  Line 36:   videos: string[]
  Line 39:   addVideo: (value: string) => void
  Line 110:   addVideo,
  Line 219:     addVideo,
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupManager.ts",
  "oldString": "  images: string[]\n  videos: string[]\n  addImage: (value: string) => void\n  addVideo: (value: string) => void\n",
  "newString": "  images: string[]\n  videos: string[]\n  pdfs: string[]\n  addImage: (value: string) => void\n  addVideo: (value: string) => void\n  addPdf: (value: string) => void\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupManager.ts",
  "oldString": "  images,\n  videos,\n  addImage,\n  addVideo,\n",
  "newString": "  images,\n  videos,\n  pdfs,\n  addImage,\n  addVideo,\n  addPdf,\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupManager.ts",
  "oldString": "    images,\n    videos,\n    addImage,\n    addVideo,\n",
  "newString": "    images,\n    videos,\n    pdfs,\n    addImage,\n    addVideo,\n    addPdf,\n"
}
```
```
---
## Assistant
_Thinking:_
I need to revisit the code to fix the earlier issue with the useCommandScreenBindings, which was failing due to the type. With the updated options type, it should compile successfully now. I also need to make sure to include PDFs and the addPdf function in the options that I'm passing to useCommandScreenPopupManager. Alright, time to reapply these updates to useCommandScreenBindings to get everything running smoothly!
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "    images: context.images,\n    videos: context.videos,\n    addImage: context.addImage,\n    addVideo: context.addVideo,\n",
  "newString": "    images: context.images,\n    videos: context.videos,\n    pdfs: context.pdfs,\n    addImage: context.addImage,\n    addVideo: context.addVideo,\n    addPdf: context.addPdf,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [47:5] An object literal cannot have multiple properties with the same name.
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "offset": 24,
  "limit": 35
}
```
**Output:**
```
<file>
00025| ): UseCommandScreenBindingsResult => {
00026|   const { context, input, popup, history, generation } = options
00027| 
00028|   const popupManager = useCommandScreenPopupManager({
00029|     currentModel: generation.currentModel,
00030|     polishModelId: generation.polishModelId,
00031|     currentTargetModel: generation.currentTargetModel,
00032| 
00033|     modelOptions: generation.modelOptions,
00034|     smartContextEnabled: context.smartContextEnabled,
00035|     smartContextRoot: context.smartContextRoot,
00036|     toggleSmartContext: context.toggleSmartContext,
00037|     setSmartRoot: context.setSmartRoot,
00038|     urls: context.urls,
00039|     addUrl: context.addUrl,
00040|     images: context.images,
00041|     videos: context.videos,
00042|     pdfs: context.pdfs,
00043|     addImage: context.addImage,
00044|     addVideo: context.addVideo,
00045|     addPdf: context.addPdf,
00046| 
00047|     addPdf: context.addPdf,
00048|     lastTestFile: history.lastTestFile,
00049|     ...(context.interactiveTransportPath
00050|       ? { interactiveTransportPath: context.interactiveTransportPath }
00051|       : {}),
00052|     isGenerating: generation.isGenerating,
00053|     lastUserIntentRef: input.lastUserIntentRef,
00054|     lastTypedIntentRef: input.lastTypedIntentRef,
00055|     pushHistoryProxy: history.pushHistoryProxy,
00056|     notify: context.notify,
00057|     setInputValue: input.setInputValue,
00058|     runGeneration: generation.runGeneration,
00059|     runSeriesGeneration: generation.runSeriesGeneration,
(File has more lines. Use 'offset' parameter to read beyond line 59)
</file>
```
```
---
## Assistant
_Thinking:_
**Reviewing code for duplicates**
I noticed there's a duplicate line involving addPdf. I need to remove this duplication to tidy up the code. It makes sense to go in and edit it, ensuring everything runs smoothly without unnecessary repetition. Cleaning this up will help maintain clarity in the code, making it easier to read and manage. Letâs get that sorted!
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "    addImage: context.addImage,\n    addVideo: context.addVideo,\n    addPdf: context.addPdf,\n\n    addPdf: context.addPdf,\n    lastTestFile: history.lastTestFile,\n",
  "newString": "    addImage: context.addImage,\n    addVideo: context.addVideo,\n    addPdf: context.addPdf,\n    lastTestFile: history.lastTestFile,\n"
}
```
```
---
## Assistant
_Thinking:_
**Updating screen bindings**
I need to update the rest of the `useCommandScreenBindings` function. It's essential to include the context for the popup bindings, specifically for handling PDFs with `addPdf` and `removePdf` features. I should focus around the context section, near line 192. It seems like an important patch, and I want to ensure everything integrates smoothly. I should probably double-check the related areas as well to avoid any issues. Let's patch this up!
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "      context: {\n        droppedFilePath,\n        files: context.files,\n        urls: context.urls,\n        images: context.images,\n        videos: context.videos,\n        smartContextEnabled: context.smartContextEnabled,\n",
  "newString": "      context: {\n        droppedFilePath,\n        files: context.files,\n        urls: context.urls,\n        images: context.images,\n        videos: context.videos,\n        pdfs: context.pdfs,\n        smartContextEnabled: context.smartContextEnabled,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [200:7] Object literal may only specify known properties, and 'pdfs' does not exist in type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
**Locating update types**
I need to update the types for options.context in the useCommandScreenPopupBindingsOptions. It seems like this is coming from the useCommandScreenPopupBindings.types file, but I'm thinking that useCommandScreenPopupBindings.ts also uses options defined in UseCommandScreenPopupBindingsOptions. I should probably take a look at the top of the file to confirm this. It feels a bit complicated, but Iâm curious to see how it's structured!
```
Tool: grep
**Input:**
```json
{
  "pattern": "export type UseCommandScreenPopupBindingsOptions|context: \\{",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Output:**
```
Found 4 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts:
  Line 32: export type UseCommandScreenPopupBindingsOptions = {
  Line 100:   context: {
  Line 160:     context: {
  Line 465:       context: {
```
```
---
## Assistant
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "offset": 20,
  "limit": 130
}
```
**Output:**
```
<file>
00021| import {
00022|   useCommandScreenHistoryIntentPopupBindings,
00023|   type UseCommandScreenHistoryIntentPopupBindingsOptions,
00024| } from './useCommandScreenHistoryIntentPopupBindings'
00025| import {
00026|   useCommandScreenSubmitBindings,
00027|   type UseCommandScreenSubmitBindingsOptions,
00028| } from './useCommandScreenSubmitBindings'
00029| 
00030| type SetPopupState = import('react').Dispatch<import('react').SetStateAction<PopupState>>
00031| 
00032| export type UseCommandScreenPopupBindingsOptions = {
00033|   input: {
00034|     value: string
00035|     setValue: (value: string | ((prev: string) => string)) => void
00036|     setPasteActive: (active: boolean) => void
00037| 
00038|     consumeSuppressedTextInputChange: () => boolean
00039|     suppressNextInput: () => void
00040|     updateLastTypedIntent: (next: string) => void
00041| 
00042|     intentFilePath: string
00043|     lastUserIntentRef: import('react').MutableRefObject<string | null>
00044|   }
00045| 
00046|   popup: {
00047|     state: PopupState
00048|     setState: SetPopupState
00049|     isOpen: boolean
00050|     helpOpen: boolean
00051|     close: () => void
00052| 
00053|     actions: {
00054|       handleCommandSelection: (commandId: CommandDescriptor['id'], argsRaw?: string) => void
00055|       handleModelPopupSubmit: (option: ModelOption | null | undefined) => void
00056|       applyToggleSelection: (field: 'copy' | 'chatgpt' | 'json', value: boolean) => void
00057|       handleIntentFileSubmit: (value: string) => void
00058|       handleResumeSubmit: () => void
00059|       handleExportSubmit: () => void
00060|       handleSeriesIntentSubmit: (value: string) => void
00061|       handleBudgetsSubmit: () => void
00062|     }
00063|   }
00064| 
00065|   menu: {
00066|     isActive: boolean
00067|     selectedCommandId: CommandDescriptor['id'] | null
00068|     argsRaw: string
00069|     isCommandMode: boolean
00070| 
00071|     actions: {
00072|       handleNewCommand: (argsRaw: string) => void
00073|       handleReuseCommand: () => void
00074|     }
00075|   }
00076| 
00077|   generation: {
00078|     isGenerating: boolean
00079|     isAwaitingRefinement: boolean
00080|     submitRefinement: (value: string) => void
00081|     runGeneration: (payload: {
00082|       intent?: string
00083|       intentFile?: string
00084|       resume?:
00085|         | { kind: 'history'; selector: string; mode: import('../../../types').ResumeMode }
00086|         | { kind: 'file'; payloadPath: string; mode: import('../../../types').ResumeMode }
00087|     }) => Promise<void>
00088|   }
00089| 
00090|   history: {
00091|     pushHistory: (
00092|       content: string,
00093|       kind?: HistoryEntry['kind'],
00094|       format?: HistoryEntry['format'],
00095|     ) => void
00096|     addCommandHistoryEntry: (value: string) => void
00097|     commandHistoryValues: string[]
00098|   }
00099| 
00100|   context: {
00101|     droppedFilePath: string | null
00102| 
00103|     files: string[]
00104|     urls: string[]
00105|     images: string[]
00106|     videos: string[]
00107| 
00108|     smartContextEnabled: boolean
00109|     smartContextRoot: string | null
00110| 
00111|     addFile: (value: string) => void
00112|     removeFile: (index: number) => void
00113|     addUrl: (value: string) => void
00114|     removeUrl: (index: number) => void
00115|     updateUrl: (index: number, value: string) => void
00116|     addImage: (value: string) => void
00117|     removeImage: (index: number) => void
00118|     addVideo: (value: string) => void
00119|     removeVideo: (index: number) => void
00120|     toggleSmartContext: () => void
00121|     setSmartRoot: (value: string) => void
00122| 
00123|     notify: (message: string) => void
00124| 
00125|     modelOptions: ModelOption[]
00126| 
00127|     lastReasoning: string | null
00128|     terminalColumns: number
00129|     reasoningPopupHeight: number
00130|   }
00131| }
00132| 
00133| export type UseCommandScreenPopupBindingsResult = {
00134|   input: {
00135|     tokenLabel: (token: string) => string | null
00136|     onChange: (next: string) => void
00137|   }
00138|   submit: {
00139|     onSubmit: (value: string) => void
00140|     onSeriesSubmit: (value: string) => void
00141|   }
00142|   popup: {
00143|     model: {
00144|       options: ModelOption[]
00145|       recentCount: number
00146|       selection: number
00147|       onQueryChange: (next: string) => void
00148|     }
00149|     history: {
00150|       items: string[]
(File has more lines. Use 'offset' parameter to read beyond line 150)
</file>
```
```
---
## Assistant
_Thinking:_
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "oldString": "    images: string[]\n    videos: string[]\n",
  "newString": "    images: string[]\n    videos: string[]\n    pdfs: string[]\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "oldString": "    addVideo: (value: string) => void\n    removeVideo: (index: number) => void\n",
  "newString": "    addVideo: (value: string) => void\n    removeVideo: (index: number) => void\n    addPdf: (value: string) => void\n    removePdf: (index: number) => void\n"
}
```
```
---
## Assistant
_Thinking:_
I need to revisit the useCommandScreenBindings to include PDFs in the context that I'm passing. It seems like thatâs the next step and should help with compiling everything properly. I'll make sure to apply the earlier edit now to ensure everything flows together smoothly. Itâs a little bit of a process, but I'm focused on getting this done right! Letâs see how this goes.
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "        images: context.images,\n        videos: context.videos,\n        smartContextEnabled: context.smartContextEnabled,\n",
  "newString": "        images: context.images,\n        videos: context.videos,\n        pdfs: context.pdfs,\n        smartContextEnabled: context.smartContextEnabled,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [67:3] Argument expression expected.
ERROR [67:4] ',' expected.
ERROR [75:1] Declaration or statement expected.
ERROR [25:4] A function whose declared type is neither 'undefined', 'void', nor 'any' must return a value.
ERROR [47:7] Object literal may only specify known properties, and 'addFile' does not exist in type 'UseCommandScreenPopupManagerOptions'.
ERROR [61:7] No value exists in scope for the shorthand property 'notify'. Either declare one or provide an initializer.
</file_diagnostics>
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "        addVideo: context.addVideo,\n        removeVideo: context.removeVideo,\n",
  "newString": "        addVideo: context.addVideo,\n        removeVideo: context.removeVideo,\n        addPdf: context.addPdf,\n        removePdf: context.removePdf,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [67:3] Argument expression expected.
ERROR [67:4] ',' expected.
ERROR [75:1] Declaration or statement expected.
ERROR [25:4] A function whose declared type is neither 'undefined', 'void', nor 'any' must return a value.
ERROR [47:7] Object literal may only specify known properties, and 'addFile' does not exist in type 'UseCommandScreenPopupManagerOptions'.
ERROR [61:7] No value exists in scope for the shorthand property 'notify'. Either declare one or provide an initializer.
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
**Investigating parse errors**
It looks like the `useCommandScreenBindings` file has some parse errors, particularly around line 67. I wonder if these issues stem from a previously misapplied patch. To really get to the bottom of this, Iâll need to check out that section of the file to pinpoint the problem. It could lead to fixing these errors in a timely manner, which is important to ensure everything runs smoothly!
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "offset": 40,
  "limit": 50
}
```
**Output:**
```
<file>
00041|         videos: context.videos,
00042|         pdfs: context.pdfs,
00043|         smartContextEnabled: context.smartContextEnabled,
00044| 
00045| 
00046|       smartContextRoot: context.smartContextRoot,
00047|       addFile: context.addFile,
00048|       removeFile: context.removeFile,
00049|       addUrl: context.addUrl,
00050|       removeUrl: context.removeUrl,
00051|       updateUrl: context.updateUrl,
00052|       addImage: context.addImage,
00053|       removeImage: context.removeImage,
00054|         addVideo: context.addVideo,
00055|         removeVideo: context.removeVideo,
00056|         addPdf: context.addPdf,
00057|         removePdf: context.removePdf,
00058| 
00059|       toggleSmartContext: context.toggleSmartContext,
00060|       setSmartRoot: context.setSmartRoot,
00061|       notify,
00062|       modelOptions: generation.modelOptions,
00063|       lastReasoning: context.lastReasoning,
00064|       terminalColumns: input.terminalColumns,
00065|       reasoningPopupHeight: POPUP_HEIGHTS.reasoning,
00066|     },
00067|   })
00068| 
00069|   return {
00070|     popupManager,
00071|     shell,
00072|     bindings,
00073|     enhancedStatusChips,
00074|   }
00075| }
00076| 
(End of file - total 76 lines)
</file>
```
```
---
## Assistant
_Thinking:_
```
Tool: bash
**Input:**
```json
{
  "command": "git checkout -- src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "description": "Restore useCommandScreenBindings.ts from HEAD"
}
```
```
---
## Assistant
_Thinking:_
**Reviewing command screen bindings**
I need to reapply modifications to the useCommandScreenBindings. I should carefully include options for add pdfs, addPdf, and removePdf in the popupManager. It seems important to also pass in the context to those popup bindings for proper functionality. First, I'll read the relevant section of the restored file that involves the popupManager call to ensure I understand how everything is connected. This will help me apply the changes correctly!
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "offset": 24,
  "limit": 70
}
```
**Output:**
```
<file>
00025| ): UseCommandScreenBindingsResult => {
00026|   const { context, input, popup, history, generation } = options
00027| 
00028|   const popupManager = useCommandScreenPopupManager({
00029|     currentModel: generation.currentModel,
00030|     polishModelId: generation.polishModelId,
00031|     currentTargetModel: generation.currentTargetModel,
00032| 
00033|     modelOptions: generation.modelOptions,
00034|     smartContextEnabled: context.smartContextEnabled,
00035|     smartContextRoot: context.smartContextRoot,
00036|     toggleSmartContext: context.toggleSmartContext,
00037|     setSmartRoot: context.setSmartRoot,
00038|     urls: context.urls,
00039|     addUrl: context.addUrl,
00040|     images: context.images,
00041|     videos: context.videos,
00042|     addImage: context.addImage,
00043|     addVideo: context.addVideo,
00044|     lastTestFile: history.lastTestFile,
00045|     ...(context.interactiveTransportPath
00046|       ? { interactiveTransportPath: context.interactiveTransportPath }
00047|       : {}),
00048|     isGenerating: generation.isGenerating,
00049|     lastUserIntentRef: input.lastUserIntentRef,
00050|     lastTypedIntentRef: input.lastTypedIntentRef,
00051|     pushHistoryProxy: history.pushHistoryProxy,
00052|     notify: context.notify,
00053|     setInputValue: input.setInputValue,
00054|     runGeneration: generation.runGeneration,
00055|     runSeriesGeneration: generation.runSeriesGeneration,
00056|     runTestsFromCommandProxy: history.runTestsFromCommandProxy,
00057|     setCurrentModel: generation.selectModel,
00058|     setCurrentTargetModel: generation.selectTargetModel,
00059|     setPolishModelId: generation.selectPolishModel,
00060|     setCopyEnabled: input.setCopyEnabled,
00061|     setChatGptEnabled: input.setChatGptEnabled,
00062|     setJsonOutputEnabled: input.setJsonOutputEnabled,
00063|     intentFilePath: input.intentFilePath,
00064|     setIntentFilePath: input.setIntentFilePath,
00065|     metaInstructions: context.metaInstructions,
00066|     setMetaInstructions: context.setMetaInstructions,
00067|     budgets: {
00068|       maxContextTokens: context.maxContextTokens,
00069|       maxInputTokens: context.maxInputTokens,
00070|       contextOverflowStrategy: context.contextOverflowStrategy,
00071|     },
00072|     setBudgets: context.setBudgets,
00073|     copyEnabled: input.copyEnabled,
00074| 
00075|     chatGptEnabled: input.chatGptEnabled,
00076|     jsonOutputEnabled: input.jsonOutputEnabled,
00077|   })
00078| 
00079|   history.closeTestPopupRef.current = () => {
00080|     popupManager.setPopupState((prev) => (prev?.type === 'test' ? null : prev))
00081|   }
00082| 
00083|   useCommandScreenPopupVisibility({
00084|     isPopupOpen: popupManager.isPopupOpen,
00085|     onPopupVisibilityChange: popup.onPopupVisibilityChange,
00086|   })
00087| 
00088|   const pushHistory: PushHistory = useStableCallback((content, kind) => {
00089|     history.pushHistoryRef.current(content, kind)
00090|   })
00091| 
00092|   const droppedFilePath = useDroppedFilePath(input.inputValue)
00093| 
00094|   const shell = useCommandScreenShell({
(File has more lines. Use 'offset' parameter to read beyond line 94)
</file>
```
```
---
## Assistant
_Thinking:_
**Updating popup manager**
I need to add pdfs to the popupManager options since we've updated the useCommandScreenPopupManager. I'll edit the lines surrounding images and videos accordingly. Later, when building the context object for bindings, I must include pdfs and the add/remove functionality. For now, Iâll update the popupManager call: after videos, I'll make sure to add context.pdfs, and after addVideo, Iâll add addPdf. Let's proceed with that!
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "    images: context.images,\n    videos: context.videos,\n    addImage: context.addImage,\n    addVideo: context.addVideo,\n",
  "newString": "    images: context.images,\n    videos: context.videos,\n    pdfs: context.pdfs,\n    addImage: context.addImage,\n    addVideo: context.addVideo,\n    addPdf: context.addPdf,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [194:5] Type '{ droppedFilePath: UseDroppedFilePathResult; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; ... 15 more ...; reasoningPopupHeight: 18; }' is missing the following properties from type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; pdfs: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 16 more ...; reasoningPopupHeight: number; }': pdfs, addPdf, removePdf
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
**Fixing context bindings**
Iâm noticing some issues, like later binding context objects being missing, which is causing compile errors, but it seems like the edits were applied. Iâll need to address this later. For now, I should patch the context object for the bindings. Let's open up that area so I can make the necessary edits. It's a bit intricate, but Iâm determined to get it sorted out!
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "offset": 180,
  "limit": 50
}
```
**Output:**
```
<file>
00181|       },
00182|     },
00183|     generation: {
00184|       isGenerating: generation.isGenerating,
00185|       isAwaitingRefinement: generation.isAwaitingRefinement,
00186|       submitRefinement: generation.submitRefinement,
00187|       runGeneration: generation.runGeneration,
00188|     },
00189|     history: {
00190|       pushHistory,
00191|       addCommandHistoryEntry: history.addCommandHistoryEntry,
00192|       commandHistoryValues: history.commandHistoryValues,
00193|     },
00194|     context: {
00195|       droppedFilePath,
00196|       files: context.files,
00197|       urls: context.urls,
00198|       images: context.images,
00199|       videos: context.videos,
00200|       smartContextEnabled: context.smartContextEnabled,
00201|       smartContextRoot: context.smartContextRoot,
00202|       addFile: context.addFile,
00203|       removeFile: context.removeFile,
00204|       addUrl: context.addUrl,
00205|       removeUrl: context.removeUrl,
00206|       updateUrl: context.updateUrl,
00207|       addImage: context.addImage,
00208|       removeImage: context.removeImage,
00209|       addVideo: context.addVideo,
00210|       removeVideo: context.removeVideo,
00211|       toggleSmartContext: context.toggleSmartContext,
00212|       setSmartRoot: context.setSmartRoot,
00213|       notify,
00214|       modelOptions: generation.modelOptions,
00215|       lastReasoning: context.lastReasoning,
00216|       terminalColumns: input.terminalColumns,
00217|       reasoningPopupHeight: POPUP_HEIGHTS.reasoning,
00218|     },
00219|   })
00220| 
00221|   return {
00222|     popupManager,
00223|     shell,
00224|     bindings,
00225|     enhancedStatusChips,
00226|   }
00227| }
00228| 
(End of file - total 228 lines)
</file>
```
```
---
## Assistant
_Thinking:_
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "      images: context.images,\n      videos: context.videos,\n      smartContextEnabled: context.smartContextEnabled,\n",
  "newString": "      images: context.images,\n      videos: context.videos,\n      pdfs: context.pdfs,\n      smartContextEnabled: context.smartContextEnabled,\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "      addVideo: context.addVideo,\n      removeVideo: context.removeVideo,\n",
  "newString": "      addVideo: context.addVideo,\n      removeVideo: context.removeVideo,\n      addPdf: context.addPdf,\n      removePdf: context.removePdf,\n"
}
```
```
---
## Assistant
_Thinking:_
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "offset": 230,
  "limit": 60
}
```
**Output:**
```
<file>
00231|     inputValue: options.input.value,
00232|     popupState: options.popup.state,
00233|     helpOpen: options.popup.helpOpen,
00234|     setInputValue: options.input.setValue,
00235|     setPasteActive: options.input.setPasteActive,
00236|     consumeSuppressedTextInputChange: options.input.consumeSuppressedTextInputChange,
00237|     suppressNextInput: options.input.suppressNextInput,
00238|     updateLastTypedIntent: options.input.updateLastTypedIntent,
00239|   } satisfies UseCommandScreenPasteBindingsOptions)
00240| 
00241|   const context = useCommandScreenContextPopupBindings({
00242|     inputValue: options.input.value,
00243|     popupState: options.popup.state,
00244|     helpOpen: options.popup.helpOpen,
00245|     isPopupOpen: options.popup.isOpen,
00246|     isCommandMode: options.menu.isCommandMode,
00247|     isCommandMenuActive: options.menu.isActive,
00248|     isGenerating: options.generation.isGenerating,
00249|     droppedFilePath: options.context.droppedFilePath,
00250|     files: options.context.files,
00251|     urls: options.context.urls,
00252|     images: options.context.images,
00253|     videos: options.context.videos,
00254|     pdfs: options.context.pdfs,
00255|     smartContextEnabled: options.context.smartContextEnabled,
00256|     smartContextRoot: options.context.smartContextRoot,
00257|     addFile: options.context.addFile,
00258|     removeFile: options.context.removeFile,
00259|     addUrl: options.context.addUrl,
00260|     removeUrl: options.context.removeUrl,
00261|     updateUrl: options.context.updateUrl,
00262|     addImage: options.context.addImage,
00263|     removeImage: options.context.removeImage,
00264|     addVideo: options.context.addVideo,
00265|     removeVideo: options.context.removeVideo,
00266|     addPdf: options.context.addPdf,
00267|     removePdf: options.context.removePdf,
00268|     toggleSmartContext: options.context.toggleSmartContext,
00269|     setSmartRoot: options.context.setSmartRoot,
00270|     setInputValue: options.input.setValue,
00271|     setPopupState: options.popup.setState,
00272|     suppressNextInput: options.input.suppressNextInput,
00273|     notify,
00274|     pushHistory,
00275|     addCommandHistoryEntry: options.history.addCommandHistoryEntry,
00276|     handleCommandSelection: options.popup.actions.handleCommandSelection,
00277|     consumeSuppressedTextInputChange: options.input.consumeSuppressedTextInputChange,
00278|   } satisfies UseCommandScreenContextPopupBindingsOptions)
00279| 
00280|   const historyAndIntent = useCommandScreenHistoryIntentPopupBindings({
00281|     popupState: options.popup.state,
00282|     setPopupState: options.popup.setState,
00283|     closePopup: options.popup.close,
00284|     setInputValue: options.input.setValue,
00285|     consumeSuppressedTextInputChange: options.input.consumeSuppressedTextInputChange,
00286|     suppressNextInput: options.input.suppressNextInput,
00287|     commandHistoryValues: options.history.commandHistoryValues,
00288|   } satisfies UseCommandScreenHistoryIntentPopupBindingsOptions)
00289| 
00290|   const { modelPopupOptions, modelPopupRecentCount, modelPopupSelection } = useModelPopupData({
(File has more lines. Use 'offset' parameter to read beyond line 290)
</file>
```
```
---
## Assistant
_Thinking:_
I'm checking if everything looks good with the recent addition of the PDF config into the usePopupKeyboardShortcuts call. I need to verify where exactly it's being passed. It's important to be thorough at this stage to ensure that the integration is seamless and functions as intended. Letâs double-check the relevant areas and confirm that there are no issues with the implementation. I'll take my time with this!
```
Tool: grep
**Input:**
```json
{
  "pattern": "pdf:\\s*\\{|video:\\s*\\{",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Output:**
```
Found 4 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts:
  Line 185:       video: {
  Line 363:     video: {
  Line 370:     pdf: {
  Line 490:         video: {
```
```
---
## Assistant
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "offset": 340,
  "limit": 60
}
```
**Output:**
```
<file>
00341|       onSubmit: options.popup.actions.handleBudgetsSubmit,
00342|     },
00343| 
00344|     file: {
00345|       items: options.context.files,
00346|       suggestions: context.filePopupSuggestions,
00347|       onAdd: context.onAddFile,
00348|       onRemove: context.onRemoveFile,
00349|     },
00350| 
00351|     url: {
00352|       items: options.context.urls,
00353|       onRemove: context.onRemoveUrl,
00354|     },
00355| 
00356|     image: {
00357|       items: options.context.images,
00358|       suggestions: context.imagePopupSuggestions,
00359|       onAdd: context.onAddImage,
00360|       onRemove: context.onRemoveImage,
00361|     },
00362| 
00363|     video: {
00364|       items: options.context.videos,
00365|       suggestions: context.videoPopupSuggestions,
00366|       onAdd: context.onAddVideo,
00367|       onRemove: context.onRemoveVideo,
00368|     },
00369| 
00370|     pdf: {
00371|       items: options.context.pdfs,
00372|       suggestions: context.pdfPopupSuggestions,
00373|       onAdd: context.onAddPdf,
00374|       onRemove: context.onRemovePdf,
00375|     },
00376| 
00377|     history: {
00378|       items: historyAndIntent.history.historyPopupItems,
00379|     },
00380| 
00381|     resume: {
00382|       onSubmit: options.popup.actions.handleResumeSubmit,
00383|     },
00384| 
00385|     export: {
00386|       onSubmit: options.popup.actions.handleExportSubmit,
00387|     },
00388| 
00389|     smart: {
00390|       suggestions: context.smartPopupSuggestions,
00391|       contextRoot: options.context.smartContextRoot,
00392|       onRootSubmit: context.onSmartRootSubmit,
00393|     },
00394| 
00395|     intent: {
00396|       suggestions: historyAndIntent.intent.intentPopupSuggestions,
00397|       onFileSubmit: options.popup.actions.handleIntentFileSubmit,
00398|     },
00399| 
00400|     reasoning: {
(File has more lines. Use 'offset' parameter to read beyond line 400)
</file>
```
```
---
## Assistant
_Thinking:_
**Updating PopupArea.tsx**
Iâm working on updating PopupArea.tsx to render PDFs and accept props. First, Iâll add props fields similar to whatâs used for images and videos. Then, I need to include a function to render the PDF popup. I also want to update the switch at the bottom to handle the 'pdf' case. I noticed there was an earlier diagnostic line at 507, so Iâll make sure to patch that as well. Letâs dig deeper around the video popup and the switch.
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/components/PopupArea.tsx",
  "offset": 240,
  "limit": 120
}
```
**Output:**
```
<file>
00241|   return <ListPopup {...viewModel} />
00242| }
00243| 
00244| const renderVideoPopup = (props: PopupAreaProps, popupState: PopupStateFor<'video'>) => {
00245|   const viewModel = {
00246|     title: 'Videos',
00247|     placeholder: 'path/to/video.mp4',
00248|     draft: popupState.draft,
00249|     items: props.videos,
00250|     selectedIndex: popupState.selectionIndex,
00251|     selectedFocused: popupState.selectedFocused,
00252|     layout: 'selected-first',
00253|     emptyLabel: 'No videos attached',
00254|     instructions:
00255|       "Enter add Âˇ â/â focus list Âˇ Del/Backspace remove Âˇ Tab suggestions Âˇ Esc close\nfzf: ^start $end 'exact",
00256|     suggestedItems: props.videoPopupSuggestions,
00257|     suggestedSelectionIndex: props.videoPopupSuggestionSelectionIndex,
00258|     suggestedFocused: props.videoPopupSuggestionsFocused,
00259|     maxHeight: props.overlayHeight,
00260|     onDraftChange: props.onVideoPopupDraftChange,
00261|     onSubmitDraft: props.onAddVideo,
00262|   } satisfies ComponentProps<typeof ListPopup>
00263| 
00264|   return <ListPopup {...viewModel} />
00265| }
00266| 
00267| const renderHistoryPopup = (props: PopupAreaProps, popupState: PopupStateFor<'history'>) => {
00268|   const viewModel = {
00269|     title: 'History',
00270|     placeholder: 'Type to filterâŚ',
00271|     draft: popupState.draft,
00272|     items: props.historyPopupItems,
00273|     selectedIndex: popupState.selectionIndex,
00274|     selectedFocused: true,
00275|     emptyLabel: 'No history entries',
00276|     instructions: 'â/â select Âˇ Enter paste Âˇ Esc close',
00277|     onDraftChange: props.onHistoryPopupDraftChange,
00278|     onSubmitDraft: props.onHistoryPopupSubmit,
00279|   } satisfies ComponentProps<typeof ListPopup>
00280| 
00281|   return <ListPopup {...viewModel} />
00282| }
00283| 
00284| const renderResumePopup = (props: PopupAreaProps, popupState: PopupStateFor<'resume'>) => {
00285|   const viewModel = {
00286|     selectionIndex: popupState.selectionIndex,
00287|     sourceKind: popupState.sourceKind,
00288|     mode: popupState.mode,
00289|     historyItems: popupState.historyItems,
00290|     historySelectionIndex: popupState.historySelectionIndex,
00291|     historyErrorMessage: popupState.historyErrorMessage,
00292|     payloadPathDraft: popupState.payloadPathDraft,
00293|     suggestedItems: popupState.suggestedItems,
00294|     suggestedSelectionIndex: popupState.suggestedSelectionIndex,
00295|     suggestedFocused: popupState.suggestedFocused,
00296|     onPayloadPathChange: props.onResumePayloadPathDraftChange,
00297|     onSubmit: props.onResumeSubmit,
00298|   } satisfies ComponentProps<typeof ResumePopup>
00299| 
00300|   return <ResumePopup {...viewModel} />
00301| }
00302| 
00303| const renderExportPopup = (props: PopupAreaProps, popupState: PopupStateFor<'export'>) => {
00304|   const viewModel = {
00305|     selectionIndex: popupState.selectionIndex,
00306|     format: popupState.format,
00307|     outPathDraft: popupState.outPathDraft,
00308|     historyItems: popupState.historyItems,
00309|     historySelectionIndex: popupState.historySelectionIndex,
00310|     historyErrorMessage: popupState.historyErrorMessage,
00311|     onOutPathChange: props.onExportOutPathDraftChange,
00312|     onSubmit: props.onExportSubmit,
00313|   } satisfies ComponentProps<typeof ExportPopup>
00314| 
00315|   return <ExportPopup {...viewModel} />
00316| }
00317| 
00318| const renderIntentPopup = (props: PopupAreaProps, popupState: PopupStateFor<'intent'>) => {
00319|   const viewModel = {
00320|     draft: popupState.draft,
00321|     suggestions: props.intentPopupSuggestions,
00322|     suggestedSelectionIndex: props.intentPopupSuggestionSelectionIndex,
00323|     suggestedFocused: props.intentPopupSuggestionsFocused,
00324|     maxHeight: props.overlayHeight,
00325|     onDraftChange: props.onIntentPopupDraftChange,
00326|     onSubmitDraft: props.onIntentFileSubmit,
00327|   } satisfies ComponentProps<typeof IntentFilePopup>
00328| 
00329|   return <IntentFilePopup {...viewModel} />
00330| }
00331| 
00332| const renderSmartPopup = (props: PopupAreaProps, popupState: PopupStateFor<'smart'>) => {
00333|   const viewModel = {
00334|     savedRoot: props.smartContextRoot,
00335|     draft: popupState.draft,
00336|     suggestedItems: props.smartPopupSuggestions,
00337|     suggestedSelectionIndex: props.smartPopupSuggestionSelectionIndex,
00338|     suggestedFocused: props.smartPopupSuggestionsFocused,
00339|     maxHeight: props.overlayHeight,
00340|     onDraftChange: props.onSmartPopupDraftChange,
00341|     onSubmitRoot: props.onSmartRootSubmit,
00342|   } satisfies ComponentProps<typeof SmartPopup>
00343| 
00344|   return <SmartPopup {...viewModel} />
00345| }
00346| 
00347| const renderInstructionsPopup = (
00348|   props: PopupAreaProps,
00349|   popupState: PopupStateFor<'instructions'>,
00350| ) => {
00351|   const viewModel = {
00352|     draft: popupState.draft,
00353|     onDraftChange: props.onInstructionsDraftChange,
00354|     onSubmitDraft: props.onInstructionsSubmit,
00355|   } satisfies ComponentProps<typeof InstructionsPopup>
00356| 
00357|   return <InstructionsPopup {...viewModel} />
00358| }
00359| 
00360| const renderSeriesPopup = (props: PopupAreaProps, popupState: PopupStateFor<'series'>) => {
(File has more lines. Use 'offset' parameter to read beyond line 360)
</file>
```
```
---
## Assistant
_Thinking:_
**Updating popup structure**
I need to add a renderPdfPopup after the video popup. It seems important to define the props early on, especially for PDF items and any related suggestions or callbacks. I'll go ahead and edit the PopupAreaProps around line 60. That feels like a solid plan, and Iâll review the surrounding code to ensure everything aligns properly. Let's patch things up!
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/components/PopupArea.tsx",
  "offset": 50,
  "limit": 110
}
```
**Output:**
```
<file>
00051|   filePopupSuggestionsFocused: boolean
00052|   onFilePopupDraftChange: (next: string) => void
00053|   onAddFile: (value: string) => void
00054| 
00055|   // URL popup
00056|   urls: string[]
00057|   onUrlPopupDraftChange: (next: string) => void
00058|   onAddUrl: (value: string) => void
00059| 
00060|   // Image popup
00061|   images: string[]
00062|   imagePopupSuggestions: string[]
00063|   imagePopupSuggestionSelectionIndex: number
00064|   imagePopupSuggestionsFocused: boolean
00065|   onImagePopupDraftChange: (next: string) => void
00066|   onAddImage: (value: string) => void
00067| 
00068|   // Video popup
00069|   videos: string[]
00070|   videoPopupSuggestions: string[]
00071|   videoPopupSuggestionSelectionIndex: number
00072|   videoPopupSuggestionsFocused: boolean
00073|   onVideoPopupDraftChange: (next: string) => void
00074|   onAddVideo: (value: string) => void
00075| 
00076|   // History popup
00077|   historyPopupItems: string[]
00078|   onHistoryPopupDraftChange: (next: string) => void
00079|   onHistoryPopupSubmit: (value: string) => void
00080| 
00081|   // Resume popup
00082|   onResumePayloadPathDraftChange: (next: string) => void
00083|   onResumeSubmit: () => void
00084| 
00085|   // Export popup
00086|   onExportOutPathDraftChange: (next: string) => void
00087|   onExportSubmit: () => void
00088| 
00089|   // Intent popup
00090|   intentPopupSuggestions: string[]
00091|   intentPopupSuggestionSelectionIndex: number
00092|   intentPopupSuggestionsFocused: boolean
00093|   onIntentPopupDraftChange: (next: string) => void
00094|   onIntentFileSubmit: (value: string) => void
00095| 
00096|   // Instructions
00097|   onInstructionsDraftChange: (next: string) => void
00098|   onInstructionsSubmit: (value: string) => void
00099| 
00100|   // Series
00101|   isGenerating: boolean
00102|   onSeriesDraftChange: (next: string) => void
00103|   onSeriesSubmit: (value: string) => void
00104| 
00105|   // Test
00106|   isTestCommandRunning: boolean
00107|   onTestDraftChange: (next: string) => void
00108|   onTestSubmit: (value: string) => void
00109| 
00110|   // Tokens
00111|   tokenUsageRun: TokenUsageRun | null
00112|   tokenUsageBreakdown: TokenUsageBreakdown | null
00113|   maxContextTokens: number | null
00114|   maxInputTokens: number | null
00115|   contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
00116|   latestContextOverflow:
00117|     | import('../../../generation-pipeline-reducer').ContextOverflowDetails
00118|     | null
00119| 
00120|   // Budgets
00121|   onBudgetsMaxContextTokensDraftChange: (next: string) => void
00122|   onBudgetsMaxInputTokensDraftChange: (next: string) => void
00123|   onBudgetsSubmit: () => void
00124| 
00125|   // Settings
00126|   statusChips: string[]
00127| 
00128|   // Reasoning
00129|   reasoningPopupLines: HistoryEntry[]
00130|   reasoningPopupVisibleRows: number
00131| 
00132|   // Smart context
00133|   smartContextEnabled: boolean
00134|   smartContextRoot: string | null
00135|   smartPopupSuggestions: string[]
00136|   smartPopupSuggestionSelectionIndex: number
00137|   smartPopupSuggestionsFocused: boolean
00138|   onSmartPopupDraftChange: (next: string) => void
00139|   onSmartRootSubmit: (value: string) => void
00140| }
00141| 
00142| type NonNullPopupState = Exclude<PopupState, null>
00143| 
00144| type PopupStateFor<T extends NonNullPopupState['type']> = Extract<NonNullPopupState, { type: T }>
00145| 
00146| const renderModelPopup = (props: PopupAreaProps, popupState: PopupStateFor<'model'>) => {
00147|   const title =
00148|     popupState.kind === 'target'
00149|       ? 'Select target model'
00150|       : popupState.kind === 'polish'
00151|         ? 'Select polish model'
00152|         : 'Select model'
00153| 
00154|   const viewModel = {
00155|     title,
00156|     query: popupState.query,
00157|     options: props.modelPopupOptions,
00158|     selectedIndex: props.modelPopupSelection,
00159|     recentCount: props.modelPopupRecentCount,
00160|     maxHeight: props.overlayHeight,
(File has more lines. Use 'offset' parameter to read beyond line 160)
</file>
```
```
---
## Assistant
_Thinking:_
```
Tool: todowrite
**Input:**
```json
{
  "todos": [
    {
      "content": "Add PDF state to TUI context",
      "status": "completed",
      "priority": "high",
      "id": "1"
    },
    {
      "content": "Add PDF popup UI and commands",
      "status": "in_progress",
      "priority": "high",
      "id": "2"
    },
    {
      "content": "Wire TUI generation pipeline PDFs",
      "status": "pending",
      "priority": "high",
      "id": "3"
    },
    {
      "content": "Implement OpenAI PDF text extraction",
      "status": "pending",
      "priority": "high",
      "id": "4"
    },
    {
      "content": "Update tests for TUI and OpenAI",
      "status": "pending",
      "priority": "high",
      "id": "5"
    },
    {
      "content": "Run typecheck/tests/build",
      "status": "pending",
      "priority": "high",
      "id": "6"
    }
  ]
}
```
**Output:**
```
[
  {
    "content": "Add PDF state to TUI context",
    "status": "completed",
    "priority": "high",
    "id": "1"
  },
  {
    "content": "Add PDF popup UI and commands",
    "status": "in_progress",
    "priority": "high",
    "id": "2"
  },
  {
    "content": "Wire TUI generation pipeline PDFs",
    "status": "pending",
    "priority": "high",
    "id": "3"
  },
  {
    "content": "Implement OpenAI PDF text extraction",
    "status": "pending",
    "priority": "high",
    "id": "4"
  },
  {
    "content": "Update tests for TUI and OpenAI",
    "status": "pending",
    "priority": "high",
    "id": "5"
  },
  {
    "content": "Run typecheck/tests/build",
    "status": "pending",
    "priority": "high",
    "id": "6"
  }
]
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/components/PopupArea.tsx",
  "oldString": "  // Video popup\n  videos: string[]\n  videoPopupSuggestions: string[]\n  videoPopupSuggestionSelectionIndex: number\n  videoPopupSuggestionsFocused: boolean\n  onVideoPopupDraftChange: (next: string) => void\n  onAddVideo: (value: string) => void\n",
  "newString": "  // Video popup\n  videos: string[]\n  videoPopupSuggestions: string[]\n  videoPopupSuggestionSelectionIndex: number\n  videoPopupSuggestionsFocused: boolean\n  onVideoPopupDraftChange: (next: string) => void\n  onAddVideo: (value: string) => void\n\n  // PDF popup\n  pdfs: string[]\n  pdfPopupSuggestions: string[]\n  pdfPopupSuggestionSelectionIndex: number\n  pdfPopupSuggestionsFocused: boolean\n  onPdfPopupDraftChange: (next: string) => void\n  onAddPdf: (value: string) => void\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/components/PopupArea.tsx",
  "oldString": "const renderVideoPopup = (props: PopupAreaProps, popupState: PopupStateFor<'video'>) => {\n  const viewModel = {\n    title: 'Videos',\n    placeholder: 'path/to/video.mp4',\n    draft: popupState.draft,\n    items: props.videos,\n    selectedIndex: popupState.selectionIndex,\n    selectedFocused: popupState.selectedFocused,\n    layout: 'selected-first',\n    emptyLabel: 'No videos attached',\n    instructions:\n      \"Enter add Âˇ â/â focus list Âˇ Del/Backspace remove Âˇ Tab suggestions Âˇ Esc close\\nfzf: ^start $end 'exact\",\n    suggestedItems: props.videoPopupSuggestions,\n    suggestedSelectionIndex: props.videoPopupSuggestionSelectionIndex,\n    suggestedFocused: props.videoPopupSuggestionsFocused,\n    maxHeight: props.overlayHeight,\n    onDraftChange: props.onVideoPopupDraftChange,\n    onSubmitDraft: props.onAddVideo,\n  } satisfies ComponentProps<typeof ListPopup>\n\n  return <ListPopup {...viewModel} />\n}\n",
  "newString": "const renderVideoPopup = (props: PopupAreaProps, popupState: PopupStateFor<'video'>) => {\n  const viewModel = {\n    title: 'Videos',\n    placeholder: 'path/to/video.mp4',\n    draft: popupState.draft,\n    items: props.videos,\n    selectedIndex: popupState.selectionIndex,\n    selectedFocused: popupState.selectedFocused,\n    layout: 'selected-first',\n    emptyLabel: 'No videos attached',\n    instructions:\n      \"Enter add Âˇ â/â focus list Âˇ Del/Backspace remove Âˇ Tab suggestions Âˇ Esc close\\nfzf: ^start $end 'exact\",\n    suggestedItems: props.videoPopupSuggestions,\n    suggestedSelectionIndex: props.videoPopupSuggestionSelectionIndex,\n    suggestedFocused: props.videoPopupSuggestionsFocused,\n    maxHeight: props.overlayHeight,\n    onDraftChange: props.onVideoPopupDraftChange,\n    onSubmitDraft: props.onAddVideo,\n  } satisfies ComponentProps<typeof ListPopup>\n\n  return <ListPopup {...viewModel} />\n}\n\nconst renderPdfPopup = (props: PopupAreaProps, popupState: PopupStateFor<'pdf'>) => {\n  const viewModel = {\n    title: 'PDFs',\n    placeholder: 'path/to/document.pdf',\n    draft: popupState.draft,\n    items: props.pdfs,\n    selectedIndex: popupState.selectionIndex,\n    selectedFocused: popupState.selectedFocused,\n    layout: 'selected-first',\n    emptyLabel: 'No PDFs attached',\n    instructions:\n      \"Enter add Âˇ â/â focus list Âˇ Del/Backspace remove Âˇ Tab suggestions Âˇ Esc close\\nfzf: ^start $end 'exact\",\n    suggestedItems: props.pdfPopupSuggestions,\n    suggestedSelectionIndex: props.pdfPopupSuggestionSelectionIndex,\n    suggestedFocused: props.pdfPopupSuggestionsFocused,\n    maxHeight: props.overlayHeight,\n    onDraftChange: props.onPdfPopupDraftChange,\n    onSubmitDraft: props.onAddPdf,\n  } satisfies ComponentProps<typeof ListPopup>\n\n  return <ListPopup {...viewModel} />\n}\n"
}
```
```
```
Tool: grep
**Input:**
```json
{
  "pattern": "case 'video'\\:",
  "include": "src/tui/screens/command/components/PopupArea.tsx"
}
```
**Output:**
```
Found 1 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/components/PopupArea.tsx:
  Line 477:     case 'video':
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/components/PopupArea.tsx",
  "oldString": "    case 'video':\n      return renderVideoPopup(props, popupState)\n",
  "newString": "    case 'video':\n      return renderVideoPopup(props, popupState)\n\n    case 'pdf':\n      return renderPdfPopup(props, popupState)\n"
}
```
```
---
## Assistant
_Thinking:_
```
Tool: bash
**Input:**
```json
{
  "command": "npm run typecheck",
  "description": "Run TypeScript typecheck"
}
```
**Output:**
```
> @perceptron/prompt-maker-cli@0.1.0 typecheck
> tsc -p tsconfig.json --noEmit
src/__tests__/tui/command-screen-popup-bindings-contract.test.ts(239,7): error TS2739: Type '{ droppedFilePath: null; files: never[]; urls: never[]; images: never[]; videos: never[]; smartContextEnabled: false; smartContextRoot: null; addFile: Mock<any, any, any>; removeFile: Mock<any, any, any>; ... 13 more ...; reasoningPopupHeight: number; }' is missing the following properties from type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; pdfs: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 16 more ...; reasoningPopupHeight: number; }': pdfs, addPdf, removePdf
src/__tests__/tui/context-popup-glue-contract.test.ts(31,11): error TS2739: Type '{ inputValue: string; popupState: null; helpOpen: false; isPopupOpen: false; isCommandMode: false; isCommandMenuActive: false; isGenerating: false; droppedFilePath: null; files: never[]; urls: never[]; ... 23 more ...; isFilePath: Mock<...>; }' is missing the following properties from type 'UseContextPopupGlueOptions': pdfs, addPdf, removePdf
src/__tests__/tui/popup-manager-command-mapping.test.ts(8,9): error TS2741: Property 'pdfs' is missing in type '{ copyEnabled: false; chatGptEnabled: false; jsonOutputEnabled: false; interactiveTransportPath: undefined; urls: never[]; images: never[]; videos: never[]; smartContextEnabled: false; smartContextRoot: null; }' but required in type 'CommandMappingContext'.
src/__tests__/usePopupManager.test.ts(48,9): error TS2739: Type '{ currentModel: string; currentTargetModel: string; modelOptions: ModelOption[]; activeThemeName: string; themeMode: "dark"; themes: { name: string; label: string; }[]; smartContextEnabled: false; ... 42 more ...; setExportDefaults: Mock<...>; }' is missing the following properties from type 'UsePopupManagerOptions': pdfs, addPdf
src/tui/screens/command/hooks/useCommandScreenController.ts(115,9): error TS2739: Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
src/tui/screens/command/hooks/useCommandScreenViewModel.ts(187,11): error TS2740: Type '{ reasoningPopupLines: HistoryEntry[]; reasoningPopupVisibleRows: number; statusChips: string[]; onBudgetsMaxContextTokensDraftChange: (next: string) => void; onBudgetsMaxInputTokensDraftChange: (next: string) => void; ... 63 more ...; overlayHeight: number; }' is missing the following properties from type 'PopupAreaProps': pdfs, pdfPopupSuggestions, pdfPopupSuggestionSelectionIndex, pdfPopupSuggestionsFocused, and 2 more.
```
```
---
## Assistant
_Thinking:_
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/command-screen-popup-bindings-contract.test.ts",
  "offset": 220,
  "limit": 40
}
```
**Output:**
```
<file>
00221|         argsRaw: '',
00222|         isCommandMode: false,
00223|         actions: {
00224|           handleNewCommand: jest.fn(),
00225|           handleReuseCommand: jest.fn(),
00226|         },
00227|       },
00228|       generation: {
00229|         isGenerating: false,
00230|         isAwaitingRefinement: false,
00231|         submitRefinement: jest.fn(),
00232|         runGeneration: jest.fn(async () => undefined),
00233|       },
00234|       history: {
00235|         pushHistory: jest.fn(),
00236|         addCommandHistoryEntry: jest.fn(),
00237|         commandHistoryValues: [],
00238|       },
00239|       context: {
00240|         droppedFilePath: null,
00241|         files: [],
00242|         urls: [],
00243|         images: [],
00244|         videos: [],
00245|         smartContextEnabled: false,
00246|         smartContextRoot: null,
00247|         addFile: jest.fn(),
00248|         removeFile: jest.fn(),
00249|         addUrl: jest.fn(),
00250|         removeUrl: jest.fn(),
00251|         updateUrl: jest.fn(),
00252|         addImage: jest.fn(),
00253|         removeImage: jest.fn(),
00254|         addVideo: jest.fn(),
00255|         removeVideo: jest.fn(),
00256|         toggleSmartContext: jest.fn(),
00257|         setSmartRoot: jest.fn(),
00258|         notify: jest.fn(),
00259|         modelOptions: [],
00260|         lastReasoning: null,
(File has more lines. Use 'offset' parameter to read beyond line 260)
</file>
```
```
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/context-popup-glue-contract.test.ts",
  "offset": 1,
  "limit": 80
}
```
**Output:**
```
<file>
00002| import { JSDOM } from 'jsdom'
00003| 
00004| jest.mock('ink', () => ({
00005|   useInput: jest.fn(),
00006| }))
00007| 
00008| import {
00009|   useContextPopupGlue,
00010|   type UseContextPopupGlueOptions,
00011| } from '../../tui/screens/command/hooks/useContextPopupGlue'
00012| 
00013| const dom = new JSDOM('<!doctype html><html><body></body></html>')
00014| 
00015| type GlobalDom = { window: Window; document: Document }
00016| 
00017| beforeAll(() => {
00018|   const target = globalThis as unknown as GlobalDom
00019|   target.window = dom.window as unknown as Window
00020|   target.document = dom.window.document
00021| })
00022| 
00023| afterAll(() => {
00024|   const target = globalThis as unknown as Partial<GlobalDom>
00025|   delete target.window
00026|   delete target.document
00027| })
00028| 
00029| describe('useContextPopupGlue (shape contract)', () => {
00030|   it('returns stable keys and safe-to-call handlers', () => {
00031|     const options: UseContextPopupGlueOptions = {
00032|       inputValue: '',
00033|       popupState: null,
00034|       helpOpen: false,
00035|       isPopupOpen: false,
00036|       isCommandMode: false,
00037|       isCommandMenuActive: false,
00038|       isGenerating: false,
00039|       droppedFilePath: null,
00040|       files: [],
00041|       urls: [],
00042|       images: [],
00043|       videos: [],
00044|       smartContextEnabled: false,
00045|       smartContextRoot: null,
00046|       addFile: jest.fn(),
00047|       removeFile: jest.fn(),
00048|       addUrl: jest.fn(),
00049|       removeUrl: jest.fn(),
00050|       updateUrl: jest.fn(),
00051|       addImage: jest.fn(),
00052|       removeImage: jest.fn(),
00053|       addVideo: jest.fn(),
00054|       removeVideo: jest.fn(),
00055|       toggleSmartContext: jest.fn(),
00056|       setSmartRoot: jest.fn(),
00057|       setInputValue: jest.fn(),
00058|       setPopupState: jest.fn(),
00059|       suppressNextInput: jest.fn(),
00060|       notify: jest.fn(),
00061|       pushHistory: jest.fn(),
00062|       addCommandHistoryEntry: jest.fn(),
00063|       handleCommandSelection: jest.fn(),
00064|       consumeSuppressedTextInputChange: jest.fn(() => false),
00065|       isFilePath: jest.fn(() => true),
00066|     }
00067| 
00068|     const { result } = renderHook(() => useContextPopupGlue(options))
00069| 
00070|     const expectedKeys = [
00071|       'filePopupSuggestions',
00072|       'filePopupSuggestionSelectionIndex',
00073|       'filePopupSuggestionsFocused',
00074|       'onFilePopupDraftChange',
00075|       'onAddFile',
00076|       'onRemoveFile',
00077|       'onUrlPopupDraftChange',
00078|       'onAddUrl',
00079|       'onRemoveUrl',
00080|       'imagePopupSuggestions',
00081|       'imagePopupSuggestionSelectionIndex',
(File has more lines. Use 'offset' parameter to read beyond line 81)
</file>
```
```
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/popup-manager-command-mapping.test.ts",
  "offset": 1,
  "limit": 40
}
```
**Output:**
```
<file>
00002|   JSON_INTERACTIVE_ERROR,
00003|   mapPopupCommandSelection,
00004|   type CommandMappingContext,
00005| } from '../../tui/hooks/popup-manager/command-mapping'
00006| 
00007| describe('popup manager command mapping', () => {
00008|   const baseContext: CommandMappingContext = {
00009|     copyEnabled: false,
00010|     chatGptEnabled: false,
00011|     jsonOutputEnabled: false,
00012|     interactiveTransportPath: undefined,
00013|     urls: [],
00014|     images: [],
00015|     videos: [],
00016|     smartContextEnabled: false,
00017|     smartContextRoot: null,
00018|   }
00019| 
00020|   it('blocks /json when interactive transport is active', () => {
00021|     const result = mapPopupCommandSelection({
00022|       commandId: 'json',
00023|       argsRaw: undefined,
00024|       context: {
00025|         ...baseContext,
00026|         interactiveTransportPath: '/tmp/socket',
00027|       },
00028|     })
00029| 
00030|     expect(result).toEqual({
00031|       kind: 'steps',
00032|       steps: [
00033|         { type: 'push-history', message: JSON_INTERACTIVE_ERROR, kind: 'system' },
00034|         { type: 'set-input', value: '' },
00035|       ],
00036|     })
00037|   })
00038| 
00039|   it('maps /url args into add/warn steps with dedupe', () => {
00040|     const result = mapPopupCommandSelection({
00041|       commandId: 'url',
(File has more lines. Use 'offset' parameter to read beyond line 41)
</file>
```
```
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/usePopupManager.test.ts",
  "offset": 1,
  "limit": 80
}
```
**Output:**
```
<file>
00002| import { JSDOM } from 'jsdom'
00003| import type { MutableRefObject } from 'react'
00004| 
00005| import { resetRecentSessionModelsForTests } from '../tui/model-session'
00006| import { usePopupManager } from '../tui/hooks/usePopupManager'
00007| import type { UsePopupManagerOptions } from '../tui/hooks/usePopupManager'
00008| import type { ModelOption } from '../tui/types'
00009| 
00010| jest.mock('../tui/file-suggestions', () => ({
00011|   discoverDirectorySuggestions: jest.fn(),
00012|   discoverFileSuggestions: jest.fn(),
00013| }))
00014| 
00015| jest.mock('node:fs/promises', () => ({
00016|   readFile: jest.fn(),
00017| }))
00018| 
00019| const dom = new JSDOM('<!doctype html><html><body></body></html>')
00020| const globalScope = globalThis as typeof globalThis & {
00021|   window: Window & typeof globalThis
00022|   document: Document
00023|   navigator: Navigator
00024| }
00025| 
00026| globalScope.window = dom.window
00027| globalScope.document = dom.window.document
00028| globalScope.navigator = dom.window.navigator
00029| 
00030| beforeEach(() => {
00031|   resetRecentSessionModelsForTests()
00032| })
00033| 
00034| const defaultModelOptions: ModelOption[] = [
00035|   {
00036|     id: 'gpt-4o-mini',
00037|     label: 'gpt-4o-mini',
00038|     provider: 'openai',
00039|     description: 'test',
00040|     capabilities: [],
00041|     source: 'builtin',
00042|   },
00043| ]
00044| 
00045| const createOptions = (overrides: Partial<UsePopupManagerOptions> = {}): UsePopupManagerOptions => {
00046|   const baseRef: MutableRefObject<string | null> = { current: null }
00047| 
00048|   const defaults: UsePopupManagerOptions = {
00049|     currentModel: 'gpt-4o-mini',
00050|     currentTargetModel: 'gpt-4o-mini',
00051|     modelOptions: defaultModelOptions,
00052|     activeThemeName: 'pm-dark',
00053|     themeMode: 'dark',
00054|     themes: [
00055|       { name: 'pm-dark', label: 'Prompt Maker Dark' },
00056|       { name: 'pm-light', label: 'Prompt Maker Light' },
00057|     ],
00058|     smartContextEnabled: false,
00059|     smartContextRoot: null,
00060|     toggleSmartContext: jest.fn(),
00061|     setSmartRoot: jest.fn(),
00062|     urls: [],
00063|     addUrl: jest.fn(),
00064|     images: [],
00065|     videos: [],
00066|     addImage: jest.fn(),
00067|     addVideo: jest.fn(),
00068|     lastTestFile: null,
00069|     defaultTestFile: 'prompt.test.ts',
00070|     interactiveTransportPath: undefined,
00071|     isGenerating: false,
00072|     lastUserIntentRef: baseRef,
00073|     pushHistory: jest.fn(),
00074|     notify: jest.fn(),
00075|     setInputValue: jest.fn(),
00076|     runGeneration: jest.fn(async () => undefined),
00077|     runSeriesGeneration: jest.fn(),
00078|     runTestsFromCommand: jest.fn(),
00079|     exitApp: jest.fn(),
00080|     setCurrentModel: jest.fn(),
00081|     setCurrentTargetModel: jest.fn(),
(File has more lines. Use 'offset' parameter to read beyond line 81)
</file>
```
```
---
## Assistant
_Thinking:_
**Patching tests for PDF support**
I'm looking to make necessary changes for various tests. For the command-screen-popup-bindings-contract test, Iâll add pdfs and include mocks for addPdf/removePdf. I also need to update the context-popup-glue-contract test similarly, ensuring the expectedKeys list has the new pdf keys. For the popup-manager-command-mapping test, I'll add pdfs to the baseContext. Finally, in the usePopupManager test, I'll make sure createOptions includes pdfs and modify the type if needed. Letâs get these edits applied!
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/command-screen-popup-bindings-contract.test.ts",
  "oldString": "        images: [],\n        videos: [],\n",
  "newString": "        images: [],\n        videos: [],\n        pdfs: [],\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/command-screen-popup-bindings-contract.test.ts",
  "oldString": "        addVideo: jest.fn(),\n        removeVideo: jest.fn(),\n",
  "newString": "        addVideo: jest.fn(),\n        removeVideo: jest.fn(),\n        addPdf: jest.fn(),\n        removePdf: jest.fn(),\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/popup-manager-command-mapping.test.ts",
  "oldString": "    images: [],\n    videos: [],\n",
  "newString": "    images: [],\n    videos: [],\n    pdfs: [],\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/usePopupManager.test.ts",
  "oldString": "    images: [],\n    videos: [],\n    addImage: jest.fn(),\n    addVideo: jest.fn(),\n",
  "newString": "    images: [],\n    videos: [],\n    pdfs: [],\n    addImage: jest.fn(),\n    addVideo: jest.fn(),\n    addPdf: jest.fn(),\n"
}
```
```
---
## Assistant
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/context-popup-glue-contract.test.ts",
  "oldString": "      images: [],\n      videos: [],\n",
  "newString": "      images: [],\n      videos: [],\n      pdfs: [],\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [31:11] Type '{ inputValue: string; popupState: null; helpOpen: false; isPopupOpen: false; isCommandMode: false; isCommandMenuActive: false; isGenerating: false; droppedFilePath: null; files: never[]; urls: never[]; ... 24 more ...; isFilePath: Mock<...>; }' is missing the following properties from type 'UseContextPopupGlueOptions': addPdf, removePdf
</file_diagnostics>
```
```
---
## Assistant
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/context-popup-glue-contract.test.ts",
  "oldString": "      addVideo: jest.fn(),\n      removeVideo: jest.fn(),\n",
  "newString": "      addVideo: jest.fn(),\n      removeVideo: jest.fn(),\n      addPdf: jest.fn(),\n      removePdf: jest.fn(),\n"
}
```
```
---
## Assistant
_Thinking:_
**Updating expected keys**
I need to update the expectedKeys list to include the PDF keys. First, Iâll locate where this list exists in the file. Iâll search for `videoPopupSuggestions` and other relevant snippets. Once I find the section around expectedKeys, I can make the necessary edits to insert the PDF keys right after the video ones. It's a small but important detail to get right!
```
Tool: grep
**Input:**
```json
{
  "pattern": "'videoPopupSuggestions'|'onRemoveVideo'|'smartPopupSuggestions'",
  "include": "src/__tests__/tui/context-popup-glue-contract.test.ts"
}
```
**Output:**
```
Found 3 matches
/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/context-popup-glue-contract.test.ts:
  Line 89:       'videoPopupSuggestions',
  Line 94:       'onRemoveVideo',
  Line 95:       'smartPopupSuggestions',
```
```
---
## Assistant
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/context-popup-glue-contract.test.ts",
  "offset": 80,
  "limit": 30
}
```
**Output:**
```
<file>
00081|       'onAddUrl',
00082|       'onRemoveUrl',
00083|       'imagePopupSuggestions',
00084|       'imagePopupSuggestionSelectionIndex',
00085|       'imagePopupSuggestionsFocused',
00086|       'onImagePopupDraftChange',
00087|       'onAddImage',
00088|       'onRemoveImage',
00089|       'videoPopupSuggestions',
00090|       'videoPopupSuggestionSelectionIndex',
00091|       'videoPopupSuggestionsFocused',
00092|       'onVideoPopupDraftChange',
00093|       'onAddVideo',
00094|       'onRemoveVideo',
00095|       'smartPopupSuggestions',
00096|       'smartPopupSuggestionSelectionIndex',
00097|       'smartPopupSuggestionsFocused',
00098|       'onSmartPopupDraftChange',
00099|       'onSmartToggle',
00100|       'onSmartRootSubmit',
00101|     ].sort()
00102| 
00103|     expect(Object.keys(result.current).sort()).toEqual(expectedKeys)
00104| 
00105|     expect(() => result.current.onFilePopupDraftChange('x')).not.toThrow()
00106|     expect(() => result.current.onAddFile('/tmp/file.txt')).not.toThrow()
00107|     expect(() => result.current.onRemoveFile(0)).not.toThrow()
00108| 
00109|     expect(() => result.current.onUrlPopupDraftChange('https://example.com')).not.toThrow()
00110|     expect(() => result.current.onAddUrl('https://example.com')).not.toThrow()
(File has more lines. Use 'offset' parameter to read beyond line 110)
</file>
```
```
---
## Assistant
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/context-popup-glue-contract.test.ts",
  "oldString": "      'onAddVideo',\n      'onRemoveVideo',\n      'smartPopupSuggestions',\n",
  "newString": "      'onAddVideo',\n      'onRemoveVideo',\n      'pdfPopupSuggestions',\n      'pdfPopupSuggestionSelectionIndex',\n      'pdfPopupSuggestionsFocused',\n      'onPdfPopupDraftChange',\n      'onAddPdf',\n      'onRemovePdf',\n      'smartPopupSuggestions',\n"
}
```
```
---
## Assistant
_Thinking:_
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 80,
  "limit": 80
}
```
**Output:**
```
<file>
00081|     setLastReasoning,
00082|     setLastGeneratedPrompt,
00083|     resetContext,
00084|   } = useContextDispatch()
00085| 
00086|   const historyAndTests = useCommandScreenHistoryAndTests()
00087| 
00088|   const inputState = useCommandScreenInputState({
00089|     pushHistoryProxy: historyAndTests.pushHistoryProxy,
00090|   })
00091| 
00092|   const modelAndGeneration: UseCommandScreenModelGenerationResult = useCommandScreenModelGeneration(
00093|     {
00094|       pushHistoryProxy: historyAndTests.pushHistoryProxy,
00095|       notify,
00096|       files,
00097|       urls,
00098|       images,
00099|       videos,
00100|       smartContextEnabled,
00101|       smartContextRoot,
00102|       metaInstructions,
00103|       budgets: { maxContextTokens, maxInputTokens, contextOverflowStrategy },
00104|       ...(interactiveTransportPath ? { interactiveTransportPath } : {}),
00105|       terminalColumns: inputState.terminalColumns,
00106|       copyEnabled: inputState.copyEnabled,
00107|       chatGptEnabled: inputState.chatGptEnabled,
00108|       jsonOutputEnabled: inputState.jsonOutputEnabled,
00109|       isTestCommandRunning: historyAndTests.isTestCommandRunning,
00110|       setLastReasoning,
00111|       setLastGeneratedPrompt,
00112|     },
00113|   )
00114| 
00115|   const contextOptions: UseCommandScreenPopupAndViewOptions['context'] = useMemo(
00116|     () => ({
00117|       interactiveTransportPath,
00118|       notify,
00119|       stdout,
00120|       files,
00121|       urls,
00122|       images,
00123|       videos,
00124|       smartContextEnabled,
00125|       smartContextRoot,
00126|       metaInstructions,
00127|       maxContextTokens,
00128|       maxInputTokens,
00129|       contextOverflowStrategy,
00130|       lastReasoning,
00131|       lastGeneratedPrompt,
00132|       addFile,
00133|       removeFile,
00134|       addUrl,
00135|       removeUrl,
00136|       updateUrl,
00137|       addImage,
00138|       removeImage,
00139|       addVideo,
00140|       removeVideo,
00141|       toggleSmartContext,
00142|       setSmartRoot,
00143|       setMetaInstructions,
00144|       setBudgets,
00145|       resetContext,
00146|     }),
00147|     [
00148|       interactiveTransportPath,
00149|       notify,
00150|       stdout,
00151|       files,
00152|       urls,
00153|       images,
00154|       videos,
00155|       smartContextEnabled,
00156|       smartContextRoot,
00157|       metaInstructions,
00158|       maxContextTokens,
00159|       maxInputTokens,
00160|       contextOverflowStrategy,
(File has more lines. Use 'offset' parameter to read beyond line 160)
</file>
```
```
---
## Assistant
_Thinking:_
```
Tool: grep
**Input:**
```json
{
  "pattern": "const \\{[^}]*videos[^}]*\\} = useContextState",
  "include": "src/tui/screens/command/hooks/useCommandScreenController.ts"
}
```
**Output:**
```
No files found
```
```
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 20,
  "limit": 70
}
```
**Output:**
```
<file>
00021| export type UseCommandScreenControllerOptions = {
00022|   transport?: {
00023|     interactiveTransportPath?: string | undefined
00024|   }
00025|   popup: {
00026|     onPopupVisibilityChange?: (isOpen: boolean) => void
00027|     commandMenuSignal?: number
00028|     helpOpen: boolean
00029|     reservedRows: number
00030|   }
00031|   notify: (message: string, options?: NotifyOptions) => void
00032| }
00033| 
00034| export type UseCommandScreenControllerResult = {
00035|   view: UseCommandScreenPopupAndViewResult
00036|   actions: {
00037|     suppressNextInput: () => void
00038|   }
00039| }
00040| 
00041| export const useCommandScreenController = ({
00042|   transport,
00043|   popup,
00044|   notify,
00045| }: UseCommandScreenControllerOptions): UseCommandScreenControllerResult => {
00046|   const { stdout } = useStdout()
00047| 
00048|   const interactiveTransportPath = transport?.interactiveTransportPath
00049| 
00050|   const { onPopupVisibilityChange, commandMenuSignal, helpOpen, reservedRows } = popup
00051| 
00052|   const {
00053|     files,
00054|     urls,
00055|     images,
00056|     videos,
00057|     smartContextEnabled,
00058|     smartContextRoot,
00059|     metaInstructions,
00060|     maxContextTokens,
00061|     maxInputTokens,
00062|     contextOverflowStrategy,
00063|     lastReasoning,
00064|     lastGeneratedPrompt,
00065|   } = useContextState()
00066| 
00067|   const {
00068|     addFile,
00069|     removeFile,
00070|     addUrl,
00071|     removeUrl,
00072|     updateUrl,
00073|     addImage,
00074|     removeImage,
00075|     addVideo,
00076|     removeVideo,
00077|     toggleSmartContext,
00078|     setSmartRoot,
00079|     setMetaInstructions,
00080|     setBudgets,
00081|     setLastReasoning,
00082|     setLastGeneratedPrompt,
00083|     resetContext,
00084|   } = useContextDispatch()
00085| 
00086|   const historyAndTests = useCommandScreenHistoryAndTests()
00087| 
00088|   const inputState = useCommandScreenInputState({
00089|     pushHistoryProxy: historyAndTests.pushHistoryProxy,
00090|   })
(File has more lines. Use 'offset' parameter to read beyond line 90)
</file>
```
```
---
## Assistant
```
Tool: todowrite
**Input:**
```json
{
  "todos": [
    {
      "content": "Add PDF state to TUI context",
      "status": "completed",
      "priority": "high",
      "id": "1"
    },
    {
      "content": "Add PDF popup UI and commands",
      "status": "in_progress",
      "priority": "high",
      "id": "2"
    },
    {
      "content": "Wire TUI generation pipeline PDFs",
      "status": "pending",
      "priority": "high",
      "id": "3"
    },
    {
      "content": "Implement OpenAI PDF text extraction",
      "status": "pending",
      "priority": "high",
      "id": "4"
    },
    {
      "content": "Update tests for TUI and OpenAI",
      "status": "pending",
      "priority": "high",
      "id": "5"
    },
    {
      "content": "Run typecheck/tests/build",
      "status": "pending",
      "priority": "high",
      "id": "6"
    }
  ]
}
```
**Output:**
```
[
  {
    "content": "Add PDF state to TUI context",
    "status": "completed",
    "priority": "high",
    "id": "1"
  },
  {
    "content": "Add PDF popup UI and commands",
    "status": "in_progress",
    "priority": "high",
    "id": "2"
  },
  {
    "content": "Wire TUI generation pipeline PDFs",
    "status": "pending",
    "priority": "high",
    "id": "3"
  },
  {
    "content": "Implement OpenAI PDF text extraction",
    "status": "pending",
    "priority": "high",
    "id": "4"
  },
  {
    "content": "Update tests for TUI and OpenAI",
    "status": "pending",
    "priority": "high",
    "id": "5"
  },
  {
    "content": "Run typecheck/tests/build",
    "status": "pending",
    "priority": "high",
    "id": "6"
  }
]
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "    images,\n    videos,\n",
  "newString": "    images,\n    videos,\n    pdfs,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [79:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [80:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [82:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [83:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [122:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "    addVideo,\n    removeVideo,\n",
  "newString": "    addVideo,\n    removeVideo,\n    addPdf,\n    removePdf,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [79:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [80:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [82:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [83:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [122:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "       images,\n       videos,\n       smartContextEnabled,\n",
  "newString": "       images,\n       videos,\n       pdfs,\n       smartContextEnabled,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [79:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [80:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [82:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [83:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [122:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "       images,\n       videos,\n       smartContextEnabled,\n",
  "newString": "       images,\n       videos,\n       pdfs,\n       smartContextEnabled,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [79:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [80:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [82:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [83:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [122:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "       addVideo,\n       removeVideo,\n",
  "newString": "       addVideo,\n       removeVideo,\n       addPdf,\n       removePdf,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [79:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [80:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [82:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [83:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [122:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
**Reviewing code duplicates**
I noticed that we accidentally included addPdf/removePdf twice in the dispatch destructuring within contextOptions and/or modelAndGeneration options. I think I need to review lines 70-85 after the edits to identify the duplicates. Itâs a bit annoying when that happens, but itâs good to catch it early. Iâll make sure to double-check those areas and clean it up properly to avoid any issues down the line.
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 64,
  "limit": 30
}
```
**Output:**
```
<file>
00065|     lastReasoning,
00066|     lastGeneratedPrompt,
00067|   } = useContextState()
00068| 
00069|   const {
00070|     addFile,
00071|     removeFile,
00072|     addUrl,
00073|     removeUrl,
00074|     updateUrl,
00075|     addImage,
00076|     removeImage,
00077|     addVideo,
00078|     removeVideo,
00079|     addPdf,
00080|     removePdf,
00081| 
00082|     addPdf,
00083|     removePdf,
00084|     toggleSmartContext,
00085|     setSmartRoot,
00086|     setMetaInstructions,
00087|     setBudgets,
00088|     setLastReasoning,
00089|     setLastGeneratedPrompt,
00090|     resetContext,
00091|   } = useContextDispatch()
00092| 
00093|   const historyAndTests = useCommandScreenHistoryAndTests()
00094| 
(File has more lines. Use 'offset' parameter to read beyond line 94)
</file>
```
```
---
## Assistant
_Thinking:_
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "    addPdf,\n    removePdf,\n\n    addPdf,\n    removePdf,\n    toggleSmartContext,\n",
  "newString": "    addPdf,\n    removePdf,\n    toggleSmartContext,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [119:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
**Fixing context options**
Iâm looking at the error regarding the contextOptions object missing pdfs and the addPdf/removePdf functions. It seems we attempted to address this, but maybe they werenât inserted correctly or I placed them in the wrong location. I think it would be best to read the definition of contextOptions after line 115 to see where I might have gone wrong. It might help clarify how to properly include those functions!
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 112,
  "limit": 50
}
```
**Output:**
```
<file>
00113|       isTestCommandRunning: historyAndTests.isTestCommandRunning,
00114|       setLastReasoning,
00115|       setLastGeneratedPrompt,
00116|     },
00117|   )
00118| 
00119|   const contextOptions: UseCommandScreenPopupAndViewOptions['context'] = useMemo(
00120|     () => ({
00121|       interactiveTransportPath,
00122|       notify,
00123|       stdout,
00124|       files,
00125|       urls,
00126|       images,
00127|       videos,
00128|       smartContextEnabled,
00129|       smartContextRoot,
00130|       metaInstructions,
00131|       maxContextTokens,
00132|       maxInputTokens,
00133|       contextOverflowStrategy,
00134|       lastReasoning,
00135|       lastGeneratedPrompt,
00136|       addFile,
00137|       removeFile,
00138|       addUrl,
00139|       removeUrl,
00140|       updateUrl,
00141|       addImage,
00142|       removeImage,
00143|       addVideo,
00144|       removeVideo,
00145|       toggleSmartContext,
00146|       setSmartRoot,
00147|       setMetaInstructions,
00148|       setBudgets,
00149|       resetContext,
00150|     }),
00151|     [
00152|       interactiveTransportPath,
00153|       notify,
00154|       stdout,
00155|       files,
00156|       urls,
00157|       images,
00158|       videos,
00159|       smartContextEnabled,
00160|       smartContextRoot,
00161|       metaInstructions,
00162|       maxContextTokens,
(File has more lines. Use 'offset' parameter to read beyond line 162)
</file>
```
```
---
## Assistant
_Thinking:_
**Reviewing state structure**
Iâm noticing that the PDFs are missing from the state destructure we did earlier. We added the useContextState, which was great, but it seems like we didnât insert the PDFs properly in the state destructuring part. It might be that we overlooked this step. Let me check those earlier lines, like line 52 and see if I can find what went wrong. I want to make sure everything is accurately set up!
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 48,
  "limit": 20
}
```
**Output:**
```
<file>
00049| 
00050|   const { onPopupVisibilityChange, commandMenuSignal, helpOpen, reservedRows } = popup
00051| 
00052|   const {
00053|     files,
00054|     urls,
00055|     images,
00056|     videos,
00057|     pdfs,
00058|     smartContextEnabled,
00059| 
00060|     smartContextRoot,
00061|     metaInstructions,
00062|     maxContextTokens,
00063|     maxInputTokens,
00064|     contextOverflowStrategy,
00065|     lastReasoning,
00066|     lastGeneratedPrompt,
00067|   } = useContextState()
00068| 
(File has more lines. Use 'offset' parameter to read beyond line 68)
</file>
```
```
---
## Assistant
_Thinking:_
**Fixing context options**
I noticed thereâs a blank line after `smartContextEnabled`. I need to make sure to add PDFs in the `contextOptions` object and in the dependency list. I also have to include the `addPdf` and `removePdf` functions, as well as the PDFs. This is crucial for everything to work seamlessly! Let's patch these details up and get everything sorted out. I want to make sure I've covered all my bases!
```
Tool: grep
**Input:**
```json
{
  "pattern": "images,\\n\\s*videos,\\n\\s*smartContextEnabled",
  "include": "src/tui/screens/command/hooks/useCommandScreenController.ts"
}
```
**Error:**
```
Error: ripgrep failed: rg: the literal "\n" is not allowed in a regex
Consider enabling multiline mode with the --multiline flag (or -U for short).
When multiline mode is enabled, new line characters can be matched.
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      addVideo,\n      removeVideo,\n      toggleSmartContext,\n",
  "newString": "      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n      smartContextRoot,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n      smartContextRoot,\n"
}
```
**Error:**
```
Error: Found multiple matches for oldString. Provide more surrounding lines in oldString to identify the correct match.
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
**Adjusting code edits**
I noticed some edits failed due to multiple matches, so I plan to make targeted edits with a clearer context. In my modelAndGeneration call on line 92, it seems like I may have mistakenly added PDFs earlier. The error is related to a contextOptions type mismatch that I need to focus on. I'll modify the contextOptions object on line 120 to include PDFs and the addPdf/removePdf functions after removeVideo, along with updating the dependencies array.
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 118,
  "limit": 40
}
```
**Output:**
```
<file>
00119| 
00120|   const contextOptions: UseCommandScreenPopupAndViewOptions['context'] = useMemo(
00121|     () => ({
00122|       interactiveTransportPath,
00123|       notify,
00124|       stdout,
00125|       files,
00126|       urls,
00127|       images,
00128|       videos,
00129|       smartContextEnabled,
00130|       smartContextRoot,
00131|       metaInstructions,
00132|       maxContextTokens,
00133|       maxInputTokens,
00134|       contextOverflowStrategy,
00135|       lastReasoning,
00136|       lastGeneratedPrompt,
00137|       addFile,
00138|       removeFile,
00139|       addUrl,
00140|       removeUrl,
00141|       updateUrl,
00142|       addImage,
00143|       removeImage,
00144|       addVideo,
00145|       removeVideo,
00146|       toggleSmartContext,
00147|       setSmartRoot,
00148|       setMetaInstructions,
00149|       setBudgets,
00150|       resetContext,
00151|     }),
00152|     [
00153|       interactiveTransportPath,
00154|       notify,
00155|       stdout,
00156|       files,
00157|       urls,
00158|       images,
(File has more lines. Use 'offset' parameter to read beyond line 158)
</file>
```
```
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 150,
  "limit": 30
}
```
**Output:**
```
<file>
00151|     }),
00152|     [
00153|       interactiveTransportPath,
00154|       notify,
00155|       stdout,
00156|       files,
00157|       urls,
00158|       images,
00159|       videos,
00160|       smartContextEnabled,
00161|       smartContextRoot,
00162|       metaInstructions,
00163|       maxContextTokens,
00164|       maxInputTokens,
00165|       contextOverflowStrategy,
00166|       lastReasoning,
00167|       lastGeneratedPrompt,
00168|       addFile,
00169|       removeFile,
00170|       addUrl,
00171|       removeUrl,
00172|       updateUrl,
00173|       addImage,
00174|       removeImage,
00175|       addVideo,
00176|       removeVideo,
00177|       toggleSmartContext,
00178|       setSmartRoot,
00179|       setMetaInstructions,
00180|       setBudgets,
(File has more lines. Use 'offset' parameter to read beyond line 180)
</file>
```
```
---
## Assistant
```
Tool: glob
**Input:**
```json
{
  "pattern": "noop",
  "path": "/Users/eroberts/Projects/prompt-maker/src"
}
```
**Output:**
```
No files found
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      addVideo,\n      removeVideo,\n      toggleSmartContext,\n",
  "newString": "      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n      smartContextRoot,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n      smartContextRoot,\n"
}
```
**Error:**
```
Error: Found multiple matches for oldString. Provide more surrounding lines in oldString to identify the correct match.
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n      smartContextRoot,\n      metaInstructions,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n      smartContextRoot,\n      metaInstructions,\n"
}
```
**Error:**
```
Error: Found multiple matches for oldString. Provide more surrounding lines in oldString to identify the correct match.
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n      smartContextRoot,\n      metaInstructions,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n      smartContextRoot,\n      metaInstructions,\n"
}
```
**Error:**
```
Error: Found multiple matches for oldString. Provide more surrounding lines in oldString to identify the correct match.
```
```
---
## Assistant
_Thinking:_
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      stdout,\n      files,\n      urls,\n      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "      stdout,\n      files,\n      urls,\n      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
**Error:**
```
Error: Found multiple matches for oldString. Provide more surrounding lines in oldString to identify the correct match.
```
```
---
## Assistant
_Thinking:_
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "    () => ({\n      interactiveTransportPath,\n      notify,\n      stdout,\n      files,\n      urls,\n      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "    () => ({\n      interactiveTransportPath,\n      notify,\n      stdout,\n      files,\n      urls,\n      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 25 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': addPdf, removePdf
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
I'm noting that the PDFs are now included, which is great! However, I realize that the addPdf and removePdf features seem to be missing. I need to think about how to implement those properly, perhaps by patching the object in a similar way to what I did after removing the video. Thereâs a bit of organization needed here, but I feel like Iâm on the right track. Let's ensure everything aligns well!
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      addVideo,\n      removeVideo,\n      toggleSmartContext,\n",
  "newString": "      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 25 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': addPdf, removePdf
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
**Verifying patch application**
Iâm thinking that the patch might not have applied correctly since there could be multiple matches in play. I definitely want to double-check this by reading the specific snippet to understand better what's going on. It's important to ensure I have the correct context before moving forward. I just need to make sure Iâm thorough in this verification process to get things right!
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 118,
  "limit": 35
}
```
**Output:**
```
<file>
00119| 
00120|   const contextOptions: UseCommandScreenPopupAndViewOptions['context'] = useMemo(
00121|     () => ({
00122|       interactiveTransportPath,
00123|       notify,
00124|       stdout,
00125|       files,
00126|       urls,
00127|       images,
00128|       videos,
00129|       pdfs,
00130|       smartContextEnabled,
00131|       smartContextRoot,
00132|       metaInstructions,
00133|       maxContextTokens,
00134|       maxInputTokens,
00135|       contextOverflowStrategy,
00136|       lastReasoning,
00137|       lastGeneratedPrompt,
00138|       addFile,
00139|       removeFile,
00140|       addUrl,
00141|       removeUrl,
00142|       updateUrl,
00143|       addImage,
00144|       removeImage,
00145|       addVideo,
00146|       removeVideo,
00147|       toggleSmartContext,
00148|       setSmartRoot,
00149|       setMetaInstructions,
00150|       setBudgets,
00151|       resetContext,
00152|     }),
00153|     [
(File has more lines. Use 'offset' parameter to read beyond line 153)
</file>
```
```
---
## Assistant
_Thinking:_
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      addVideo,\n      removeVideo,\n      toggleSmartContext,\n",
  "newString": "      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 25 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': addPdf, removePdf
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
**Clarifying context**
It seems like there's an issue where a patch didn't apply, possibly due to multiple matches. I should provide additional context to make sure everything is unique. Maybe I need to include the preceding `addImage` and other relevant details to clarify what's going on. Ensuring that the information is complete will help identify any potential problems, so let's make sure I'm being thorough!
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      updateUrl,\n      addImage,\n      removeImage,\n      addVideo,\n      removeVideo,\n      toggleSmartContext,\n",
  "newString": "      updateUrl,\n      addImage,\n      removeImage,\n      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 25 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': addPdf, removePdf
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
**Reviewing possible patches**
Iâm noticing thereâs still an error, but it looks like a patch might have been applied. I think itâs important to take a moment to re-read the snippet to uncover what might have gone wrong. Maybe Iâll find a clue that helps me determine what needs to be adjusted. There's a lot to think about here, and I definitely want to get this right! Let's see what I can find in that snippet.
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 138,
  "limit": 18
}
```
**Output:**
```
<file>
00139|       removeFile,
00140|       addUrl,
00141|       removeUrl,
00142|       updateUrl,
00143|       addImage,
00144|       removeImage,
00145|       addVideo,
00146|       removeVideo,
00147|       toggleSmartContext,
00148|       setSmartRoot,
00149|       setMetaInstructions,
00150|       setBudgets,
00151|       resetContext,
00152|     }),
00153|     [
00154|       interactiveTransportPath,
00155|       notify,
00156|       stdout,
(File has more lines. Use 'offset' parameter to read beyond line 156)
</file>
```
```
---
## Assistant
_Thinking:_
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      removeVideo,\n      toggleSmartContext,\n",
  "newString": "      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 25 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': addPdf, removePdf
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 140,
  "limit": 12
}
```
**Output:**
```
<file>
00141|       removeUrl,
00142|       updateUrl,
00143|       addImage,
00144|       removeImage,
00145|       addVideo,
00146|       removeVideo,
00147|       toggleSmartContext,
00148|       setSmartRoot,
00149|       setMetaInstructions,
00150|       setBudgets,
00151|       resetContext,
00152|     }),
(File has more lines. Use 'offset' parameter to read beyond line 152)
</file>
```
```
---
## Assistant
_Thinking:_
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      addVideo,\n      removeVideo,\n      toggleSmartContext,\n      setSmartRoot,\n",
  "newString": "      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n      setSmartRoot,\n"
}
```
**Error:**
```
Error: Found multiple matches for oldString. Provide more surrounding lines in oldString to identify the correct match.
```
```
---
## Assistant
_Thinking:_
**Refining target strings**
I realize that the oldString shows up multiple times, including in the dependency array. To make sure I target the object section uniquely, I need to incorporate previous lines like `lastGeneratedPrompt`. By including `lastGeneratedPrompt` earlier, I can create a unique identification. It feels like a fun little puzzle to solve â I just want to make sure I'm handling it well! Let's patch it up to ensure clarity.
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      lastReasoning,\n      lastGeneratedPrompt,\n      addFile,\n      removeFile,\n      addUrl,\n      removeUrl,\n      updateUrl,\n      addImage,\n      removeImage,\n      addVideo,\n      removeVideo,\n      toggleSmartContext,\n      setSmartRoot,\n",
  "newString": "      lastReasoning,\n      lastGeneratedPrompt,\n      addFile,\n      removeFile,\n      addUrl,\n      removeUrl,\n      updateUrl,\n      addImage,\n      removeImage,\n      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n      setSmartRoot,\n"
}
```
**Error:**
```
Error: Found multiple matches for oldString. Provide more surrounding lines in oldString to identify the correct match.
```
```
---
## Assistant
_Thinking:_
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "    () => ({\n      interactiveTransportPath,\n      notify,\n      stdout,\n      files,\n      urls,\n      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n      smartContextRoot,\n      metaInstructions,\n      maxContextTokens,\n      maxInputTokens,\n      contextOverflowStrategy,\n      lastReasoning,\n      lastGeneratedPrompt,\n      addFile,\n      removeFile,\n      addUrl,\n      removeUrl,\n      updateUrl,\n      addImage,\n      removeImage,\n      addVideo,\n      removeVideo,\n      toggleSmartContext,\n      setSmartRoot,\n      setMetaInstructions,\n      setBudgets,\n      resetContext,\n    }),\n",
  "newString": "    () => ({\n      interactiveTransportPath,\n      notify,\n      stdout,\n      files,\n      urls,\n      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n      smartContextRoot,\n      metaInstructions,\n      maxContextTokens,\n      maxInputTokens,\n      contextOverflowStrategy,\n      lastReasoning,\n      lastGeneratedPrompt,\n      addFile,\n      removeFile,\n      addUrl,\n      removeUrl,\n      updateUrl,\n      addImage,\n      removeImage,\n      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n      setSmartRoot,\n      setMetaInstructions,\n      setBudgets,\n      resetContext,\n    }),\n"
}
```
```
---
## Assistant
_Thinking:_
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 150,
  "limit": 40
}
```
**Output:**
```
<file>
00151|       setMetaInstructions,
00152|       setBudgets,
00153|       resetContext,
00154|     }),
00155|     [
00156|       interactiveTransportPath,
00157|       notify,
00158|       stdout,
00159|       files,
00160|       urls,
00161|       images,
00162|       videos,
00163|       smartContextEnabled,
00164|       smartContextRoot,
00165|       metaInstructions,
00166|       maxContextTokens,
00167|       maxInputTokens,
00168|       contextOverflowStrategy,
00169|       lastReasoning,
00170|       lastGeneratedPrompt,
00171|       addFile,
00172|       removeFile,
00173|       addUrl,
00174|       removeUrl,
00175|       updateUrl,
00176|       addImage,
00177|       removeImage,
00178|       addVideo,
00179|       removeVideo,
00180|       toggleSmartContext,
00181|       setSmartRoot,
00182|       setMetaInstructions,
00183|       setBudgets,
00184|       resetContext,
00185|     ],
00186|   )
00187| 
00188|   const inputOptions: UseCommandScreenPopupAndViewOptions['input'] = useMemo(
00189|     () => ({
00190|       terminalRows: inputState.terminalRows,
(File has more lines. Use 'offset' parameter to read beyond line 190)
</file>
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      addVideo,\n      removeVideo,\n      toggleSmartContext,\n",
  "newString": "      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n"
}
```
```
---
## Assistant
_Thinking:_
```
Tool: grep
**Input:**
```json
{
  "pattern": "popup\\.context|popup: \\{|imagePopupSuggestions|videoPopupSuggestions",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupAndView.ts"
}
```
**Output:**
```
Found 23 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupAndView.ts:
  Line 31:     popup: {
  Line 47:         filePopupSuggestions: bindings.popup.context.file.suggestions,
  Line 48:         filePopupSuggestionSelectionIndex: bindings.popup.context.file.suggestionSelectionIndex,
  Line 49:         filePopupSuggestionsFocused: bindings.popup.context.file.suggestionsFocused,
  Line 50:         onFilePopupDraftChange: bindings.popup.context.file.onDraftChange,
  Line 51:         onAddFile: bindings.popup.context.file.onAdd,
  Line 53:         onUrlPopupDraftChange: bindings.popup.context.url.onDraftChange,
  Line 54:         onAddUrl: bindings.popup.context.url.onAdd,
  Line 56:         imagePopupSuggestions: bindings.popup.context.image.suggestions,
  Line 57:         imagePopupSuggestionSelectionIndex: bindings.popup.context.image.suggestionSelectionIndex,
  Line 58:         imagePopupSuggestionsFocused: bindings.popup.context.image.suggestionsFocused,
  Line 59:         onImagePopupDraftChange: bindings.popup.context.image.onDraftChange,
  Line 60:         onAddImage: bindings.popup.context.image.onAdd,
  Line 62:         videoPopupSuggestions: bindings.popup.context.video.suggestions,
  Line 63:         videoPopupSuggestionSelectionIndex: bindings.popup.context.video.suggestionSelectionIndex,
  Line 64:         videoPopupSuggestionsFocused: bindings.popup.context.video.suggestionsFocused,
  Line 65:         onVideoPopupDraftChange: bindings.popup.context.video.onDraftChange,
  Line 66:         onAddVideo: bindings.popup.context.video.onAdd,
  Line 69:         smartPopupSuggestions: bindings.popup.context.smart.suggestions,
  Line 70:         smartPopupSuggestionSelectionIndex: bindings.popup.context.smart.suggestionSelectionIndex,
  Line 71:         smartPopupSuggestionsFocused: bindings.popup.context.smart.suggestionsFocused,
  Line 72:         onSmartPopupDraftChange: bindings.popup.context.smart.onDraftChange,
  Line 73:         onSmartRootSubmit: bindings.popup.context.smart.onRootSubmit,
```
```
---
## Assistant
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupAndView.ts",
  "offset": 1,
  "limit": 120
}
```
**Output:**
```
<file>
00002| import { useCommandScreenViewModel } from './useCommandScreenViewModel'
00003| 
00004| import type {
00005|   UseCommandScreenPopupAndViewOptions,
00006|   UseCommandScreenPopupAndViewResult,
00007| } from './useCommandScreenPopupAndView.types'
00008| 
00009| export const useCommandScreenPopupAndView = (
00010|   options: UseCommandScreenPopupAndViewOptions,
00011| ): UseCommandScreenPopupAndViewResult => {
00012|   const { context, input, popup, history, generation } = options
00013| 
00014|   const { popupManager, shell, bindings, enhancedStatusChips } = useCommandScreenBindings(options)
00015| 
00016|   const viewModel = useCommandScreenViewModel({
00017|     transport: { isAwaitingTransportInput: shell.isAwaitingTransportInput },
00018|     panes: {
00019|       history: {
00020|         lines: shell.history,
00021|         visibleRows: shell.historyRows,
00022|         scrollOffset: shell.scrollOffset,
00023|       },
00024|       menu: {
00025|         isActive: shell.isCommandMenuActive,
00026|         height: shell.menuHeight,
00027|         commands: shell.visibleCommands,
00028|         selectedIndex: input.commandSelectionIndex,
00029|       },
00030|     },
00031|     popup: {
00032|       base: {
00033|         popupState: popupManager.popupState,
00034|         helpOpen: popup.helpOpen,
00035|         overlayHeight: shell.overlayHeight,
00036|       },
00037|       model: {
00038|         modelPopupOptions: bindings.popup.model.options,
00039|         modelPopupSelection: bindings.popup.model.selection,
00040|         modelPopupRecentCount: bindings.popup.model.recentCount,
00041|         providerStatuses: generation.providerStatuses,
00042|         onModelPopupQueryChange: bindings.popup.model.onQueryChange,
00043|         onModelPopupSubmit: popupManager.actions.handleModelPopupSubmit,
00044|       },
00045|       context: {
00046|         files: context.files,
00047|         filePopupSuggestions: bindings.popup.context.file.suggestions,
00048|         filePopupSuggestionSelectionIndex: bindings.popup.context.file.suggestionSelectionIndex,
00049|         filePopupSuggestionsFocused: bindings.popup.context.file.suggestionsFocused,
00050|         onFilePopupDraftChange: bindings.popup.context.file.onDraftChange,
00051|         onAddFile: bindings.popup.context.file.onAdd,
00052|         urls: context.urls,
00053|         onUrlPopupDraftChange: bindings.popup.context.url.onDraftChange,
00054|         onAddUrl: bindings.popup.context.url.onAdd,
00055|         images: context.images,
00056|         imagePopupSuggestions: bindings.popup.context.image.suggestions,
00057|         imagePopupSuggestionSelectionIndex: bindings.popup.context.image.suggestionSelectionIndex,
00058|         imagePopupSuggestionsFocused: bindings.popup.context.image.suggestionsFocused,
00059|         onImagePopupDraftChange: bindings.popup.context.image.onDraftChange,
00060|         onAddImage: bindings.popup.context.image.onAdd,
00061|         videos: context.videos,
00062|         videoPopupSuggestions: bindings.popup.context.video.suggestions,
00063|         videoPopupSuggestionSelectionIndex: bindings.popup.context.video.suggestionSelectionIndex,
00064|         videoPopupSuggestionsFocused: bindings.popup.context.video.suggestionsFocused,
00065|         onVideoPopupDraftChange: bindings.popup.context.video.onDraftChange,
00066|         onAddVideo: bindings.popup.context.video.onAdd,
00067|         smartContextEnabled: context.smartContextEnabled,
00068|         smartContextRoot: context.smartContextRoot,
00069|         smartPopupSuggestions: bindings.popup.context.smart.suggestions,
00070|         smartPopupSuggestionSelectionIndex: bindings.popup.context.smart.suggestionSelectionIndex,
00071|         smartPopupSuggestionsFocused: bindings.popup.context.smart.suggestionsFocused,
00072|         onSmartPopupDraftChange: bindings.popup.context.smart.onDraftChange,
00073|         onSmartRootSubmit: bindings.popup.context.smart.onRootSubmit,
00074|       },
00075|       history: {
00076|         historyPopupItems: bindings.popup.history.items,
00077|         onHistoryPopupDraftChange: bindings.popup.history.onDraftChange,
00078|         onHistoryPopupSubmit: bindings.popup.history.onSubmit,
00079|       },
00080|       resume: {
00081|         onResumePayloadPathDraftChange: bindings.popup.misc.onResumePayloadPathDraftChange,
00082|         onResumeSubmit: popupManager.actions.handleResumeSubmit,
00083|       },
00084|       export: {
00085|         onExportOutPathDraftChange: bindings.popup.misc.onExportOutPathDraftChange,
00086|         onExportSubmit: popupManager.actions.handleExportSubmit,
00087|       },
00088|       intent: {
00089|         intentPopupSuggestions: bindings.popup.intent.suggestions,
00090|         intentPopupSuggestionSelectionIndex: bindings.popup.intent.suggestionSelectionIndex,
00091|         intentPopupSuggestionsFocused: bindings.popup.intent.suggestionsFocused,
00092|         onIntentPopupDraftChange: bindings.popup.intent.onDraftChange,
00093|         onIntentFileSubmit: popupManager.actions.handleIntentFileSubmit,
00094|       },
00095|       instructions: {
00096|         onInstructionsDraftChange: bindings.popup.misc.onInstructionsDraftChange,
00097|         onInstructionsSubmit: popupManager.actions.handleInstructionsSubmit,
00098|       },
00099|       series: {
00100|         isGenerating: generation.isGenerating,
00101|         onSeriesDraftChange: bindings.popup.misc.onSeriesDraftChange,
00102|         onSeriesSubmit: bindings.submit.onSeriesSubmit,
00103|       },
00104|       test: {
00105|         isTestCommandRunning: history.isTestCommandRunning,
00106|         onTestDraftChange: bindings.popup.misc.onTestDraftChange,
00107|         onTestSubmit: history.onTestPopupSubmit,
00108|       },
00109|       tokens: {
00110|         tokenUsageRun: generation.tokenUsageRun,
00111|         tokenUsageBreakdown: generation.tokenUsageBreakdown,
00112|         maxContextTokens: context.maxContextTokens,
00113|         maxInputTokens: context.maxInputTokens,
00114|         contextOverflowStrategy: context.contextOverflowStrategy,
00115|         latestContextOverflow: generation.latestContextOverflow,
00116|       },
00117|       budgets: {
00118|         onBudgetsMaxContextTokensDraftChange:
00119|           bindings.popup.misc.onBudgetsMaxContextTokensDraftChange,
00120|         onBudgetsMaxInputTokensDraftChange: bindings.popup.misc.onBudgetsMaxInputTokensDraftChange,
00121|         onBudgetsSubmit: popupManager.actions.handleBudgetsSubmit,
(File has more lines. Use 'offset' parameter to read beyond line 121)
</file>
```
```
---
## Assistant
_Thinking:_
**Extending context for PDFs**
I need to add PDFs to the popup context by incorporating `pdfs: context.pdfs` and ensuring that suggestions are passed from `bindings.popup.context.pdf`. Iâm wondering if the bindings actually include the PDF part. While we added PDF glue, I have to extend the `useCommandScreenPopupBindings` result types and structure. Right now, it only includes `context.video`, so I need to locate where the output object is built in `useCommandScreenPopupBindings.ts` around the relevant return point.
```
Tool: grep
**Input:**
```json
{
  "pattern": "popup:\\s*\\{\\s*context",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Output:**
```
No files found
```
```
---
## Assistant
```
Tool: grep
**Input:**
```json
{
  "pattern": "context:\\s*\\{\\s*file:",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Output:**
```
No files found
```
```
---
## Assistant
```
Tool: grep
**Input:**
```json
{
  "pattern": "popup\\.context|context: \\{\\s*file",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Output:**
```
No files found
```
```
---
## Assistant
_Thinking:_
**Inspecting object structure**
I think the return object structure might be different, so I need to take a closer look at the part where it's building the return object at the bottom. It seems wise to search for "return {" around offset 420 to find out what's going on there. This method should help me uncover any inconsistencies or errors in the current structure. Letâs see what I can find!
```
Tool: grep
**Input:**
```json
{
  "pattern": "return \\{|popup: \\{",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Output:**
```
Found 2 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts:
  Line 46:   popup: {
  Line 145:   popup: {
```
```
---
## Assistant
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "offset": 140,
  "limit": 90
}
```
**Output:**
```
<file>
00141|   submit: {
00142|     onSubmit: (value: string) => void
00143|     onSeriesSubmit: (value: string) => void
00144|   }
00145|   popup: {
00146|     model: {
00147|       options: ModelOption[]
00148|       recentCount: number
00149|       selection: number
00150|       onQueryChange: (next: string) => void
00151|     }
00152|     history: {
00153|       items: string[]
00154|       onDraftChange: (next: string) => void
00155|       onSubmit: (value: string) => void
00156|     }
00157|     intent: {
00158|       suggestions: string[]
00159|       suggestionSelectionIndex: number
00160|       suggestionsFocused: boolean
00161|       onDraftChange: (next: string) => void
00162|     }
00163|     context: {
00164|       file: {
00165|         suggestions: string[]
00166|         suggestionSelectionIndex: number
00167|         suggestionsFocused: boolean
00168|         onDraftChange: (next: string) => void
00169|         onAdd: (value: string) => void
00170|         onRemove: (index: number) => void
00171|       }
00172|       url: {
00173|         onDraftChange: (next: string) => void
00174|         onAdd: (value: string) => void
00175|         onRemove: (index: number) => void
00176|       }
00177|       image: {
00178|         suggestions: string[]
00179|         suggestionSelectionIndex: number
00180|         suggestionsFocused: boolean
00181|         onDraftChange: (next: string) => void
00182|         onAdd: (value: string) => void
00183|         onRemove: (index: number) => void
00184|       }
00185|       video: {
00186|         suggestions: string[]
00187|         suggestionSelectionIndex: number
00188|         suggestionsFocused: boolean
00189|         onDraftChange: (next: string) => void
00190|         onAdd: (value: string) => void
00191|         onRemove: (index: number) => void
00192|       }
00193|       smart: {
00194|         suggestions: string[]
00195|         suggestionSelectionIndex: number
00196|         suggestionsFocused: boolean
00197|         onDraftChange: (next: string) => void
00198|         onRootSubmit: (value: string) => void
00199|       }
00200|     }
00201|     misc: {
00202|       onSeriesDraftChange: (next: string) => void
00203|       onInstructionsDraftChange: (next: string) => void
00204|       onTestDraftChange: (next: string) => void
00205|       onBudgetsMaxContextTokensDraftChange: (next: string) => void
00206|       onBudgetsMaxInputTokensDraftChange: (next: string) => void
00207|       onResumePayloadPathDraftChange: (next: string) => void
00208|       onExportOutPathDraftChange: (next: string) => void
00209|     }
00210|     reasoning: {
00211|       lines: HistoryEntry[]
00212|       visibleRows: number
00213|     }
00214|   }
00215| }
00216| 
00217| export const useCommandScreenPopupBindings = (
00218|   options: UseCommandScreenPopupBindingsOptions,
00219| ): UseCommandScreenPopupBindingsResult => {
00220|   const notify = useStableCallback((message: string) => {
00221|     options.context.notify(message)
00222|   })
00223| 
00224|   const pushHistory = useStableCallback(
00225|     (content: string, kind: HistoryEntry['kind'] = 'system', format?: HistoryEntry['format']) => {
00226|       options.history.pushHistory(content, kind, format)
00227|     },
00228|   )
00229| 
00230|   const paste = useCommandScreenPasteBindings({
(File has more lines. Use 'offset' parameter to read beyond line 230)
</file>
```
```
---
## Assistant
_Thinking:_
**Patching the popup object**
I need to add a PDF in the `popup.context` type and make sure itâs included in the returned object at the bottom. So, what comes next is to patch this correctly. When I'm assembling the `popup` object, particularly around the return section at the end, Iâll need to locate the relevant part to do this. I think searching for `pdfPopupSuggestions` could help, and I might use grep for that.
```
Tool: grep
**Input:**
```json
{
  "pattern": "pdfPopupSuggestions|context: \\{\\n\\s*file|videoPopupSuggestions",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Error:**
```
Error: ripgrep failed: rg: the literal "\n" is not allowed in a regex
Consider enabling multiline mode with the --multiline flag (or -U for short).
When multiline mode is enabled, new line characters can be matched.
```
```
---
## Assistant
```
Tool: grep
**Input:**
```json
{
  "pattern": "videoPopupSuggestions|pdfPopupSuggestions|onAddPdf",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Output:**
```
Found 7 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts:
  Line 365:       suggestions: context.videoPopupSuggestions,
  Line 372:       suggestions: context.pdfPopupSuggestions,
  Line 373:       onAdd: context.onAddPdf,
  Line 491:           suggestions: context.videoPopupSuggestions,
  Line 493:           suggestionsFocused: context.videoPopupSuggestionsFocused,
  Line 548:       context.videoPopupSuggestions,
  Line 550:       context.videoPopupSuggestionsFocused,
```
```
---
## Assistant
_Thinking:_
Iâm noticing that the return object already has the PDF configuration, but itâs missing type definitions. To correct this, Iâll patch the type area around line 185. I need to edit the UseCommandScreenPopupBindingsResult type to include the PDF in the popup context. Thatâs located at the top, so my plan is to insert âpdfâ right after âvideo.â This should help clarify things and ensure everything aligns correctly.
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "oldString": "      video: {\n        suggestions: string[]\n        suggestionSelectionIndex: number\n        suggestionsFocused: boolean\n        onDraftChange: (next: string) => void\n        onAdd: (value: string) => void\n        onRemove: (index: number) => void\n      }\n      smart: {\n",
  "newString": "      video: {\n        suggestions: string[]\n        suggestionSelectionIndex: number\n        suggestionsFocused: boolean\n        onDraftChange: (next: string) => void\n        onAdd: (value: string) => void\n        onRemove: (index: number) => void\n      }\n      pdf: {\n        suggestions: string[]\n        suggestionSelectionIndex: number\n        suggestionsFocused: boolean\n        onDraftChange: (next: string) => void\n        onAdd: (value: string) => void\n        onRemove: (index: number) => void\n      }\n      smart: {\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [577:3] Type '{ input: { tokenLabel: (token: string) => string | null; onChange: (next: string) => void; }; submit: { onSubmit: (value: string) => void; onSeriesSubmit: (value: string) => void; }; popup: { ...; }; }' is not assignable to type 'UseCommandScreenPopupBindingsResult'.
  The types of 'popup.context' are incompatible between these types.
    Property 'pdf' is missing in type '{ file: { suggestions: string[]; suggestionSelectionIndex: number; suggestionsFocused: boolean; onDraftChange: (next: string) => void; onAdd: (value: string) => void; onRemove: (index: number) => void; }; url: { ...; }; image: { ...; }; video: { ...; }; smart: { ...; }; }' but required in type '{ file: { suggestions: string[]; suggestionSelectionIndex: number; suggestionsFocused: boolean; onDraftChange: (next: string) => void; onAdd: (value: string) => void; onRemove: (index: number) => void; }; ... 4 more ...; smart: { ...; }; }'.
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "offset": 450,
  "limit": 160
}
```
**Output:**
```
<file>
00451|       onSubmit: submit.handleSubmit,
00452|       onSeriesSubmit: submit.onSeriesSubmit,
00453|     }),
00454|     [submit.handleSubmit, submit.onSeriesSubmit],
00455|   )
00456| 
00457|   const popup = useMemo(
00458|     () => ({
00459|       model: {
00460|         options: modelPopupOptions,
00461|         recentCount: modelPopupRecentCount,
00462|         selection: modelPopupSelection,
00463|         onQueryChange: miscDraftHandlers.onModelPopupQueryChange,
00464|       },
00465|       history: {
00466|         items: historyAndIntent.history.historyPopupItems,
00467|         onDraftChange: historyAndIntent.history.onHistoryPopupDraftChange,
00468|         onSubmit: historyAndIntent.history.onHistoryPopupSubmit,
00469|       },
00470|       intent: {
00471|         suggestions: historyAndIntent.intent.intentPopupSuggestions,
00472|         suggestionSelectionIndex: historyAndIntent.intent.intentPopupSuggestionSelectionIndex,
00473|         suggestionsFocused: historyAndIntent.intent.intentPopupSuggestionsFocused,
00474|         onDraftChange: historyAndIntent.intent.onIntentPopupDraftChange,
00475|       },
00476|       context: {
00477|         file: {
00478|           suggestions: context.filePopupSuggestions,
00479|           suggestionSelectionIndex: context.filePopupSuggestionSelectionIndex,
00480|           suggestionsFocused: context.filePopupSuggestionsFocused,
00481|           onDraftChange: context.onFilePopupDraftChange,
00482|           onAdd: context.onAddFile,
00483|           onRemove: context.onRemoveFile,
00484|         },
00485|         url: {
00486|           onDraftChange: context.onUrlPopupDraftChange,
00487|           onAdd: context.onAddUrl,
00488|           onRemove: context.onRemoveUrl,
00489|         },
00490|         image: {
00491|           suggestions: context.imagePopupSuggestions,
00492|           suggestionSelectionIndex: context.imagePopupSuggestionSelectionIndex,
00493|           suggestionsFocused: context.imagePopupSuggestionsFocused,
00494|           onDraftChange: context.onImagePopupDraftChange,
00495|           onAdd: context.onAddImage,
00496|           onRemove: context.onRemoveImage,
00497|         },
00498|         video: {
00499|           suggestions: context.videoPopupSuggestions,
00500|           suggestionSelectionIndex: context.videoPopupSuggestionSelectionIndex,
00501|           suggestionsFocused: context.videoPopupSuggestionsFocused,
00502|           onDraftChange: context.onVideoPopupDraftChange,
00503|           onAdd: context.onAddVideo,
00504|           onRemove: context.onRemoveVideo,
00505|         },
00506|         smart: {
00507|           suggestions: context.smartPopupSuggestions,
00508|           suggestionSelectionIndex: context.smartPopupSuggestionSelectionIndex,
00509|           suggestionsFocused: context.smartPopupSuggestionsFocused,
00510|           onDraftChange: context.onSmartPopupDraftChange,
00511|           onRootSubmit: context.onSmartRootSubmit,
00512|         },
00513|       },
00514|       misc: {
00515|         onSeriesDraftChange: miscDraftHandlers.onSeriesDraftChange,
00516|         onInstructionsDraftChange: miscDraftHandlers.onInstructionsDraftChange,
00517|         onTestDraftChange: miscDraftHandlers.onTestDraftChange,
00518|         onBudgetsMaxContextTokensDraftChange:
00519|           miscDraftHandlers.onBudgetsMaxContextTokensDraftChange,
00520|         onBudgetsMaxInputTokensDraftChange: miscDraftHandlers.onBudgetsMaxInputTokensDraftChange,
00521|         onResumePayloadPathDraftChange: miscDraftHandlers.onResumePayloadPathDraftChange,
00522|         onExportOutPathDraftChange: miscDraftHandlers.onExportOutPathDraftChange,
00523|       },
00524|       reasoning: {
00525|         lines: reasoningPopupLines,
00526|         visibleRows: reasoningPopupVisibleRows,
00527|       },
00528|     }),
00529|     [
00530|       modelPopupOptions,
00531|       modelPopupRecentCount,
00532|       modelPopupSelection,
00533|       miscDraftHandlers.onModelPopupQueryChange,
00534|       historyAndIntent.history.historyPopupItems,
00535|       historyAndIntent.history.onHistoryPopupDraftChange,
00536|       historyAndIntent.history.onHistoryPopupSubmit,
00537|       historyAndIntent.intent.intentPopupSuggestions,
00538|       historyAndIntent.intent.intentPopupSuggestionSelectionIndex,
00539|       historyAndIntent.intent.intentPopupSuggestionsFocused,
00540|       historyAndIntent.intent.onIntentPopupDraftChange,
00541|       context.filePopupSuggestions,
00542|       context.filePopupSuggestionSelectionIndex,
00543|       context.filePopupSuggestionsFocused,
00544|       context.onFilePopupDraftChange,
00545|       context.onAddFile,
00546|       context.onRemoveFile,
00547|       context.onUrlPopupDraftChange,
00548|       context.onAddUrl,
00549|       context.onRemoveUrl,
00550|       context.imagePopupSuggestions,
00551|       context.imagePopupSuggestionSelectionIndex,
00552|       context.imagePopupSuggestionsFocused,
00553|       context.onImagePopupDraftChange,
00554|       context.onAddImage,
00555|       context.onRemoveImage,
00556|       context.videoPopupSuggestions,
00557|       context.videoPopupSuggestionSelectionIndex,
00558|       context.videoPopupSuggestionsFocused,
00559|       context.onVideoPopupDraftChange,
00560|       context.onAddVideo,
00561|       context.onRemoveVideo,
00562|       context.smartPopupSuggestions,
00563|       context.smartPopupSuggestionSelectionIndex,
00564|       context.smartPopupSuggestionsFocused,
00565|       context.onSmartPopupDraftChange,
00566|       context.onSmartRootSubmit,
00567|       miscDraftHandlers.onSeriesDraftChange,
00568|       miscDraftHandlers.onInstructionsDraftChange,
00569|       miscDraftHandlers.onTestDraftChange,
00570|       miscDraftHandlers.onBudgetsMaxContextTokensDraftChange,
00571|       miscDraftHandlers.onBudgetsMaxInputTokensDraftChange,
00572|       reasoningPopupLines,
00573|       reasoningPopupVisibleRows,
00574|     ],
00575|   )
00576| 
00577|   return useMemo(
00578|     () => ({
00579|       input,
00580|       submit: submitGroup,
00581|       popup,
00582|     }),
00583|     [input, submitGroup, popup],
00584|   )
00585| }
00586| 
(End of file - total 586 lines)
</file>
```
```
---
## Assistant
_Thinking:_
**Updating PDF context**
Iâm noticing that the return object isnât including the PDF context block, which needs my attention. I have to add a `pdf` section to `popup.context` in the `useMemo` and make sure it's included in the dependencies list. We had the PDF in the keyboard shortcuts call but not in the returned values. Iâll patch this by adding the PDF block after the video block and adjusting the dependencies accordingly. Time to proceed with the edit!
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "oldString": "        video: {\n          suggestions: context.videoPopupSuggestions,\n          suggestionSelectionIndex: context.videoPopupSuggestionSelectionIndex,\n          suggestionsFocused: context.videoPopupSuggestionsFocused,\n          onDraftChange: context.onVideoPopupDraftChange,\n          onAdd: context.onAddVideo,\n          onRemove: context.onRemoveVideo,\n        },\n        smart: {\n",
  "newString": "        video: {\n          suggestions: context.videoPopupSuggestions,\n          suggestionSelectionIndex: context.videoPopupSuggestionSelectionIndex,\n          suggestionsFocused: context.videoPopupSuggestionsFocused,\n          onDraftChange: context.onVideoPopupDraftChange,\n          onAdd: context.onAddVideo,\n          onRemove: context.onRemoveVideo,\n        },\n        pdf: {\n          suggestions: context.pdfPopupSuggestions,\n          suggestionSelectionIndex: context.pdfPopupSuggestionSelectionIndex,\n          suggestionsFocused: context.pdfPopupSuggestionsFocused,\n          onDraftChange: context.onPdfPopupDraftChange,\n          onAdd: context.onAddPdf,\n          onRemove: context.onRemovePdf,\n        },\n        smart: {\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "oldString": "      context.videoPopupSuggestions,\n      context.videoPopupSuggestionSelectionIndex,\n      context.videoPopupSuggestionsFocused,\n      context.onVideoPopupDraftChange,\n      context.onAddVideo,\n      context.onRemoveVideo,\n      context.smartPopupSuggestions,\n",
  "newString": "      context.videoPopupSuggestions,\n      context.videoPopupSuggestionSelectionIndex,\n      context.videoPopupSuggestionsFocused,\n      context.onVideoPopupDraftChange,\n      context.onAddVideo,\n      context.onRemoveVideo,\n      context.pdfPopupSuggestions,\n      context.pdfPopupSuggestionSelectionIndex,\n      context.pdfPopupSuggestionsFocused,\n      context.onPdfPopupDraftChange,\n      context.onAddPdf,\n      context.onRemovePdf,\n      context.smartPopupSuggestions,\n"
}
```
```
---
## Assistant
_Thinking:_
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupAndView.ts",
  "oldString": "        videos: context.videos,\n        videoPopupSuggestions: bindings.popup.context.video.suggestions,\n        videoPopupSuggestionSelectionIndex: bindings.popup.context.video.suggestionSelectionIndex,\n        videoPopupSuggestionsFocused: bindings.popup.context.video.suggestionsFocused,\n        onVideoPopupDraftChange: bindings.popup.context.video.onDraftChange,\n        onAddVideo: bindings.popup.context.video.onAdd,\n        smartContextEnabled: context.smartContextEnabled,\n",
  "newString": "        videos: context.videos,\n        videoPopupSuggestions: bindings.popup.context.video.suggestions,\n        videoPopupSuggestionSelectionIndex: bindings.popup.context.video.suggestionSelectionIndex,\n        videoPopupSuggestionsFocused: bindings.popup.context.video.suggestionsFocused,\n        onVideoPopupDraftChange: bindings.popup.context.video.onDraftChange,\n        onAddVideo: bindings.popup.context.video.onAdd,\n        pdfs: context.pdfs,\n        pdfPopupSuggestions: bindings.popup.context.pdf.suggestions,\n        pdfPopupSuggestionSelectionIndex: bindings.popup.context.pdf.suggestionSelectionIndex,\n        pdfPopupSuggestionsFocused: bindings.popup.context.pdf.suggestionsFocused,\n        onPdfPopupDraftChange: bindings.popup.context.pdf.onDraftChange,\n        onAddPdf: bindings.popup.context.pdf.onAdd,\n        smartContextEnabled: context.smartContextEnabled,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [67:9] Object literal may only specify known properties, and 'pdfs' does not exist in type '{ files: string[]; filePopupSuggestions: string[]; filePopupSuggestionSelectionIndex: number; filePopupSuggestionsFocused: boolean; onFilePopupDraftChange: (next: string) => void; onAddFile: (value: string) => void; ... 21 more ...; onSmartRootSubmit: (value: string) => void; }'.
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
**Searching for options type**
I need to check `useCommandScreenViewModel.types`, possibly in the `useCommandScreenViewModel.ts` file, to see if it defines the options type. It seems like the same thought repeated, so Iâll definitely search for it. I just want to make sure Iâm looking in the right file to find the information I need. Let's see what I can find in there!
```
Tool: grep
**Input:**
```json
{
  "pattern": "context:\\s*\\{|popup:\\s*\\{\\s*base|type UseCommandScreenViewModelOptions",
  "include": "src/tui/screens/command/hooks/useCommandScreenViewModel.ts"
}
```
**Output:**
```
Found 2 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenViewModel.ts:
  Line 12: export type UseCommandScreenViewModelOptions = {
  Line 38:     context: {
```
```
---
## Assistant
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenViewModel.ts",
  "offset": 1,
  "limit": 80
}
```
**Output:**
```
<file>
00002| 
00003| import type { DebugKeyEvent } from '../../../components/core/MultilineTextInput'
00004| import type { HistoryEntry, ModelOption, PopupState, ProviderStatusMap } from '../../../types'
00005| import type { TokenUsageBreakdown, TokenUsageRun } from '../../../token-usage-store'
00006| 
00007| import type { CommandInputProps } from '../components/CommandInput'
00008| import type { CommandMenuPaneProps } from '../components/CommandMenuPane'
00009| import type { HistoryPaneProps } from '../components/HistoryPane'
00010| import type { PopupAreaProps } from '../components/PopupArea'
00011| 
00012| export type UseCommandScreenViewModelOptions = {
00013|   transport: {
00014|     isAwaitingTransportInput: boolean
00015|   }
00016| 
00017|   panes: {
00018|     history: HistoryPaneProps
00019|     menu: CommandMenuPaneProps
00020|   }
00021| 
00022|   popup: {
00023|     base: {
00024|       popupState: PopupState
00025|       helpOpen: boolean
00026|       overlayHeight: number
00027|     }
00028| 
00029|     model: {
00030|       modelPopupOptions: ModelOption[]
00031|       modelPopupSelection: number
00032|       modelPopupRecentCount: number
00033|       providerStatuses: ProviderStatusMap
00034|       onModelPopupQueryChange: (next: string) => void
00035|       onModelPopupSubmit: (option: ModelOption | null | undefined) => void
00036|     }
00037| 
00038|     context: {
00039|       files: string[]
00040|       filePopupSuggestions: string[]
00041|       filePopupSuggestionSelectionIndex: number
00042|       filePopupSuggestionsFocused: boolean
00043|       onFilePopupDraftChange: (next: string) => void
00044|       onAddFile: (value: string) => void
00045| 
00046|       urls: string[]
00047|       onUrlPopupDraftChange: (next: string) => void
00048|       onAddUrl: (value: string) => void
00049| 
00050|       images: string[]
00051|       imagePopupSuggestions: string[]
00052|       imagePopupSuggestionSelectionIndex: number
00053|       imagePopupSuggestionsFocused: boolean
00054|       onImagePopupDraftChange: (next: string) => void
00055|       onAddImage: (value: string) => void
00056| 
00057|       videos: string[]
00058|       videoPopupSuggestions: string[]
00059|       videoPopupSuggestionSelectionIndex: number
00060|       videoPopupSuggestionsFocused: boolean
00061|       onVideoPopupDraftChange: (next: string) => void
00062|       onAddVideo: (value: string) => void
00063| 
00064|       smartContextEnabled: boolean
00065|       smartContextRoot: string | null
00066|       smartPopupSuggestions: string[]
00067|       smartPopupSuggestionSelectionIndex: number
00068|       smartPopupSuggestionsFocused: boolean
00069|       onSmartPopupDraftChange: (next: string) => void
00070|       onSmartRootSubmit: (value: string) => void
00071|     }
00072| 
00073|     history: {
00074|       historyPopupItems: string[]
00075|       onHistoryPopupDraftChange: (next: string) => void
00076|       onHistoryPopupSubmit: (value: string) => void
00077|     }
00078| 
00079|     resume: {
00080|       onResumePayloadPathDraftChange: (next: string) => void
00081|       onResumeSubmit: () => void
(File has more lines. Use 'offset' parameter to read beyond line 81)
</file>
```
```
---
## Assistant
_Thinking:_
I need to add a PDF section to the options type. It seems like I should patch this in after the videos section. So, I'll go ahead and edit the file to make sure everything fits together nicely. Letâs see, I need to make sure this change aligns with the overall structure and that itâs clear for anyone who will use it. Iâm double-checking everything to avoid any hiccups later!
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenViewModel.ts",
  "oldString": "      videos: string[]\n      videoPopupSuggestions: string[]\n      videoPopupSuggestionSelectionIndex: number\n      videoPopupSuggestionsFocused: boolean\n      onVideoPopupDraftChange: (next: string) => void\n      onAddVideo: (value: string) => void\n\n      smartContextEnabled: boolean\n",
  "newString": "      videos: string[]\n      videoPopupSuggestions: string[]\n      videoPopupSuggestionSelectionIndex: number\n      videoPopupSuggestionsFocused: boolean\n      onVideoPopupDraftChange: (next: string) => void\n      onAddVideo: (value: string) => void\n\n      pdfs: string[]\n      pdfPopupSuggestions: string[]\n      pdfPopupSuggestionSelectionIndex: number\n      pdfPopupSuggestionsFocused: boolean\n      onPdfPopupDraftChange: (next: string) => void\n      onAddPdf: (value: string) => void\n\n      smartContextEnabled: boolean\n"
}
```
```
---
## Assistant
_Thinking:_
**Evaluating the editing process**
I'm attempting to use the edit tool again since the previous attempt failed due to a type issue. I think my adjustments might succeed this time, but I need to check if the file is still unchanged. It seems like the initial edit may have applied, even though I encountered an error. So, I'll go ahead and reapply the edit to the same block now and see if it works. Let's find out!
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupAndView.ts",
  "oldString": "        videos: context.videos,\n        videoPopupSuggestions: bindings.popup.context.video.suggestions,\n        videoPopupSuggestionSelectionIndex: bindings.popup.context.video.suggestionSelectionIndex,\n        videoPopupSuggestionsFocused: bindings.popup.context.video.suggestionsFocused,\n        onVideoPopupDraftChange: bindings.popup.context.video.onDraftChange,\n        onAddVideo: bindings.popup.context.video.onAdd,\n        smartContextEnabled: context.smartContextEnabled,\n",
  "newString": "        videos: context.videos,\n        videoPopupSuggestions: bindings.popup.context.video.suggestions,\n        videoPopupSuggestionSelectionIndex: bindings.popup.context.video.suggestionSelectionIndex,\n        videoPopupSuggestionsFocused: bindings.popup.context.video.suggestionsFocused,\n        onVideoPopupDraftChange: bindings.popup.context.video.onDraftChange,\n        onAddVideo: bindings.popup.context.video.onAdd,\n        pdfs: context.pdfs,\n        pdfPopupSuggestions: bindings.popup.context.pdf.suggestions,\n        pdfPopupSuggestionSelectionIndex: bindings.popup.context.pdf.suggestionSelectionIndex,\n        pdfPopupSuggestionsFocused: bindings.popup.context.pdf.suggestionsFocused,\n        onPdfPopupDraftChange: bindings.popup.context.pdf.onDraftChange,\n        onAddPdf: bindings.popup.context.pdf.onAdd,\n        smartContextEnabled: context.smartContextEnabled,\n"
}
```
```
---
## Assistant
_Thinking:_
**Fixing typecheck errors**
I'm focusing on making sure the bindings.popup.context includes the PDF section. Iâve updated the useCommandScreenPopupBindingsResult type and the return object to incorporate PDF, which is good progress! Now, I need to tackle the remaining typecheck errors that might still be lurking around. So, letâs go ahead and rerun the typecheck to see what needs fixing next. It feels like I'm getting close to wrapping this up!
```
Tool: bash
**Input:**
```json
{
  "command": "npm run typecheck",
  "description": "Run TypeScript typecheck"
}
```
**Output:**
```
> @perceptron/prompt-maker-cli@0.1.0 typecheck
> tsc -p tsconfig.json --noEmit
```
```
---
## Assistant
_Thinking:_
```
Tool: grep
**Input:**
```json
{
  "pattern": "videos\\b|images\\b|pdfs\\b",
  "include": "src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts"
}
```
**Output:**
```
Found 6 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts:
  Line 23:   images: string[]
  Line 24:   videos: string[]
  Line 62:   images,
  Line 63:   videos,
  Line 125:     images,
  Line 126:     videos,
```
```
---
## Assistant
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts",
  "offset": 1,
  "limit": 160
}
```
**Output:**
```
<file>
00002| import { useCallback, useEffect, useRef, useState } from 'react'
00003| 
00004| import type { BudgetSettings } from '../../../budget-settings'
00005| import type { HistoryEntry, ModelOption } from '../../../types'
00006| 
00007| import { DEFAULT_MODEL_ID, getPreferredModelId } from '../../../model-options'
00008| import { resolveDefaultGenerateModel } from '../../../../prompt-generator-service'
00009| 
00010| import { useModelProviderState } from './useModelProviderState'
00011| import { useCommandGenerationPipeline } from './useCommandGenerationPipeline'
00012| 
00013| export type UseCommandScreenModelGenerationOptions = {
00014|   pushHistoryProxy: (
00015|     content: string,
00016|     kind?: HistoryEntry['kind'],
00017|     format?: HistoryEntry['format'],
00018|   ) => void
00019|   notify: (message: string, options?: NotifyOptions) => void
00020| 
00021|   files: string[]
00022|   urls: string[]
00023|   images: string[]
00024|   videos: string[]
00025| 
00026|   smartContextEnabled: boolean
00027|   smartContextRoot: string | null
00028| 
00029|   metaInstructions: string
00030|   budgets: BudgetSettings
00031|   interactiveTransportPath?: string | undefined
00032|   terminalColumns: number
00033| 
00034|   copyEnabled: boolean
00035|   chatGptEnabled: boolean
00036|   jsonOutputEnabled: boolean
00037| 
00038|   isTestCommandRunning: boolean
00039| 
00040|   setLastReasoning: (value: string | null) => void
00041|   setLastGeneratedPrompt: (value: string | null) => void
00042| }
00043| 
00044| export type UseCommandScreenModelGenerationResult = {
00045|   modelOptions: ReturnType<typeof useModelProviderState>['modelOptions']
00046|   currentModel: ReturnType<typeof useModelProviderState>['currentModel']
00047|   selectModel: ReturnType<typeof useModelProviderState>['selectModel']
00048|   polishModelId: ModelOption['id'] | null
00049|   selectPolishModel: (nextId: ModelOption['id'] | null) => void
00050|   currentTargetModel: ModelOption['id']
00051|   selectTargetModel: (nextId: ModelOption['id']) => void
00052|   providerStatuses: ReturnType<typeof useModelProviderState>['providerStatuses']
00053|   updateProviderStatus: ReturnType<typeof useModelProviderState>['updateProviderStatus']
00054|   pipeline: ReturnType<typeof useCommandGenerationPipeline>
00055| }
00056| 
00057| export const useCommandScreenModelGeneration = ({
00058|   pushHistoryProxy,
00059|   notify,
00060|   files,
00061|   urls,
00062|   images,
00063|   videos,
00064|   smartContextEnabled,
00065|   smartContextRoot,
00066|   metaInstructions,
00067|   budgets,
00068|   interactiveTransportPath,
00069|   terminalColumns,
00070|   copyEnabled,
00071|   chatGptEnabled,
00072|   jsonOutputEnabled,
00073|   isTestCommandRunning,
00074|   setLastReasoning,
00075|   setLastGeneratedPrompt,
00076| }: UseCommandScreenModelGenerationOptions): UseCommandScreenModelGenerationResult => {
00077|   const { modelOptions, currentModel, selectModel, providerStatuses, updateProviderStatus } =
00078|     useModelProviderState({ pushHistory: pushHistoryProxy })
00079| 
00080|   const [polishModelId, setPolishModelIdState] = useState<ModelOption['id'] | null>(null)
00081| 
00082|   const selectPolishModel = useCallback((nextId: ModelOption['id'] | null) => {
00083|     setPolishModelIdState((prev) => (prev === nextId ? prev : nextId))
00084|   }, [])
00085| 
00086|   const [currentTargetModel, setCurrentTargetModelState] =
00087|     useState<ModelOption['id']>(DEFAULT_MODEL_ID)
00088| 
00089|   const userSelectedTargetModelRef = useRef(false)
00090| 
00091|   const selectTargetModel = useCallback((nextId: ModelOption['id']) => {
00092|     userSelectedTargetModelRef.current = true
00093|     setCurrentTargetModelState((prev) => (prev === nextId ? prev : nextId))
00094|   }, [])
00095| 
00096|   useEffect(() => {
00097|     let cancelled = false
00098| 
00099|     const syncDefaultTargetModel = async (): Promise<void> => {
00100|       if (userSelectedTargetModelRef.current) {
00101|         return
00102|       }
00103| 
00104|       const resolvedDefault = await resolveDefaultGenerateModel().catch(() => null)
00105|       if (cancelled || userSelectedTargetModelRef.current) {
00106|         return
00107|       }
00108| 
00109|       const preferred = getPreferredModelId(modelOptions, resolvedDefault)
00110|       setCurrentTargetModelState((prev) => (prev === preferred ? prev : preferred))
00111|     }
00112| 
00113|     void syncDefaultTargetModel()
00114| 
00115|     return () => {
00116|       cancelled = true
00117|     }
00118|   }, [modelOptions])
00119| 
00120|   const pipeline = useCommandGenerationPipeline({
00121|     pushHistory: pushHistoryProxy,
00122|     notify,
00123|     files,
00124|     urls,
00125|     images,
00126|     videos,
00127|     smartContextEnabled,
00128|     smartContextRoot,
00129|     metaInstructions,
00130|     budgets,
00131|     currentModel,
00132|     targetModel: currentTargetModel,
00133|     interactiveTransportPath,
00134|     terminalColumns,
00135|     polishModelId,
00136|     jsonOutputEnabled,
00137| 
00138|     copyEnabled,
00139|     chatGptEnabled,
00140|     isTestCommandRunning,
00141|     onProviderStatusUpdate: updateProviderStatus,
00142|     onReasoningUpdate: setLastReasoning,
00143|     onLastGeneratedPromptUpdate: setLastGeneratedPrompt,
00144|   })
00145| 
00146|   return {
00147|     modelOptions,
00148|     currentModel,
00149|     selectModel,
00150|     polishModelId,
00151|     selectPolishModel,
00152|     currentTargetModel,
00153|     selectTargetModel,
00154| 
00155|     providerStatuses,
00156|     updateProviderStatus,
00157|     pipeline,
00158|   }
00159| }
00160| 
(End of file - total 160 lines)
</file>
```
```
---
## Assistant
_Thinking:_
**Inspecting command generation**
I need to add PDFs to the options for the command-generation pipeline. It looks like we're using `useCommandGenerationPipeline.ts`, so Iâll need to update that to accept PDFs and pass them along to generate the necessary arguments. Itâs a good idea to quickly inspect the `useCommandGenerationPipeline.ts` file to see how we can efficiently implement this change. Let's get started!
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "offset": 1,
  "limit": 140
}
```
**Output:**
```
<file>
00002| 
00003| import type { BudgetSettings } from '../../../budget-settings'
00004| import { useGenerationPipeline } from '../../../hooks/useGenerationPipeline'
00005| import type { NotifyOptions } from '../../../notifier'
00006| import { createTokenUsageStore } from '../../../token-usage-store'
00007| import type { HistoryEntry, ProviderStatus } from '../../../types'
00008| import type { TokenUsageBreakdown, TokenUsageRun } from '../../../token-usage-store'
00009| 
00010| export type UseCommandGenerationPipelineOptions = {
00011|   pushHistory: (
00012|     content: string,
00013|     kind?: HistoryEntry['kind'],
00014|     format?: HistoryEntry['format'],
00015|   ) => void
00016|   notify?: (message: string, options?: NotifyOptions) => void
00017| 
00018|   files: string[]
00019|   urls: string[]
00020|   images: string[]
00021|   videos: string[]
00022| 
00023|   smartContextEnabled: boolean
00024|   smartContextRoot: string | null
00025| 
00026|   metaInstructions: string
00027|   budgets: BudgetSettings
00028|   currentModel: string
00029|   targetModel: string
00030|   interactiveTransportPath?: string | undefined
00031|   terminalColumns: number
00032| 
00033|   polishModelId: string | null
00034|   jsonOutputEnabled: boolean
00035|   copyEnabled: boolean
00036|   chatGptEnabled: boolean
00037| 
00038|   isTestCommandRunning: boolean
00039| 
00040|   onProviderStatusUpdate: (status: ProviderStatus) => void
00041|   onReasoningUpdate: (reasoning: string | null) => void
00042|   onLastGeneratedPromptUpdate: (prompt: string) => void
00043| }
00044| 
00045| export type UseCommandGenerationPipelineResult = {
00046|   isGenerating: boolean
00047|   runGeneration: ReturnType<typeof useGenerationPipeline>['runGeneration']
00048|   runSeriesGeneration: ReturnType<typeof useGenerationPipeline>['runSeriesGeneration']
00049|   statusChips: string[]
00050|   latestContextOverflow: ReturnType<typeof useGenerationPipeline>['latestContextOverflow']
00051|   isAwaitingRefinement: boolean
00052|   submitRefinement: ReturnType<typeof useGenerationPipeline>['submitRefinement']
00053|   awaitingInteractiveMode: ReturnType<typeof useGenerationPipeline>['awaitingInteractiveMode']
00054| 
00055|   tokenUsageRun: TokenUsageRun | null
00056|   tokenUsageBreakdown: TokenUsageBreakdown | null
00057| }
00058| 
00059| export const useCommandGenerationPipeline = ({
00060|   pushHistory,
00061|   notify,
00062|   files,
00063|   urls,
00064|   images,
00065|   videos,
00066|   smartContextEnabled,
00067|   smartContextRoot,
00068|   metaInstructions,
00069|   budgets,
00070|   currentModel,
00071|   targetModel,
00072|   interactiveTransportPath,
00073|   terminalColumns,
00074|   polishModelId,
00075|   jsonOutputEnabled,
00076|   copyEnabled,
00077|   chatGptEnabled,
00078|   isTestCommandRunning,
00079|   onProviderStatusUpdate,
00080|   onReasoningUpdate,
00081|   onLastGeneratedPromptUpdate,
00082| }: UseCommandGenerationPipelineOptions): UseCommandGenerationPipelineResult => {
00083|   const tokenUsageStoreRef = useRef<ReturnType<typeof createTokenUsageStore> | null>(null)
00084|   if (!tokenUsageStoreRef.current) {
00085|     tokenUsageStoreRef.current = createTokenUsageStore()
00086|   }
00087| 
00088|   const trimmedMetaInstructions = metaInstructions.trim()
00089| 
00090|   const pipeline = useGenerationPipeline({
00091|     pushHistory,
00092|     ...(notify ? { notify } : {}),
00093|     files,
00094|     urls,
00095|     images,
00096|     videos,
00097|     smartContextEnabled,
00098|     smartContextRoot,
00099|     currentModel,
00100|     targetModel,
00101|     ...(interactiveTransportPath ? { interactiveTransportPath } : {}),
00102|     terminalColumns,
00103|     metaInstructions: trimmedMetaInstructions,
00104|     budgets,
00105|     polishModelId,
00106|     jsonOutputEnabled,
00107| 
00108|     copyEnabled,
00109|     chatGptEnabled,
00110|     isTestCommandRunning,
00111|     tokenUsageStore: tokenUsageStoreRef.current,
00112|     onProviderStatusUpdate,
00113|     onReasoningUpdate,
00114|     onLastGeneratedPromptUpdate,
00115|   })
00116| 
00117|   const tokenUsageRun = tokenUsageStoreRef.current?.getLatestRun() ?? null
00118|   const tokenUsageBreakdown = tokenUsageStoreRef.current?.getLatestBreakdown() ?? null
00119| 
00120|   return {
00121|     isGenerating: pipeline.isGenerating,
00122|     runGeneration: pipeline.runGeneration,
00123|     runSeriesGeneration: pipeline.runSeriesGeneration,
00124|     statusChips: pipeline.statusChips,
00125|     latestContextOverflow: pipeline.latestContextOverflow,
00126|     isAwaitingRefinement: pipeline.isAwaitingRefinement,
00127|     submitRefinement: pipeline.submitRefinement,
00128|     awaitingInteractiveMode: pipeline.awaitingInteractiveMode,
00129|     tokenUsageRun,
00130|     tokenUsageBreakdown,
00131|   }
00132| }
00133| 
(End of file - total 133 lines)
</file>
```
```
---
## Assistant
_Thinking:_
**Updating pipeline for PDFs**
I need to add PDFs to both the option type and pass them into the `useGenerationPipeline`. Currently, the `useGenerationPipeline` hook in the specified file doesnât handle PDFs. Earlier, we set `pdf: []` for arguments, but not for the pipeline context. My next step is updating that. The core pipeline builds `GenerateArgs` and calls the generate pipeline, so we have to ensure actual PDFs get included. I'll inspect the options type to incorporate images and videos as well.
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/hooks/useGenerationPipeline.ts",
  "offset": 40,
  "limit": 120
}
```
**Output:**
```
<file>
00041| import type { PromptGenerationRequest, SeriesResponse } from '../../prompt-generator-service'
00042| import { resolveFileContext } from '../../file-context'
00043| import { resolveSmartContextFiles } from '../../smart-context-service'
00044| import { resolveUrlContext } from '../../url-context'
00045| import type { UploadStateChange } from '../../prompt-generator-service'
00046| import { MODEL_PROVIDER_LABELS } from '../../model-providers'
00047| import { checkModelProviderStatus } from '../provider-status'
00048| import type { TokenUsageStore } from '../token-usage-store'
00049| import type { BudgetSettings } from '../budget-settings'
00050| import type { NotifyOptions } from '../notifier'
00051| import type { HistoryEntry, ProviderStatus, ResumeMode } from '../types'
00052| 
00053| export type UseGenerationPipelineOptions = {
00054|   pushHistory: (
00055|     content: string,
00056|     kind?: HistoryEntry['kind'],
00057|     format?: HistoryEntry['format'],
00058|   ) => void
00059|   notify?: (message: string, options?: NotifyOptions) => void
00060|   files: string[]
00061|   urls: string[]
00062|   images: string[]
00063|   videos: string[]
00064|   smartContextEnabled: boolean
00065|   smartContextRoot: string | null
00066|   metaInstructions: string
00067|   budgets: BudgetSettings
00068|   currentModel: string
00069|   targetModel?: string
00070|   interactiveTransportPath?: string | undefined
00071|   terminalColumns: number
00072|   polishModelId: string | null
00073|   jsonOutputEnabled: boolean
00074|   copyEnabled: boolean
00075|   chatGptEnabled: boolean
00076|   isTestCommandRunning: boolean
00077|   onProviderStatusUpdate?: (status: ProviderStatus) => void
00078|   tokenUsageStore?: TokenUsageStore
00079|   onReasoningUpdate?: (reasoning: string | null) => void
00080|   onLastGeneratedPromptUpdate?: (prompt: string) => void
00081| }
00082| 
00083| export const useGenerationPipeline = ({
00084|   pushHistory,
00085|   notify,
00086|   files,
00087|   urls,
00088|   images,
00089|   videos,
00090|   smartContextEnabled,
00091|   smartContextRoot,
00092|   metaInstructions,
00093|   budgets,
00094|   currentModel,
00095|   targetModel,
00096|   interactiveTransportPath,
00097|   terminalColumns,
00098|   polishModelId,
00099|   jsonOutputEnabled,
00100|   copyEnabled,
00101|   chatGptEnabled,
00102|   isTestCommandRunning,
00103|   onProviderStatusUpdate,
00104|   tokenUsageStore,
00105|   onReasoningUpdate,
00106|   onLastGeneratedPromptUpdate,
00107| }: UseGenerationPipelineOptions) => {
00108|   const [pipelineState, dispatch] = useReducer(
00109|     generationPipelineReducer,
00110|     INITIAL_GENERATION_PIPELINE_STATE,
00111|   )
00112| 
00113|   const {
00114|     isGenerating,
00115|     statusMessage,
00116|     isAwaitingRefinement,
00117|     awaitingInteractiveMode,
00118|     latestTelemetry,
00119|   } = pipelineState
00120|   const normalizedMetaInstructions = metaInstructions.trim()
00121| 
00122|   // âStale closureâ explanation (plain-English):
00123|   // React callbacks capture the variables that were in scope when they were created.
00124|   // If we keep a callback stable (so we donât recreate it every render), it would
00125|   // otherwise keep using old values.
00126|   //
00127|   // Example: if `handleStreamEvent` closed over an old `terminalColumns`, it would
00128|   // keep wrapping text to the wrong width after the terminal is resized.
00129|   //
00130|   // Solution used here: keep the callback stable, but read changing values from refs
00131|   // (kept fresh via useLatestRef).
00132|   const pushHistoryRef = useLatestRef(pushHistory)
00133|   const tokenUsageStoreRef = useLatestRef(tokenUsageStore)
00134|   const terminalColumnsRef = useLatestRef(terminalColumns)
00135|   const interactiveTransportPathRef = useLatestRef(interactiveTransportPath)
00136|   const notifyRef = useLatestRef(notify)
00137| 
00138|   const activeRunIdRef = useRef<string | null>(null)
00139|   const lastGeneratedPromptUpdateRef = useLatestRef<((prompt: string) => void) | null>(
00140|     onLastGeneratedPromptUpdate ?? null,
00141|   )
00142| 
00143|   type PendingRefinement = {
00144|     requestId: number
00145|     resolveText: (text: string) => void
00146|   }
00147| 
00148|   const pendingRefinementRef = useRef<PendingRefinement | null>(null)
00149|   const refinementRequestIdRef = useRef(0)
00150|   const isGeneratingRef = useLatestRef(isGenerating)
00151|   const transportAwaitingHintShownRef = useRef(false)
00152| 
00153|   const setAwaitingInteractiveMode = useCallback(
00154|     (nextMode: InteractiveAwaitingMode | null, nextStatusMessage?: string): void => {
00155|       dispatch({
00156|         type: 'set-awaiting-interactive',
00157|         awaitingInteractiveMode: nextMode,
00158|         ...(nextStatusMessage ? { statusMessage: nextStatusMessage } : {}),
00159|       })
00160|     },
(File has more lines. Use 'offset' parameter to read beyond line 160)
</file>
```
```
---
## Assistant
_Thinking:_
```
Tool: grep
**Input:**
```json
{
  "pattern": "images:|videos:",
  "include": "src/tui/hooks/useGenerationPipeline.ts"
}
```
**Output:**
```
Found 5 matches
/Users/eroberts/Projects/prompt-maker/src/tui/hooks/useGenerationPipeline.ts:
  Line 62:   images: string[]
  Line 63:   videos: string[]
  Line 572:           images: [...images],
  Line 865:           images: [...images],
  Line 866:           videos: [...videos],
```
```
---
## Assistant
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/hooks/useGenerationPipeline.ts",
  "offset": 540,
  "limit": 80
}
```
**Output:**
```
<file>
00541|       setLatestTelemetry(null)
00542|       onReasoningUpdate?.(null)
00543| 
00544|       dispatch({ type: 'generation-start', statusMessage: 'Preparing generationâŚ' })
00545|       transportAwaitingHintShownRef.current = false
00546|       pushHistoryRef.current('Starting generationâŚ')
00547| 
00548|       let stopStatusMessage: string | undefined
00549| 
00550|       try {
00551|         const transportPath = interactiveTransportPathRef.current
00552|         const usesTransportInteractive = Boolean(transportPath)
00553| 
00554|         const usesTuiInteractiveDelegate = !usesTransportInteractive && !jsonOutputEnabled
00555| 
00556|         const shouldIgnoreContextForResume = Boolean(resume)
00557| 
00558|         const args: GenerateArgs = {
00559|           interactive: usesTransportInteractive || usesTuiInteractiveDelegate,
00560|           copy: false,
00561|           openChatGpt: false,
00562|           polish: polishEnabled,
00563|           json: jsonOutputEnabled,
00564|           quiet: true,
00565|           progress: false,
00566|           stream: 'none',
00567|           showContext: false,
00568|           contextFormat: 'text',
00569|           help: false,
00570|           context: shouldIgnoreContextForResume ? [] : [...files],
00571|           urls: shouldIgnoreContextForResume ? [] : [...urls],
00572|           images: [...images],
00573|           video: [...videos],
00574|           pdf: [],
00575|           smartContext: shouldIgnoreContextForResume ? false : smartContextEnabled,
00576|           model: generationModel,
00577|           target: normalizedTargetModel,
00578|           ...(budgets.maxInputTokens !== null ? { maxInputTokens: budgets.maxInputTokens } : {}),
00579|           ...(budgets.maxContextTokens !== null
00580|             ? { maxContextTokens: budgets.maxContextTokens }
00581|             : {}),
00582|           ...(budgets.contextOverflowStrategy !== null
00583|             ? { contextOverflow: budgets.contextOverflowStrategy }
00584|             : {}),
00585|         }
00586|         if (normalizedMetaInstructions) {
00587|           args.metaInstructions = normalizedMetaInstructions
00588|         }
00589|         if (trimmedIntentFile) {
00590|           args.intentFile = trimmedIntentFile
00591|         } else if (trimmedIntent) {
00592|           args.intent = trimmedIntent
00593|         }
00594| 
00595|         if (resume) {
00596|           args.resumeMode = resume.mode
00597|           if (resume.kind === 'history') {
00598|             args.resume = resume.selector
00599|           } else {
00600|             args.resumeFrom = resume.payloadPath
00601|           }
00602|         }
00603|         if (polishEnabled) {
00604|           args.polishModel = normalizedPolishModel
00605|         }
00606| 
00607|         if (!shouldIgnoreContextForResume && smartContextEnabled && smartContextRoot) {
00608|           args.smartContextRoot = smartContextRoot
00609|         }
00610|         if (transportPath) {
00611|           args.interactiveTransport = transportPath
00612|         }
00613| 
00614|         const options: GeneratePipelineOptions = {
00615|           onStreamEvent: handleStreamEvent,
00616|           ...(usesTuiInteractiveDelegate ? { interactiveDelegate } : {}),
00617|         }
00618| 
00619|         const result: GeneratePipelineResult = await runGeneratePipeline(args, options)
00620|         onReasoningUpdate?.(result.reasoning ?? null)
(File has more lines. Use 'offset' parameter to read beyond line 620)
</file>
```
```
---
## Assistant
_Thinking:_
Next Iâll wire `pdfs` through the TUI generation pipeline so PDFs selected in the UI are included in the actual generation requests (same as images/videos).
```
Tool: todowrite
**Input:**
```json
{
  "todos": [
    {
      "content": "Add PDF state to TUI context",
      "status": "completed",
      "priority": "high",
      "id": "1"
    },
    {
      "content": "Add PDF popup UI and commands",
      "status": "completed",
      "priority": "high",
      "id": "2"
    },
    {
      "content": "Wire TUI generation pipeline PDFs",
      "status": "in_progress",
      "priority": "high",
      "id": "3"
    },
    {
      "content": "Implement OpenAI PDF text extraction",
      "status": "pending",
      "priority": "high",
      "id": "4"
    },
    {
      "content": "Update tests for TUI and OpenAI",
      "status": "pending",
      "priority": "high",
      "id": "5"
    },
    {
      "content": "Run typecheck/tests/build",
      "status": "pending",
      "priority": "high",
      "id": "6"
    }
  ]
}
```
**Output:**
```
[
  {
    "content": "Add PDF state to TUI context",
    "status": "completed",
    "priority": "high",
    "id": "1"
  },
  {
    "content": "Add PDF popup UI and commands",
    "status": "completed",
    "priority": "high",
    "id": "2"
  },
  {
    "content": "Wire TUI generation pipeline PDFs",
    "status": "in_progress",
    "priority": "high",
    "id": "3"
  },
  {
    "content": "Implement OpenAI PDF text extraction",
    "status": "pending",
    "priority": "high",
    "id": "4"
  },
  {
    "content": "Update tests for TUI and OpenAI",
    "status": "pending",
    "priority": "high",
    "id": "5"
  },
  {
    "content": "Run typecheck/tests/build",
    "status": "pending",
    "priority": "high",
    "id": "6"
  }
]
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/hooks/useGenerationPipeline.ts",
  "oldString": "  images: string[]\n  videos: string[]\n",
  "newString": "  images: string[]\n  videos: string[]\n  pdfs: string[]\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/hooks/useGenerationPipeline.ts",
  "oldString": "  images,\n  videos,\n  smartContextEnabled,\n",
  "newString": "  images,\n  videos,\n  pdfs,\n  smartContextEnabled,\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/hooks/useGenerationPipeline.ts",
  "oldString": "          images: [...images],\n          video: [...videos],\n          pdf: [],\n",
  "newString": "          images: [...images],\n          video: [...videos],\n          pdf: [...pdfs],\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/hooks/useGenerationPipeline.ts",
  "oldString": "          images: [...images],\n          videos: [...videos],\n",
  "newString": "          images: [...images],\n          videos: [...videos],\n          pdfs: [...pdfs],\n"
}
```
```
---
## Assistant
_Thinking:_
**Updating arguments**
I'm considering whether I need to update the other arguments around the 865 mark. I previously inserted something in the prompt generation request, but there could be other instances where args.pdf is set to an empty list. So, I think it would be a good idea to do a search to find those occurrences and ensure everything is in line. Let's make sure all the necessary updates are consistent!
```
Tool: grep
**Input:**
```json
{
  "pattern": "pdf:\\s*\\[\\]|pdfs:\\s*\\[\\]",
  "include": "src/tui/hooks/useGenerationPipeline.ts"
}
```
**Output:**
```
No files found
```
```
---
## Assistant
_Thinking:_
```
Tool: todowrite
**Input:**
```json
{
  "todos": [
    {
      "content": "Add PDF state to TUI context",
      "status": "completed",
      "priority": "high",
      "id": "1"
    },
    {
      "content": "Add PDF popup UI and commands",
      "status": "completed",
      "priority": "high",
      "id": "2"
    },
    {
      "content": "Wire TUI generation pipeline PDFs",
      "status": "in_progress",
      "priority": "high",
      "id": "3"
    },
    {
      "content": "Implement OpenAI PDF text extraction",
      "status": "pending",
      "priority": "high",
      "id": "4"
    },
    {
      "content": "Update tests for TUI and OpenAI",
      "status": "pending",
      "priority": "high",
      "id": "5"
    },
    {
      "content": "Run typecheck/tests/build",
      "status": "pending",
      "priority": "high",
      "id": "6"
    }
  ]
}
```
**Output:**
```
[
  {
    "content": "Add PDF state to TUI context",
    "status": "completed",
    "priority": "high",
    "id": "1"
  },
  {
    "content": "Add PDF popup UI and commands",
    "status": "completed",
    "priority": "high",
    "id": "2"
  },
  {
    "content": "Wire TUI generation pipeline PDFs",
    "status": "in_progress",
    "priority": "high",
    "id": "3"
  },
  {
    "content": "Implement OpenAI PDF text extraction",
    "status": "pending",
    "priority": "high",
    "id": "4"
  },
  {
    "content": "Update tests for TUI and OpenAI",
    "status": "pending",
    "priority": "high",
    "id": "5"
  },
  {
    "content": "Run typecheck/tests/build",
    "status": "pending",
    "priority": "high",
    "id": "6"
  }
]
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "oldString": "  images: string[]\n  videos: string[]\n",
  "newString": "  images: string[]\n  videos: string[]\n  pdfs: string[]\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "oldString": "  images,\n  videos,\n  smartContextEnabled,\n",
  "newString": "  images,\n  videos,\n  pdfs,\n  smartContextEnabled,\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "oldString": "    images,\n    videos,\n    smartContextEnabled,\n",
  "newString": "    images,\n    videos,\n    pdfs,\n    smartContextEnabled,\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "oldString": "     images,\n     videos,\n     smartContextEnabled,\n",
  "newString": "     images,\n     videos,\n     pdfs,\n     smartContextEnabled,\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "oldString": "     images,\n     videos,\n     smartContextEnabled,\n",
  "newString": "     images,\n     videos,\n     pdfs,\n     smartContextEnabled,\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "oldString": "     images,\n     videos,\n     smartContextEnabled,\n",
  "newString": "     images,\n     videos,\n     pdfs,\n     smartContextEnabled,\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "oldString": "    images,\n    videos,\n    smartContextEnabled,\n",
  "newString": "    images,\n    videos,\n    pdfs,\n    smartContextEnabled,\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "oldString": "    images,\n    videos,\n    smartContextEnabled,\n",
  "newString": "    images,\n    videos,\n    pdfs,\n    smartContextEnabled,\n"
}
```
```
---
## Assistant
_Thinking:_
**Inspecting file for duplicates**
I think we might have made too many redundant edits, which could risk corrupting the file. I need to take a closer look at the file to ensure everything is correct and to remove any duplicates if I find them. It's important to read the relevant portion carefully to catch any mistakes. Let's make sure everything is in good shape before proceeding further!
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "offset": 10,
  "limit": 40
}
```
**Output:**
```
<file>
00011|   pushHistory: (
00012|     content: string,
00013|     kind?: HistoryEntry['kind'],
00014|     format?: HistoryEntry['format'],
00015|   ) => void
00016|   notify?: (message: string, options?: NotifyOptions) => void
00017| 
00018|   files: string[]
00019|   urls: string[]
00020|   images: string[]
00021|   videos: string[]
00022|   pdfs: string[]
00023| 
00024|   smartContextEnabled: boolean
00025|   smartContextRoot: string | null
00026| 
00027|   metaInstructions: string
00028|   budgets: BudgetSettings
00029|   currentModel: string
00030|   targetModel: string
00031|   interactiveTransportPath?: string | undefined
00032|   terminalColumns: number
00033| 
00034|   polishModelId: string | null
00035|   jsonOutputEnabled: boolean
00036|   copyEnabled: boolean
00037|   chatGptEnabled: boolean
00038| 
00039|   isTestCommandRunning: boolean
00040| 
00041|   onProviderStatusUpdate: (status: ProviderStatus) => void
00042|   onReasoningUpdate: (reasoning: string | null) => void
00043|   onLastGeneratedPromptUpdate: (prompt: string) => void
00044| }
00045| 
00046| export type UseCommandGenerationPipelineResult = {
00047|   isGenerating: boolean
00048|   runGeneration: ReturnType<typeof useGenerationPipeline>['runGeneration']
00049|   runSeriesGeneration: ReturnType<typeof useGenerationPipeline>['runSeriesGeneration']
00050|   statusChips: string[]
(File has more lines. Use 'offset' parameter to read beyond line 50)
</file>
```
```
---
## Assistant
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "offset": 56,
  "limit": 70
}
```
**Output:**
```
<file>
00057|   tokenUsageBreakdown: TokenUsageBreakdown | null
00058| }
00059| 
00060| export const useCommandGenerationPipeline = ({
00061|   pushHistory,
00062|   notify,
00063|   files,
00064|   urls,
00065|   images,
00066|   videos,
00067|   pdfs,
00068|   smartContextEnabled,
00069| 
00070|   smartContextRoot,
00071|   metaInstructions,
00072|   budgets,
00073|   currentModel,
00074|   targetModel,
00075|   interactiveTransportPath,
00076|   terminalColumns,
00077|   polishModelId,
00078|   jsonOutputEnabled,
00079|   copyEnabled,
00080|   chatGptEnabled,
00081|   isTestCommandRunning,
00082|   onProviderStatusUpdate,
00083|   onReasoningUpdate,
00084|   onLastGeneratedPromptUpdate,
00085| }: UseCommandGenerationPipelineOptions): UseCommandGenerationPipelineResult => {
00086|   const tokenUsageStoreRef = useRef<ReturnType<typeof createTokenUsageStore> | null>(null)
00087|   if (!tokenUsageStoreRef.current) {
00088|     tokenUsageStoreRef.current = createTokenUsageStore()
00089|   }
00090| 
00091|   const trimmedMetaInstructions = metaInstructions.trim()
00092| 
00093|   const pipeline = useGenerationPipeline({
00094|     pushHistory,
00095|     ...(notify ? { notify } : {}),
00096|     files,
00097|     urls,
00098|     images,
00099|     videos,
00100|     pdfs,
00101|     smartContextEnabled,
00102|     smartContextRoot,
00103|     currentModel,
00104|     targetModel,
00105|     ...(interactiveTransportPath ? { interactiveTransportPath } : {}),
00106|     terminalColumns,
00107|     metaInstructions: trimmedMetaInstructions,
00108|     budgets,
00109|     polishModelId,
00110|     jsonOutputEnabled,
00111| 
00112|     copyEnabled,
00113|     chatGptEnabled,
00114|     isTestCommandRunning,
00115|     tokenUsageStore: tokenUsageStoreRef.current,
00116|     onProviderStatusUpdate,
00117|     onReasoningUpdate,
00118|     onLastGeneratedPromptUpdate,
00119|   })
00120| 
00121|   const tokenUsageRun = tokenUsageStoreRef.current?.getLatestRun() ?? null
00122|   const tokenUsageBreakdown = tokenUsageStoreRef.current?.getLatestBreakdown() ?? null
00123| 
00124|   return {
00125|     isGenerating: pipeline.isGenerating,
00126|     runGeneration: pipeline.runGeneration,
(File has more lines. Use 'offset' parameter to read beyond line 126)
</file>
```
```
---
## Assistant
_Thinking:_
```
Tool: grep
**Input:**
```json
{
  "pattern": "pdfs",
  "include": "src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts"
}
```
**Output:**
```
Found 1 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts:
  Line 25:   pdfs: string[]
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts",
  "oldString": "  images: string[]\n  videos: string[]\n",
  "newString": "  images: string[]\n  videos: string[]\n  pdfs: string[]\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts",
  "oldString": "  images,\n  videos,\n",
  "newString": "  images,\n  videos,\n  pdfs,\n"
}
```
```
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts",
  "oldString": "    images,\n    videos,\n    smartContextEnabled,\n",
  "newString": "    images,\n    videos,\n    pdfs,\n    smartContextEnabled,\n"
}
```
```
---
## Assistant
_Thinking:_
**Updating command screen controller**
I need to update `useCommandScreenController.ts` to ensure that PDFs are correctly passed to the model generation pipeline. I remember that earlier, we tried to insert these PDFs into the `modelGeneration` call, but Iâm not certain if it was done correctly or if we just attempted it. I'll check around line 92 to find the relevant piece of code and confirm if the implementation is as intended. Let's make sure itâs working properly!
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 88,
  "limit": 30
}
```
**Output:**
```
<file>
00089|   } = useContextDispatch()
00090| 
00091|   const historyAndTests = useCommandScreenHistoryAndTests()
00092| 
00093|   const inputState = useCommandScreenInputState({
00094|     pushHistoryProxy: historyAndTests.pushHistoryProxy,
00095|   })
00096| 
00097|   const modelAndGeneration: UseCommandScreenModelGenerationResult = useCommandScreenModelGeneration(
00098|     {
00099|       pushHistoryProxy: historyAndTests.pushHistoryProxy,
00100|       notify,
00101|       files,
00102|       urls,
00103|       images,
00104|       videos,
00105|       smartContextEnabled,
00106|       smartContextRoot,
00107|       metaInstructions,
00108|       budgets: { maxContextTokens, maxInputTokens, contextOverflowStrategy },
00109|       ...(interactiveTransportPath ? { interactiveTransportPath } : {}),
00110|       terminalColumns: inputState.terminalColumns,
00111|       copyEnabled: inputState.copyEnabled,
00112|       chatGptEnabled: inputState.chatGptEnabled,
00113|       jsonOutputEnabled: inputState.jsonOutputEnabled,
00114|       isTestCommandRunning: historyAndTests.isTestCommandRunning,
00115|       setLastReasoning,
00116|       setLastGeneratedPrompt,
00117|     },
00118|   )
(File has more lines. Use 'offset' parameter to read beyond line 118)
</file>
```
```
---
## Assistant
_Thinking:_
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [98:5] Argument of type '{ terminalColumns: number; copyEnabled: boolean; chatGptEnabled: boolean; jsonOutputEnabled: boolean; isTestCommandRunning: boolean; setLastReasoning: (value: string | null) => void; ... 11 more ...; budgets: { ...; }; }' is not assignable to parameter of type 'UseCommandScreenModelGenerationOptions'.
  Property 'pdfs' is missing in type '{ terminalColumns: number; copyEnabled: boolean; chatGptEnabled: boolean; jsonOutputEnabled: boolean; isTestCommandRunning: boolean; setLastReasoning: (value: string | null) => void; ... 11 more ...; budgets: { ...; }; }' but required in type 'UseCommandScreenModelGenerationOptions'.
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
**Updating file context**
I see that the edit didnât apply due to multiple matches. Thatâs likely why the diagnostic from the file is still missing the PDFs. I need to ensure everything is updated properly. So, it seems I need to patch within the object to include more context. Iâm planning to edit that call and replace `images,\n      videos,` with the inclusion of PDFs. This should help cover whatâs missing!
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      urls,\n      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "      urls,\n      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
**Output:**
```
This file has errors, please fix
<file_diagnostics>
ERROR [98:5] Argument of type '{ terminalColumns: number; copyEnabled: boolean; chatGptEnabled: boolean; jsonOutputEnabled: boolean; isTestCommandRunning: boolean; setLastReasoning: (value: string | null) => void; ... 11 more ...; budgets: { ...; }; }' is not assignable to parameter of type 'UseCommandScreenModelGenerationOptions'.
  Property 'pdfs' is missing in type '{ terminalColumns: number; copyEnabled: boolean; chatGptEnabled: boolean; jsonOutputEnabled: boolean; isTestCommandRunning: boolean; setLastReasoning: (value: string | null) => void; ... 11 more ...; budgets: { ...; }; }' but required in type 'UseCommandScreenModelGenerationOptions'.
</file_diagnostics>
```
```
---
## Assistant
_Thinking:_
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 98,
  "limit": 12
}
```
**Output:**
```
<file>
00099|       pushHistoryProxy: historyAndTests.pushHistoryProxy,
00100|       notify,
00101|       files,
00102|       urls,
00103|       images,
00104|       videos,
00105|       smartContextEnabled,
00106|       smartContextRoot,
00107|       metaInstructions,
00108|       budgets: { maxContextTokens, maxInputTokens, contextOverflowStrategy },
00109|       ...(interactiveTransportPath ? { interactiveTransportPath } : {}),
00110|       terminalColumns: inputState.terminalColumns,
(File has more lines. Use 'offset' parameter to read beyond line 110)
</file>
```
```
---
## Assistant
_Thinking:_
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      pushHistoryProxy: historyAndTests.pushHistoryProxy,\n      notify,\n      files,\n      urls,\n      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "      pushHistoryProxy: historyAndTests.pushHistoryProxy,\n      notify,\n      files,\n      urls,\n      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
```
---
## Assistant
_Thinking:_
```
Tool: bash
**Input:**
```json
{
  "command": "npm run typecheck",
  "description": "Run TypeScript typecheck"
}
```
**Output:**
```
> @perceptron/prompt-maker-cli@0.1.0 typecheck
> tsc -p tsconfig.json --noEmit
src/__tests__/useGenerationPipeline.test.ts(143,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; onProviderStatusUpdate: jest.Mock<any, any, any>; files: string[]; urls: string[]; images: string[]; videos: string[]; ... 11 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; onProviderStatusUpdate: jest.Mock<any, any, any>; files: string[]; urls: string[]; images: string[]; videos: string[]; ... 11 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(174,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(201,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; onLastGeneratedPromptUpdate: jest.Mock<any, any, any>; files: string[]; urls: string[]; images: string[]; ... 12 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; onLastGeneratedPromptUpdate: jest.Mock<any, any, any>; files: string[]; urls: string[]; images: string[]; ... 12 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(228,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; smartContextEnabled: true; images: string[]; videos: string[]; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; smartContextEnabled: true; images: string[]; videos: string[]; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(271,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; polishModelId: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; polishModelId: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(296,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(345,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; notify: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; isTestCommandRunning: boolean; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; notify: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; isTestCommandRunning: boolean; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(403,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; interactiveTransportPath: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; interactiveTransportPath: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(467,31): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; isTestCommandRunning: boolean; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; isTestCommandRunning: boolean; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(505,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; metaInstructions: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; metaInstructions: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(532,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; images: string[]; videos: string[]; files: string[]; urls: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; images: string[]; videos: string[]; files: string[]; urls: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(559,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; videos: string[]; files: string[]; urls: string[]; images: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; videos: string[]; files: string[]; urls: string[]; images: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(593,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
```
```
---
## Assistant
_Thinking:_
**Updating test options**
I'm planning to update the useGenerationPipeline tests to include `pdfs: []` in the createOptions objects. It seems there will be quite a few instances to replace in the test file, so I'll need to add `pdfs: []` to the base options in those objects. My next step is to open the file and get started on making those changes. This should help ensure everything functions smoothly!
```
Tool: read
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/useGenerationPipeline.test.ts",
  "offset": 60,
  "limit": 60
}
```
**Output:**
```
<file>
00061| 
00062| const urlContextModule = jest.requireMock('../url-context') as {
00063|   resolveUrlContext: jest.Mock
00064| }
00065| 
00066| const smartContextModule = jest.requireMock('../smart-context-service') as {
00067|   resolveSmartContextFiles: jest.Mock
00068| }
00069| 
00070| const fsPromisesModule = jest.requireMock('node:fs/promises') as {
00071|   mkdir: jest.Mock
00072|   writeFile: jest.Mock
00073| }
00074| 
00075| const dom = new JSDOM('<!doctype html><html><body></body></html>')
00076| const globalEnv = globalThis as typeof globalThis & {
00077|   window: Window & typeof globalThis
00078|   document: Document
00079|   navigator: Navigator
00080| }
00081| globalEnv.window = dom.window as typeof globalEnv.window
00082| globalEnv.document = dom.window.document as Document
00083| globalEnv.navigator = dom.window.navigator
00084| 
00085| describe('useGenerationPipeline', () => {
00086|   const baseOptions = {
00087|     files: [] as string[],
00088|     urls: [] as string[],
00089|     images: [] as string[],
00090|     videos: [] as string[],
00091|     smartContextEnabled: false,
00092|     smartContextRoot: null,
00093|     metaInstructions: '',
00094|     budgets: {
00095|       maxContextTokens: null,
00096|       maxInputTokens: null,
00097|       contextOverflowStrategy: null,
00098|     },
00099|     interactiveTransportPath: undefined as string | undefined,
00100|     terminalColumns: 80,
00101|     polishModelId: null,
00102|     jsonOutputEnabled: false,
00103|     copyEnabled: false,
00104|     chatGptEnabled: false,
00105|     isTestCommandRunning: false,
00106|     notify: jest.fn(),
00107|   }
00108| 
00109|   beforeEach(() => {
00110|     jest.clearAllMocks()
00111| 
00112|     generateCommandModule.runGeneratePipeline.mockResolvedValue({
00113|       finalPrompt: 'Prompt',
00114|       model: 'gpt-4o-mini',
00115|       iterations: 1,
00116|       telemetry: null,
00117|       payload: {},
00118|     })
00119| 
00120|     promptGeneratorModule.generatePromptSeries.mockResolvedValue({
(File has more lines. Use 'offset' parameter to read beyond line 120)
</file>
```
```
---
## Assistant
_Thinking:_
**Evaluating baseOptions for PDFs**
I notice that the baseOptions are missing PDFs. To fix this, I'll add `pdfs: [] as string[]`. That should address the issue since the tests utilize spread baseOptions. It seems like a simple patch, but I want to make sure I'm covering all bases! I want to ensure everything works smoothly and fulfills the requirements, so Iâll double-check before making any changes to confirm it all integrates properly.
```
Tool: edit
**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/useGenerationPipeline.test.ts",
  "oldString": "    images: [] as string[],\n    videos: [] as string[],\n",
  "newString": "    images: [] as string[],\n    videos: [] as string[],\n    pdfs: [] as string[],\n"
}
```
```
---
## Assistant
```
Tool: bash
**Input:**
```json
{
  "command": "npm run typecheck",
  "description": "Run TypeScript typecheck"
}
```
**Output:**
```
> @perceptron/prompt-maker-cli@0.1.0 typecheck
> tsc -p tsconfig.json --noEmit
```
```
---
## Assistant
---
## User
please continue
---
## Assistant
---
</file>
<file path="docs/tui-design.md">
# Prompt Maker CLI TUI Design
This document describes the _current_ Ink-based TUI: its UX goals, input routing invariants, and how the code is organized.
If youâre looking for a user guide, the fastest way to learn the TUI is to run it and type `/help`.
## Goals
- **Keyboard-first**: no mouse required; command palette + popups cover most actions.
- **Safe input routing**: help/popup layers must âownâ the keyboard (no fallthrough).
- **Shared core logic**: the TUI calls the same generation/test pipeline as the CLI.
- **Automation-friendly**: the CLI continues to support JSON/JSONL output; the TUI is optional.
## Entry points
- CLI routing: `src/index.ts`
  - No args defaults to `ui`.
  - `test` runs the prompt test runner.
  - Everything else runs the generate workflow.
- TUI renderer: `src/tui/index.tsx` (parses `--interactive-transport`, then mounts `AppContainer`).
## Global controls
Global keybinds are defined by `src/tui/app-container-keymap.ts`. The help overlay is opened via the `/help` command.
- `Ctrl+G`: open command palette (Generate view)
- `Ctrl+T`: switch to Test Runner view
- `/help`: open help overlay
- `Ctrl+C`: exit
- `Esc`: dismiss UI elements (never exits)
## Input routing invariants
Priority order (highest wins):
1. **Help overlay**: when help is open, it consumes almost all keys.
2. **Popup input**: when a popup is open, it âownsâ the keyboard.
3. **Screen input**: active screen handles keys (history scrolling, typing, submission).
4. **AppContainer global keys**: only truly-global shortcuts should run here.
This is enforced by:
- `AppContainer` tracking `isHelpOpen` + `isPopupOpen` and gating global actions.
- Screens using `useInput(..., { isActive })` to ensure only the top layer handles keys.
## Screen model
The TUI has two views:
- **Generate** (`src/tui/screens/command/CommandScreen.tsx`)
- **Test Runner** (`src/tui/screens/test-runner/TestRunnerScreen.tsx`)
The view is managed by `src/tui/AppContainer.tsx`.
## Generate view UX
The Generate view is a scrollable history pane plus a single input bar:
- Type normal text â treated as an intent.
- Type `/` â command mode (command palette opens and filters).
- Drag & drop an absolute file path, then press `Tab` to attach it.
Commands are defined in `src/tui/config.ts` (`COMMAND_DESCRIPTORS`). The palette uses matching helpers in `src/tui/command-filter.ts`.
### Popups
Most commands open a popup:
- `/model`, `/target` â model selection
- `/file`, `/url`, `/image`, `/video` â add context items
- `/smart`, `/smart-root` â smart context controls
- `/theme`, `/theme-mode` â theme controls
- `/tokens`, `/budgets`, `/resume`, `/export`, `/settings`, `/history`, `/reasoning` â informational/workflow views
Popup transitions are managed with a pure reducer (`src/tui/popup-reducer.ts`) and a hook for effects (`src/tui/hooks/usePopupManager.ts`).
### Series generation
`/series` (or pressing `Tab`) produces a set of standalone âatomic promptsâ (no cross references between steps). It writes the artifacts under a timestamped folder (see `src/tui/hooks/useGenerationPipeline.ts`).
## Test Runner UX
The Test Runner view runs suites defined by `prompt-tests.yaml` (or another YAML file). It uses the same underlying runner exported by `src/test-command.ts`.
## Theming
- Theme definitions and loader: `src/tui/theme/*`
- Some TUI settings are persisted into CLI config via `src/config.ts` (theme, budgets, resume defaults, export defaults).
Theme JSON examples live in `src/tui/theme/examples/`.
</file>
<file path="docs/tui-updates.md">
# TUI Parity Updates (implemented in PR #15)
This document originally described a plan to update the Ink + React TUI to support CLI âauditable workflowsâ introduced in PR #14.
Those planned items are now implemented on `feature/tui-cli-parity-refacto` (PR #15).
Scope constraints (still apply):
- Preserve input routing priority (**Help overlay > Popup > Screen > AppContainer globals**) as described in `docs/tui-design.md` and `src/tui/DEVELOPER_NOTE.md`.
- Keep reducers pure (`*-reducer.ts`), and keep effects in hooks.
---
## Summary
User-facing TUI parity improvements:
- `/budgets`: configure token budgets + overflow strategy (persists to config).
- `/resume`: resume generation from history or an exported payload file (persists default mode/source).
- `/export`: export a selected history payload to JSON/YAML (persists default format/output directory).
- Help overlay includes a âWorkflowsâ section to make these discoverable.
CLI behavior/UX improvements related to these workflows:
- `export --from-history <selector>` now validates schema compatibility for the selected history entry and fails with an actionable error if the selected entry has an unsupported `schemaVersion`.
---
## TUI Workflow Commands
All of these commands are listed in the command palette (`Ctrl+G` or type `/`) and open a popup (no inline args parsing).
### `/budgets`
- Popup fields:
  - `Max context tokens` (optional integer)
  - `Max input tokens` (optional integer)
  - `Overflow strategy` (`fail`, `drop-smart`, `drop-url`, `drop-largest`, `drop-oldest`, or unset)
- Behavior:
  - Budgets are enabled when either token field is non-empty.
  - If budgets are enabled and overflow strategy is unset, the effective strategy defaults to `fail`.
  - Clearing both token fields disables budgets.
- Persistence:
  - Writes to CLI config under `promptGenerator.maxInputTokens`, `promptGenerator.maxContextTokens`, and `promptGenerator.contextOverflowStrategy`.
Implementation entrypoints:
- UI: `src/tui/components/popups/BudgetsPopup.tsx`
- Parsing: `src/tui/budget-settings.ts`
- Persistence: `src/config.ts` (`updateCliPromptGeneratorSettings`)
### `/resume`
- Popup fields:
  - `Source`: `history` or `file`
  - `Mode`: `best-effort` or `strict`
  - History picker (for `history`) or payload path input (for `file`)
- Behavior:
  - Resume runs can start even when the typed intent is empty (the resumed payload supplies the intent).
  - Resume only reuses `source:"file"` context paths. URL/smart entries are treated as missing.
  - `best-effort` warns (and continues) if some resumed files are missing.
  - `strict` fails if any resumed `source:"file"` context paths are missing.
- Persistence:
  - Writes `resumeMode` and `resumeSourceKind` to CLI config.
Implementation entrypoints:
- UI: `src/tui/components/popups/ResumePopup.tsx`
- History list: `src/tui/resume-history.ts`
- Persistence: `src/config.ts` (`updateCliResumeSettings`)
### `/export`
- Popup fields:
  - `Format`: `json` or `yaml`
  - `Out`: output file path (relative to cwd or absolute)
  - History picker (newest-first list of `last`, `last:2`, âŚ)
- Behavior:
  - Exports write the selected history payload to the chosen output path.
  - History entries with an unsupported `schemaVersion` are surfaced in the picker as unsupported; export fails if you attempt to export an unsupported entry.
- Persistence:
  - Writes `exportFormat` and `exportOutDir` (directory path) to CLI config.
Implementation entrypoints:
- UI: `src/tui/components/popups/ExportPopup.tsx`
- History picker + schema gating: `src/history/generate-history.ts`
- File writing helper: `src/export/export-generate-payload.ts`
- Persistence: `src/config.ts` (`updateCliExportSettings`)
---
## Config keys (relevant to parity workflows)
Config is loaded from (`src/config.ts`):
1. `PROMPT_MAKER_CLI_CONFIG=/path/to/config.json`
2. `~/.config/prompt-maker-cli/config.json`
3. `~/.prompt-maker-cli.json`
Relevant persisted keys:
- Budgets:
  - `promptGenerator.maxInputTokens`
  - `promptGenerator.maxContextTokens`
  - `promptGenerator.contextOverflowStrategy`
- Resume defaults:
  - `resumeMode` (`best-effort` | `strict`)
  - `resumeSourceKind` (`history` | `file`)
- Export defaults:
  - `exportFormat` (`json` | `yaml`)
  - `exportOutDir` (directory path)
---
## Help overlay
Help overlay text is generated in `src/tui/help-config.ts`.
PR #15 adds a âWorkflowsâ section that calls out:
- `/budgets`
- `/resume`
- `/export`
- A note that `compose` is CLI-only (not implemented in the TUI)
</file>
<file path="src/__tests__/generate/resume-from-file.test.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
import { appendToHistory } from '../../history-logger'
import { runGeneratePipeline } from '../../generate/pipeline'
import {
  GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
  type GenerateArgs,
  type GenerateJsonPayload,
  type StreamEventInput,
} from '../../generate/types'
import { serializeGeneratePayload } from '../../generate/payload-io'
import { createPromptGeneratorService } from '../../prompt-generator-service'
import { resolveHistoryFilePath } from '../../history-logger'
const promptService = {
  generatePrompt: jest.fn<Promise<string>, []>(),
}
jest.mock('clipboardy', () => ({ write: jest.fn() }))
jest.mock('open', () => jest.fn())
jest.mock('../../config', () => ({
  loadCliConfig: jest.fn().mockResolvedValue(null),
}))
jest.mock('../../prompt-generator-service', () => ({
  createPromptGeneratorService: jest.fn(),
  resolveDefaultGenerateModel: jest.fn().mockResolvedValue('gpt-4o-mini'),
  isGemini: jest.fn(() => false),
}))
jest.mock('../../generate/models', () => ({
  resolveTargetModel: jest.fn(
    async ({
      defaultTargetModel,
      explicitTarget,
    }: {
      defaultTargetModel: string
      explicitTarget?: string
    }) => explicitTarget ?? defaultTargetModel,
  ),
  resolveGeminiVideoModel: jest.fn().mockResolvedValue('gemini-1.5-pro'),
}))
jest.mock('../../history-logger', () => ({
  appendToHistory: jest.fn().mockResolvedValue(undefined),
  resolveHistoryFilePath: jest.fn(() => 'history.jsonl'),
}))
jest.mock('../../io', () => ({
  readFromStdin: jest.fn().mockResolvedValue(null),
}))
const mockAppendToHistory = appendToHistory as jest.Mock
const mockResolveHistoryFilePath = resolveHistoryFilePath as jest.Mock
const mockCreatePromptGeneratorService = createPromptGeneratorService as jest.Mock
const createTempDir = async (): Promise<string> =>
  fs.mkdtemp(path.join(os.tmpdir(), 'prompt-maker-resume-from-file-'))
const createBaseArgs = (): GenerateArgs => ({
  interactive: false,
  copy: false,
  openChatGpt: false,
  polish: false,
  json: false,
  quiet: true,
  progress: false,
  stream: 'none',
  showContext: false,
  contextFormat: 'text',
  help: false,
  context: [],
  urls: [],
  images: [],
  video: [],
  pdf: [],
  smartContext: false,
})
describe('runGeneratePipeline --resume-from', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    mockCreatePromptGeneratorService.mockResolvedValue(promptService)
  })
  it('loads a JSON payload file, seeds state, and emits resume.loaded with source=file', async () => {
    const dir = await createTempDir()
    try {
      const existingPath = path.join(dir, 'existing.md')
      const missingPath = path.join(dir, 'missing.md')
      await fs.writeFile(existingPath, 'existing content', 'utf8')
      const payloadPath = path.join(dir, 'payload.json')
      const resumedPayload: GenerateJsonPayload = {
        schemaVersion: GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
        intent: 'resumed intent',
        model: 'gpt-4o-mini',
        targetModel: 'gpt-4o-mini',
        prompt: 'previous prompt',
        polishedPrompt: 'previous polished',
        polishModel: 'polish-model',
        metaInstructions: 'Be concise',
        refinements: ['prior refinement'],
        iterations: 2,
        interactive: true,
        timestamp: '2025-01-01T00:00:00.000Z',
        contextPaths: [
          { path: existingPath, source: 'file' },
          { path: missingPath, source: 'file' },
          { path: 'url:https://example.com', source: 'url' },
        ],
      }
      await fs.writeFile(payloadPath, serializeGeneratePayload(resumedPayload, 'json'), 'utf8')
      const events: StreamEventInput[] = []
      const warnSpy = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
      await runGeneratePipeline(
        {
          ...createBaseArgs(),
          resumeFrom: payloadPath,
          resumeMode: 'best-effort',
        },
        {
          onStreamEvent: (event) => {
            events.push(event)
          },
        },
      )
      warnSpy.mockRestore()
      const resumeEvent = events.find(
        (event): event is Extract<StreamEventInput, { event: 'resume.loaded' }> =>
          event.event === 'resume.loaded',
      )
      expect(resumeEvent).toBeDefined()
      expect(resumeEvent?.source).toBe('file')
      expect(resumeEvent?.reusedContextPaths).toEqual([{ path: existingPath, source: 'file' }])
      expect(resumeEvent?.missingContextPaths).toEqual(
        expect.arrayContaining([
          { path: missingPath, source: 'file' },
          { path: 'url:https://example.com', source: 'url' },
        ]),
      )
      expect(mockResolveHistoryFilePath).not.toHaveBeenCalled()
      expect(promptService.generatePrompt).not.toHaveBeenCalled()
      expect(mockAppendToHistory).toHaveBeenCalledWith(
        expect.objectContaining({ intent: 'resumed intent', prompt: 'previous polished' }),
      )
    } finally {
      await fs.rm(dir, { recursive: true, force: true })
    }
  })
  it('loads a YAML payload file and emits resume.loaded with source=file', async () => {
    const dir = await createTempDir()
    try {
      const existingPath = path.join(dir, 'existing.md')
      await fs.writeFile(existingPath, 'existing content', 'utf8')
      const payloadPath = path.join(dir, 'payload.yaml')
      const resumedPayload: GenerateJsonPayload = {
        schemaVersion: GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
        intent: 'resumed intent',
        model: 'gpt-4o-mini',
        targetModel: 'gpt-4o-mini',
        prompt: 'previous prompt',
        refinements: [],
        iterations: 1,
        interactive: false,
        timestamp: '2025-01-01T00:00:00.000Z',
        contextPaths: [{ path: existingPath, source: 'file' }],
      }
      await fs.writeFile(payloadPath, serializeGeneratePayload(resumedPayload, 'yaml'), 'utf8')
      const events: StreamEventInput[] = []
      await runGeneratePipeline(
        {
          ...createBaseArgs(),
          resumeFrom: payloadPath,
          resumeMode: 'best-effort',
        },
        {
          onStreamEvent: (event) => {
            events.push(event)
          },
        },
      )
      const resumeEvent = events.find(
        (event): event is Extract<StreamEventInput, { event: 'resume.loaded' }> =>
          event.event === 'resume.loaded',
      )
      expect(resumeEvent).toBeDefined()
      expect(resumeEvent?.source).toBe('file')
      expect(resumeEvent?.reusedContextPaths).toEqual([{ path: existingPath, source: 'file' }])
      expect(resumeEvent?.missingContextPaths).toEqual([])
      expect(mockResolveHistoryFilePath).not.toHaveBeenCalled()
      expect(promptService.generatePrompt).not.toHaveBeenCalled()
    } finally {
      await fs.rm(dir, { recursive: true, force: true })
    }
  })
  it('fails fast with a descriptive error when the payload file cannot be parsed', async () => {
    const dir = await createTempDir()
    try {
      const payloadPath = path.join(dir, 'payload.json')
      await fs.writeFile(payloadPath, '{ not-valid-json', 'utf8')
      await expect(
        runGeneratePipeline({
          ...createBaseArgs(),
          resumeFrom: payloadPath,
          resumeMode: 'best-effort',
        }),
      ).rejects.toThrow(/Failed to parse JSON/i)
      expect(mockCreatePromptGeneratorService).not.toHaveBeenCalled()
      expect(mockAppendToHistory).not.toHaveBeenCalled()
    } finally {
      await fs.rm(dir, { recursive: true, force: true })
    }
  })
})
</file>
<file path="src/__tests__/tui/buffered-history-writer.test.ts">
import { createBufferedHistoryWriter } from '../../tui/hooks/buffered-history-writer'
describe('buffered-history-writer', () => {
  it('buffers writes until flush, preserving order', () => {
    const push = jest.fn()
    let scheduleCalls = 0
    const writer = createBufferedHistoryWriter({
      push,
      scheduleFlush: () => {
        scheduleCalls += 1
      },
    })
    writer.pushBuffered('first')
    writer.pushBuffered('second', 'progress')
    expect(push).not.toHaveBeenCalled()
    expect(scheduleCalls).toBe(1)
    writer.flush()
    expect(push.mock.calls).toEqual([
      ['first', undefined, undefined],
      ['second', 'progress', undefined],
    ])
    writer.pushBuffered('third')
    expect(push.mock.calls).toHaveLength(2)
    expect(scheduleCalls).toBe(2)
    writer.flush()
    expect(push.mock.calls).toEqual([
      ['first', undefined, undefined],
      ['second', 'progress', undefined],
      ['third', undefined, undefined],
    ])
  })
  it('pushManyBuffered schedules once and preserves kinds', () => {
    const push = jest.fn()
    let scheduleCalls = 0
    const writer = createBufferedHistoryWriter({
      push,
      scheduleFlush: () => {
        scheduleCalls += 1
      },
    })
    writer.pushManyBuffered([
      { content: 'a', kind: 'system' },
      { content: 'b', kind: 'progress' },
      { content: 'c' },
    ])
    expect(push).not.toHaveBeenCalled()
    expect(scheduleCalls).toBe(1)
    writer.flush()
    expect(push.mock.calls).toEqual([
      ['a', 'system', undefined],
      ['b', 'progress', undefined],
      ['c', undefined, undefined],
    ])
  })
})
</file>
<file path="src/__tests__/tui/context-popup-glue-contract.test.ts">
import { renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
jest.mock('ink', () => ({
  useInput: jest.fn(),
}))
import {
  useContextPopupGlue,
  type UseContextPopupGlueOptions,
} from '../../tui/screens/command/hooks/useContextPopupGlue'
const dom = new JSDOM('<!doctype html><html><body></body></html>')
type GlobalDom = { window: Window; document: Document }
beforeAll(() => {
  const target = globalThis as unknown as GlobalDom
  target.window = dom.window as unknown as Window
  target.document = dom.window.document
})
afterAll(() => {
  const target = globalThis as unknown as Partial<GlobalDom>
  delete target.window
  delete target.document
})
describe('useContextPopupGlue (shape contract)', () => {
  it('returns stable keys and safe-to-call handlers', () => {
    const options: UseContextPopupGlueOptions = {
      inputValue: '',
      popupState: null,
      helpOpen: false,
      isPopupOpen: false,
      isCommandMode: false,
      isCommandMenuActive: false,
      isGenerating: false,
      droppedFilePath: null,
      files: [],
      urls: [],
      images: [],
      videos: [],
      pdfs: [],
      smartContextEnabled: false,
      smartContextRoot: null,
      addFile: jest.fn(),
      removeFile: jest.fn(),
      addUrl: jest.fn(),
      removeUrl: jest.fn(),
      updateUrl: jest.fn(),
      addImage: jest.fn(),
      removeImage: jest.fn(),
      addVideo: jest.fn(),
      removeVideo: jest.fn(),
      addPdf: jest.fn(),
      removePdf: jest.fn(),
      toggleSmartContext: jest.fn(),
      setSmartRoot: jest.fn(),
      setInputValue: jest.fn(),
      setPopupState: jest.fn(),
      suppressNextInput: jest.fn(),
      notify: jest.fn(),
      pushHistory: jest.fn(),
      addCommandHistoryEntry: jest.fn(),
      handleCommandSelection: jest.fn(),
      consumeSuppressedTextInputChange: jest.fn(() => false),
      isFilePath: jest.fn(() => true),
    }
    const { result } = renderHook(() => useContextPopupGlue(options))
    const expectedKeys = [
      'filePopupSuggestions',
      'filePopupSuggestionSelectionIndex',
      'filePopupSuggestionsFocused',
      'onFilePopupDraftChange',
      'onAddFile',
      'onRemoveFile',
      'onUrlPopupDraftChange',
      'onAddUrl',
      'onRemoveUrl',
      'imagePopupSuggestions',
      'imagePopupSuggestionSelectionIndex',
      'imagePopupSuggestionsFocused',
      'onImagePopupDraftChange',
      'onAddImage',
      'onRemoveImage',
      'videoPopupSuggestions',
      'videoPopupSuggestionSelectionIndex',
      'videoPopupSuggestionsFocused',
      'onVideoPopupDraftChange',
      'onAddVideo',
      'onRemoveVideo',
      'pdfPopupSuggestions',
      'pdfPopupSuggestionSelectionIndex',
      'pdfPopupSuggestionsFocused',
      'onPdfPopupDraftChange',
      'onAddPdf',
      'onRemovePdf',
      'smartPopupSuggestions',
      'smartPopupSuggestionSelectionIndex',
      'smartPopupSuggestionsFocused',
      'onSmartPopupDraftChange',
      'onSmartToggle',
      'onSmartRootSubmit',
    ].sort()
    expect(Object.keys(result.current).sort()).toEqual(expectedKeys)
    expect(() => result.current.onFilePopupDraftChange('x')).not.toThrow()
    expect(() => result.current.onAddFile('/tmp/file.txt')).not.toThrow()
    expect(() => result.current.onRemoveFile(0)).not.toThrow()
    expect(() => result.current.onUrlPopupDraftChange('https://example.com')).not.toThrow()
    expect(() => result.current.onAddUrl('https://example.com')).not.toThrow()
    expect(() => result.current.onRemoveUrl(0)).not.toThrow()
    expect(() => result.current.onImagePopupDraftChange('x')).not.toThrow()
    expect(() => result.current.onAddImage('/tmp/image.png')).not.toThrow()
    expect(() => result.current.onRemoveImage(0)).not.toThrow()
    expect(() => result.current.onVideoPopupDraftChange('x')).not.toThrow()
    expect(() => result.current.onAddVideo('/tmp/video.mp4')).not.toThrow()
    expect(() => result.current.onRemoveVideo(0)).not.toThrow()
    expect(() => result.current.onSmartPopupDraftChange('x')).not.toThrow()
    expect(() => result.current.onSmartToggle(true)).not.toThrow()
    expect(() => result.current.onSmartRootSubmit('/tmp')).not.toThrow()
  })
})
</file>
<file path="src/__tests__/tui/generation-history-formatters.test.ts">
import {
  buildIterationCompleteHistoryMessages,
  buildJsonPayloadHistoryMessages,
  wrapTextForHistory,
} from '../../tui/hooks/generation-history-formatters'
jest.mock('wrap-ansi', () => jest.fn())
const wrapAnsiMock = jest.requireMock('wrap-ansi') as jest.Mock
describe('generation-history-formatters', () => {
  beforeEach(() => {
    wrapAnsiMock.mockReset()
  })
  it('wrapTextForHistory preserves order and splits wrapped lines', () => {
    wrapAnsiMock.mockImplementation((text: string) => {
      if (text === 'abc') {
        return 'a\nbc'
      }
      return text
    })
    expect(wrapTextForHistory('abc\ndef', 12)).toEqual(['a', 'bc', 'def'])
    expect(wrapAnsiMock).toHaveBeenCalledTimes(2)
    expect(wrapAnsiMock).toHaveBeenCalledWith('abc', 12, expect.objectContaining({ hard: true }))
    expect(wrapAnsiMock).toHaveBeenCalledWith('def', 12, expect.objectContaining({ hard: true }))
  })
  it('buildIterationCompleteHistoryMessages matches existing text output', () => {
    wrapAnsiMock.mockImplementation((text: string) => text)
    expect(
      buildIterationCompleteHistoryMessages({
        iteration: 2,
        tokens: 123,
        prompt: 'Line 1\nLine 2',
        wrapWidth: 80,
      }),
    ).toEqual([
      { content: 'Iteration 2 complete (123 tokens)', kind: 'progress' },
      { content: 'Prompt (iteration 2):', kind: 'system' },
      { content: 'Line 1', kind: 'system', format: 'markdown' },
      { content: 'Line 2', kind: 'system', format: 'markdown' },
    ])
    expect(
      buildIterationCompleteHistoryMessages({
        iteration: 1,
        tokens: 100,
        reasoningTokens: 50,
        prompt: 'Only line',
        wrapWidth: 80,
      })[0],
    ).toEqual({
      content: 'Iteration 1 complete (100 prompt tokens Âˇ 50 reasoning tokens)',
      kind: 'progress',
    })
  })
  it('buildJsonPayloadHistoryMessages pretty prints the payload', () => {
    wrapAnsiMock.mockImplementation((text: string) => text)
    expect(buildJsonPayloadHistoryMessages({ a: 1 }, 80)).toEqual([
      { content: 'JSON payload:', kind: 'system' },
      { content: '{', kind: 'system' },
      { content: '  "a": 1', kind: 'system' },
      { content: '}', kind: 'system' },
    ])
  })
})
</file>
<file path="src/__tests__/tui/list-windowing.test.ts">
import {
  clampSelectionIndex,
  ensureLeadingHeaderVisible,
  resolveWindowedValues,
} from '../../tui/components/popups/list-windowing'
describe('list-windowing helpers', () => {
  describe('clampSelectionIndex', () => {
    it('returns 0 for empty lists', () => {
      expect(clampSelectionIndex(0, 0)).toBe(0)
      expect(clampSelectionIndex(0, 10)).toBe(0)
      expect(clampSelectionIndex(0, -5)).toBe(0)
    })
    it('clamps within list bounds', () => {
      expect(clampSelectionIndex(3, -1)).toBe(0)
      expect(clampSelectionIndex(3, 0)).toBe(0)
      expect(clampSelectionIndex(3, 2)).toBe(2)
      expect(clampSelectionIndex(3, 3)).toBe(2)
    })
  })
  describe('resolveWindowedValues', () => {
    it('matches resolveWindowedList boundaries', async () => {
      const { resolveWindowedList } = await import('../../tui/components/popups/list-window')
      const items = ['a', 'b', 'c', 'd', 'e', 'f']
      const result = resolveWindowedValues(items, 3, 4)
      const window = resolveWindowedList({
        itemCount: items.length,
        selectedIndex: 3,
        maxVisibleRows: 4,
      })
      expect(result.start).toBe(window.start)
      expect(result.end).toBe(window.end)
      expect(result.showBefore).toBe(window.showBefore)
      expect(result.showAfter).toBe(window.showAfter)
      expect(result.values).toEqual(items.slice(window.start, window.end))
    })
    it('handles empty items', () => {
      expect(resolveWindowedValues([], 0, 3)).toEqual({
        start: 0,
        end: 0,
        values: [],
        showBefore: false,
        showAfter: false,
      })
    })
    it('shows a single item without indicators', () => {
      expect(resolveWindowedValues(['only'], 0, 3)).toEqual({
        start: 0,
        end: 1,
        values: ['only'],
        showBefore: false,
        showAfter: false,
      })
    })
    it('windows with small row counts and indicators', () => {
      const items = ['a', 'b', 'c', 'd', 'e']
      expect(resolveWindowedValues(items, 0, 3)).toEqual({
        start: 0,
        end: 2,
        values: ['a', 'b'],
        showBefore: false,
        showAfter: true,
      })
      expect(resolveWindowedValues(items, 2, 3)).toEqual({
        start: 2,
        end: 3,
        values: ['c'],
        showBefore: true,
        showAfter: true,
      })
      expect(resolveWindowedValues(items, 4, 3)).toEqual({
        start: 3,
        end: 5,
        values: ['d', 'e'],
        showBefore: true,
        showAfter: false,
      })
    })
    it('supports a custom lead', () => {
      const items = Array.from({ length: 10 }, (_, index) => String(index))
      expect(resolveWindowedValues(items, 5, 5, { lead: 1 })).toEqual({
        start: 4,
        end: 7,
        values: ['4', '5', '6'],
        showBefore: true,
        showAfter: true,
      })
    })
    it('moves the window as the selection changes', () => {
      const items = Array.from({ length: 10 }, (_, index) => String(index))
      expect(resolveWindowedValues(items, 0, 5).start).toBe(0)
      expect(resolveWindowedValues(items, 5, 5).start).toBe(3)
      expect(resolveWindowedValues(items, 9, 5)).toEqual({
        start: 6,
        end: 10,
        values: ['6', '7', '8', '9'],
        showBefore: true,
        showAfter: false,
      })
    })
  })
  describe('ensureLeadingHeaderVisible', () => {
    type Row =
      | { type: 'header'; title: string }
      | { type: 'spacer' }
      | { type: 'option'; id: string }
    it('pulls the header into view when there is slack', () => {
      const rows: Row[] = [
        { type: 'header', title: 'Recent' },
        { type: 'option', id: 'a' },
        { type: 'option', id: 'b' },
      ]
      expect(ensureLeadingHeaderVisible(rows, { start: 1, end: 3 }, 5, 'header', 'option')).toEqual(
        {
          start: 0,
          end: 3,
        },
      )
    })
    it('does not shift the window when already full', () => {
      const rows: Row[] = [
        { type: 'header', title: 'Recent' },
        { type: 'option', id: 'a' },
        { type: 'option', id: 'b' },
        { type: 'option', id: 'c' },
        { type: 'option', id: 'd' },
        { type: 'option', id: 'e' },
      ]
      expect(ensureLeadingHeaderVisible(rows, { start: 1, end: 6 }, 5, 'header', 'option')).toEqual(
        {
          start: 1,
          end: 6,
        },
      )
    })
    it('does not shift when the previous row is not a header', () => {
      const rows: Row[] = [
        { type: 'spacer' },
        { type: 'option', id: 'a' },
        { type: 'option', id: 'b' },
      ]
      expect(ensureLeadingHeaderVisible(rows, { start: 1, end: 3 }, 5, 'header', 'option')).toEqual(
        {
          start: 1,
          end: 3,
        },
      )
    })
  })
})
</file>
<file path="src/__tests__/tui/model-popup-model.test.ts">
import {
  buildModelPopupRows,
  resolveModelPopupListRows,
  resolveModelPopupVisibleRows,
} from '../../tui/components/popups/model-popup-model'
import type { ModelProvider } from '../../model-providers'
import type { ModelOption } from '../../tui/types'
import type { ModelPopupRow } from '../../tui/components/popups/model-popup-model'
const makeOption = (id: string, provider: ModelProvider): ModelOption => ({
  id,
  label: id,
  provider,
  description: '',
  capabilities: [],
  source: 'builtin',
})
const describeRows = (rows: readonly ModelPopupRow[]): string[] =>
  rows.map((row) => {
    if (row.type === 'header') {
      return `H:${row.title}`
    }
    if (row.type === 'spacer') {
      return 'S'
    }
    return `O:${row.option.id}:${row.optionIndex}`
  })
describe('model-popup-model', () => {
  describe('resolveModelPopupListRows', () => {
    it('reserves padding and fixed rows', () => {
      expect(resolveModelPopupListRows({ popupHeight: 16, paddingY: 2, fixedRows: 6 })).toBe(6)
    })
    it('clamps to at least one row', () => {
      expect(resolveModelPopupListRows({ popupHeight: 0, paddingY: 2, fixedRows: 6 })).toBe(1)
    })
  })
  describe('buildModelPopupRows', () => {
    it('groups recents separately from provider sections', () => {
      const options = [
        makeOption('o0', 'openai'),
        makeOption('o1', 'openai'),
        makeOption('g2', 'gemini'),
        makeOption('g3', 'gemini'),
        makeOption('c4', 'other'),
      ]
      const rows = buildModelPopupRows(options, 2)
      expect(describeRows(rows)).toEqual([
        'H:Recent',
        'O:o0:0',
        'O:o1:1',
        'S',
        'H:Gemini',
        'O:g2:2',
        'O:g3:3',
        'H:Custom',
        'O:c4:4',
      ])
    })
    it('clamps recentCount to the option length', () => {
      const options = [makeOption('o0', 'openai'), makeOption('o1', 'openai')]
      const rows = buildModelPopupRows(options, 99)
      expect(describeRows(rows)).toEqual(['H:Recent', 'O:o0:0', 'O:o1:1'])
    })
  })
  describe('resolveModelPopupVisibleRows', () => {
    const options = [
      makeOption('openai-0', 'openai'),
      makeOption('openai-1', 'openai'),
      makeOption('openai-2', 'openai'),
      makeOption('openai-3', 'openai'),
      makeOption('openai-4', 'openai'),
      makeOption('openai-5', 'openai'),
      makeOption('gemini-6', 'gemini'),
      makeOption('gemini-7', 'gemini'),
      makeOption('gemini-8', 'gemini'),
      makeOption('gemini-9', 'gemini'),
    ]
    const rows = buildModelPopupRows(options, 0)
    it('pulls a section header into view when there is slack', () => {
      const result = resolveModelPopupVisibleRows({
        rows,
        selectedOptionIndex: 9,
        maxVisibleRows: 5,
      })
      expect(result.slice.start).toBe(7)
      expect(describeRows(result.visibleRows)).toEqual([
        'H:Gemini',
        'O:gemini-6:6',
        'O:gemini-7:7',
        'O:gemini-8:8',
        'O:gemini-9:9',
      ])
    })
    it('moves the window as the selection changes', () => {
      const top = resolveModelPopupVisibleRows({
        rows,
        selectedOptionIndex: 0,
        maxVisibleRows: 5,
      })
      const middle = resolveModelPopupVisibleRows({
        rows,
        selectedOptionIndex: 5,
        maxVisibleRows: 5,
      })
      const end = resolveModelPopupVisibleRows({
        rows,
        selectedOptionIndex: 9,
        maxVisibleRows: 5,
      })
      expect(top.slice.start).toBe(0)
      expect(describeRows(top.visibleRows)).toEqual([
        'H:OpenAI',
        'O:openai-0:0',
        'O:openai-1:1',
        'O:openai-2:2',
        'S',
      ])
      expect(middle.slice.start).toBe(4)
      expect(describeRows(middle.visibleRows)).toEqual([
        'O:openai-3:3',
        'O:openai-4:4',
        'O:openai-5:5',
        'S',
        'S',
      ])
      expect(end.slice.start).toBe(7)
    })
  })
})
</file>
<file path="src/__tests__/tui/tui-exit-clear-screen.test.ts">
import { runTuiCommand } from '../../tui'
type InkModule = {
  render: jest.Mock
}
jest.mock('ink', () => ({
  render: jest.fn(),
}))
jest.mock('../../tui/AppContainer', () => ({
  __esModule: true,
  AppContainer: () => null,
}))
describe('runTuiCommand exit clear', () => {
  const getInkMock = (): InkModule => jest.requireMock('ink') as InkModule
  const mockStdoutIsTTY = (value: boolean): (() => void) => {
    const proto = Object.getPrototypeOf(process.stdout) as object | null
    const descriptor =
      Object.getOwnPropertyDescriptor(process.stdout, 'isTTY') ??
      (proto ? Object.getOwnPropertyDescriptor(proto, 'isTTY') : undefined)
    if (descriptor?.get) {
      const spy = jest
        .spyOn(process.stdout as unknown as { isTTY: boolean }, 'isTTY', 'get')
        .mockReturnValue(value)
      return () => spy.mockRestore()
    }
    const original = process.stdout.isTTY
    Object.defineProperty(process.stdout, 'isTTY', { value, configurable: true })
    return () => {
      Object.defineProperty(process.stdout, 'isTTY', { value: original, configurable: true })
    }
  }
  it('clears terminal after Ink exits', async () => {
    const stdoutWrite = jest.spyOn(process.stdout, 'write').mockImplementation(() => true)
    const restoreIsTTY = mockStdoutIsTTY(true)
    try {
      const waitUntilExit = jest.fn(async () => {})
      getInkMock().render.mockReturnValue({ waitUntilExit })
      await runTuiCommand([])
      expect(waitUntilExit).toHaveBeenCalledTimes(1)
      expect(stdoutWrite).toHaveBeenCalledWith('\u001b[0m\u001b[2J\u001b[H')
    } finally {
      stdoutWrite.mockRestore()
      restoreIsTTY()
    }
  })
  it('skips clear when not a TTY', async () => {
    const stdoutWrite = jest.spyOn(process.stdout, 'write').mockImplementation(() => true)
    const restoreIsTTY = mockStdoutIsTTY(false)
    try {
      const waitUntilExit = jest.fn(async () => {})
      getInkMock().render.mockReturnValue({ waitUntilExit })
      await runTuiCommand([])
      expect(waitUntilExit).toHaveBeenCalledTimes(1)
      expect(stdoutWrite).not.toHaveBeenCalled()
    } finally {
      stdoutWrite.mockRestore()
      restoreIsTTY()
    }
  })
})
</file>
<file path="src/__tests__/generate-args.test.ts">
import { extractIntentArg, parseGenerateArgs, stripHelpFlags } from '../generate/args'
describe('generate args helpers', () => {
  describe('extractIntentArg', () => {
    it('detects positional intent after -i/--interactive', () => {
      const result = extractIntentArg(['-i', 'intent.md', '--json'])
      expect(result.positionalIntent).toBe('intent.md')
      expect(result.positionalIntentAfterInteractive).toBe(true)
      expect(result.optionArgs).toEqual(['-i', '--json'])
    })
    it('consumes value flags without treating them as intent', () => {
      const result = extractIntentArg(['--context', 'src/**/*.ts', 'do the thing'])
      expect(result.positionalIntent).toBe('do the thing')
      expect(result.optionArgs).toEqual(['--context', 'src/**/*.ts'])
    })
    it('treats resume selectors as value flags', () => {
      const fromHistory = extractIntentArg(['--resume', 'last:2', 'do the thing'])
      expect(fromHistory.positionalIntent).toBe('do the thing')
      expect(fromHistory.optionArgs).toEqual(['--resume', 'last:2'])
      const fromFile = extractIntentArg(['--resume-from', 'payload.jsonl', 'do the thing'])
      expect(fromFile.positionalIntent).toBe('do the thing')
      expect(fromFile.optionArgs).toEqual(['--resume-from', 'payload.jsonl'])
    })
  })
  describe('stripHelpFlags', () => {
    it('removes help flags before -- passthrough', () => {
      const result = stripHelpFlags(['--help', '-h', '--json'])
      expect(result.helpRequested).toBe(true)
      expect(result.optionArgs).toEqual(['--json'])
    })
    it('preserves help tokens after -- passthrough', () => {
      const result = stripHelpFlags(['--help', '--', '--help', '-h'])
      expect(result.helpRequested).toBe(true)
      expect(result.optionArgs).toEqual(['--', '--help', '-h'])
    })
  })
  describe('parseGenerateArgs', () => {
    it('parses token budget flags without consuming intent', () => {
      const parsed = parseGenerateArgs(['--max-input-tokens', '100', 'do the thing'])
      expect(parsed.args.maxInputTokens).toBe(100)
      expect(parsed.args.intent).toBe('do the thing')
    })
    it('rejects non-positive integer token budgets', () => {
      expect(() => parseGenerateArgs(['--max-input-tokens', '0'])).toThrow(
        '--max-input-tokens must be a positive integer.',
      )
      expect(() => parseGenerateArgs(['--max-context-tokens', '1.5'])).toThrow(
        '--max-context-tokens must be a positive integer.',
      )
    })
    it('parses validated context overflow strategy', () => {
      const parsed = parseGenerateArgs(['--context-overflow', 'drop-oldest'])
      expect(parsed.args.contextOverflow).toBe('drop-oldest')
    })
    it('rejects invalid context overflow strategy', () => {
      expect(() => parseGenerateArgs(['--context-overflow', 'nope'])).toThrow()
    })
    it('parses --resume selector without consuming intent', () => {
      const parsed = parseGenerateArgs(['--resume', 'last', 'do the thing'])
      expect(parsed.args.resume).toBe('last')
      expect(parsed.args.resumeMode).toBe('best-effort')
      expect(parsed.args.intent).toBe('do the thing')
    })
    it('rejects conflicting resume flags', () => {
      expect(() => parseGenerateArgs(['--resume', 'last', '--resume-last'])).toThrow(
        /--resume and --resume-last cannot be combined/i,
      )
      expect(() =>
        parseGenerateArgs(['--resume-from', 'payload.jsonl', '--resume', 'last']),
      ).toThrow(/--resume-from cannot be combined/i)
      expect(() => parseGenerateArgs(['--resume-from', 'payload.jsonl', '--resume-last'])).toThrow(
        /--resume-from cannot be combined/i,
      )
    })
    it('rejects invalid resume-mode choices', () => {
      expect(() => parseGenerateArgs(['--resume-mode', 'nope'])).toThrow(/resume-mode/i)
    })
  })
})
</file>
<file path="src/__tests__/media-loader.test.ts">
import {
  inferPdfMimeType,
  inferVideoMimeType,
  uploadFileForGemini,
  uploadFileForGeminiWithMimeType,
} from '../media-loader'
jest.mock('node:fs/promises', () => ({
  access: jest.fn(),
}))
jest.mock('@google/generative-ai/server', () => {
  const manager = {
    uploadFile: jest.fn(),
    getFile: jest.fn(),
  }
  return {
    GoogleAIFileManager: jest.fn().mockImplementation(() => manager),
    __managerMock: manager,
  }
})
const fs = jest.requireMock('node:fs/promises') as { access: jest.Mock }
const googleModule = jest.requireMock('@google/generative-ai/server') as {
  GoogleAIFileManager: jest.Mock
  __managerMock: { uploadFile: jest.Mock; getFile: jest.Mock }
}
const manager = googleModule.__managerMock
describe('media-loader', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    googleModule.GoogleAIFileManager.mockImplementation(() => manager)
    process.env.GEMINI_API_KEY = 'gem-key'
    manager.uploadFile.mockReset()
    manager.getFile.mockReset()
    manager.uploadFile.mockResolvedValue({ file: { name: 'files/123' } })
    manager.getFile.mockResolvedValue({ state: 'ACTIVE', uri: 'gs://files/123' })
    fs.access.mockResolvedValue(undefined)
  })
  it('infers known video mime types', () => {
    expect(inferVideoMimeType('clip.mp4')).toBe('video/mp4')
    expect(() => inferVideoMimeType('clip.txt')).toThrow('Unsupported media type')
  })
  it('infers PDF mime type', () => {
    expect(inferPdfMimeType('doc.pdf')).toBe('application/pdf')
    expect(() => inferPdfMimeType('doc.txt')).toThrow('Unsupported PDF type')
  })
  it('uploads file and waits for active state', async () => {
    process.env.GEMINI_API_KEY = 'env-key'
    const uri = await uploadFileForGemini('clip.mp4', 'injected-key')
    expect(googleModule.GoogleAIFileManager).toHaveBeenCalledWith('injected-key')
    expect(manager.uploadFile).toHaveBeenCalledWith('clip.mp4', {
      mimeType: 'video/mp4',
      displayName: 'clip.mp4',
    })
    expect(uri).toBe('gs://files/123')
  })
  it('uploads arbitrary mime types via uploadFileForGeminiWithMimeType', async () => {
    const uri = await uploadFileForGeminiWithMimeType('doc.pdf', 'application/pdf', 'injected-key')
    expect(googleModule.GoogleAIFileManager).toHaveBeenCalledWith('injected-key')
    expect(manager.uploadFile).toHaveBeenCalledWith('doc.pdf', {
      mimeType: 'application/pdf',
      displayName: 'doc.pdf',
    })
    expect(uri).toBe('gs://files/123')
  })
  it('falls back to GEMINI_API_KEY when no key is provided', async () => {
    const uri = await uploadFileForGemini('clip.mp4')
    expect(googleModule.GoogleAIFileManager).toHaveBeenCalledWith('gem-key')
    expect(uri).toBe('gs://files/123')
  })
  it('throws when file is not readable', async () => {
    fs.access.mockRejectedValue(new Error('denied'))
    await expect(uploadFileForGemini('missing.mp4')).rejects.toThrow('is not readable')
  })
  it('throws when Gemini reports failure', async () => {
    manager.getFile.mockResolvedValueOnce({ state: 'FAILED', error: { message: 'bad' } })
    await expect(uploadFileForGemini('clip.mp4')).rejects.toThrow('bad')
  })
  it('requires GEMINI_API_KEY to be present', async () => {
    process.env.GEMINI_API_KEY = ''
    await expect(uploadFileForGemini('clip.mp4')).rejects.toThrow('Gemini API key')
  })
})
</file>
<file path="src/generate/interactive.ts">
import chalk from 'chalk'
import enquirer from 'enquirer'
import type {
  createPromptGeneratorService,
  PromptGenerationRequest,
  UploadStateChange,
} from '../prompt-generator-service'
import { countTokens } from '../token-counter'
import { displayPrompt } from './display'
import type { StreamDispatcher } from './stream'
import { displayTokenSummary } from './token-telemetry'
import type { InteractiveDelegate, InteractiveMode, LoopContext, TokenTelemetry } from './types'
import type { InteractiveTransport } from './interactive-transport'
const { prompt } = enquirer as typeof import('enquirer')
type PromptGenerator = Awaited<ReturnType<typeof createPromptGeneratorService>>
type GenerateAndMaybeDisplayContext = LoopContext & {
  iteration: number
  previousPrompt?: string
  latestRefinement?: string
}
export const runGenerationWorkflow = async ({
  service,
  context,
  telemetry,
  interactiveMode,
  interactiveTransport,
  interactiveDelegate,
  display,
  stream,
  onUploadStateChange,
  resume,
}: {
  service: PromptGenerator
  context: LoopContext
  telemetry: TokenTelemetry
  interactiveMode: InteractiveMode
  interactiveTransport?: InteractiveTransport | null
  interactiveDelegate?: InteractiveDelegate | undefined
  display: boolean
  stream: StreamDispatcher
  onUploadStateChange?: UploadStateChange
  resume?: { prompt: string; iterations: number } | undefined
}): Promise<{ prompt: string; reasoning?: string; iterations: number }> => {
  let iteration = 0
  let currentPrompt = ''
  let currentReasoning: string | undefined
  if (display) {
    displayTokenSummary(telemetry)
  }
  const inputTokens = telemetry.totalTokens
  if (resume) {
    currentPrompt = resume.prompt
    iteration = Math.max(0, resume.iterations)
    if (display) {
      const displayedIteration = iteration > 0 ? iteration : 1
      displayPrompt(currentPrompt, displayedIteration, countTokens(currentPrompt))
    }
  } else {
    iteration += 1
    const initialGeneration = await generateAndMaybeDisplay(
      service,
      { ...context, iteration },
      display,
      stream,
      inputTokens,
      interactiveMode !== 'none',
      onUploadStateChange,
    )
    currentPrompt = initialGeneration.prompt
    currentReasoning = initialGeneration.reasoning
  }
  if (interactiveMode !== 'none') {
    stream.emit({ event: 'interactive.state', phase: 'start', iteration })
    stream.emit({ event: 'interactive.state', phase: 'prompt', iteration })
    if (interactiveMode === 'transport' && interactiveTransport) {
      // eslint-disable-next-line no-constant-condition
      while (true) {
        stream.emit({ event: 'interactive.awaiting', mode: interactiveMode })
        const command = await interactiveTransport.nextCommand()
        if (!command || command.type === 'finish') {
          break
        }
        const instruction = command.instruction.trim()
        if (!instruction) {
          continue
        }
        stream.emit({ event: 'interactive.state', phase: 'refine', iteration })
        context.refinements.push(instruction)
        iteration += 1
        const refinementGeneration = await generateAndMaybeDisplay(
          service,
          {
            ...context,
            iteration,
            previousPrompt: currentPrompt,
            latestRefinement: instruction,
          },
          display,
          stream,
          inputTokens,
          true,
          onUploadStateChange,
        )
        currentPrompt = refinementGeneration.prompt
        currentReasoning = refinementGeneration.reasoning
        stream.emit({ event: 'interactive.state', phase: 'prompt', iteration })
      }
    } else if (interactiveDelegate) {
      // eslint-disable-next-line no-constant-condition
      while (true) {
        stream.emit({ event: 'interactive.awaiting', mode: interactiveMode })
        const action = await interactiveDelegate.getNextAction({ iteration, currentPrompt })
        if (!action || action.type === 'finish') {
          break
        }
        const refinement = action.instruction.trim()
        if (!refinement) {
          continue
        }
        stream.emit({ event: 'interactive.state', phase: 'refine', iteration })
        context.refinements.push(refinement)
        iteration += 1
        const refinementGeneration = await generateAndMaybeDisplay(
          service,
          {
            ...context,
            iteration,
            previousPrompt: currentPrompt,
            latestRefinement: refinement,
          },
          display,
          stream,
          inputTokens,
          true,
          onUploadStateChange,
        )
        currentPrompt = refinementGeneration.prompt
        currentReasoning = refinementGeneration.reasoning
        stream.emit({ event: 'interactive.state', phase: 'prompt', iteration })
      }
    } else {
      // eslint-disable-next-line no-constant-condition
      while (true) {
        stream.emit({ event: 'interactive.awaiting', mode: interactiveMode })
        const wantsRefine = await askShouldRefine()
        if (!wantsRefine) {
          break
        }
        stream.emit({ event: 'interactive.state', phase: 'refine', iteration })
        const refinement = await collectRefinementInstruction()
        if (!refinement) {
          console.log(chalk.dim('No refinement provided. Ending interactive session.'))
          break
        }
        context.refinements.push(refinement)
        iteration += 1
        const refinementGeneration = await generateAndMaybeDisplay(
          service,
          {
            ...context,
            iteration,
            previousPrompt: currentPrompt,
            latestRefinement: refinement,
          },
          display,
          stream,
          inputTokens,
          true,
          onUploadStateChange,
        )
        currentPrompt = refinementGeneration.prompt
        currentReasoning = refinementGeneration.reasoning
        stream.emit({ event: 'interactive.state', phase: 'prompt', iteration })
      }
    }
    stream.emit({ event: 'interactive.state', phase: 'complete', iteration })
  }
  return {
    prompt: currentPrompt,
    ...(typeof currentReasoning === 'string' ? { reasoning: currentReasoning } : {}),
    iterations: iteration,
  }
}
const generateAndMaybeDisplay = async (
  service: PromptGenerator,
  context: GenerateAndMaybeDisplayContext,
  display: boolean,
  stream: StreamDispatcher,
  inputTokens: number,
  interactive: boolean,
  onUploadStateChange?: UploadStateChange,
): Promise<{ prompt: string; reasoning?: string }> => {
  const request: PromptGenerationRequest = {
    intent: context.intent,
    model: context.model,
    targetModel: context.targetModel,
    fileContext: context.fileContext,
    images: context.images,
    videos: context.videos,
    pdfs: context.pdfs,
    onPromptAutoRepairAttempt: (detail) => {
      stream.emit({
        event: 'progress.update',
        scope: 'generate',
        state: 'update',
        label: `Auto-repair (${detail.kind}) ${detail.reason} (${detail.attempt}/${detail.maxAttempts})`,
      })
    },
  }
  if (context.metaInstructions) {
    request.metaInstructions = context.metaInstructions
  }
  if (onUploadStateChange) {
    request.onUploadStateChange = onUploadStateChange
  }
  if (context.previousPrompt && context.latestRefinement) {
    request.previousPrompt = context.previousPrompt
    request.refinementInstruction = context.latestRefinement
  }
  stream.emit({
    event: 'generation.iteration.start',
    iteration: context.iteration,
    intent: context.intent,
    model: context.model,
    interactive,
    inputTokens,
    refinements: [...context.refinements],
    ...(context.latestRefinement ? { latestRefinement: context.latestRefinement } : {}),
  })
  const generator = service as unknown as {
    generatePrompt: (request: PromptGenerationRequest) => Promise<string>
    generatePromptDetailed?: (request: PromptGenerationRequest) => Promise<{
      prompt: string
      reasoning?: string
    }>
  }
  const detailed =
    typeof generator.generatePromptDetailed === 'function'
      ? await generator.generatePromptDetailed(request)
      : { prompt: await generator.generatePrompt(request) }
  const outputTokens = countTokens(detailed.prompt)
  const reasoningTokens = detailed.reasoning ? countTokens(detailed.reasoning) : 0
  stream.emit({
    event: 'generation.iteration.complete',
    iteration: context.iteration,
    prompt: detailed.prompt,
    tokens: outputTokens,
    ...(reasoningTokens > 0 ? { reasoningTokens } : {}),
  })
  if (display) {
    displayPrompt(detailed.prompt, context.iteration, outputTokens)
  }
  return {
    prompt: detailed.prompt,
    ...(typeof detailed.reasoning === 'string' ? { reasoning: detailed.reasoning } : {}),
  }
}
const askShouldRefine = async (): Promise<boolean> => {
  try {
    const response = await prompt<{ refine: boolean }>({
      type: 'confirm',
      name: 'refine',
      message: chalk.cyan('Refine the generated prompt?'),
      initial: false,
    })
    return Boolean(response.refine)
  } catch (error) {
    if (isPromptCancellation(error)) {
      console.log(chalk.dim('\nInteractive session cancelled.'))
      return false
    }
    throw error
  }
}
const collectRefinementInstruction = async (): Promise<string | null> => {
  try {
    const response = await prompt<{ refinement: string }>({
      type: 'input',
      name: 'refinement',
      message: chalk.cyan('Describe the refinement (blank to finish):'),
      multiline: true,
    })
    const refinement = response.refinement?.trim()
    return refinement || null
  } catch (error) {
    if (isPromptCancellation(error)) {
      console.log(chalk.dim('\nRefinement input cancelled.'))
      return null
    }
    throw error
  }
}
const isPromptCancellation = (error: unknown): boolean => {
  if (typeof error === 'string') {
    return true
  }
  if (error instanceof Error) {
    const message = error.message.toLowerCase()
    return message.includes('cancel') || message.includes('abort')
  }
  return false
}
</file>
<file path="src/prompt-generator/message-builders.ts">
import type { MessageContent, TextPart } from '@prompt-maker/core'
import { formatContextForPrompt, type FileContext } from '../file-context'
import { resolveImageParts } from '../image-loader'
import { resolvePdfParts } from './pdf-parts'
import type { UploadStateChange } from './types'
import { resolveVideoParts } from './video-parts'
const buildPdfAttachmentSection = (pdfPaths: string[]): string | null => {
  if (pdfPaths.length === 0) {
    return null
  }
  return [
    'PDF Attachments (already provided as context):',
    pdfPaths.join('\n'),
    '',
    'Non-negotiable requirements for the prompt contract you will generate:',
    '- Treat the attached PDF as the source document and as already-available input.',
    '- In the contract "Inputs" section, do NOT ask the user to paste the document or provide a file/path.',
    '- Proceed assuming the executing assistant has the PDF attached.',
    '- Only request OCR / extracted text if you cannot access readable text from the PDF and you explicitly say so.',
  ].join('\n')
}
export const buildInitialUserMessageText = (
  intent: string,
  files: FileContext[],
  pdfPaths: string[],
  metaInstructions?: string,
): string => {
  const sections: string[] = []
  if (files.length > 0) {
    sections.push('Context Files:\n' + formatContextForPrompt(files))
  }
  const pdfSection = buildPdfAttachmentSection(pdfPaths)
  if (pdfSection) {
    sections.push(pdfSection)
  }
  sections.push(`User Intent:\n${intent.trim()}`)
  const trimmedInstructions = metaInstructions?.trim()
  if (trimmedInstructions) {
    sections.push(`Meta-Instructions:\n${trimmedInstructions}`)
  }
  sections.push(
    [
      'Return the final structured prompt contract now.',
      'Do NOT perform the task yourself; only craft instructions for another assistant using the required sections.',
    ].join(' '),
  )
  return sections.join('\n\n')
}
export const buildInitialUserMessage = async (
  intent: string,
  files: FileContext[],
  imagePaths: string[],
  videoPaths: string[],
  pdfPaths: string[],
  metaInstructions?: string,
  onUploadStateChange?: UploadStateChange,
  apiKey?: string,
): Promise<MessageContent> => {
  const text = buildInitialUserMessageText(intent, files, pdfPaths, metaInstructions)
  return await mergeMediaWithText(
    text,
    imagePaths,
    videoPaths,
    pdfPaths,
    onUploadStateChange,
    apiKey,
  )
}
export const buildRefinementMessageText = (
  previousPrompt: string,
  refinementInstruction: string,
  intent: string,
  files: FileContext[],
  pdfPaths: string[],
  metaInstructions?: string,
): string => {
  const sections: string[] = []
  if (files.length > 0) {
    sections.push('Context Files:\n' + formatContextForPrompt(files))
  }
  const pdfSection = buildPdfAttachmentSection(pdfPaths)
  if (pdfSection) {
    sections.push(pdfSection)
  }
  sections.push(`Original Intent (for reference):\n${intent.trim()}`)
  sections.push(`Current Prompt Draft:\n${previousPrompt}`)
  sections.push(`Refinement Instruction:\n${refinementInstruction.trim()}`)
  const trimmedInstructions = metaInstructions?.trim()
  if (trimmedInstructions) {
    sections.push(`Meta-Instructions:\n${trimmedInstructions}`)
  }
  sections.push(
    [
      'Return the fully updated prompt contract.',
      'Maintain the required sections and continue to avoid performing the task yourself.',
    ].join(' '),
  )
  return sections.join('\n\n')
}
export const buildRefinementMessage = async (
  previousPrompt: string,
  refinementInstruction: string,
  intent: string,
  files: FileContext[],
  imagePaths: string[],
  videoPaths: string[],
  pdfPaths: string[],
  metaInstructions?: string,
  onUploadStateChange?: UploadStateChange,
  apiKey?: string,
): Promise<MessageContent> => {
  const text = buildRefinementMessageText(
    previousPrompt,
    refinementInstruction,
    intent,
    files,
    pdfPaths,
    metaInstructions,
  )
  return await mergeMediaWithText(
    text,
    imagePaths,
    videoPaths,
    pdfPaths,
    onUploadStateChange,
    apiKey,
  )
}
export const buildSeriesUserMessageText = (
  intent: string,
  files: FileContext[],
  pdfPaths: string[],
  metaInstructions?: string,
): string => {
  const sections: string[] = []
  if (files.length > 0) {
    sections.push('Context Files:\n' + formatContextForPrompt(files))
  }
  const pdfSection = buildPdfAttachmentSection(pdfPaths)
  if (pdfSection) {
    sections.push(pdfSection)
  }
  sections.push(`User Intent:\n${intent.trim()}`)
  const trimmedInstructions = metaInstructions?.trim()
  if (trimmedInstructions) {
    sections.push(`Meta-Instructions:\n${trimmedInstructions}`)
  }
  sections.push(
    [
      'Task:',
      'Design a planning artifact consisting of one overview prompt plus a set of atomic prompts.',
      'Each atomic prompt must be self-contained, target a specific verifiable state change, and include a "Validation" section describing how a human can confirm completion.',
      'Do not perform the tasks; only describe them.',
    ].join(' '),
  )
  sections.push(
    [
      'Output Requirements:',
      'Return strict JSON matching the schema { "reasoning": string, "overviewPrompt": string, "atomicPrompts": Array<{ "title": string; "content": string }> }.',
      'Never wrap the JSON in markdown code fences and never add extra keys.',
    ].join(' '),
  )
  return sections.join('\n\n')
}
export const buildSeriesUserMessage = async (
  intent: string,
  files: FileContext[],
  imagePaths: string[],
  videoPaths: string[],
  pdfPaths: string[],
  metaInstructions?: string,
  onUploadStateChange?: UploadStateChange,
  apiKey?: string,
): Promise<MessageContent> => {
  const text = buildSeriesUserMessageText(intent, files, pdfPaths, metaInstructions)
  return await mergeMediaWithText(
    text,
    imagePaths,
    videoPaths,
    pdfPaths,
    onUploadStateChange,
    apiKey,
  )
}
const isTextPart = (part: Exclude<MessageContent, string>[number]): part is TextPart => {
  return part.type === 'text'
}
export const mergeResolvedMediaWithText = (
  content: MessageContent,
  text: string,
): MessageContent => {
  if (typeof content === 'string') {
    return text
  }
  const mediaParts = content.filter((part) => !isTextPart(part))
  if (mediaParts.length === 0) {
    return text
  }
  return [...mediaParts, { type: 'text', text }]
}
const mergeMediaWithText = async (
  text: string,
  imagePaths: string[],
  videoPaths: string[],
  pdfPaths: string[],
  onUploadStateChange?: UploadStateChange,
  apiKey?: string,
): Promise<MessageContent> => {
  const [imageParts, videoParts, pdfParts] = await Promise.all([
    resolveImageParts(imagePaths, onUploadStateChange),
    resolveVideoParts(videoPaths, onUploadStateChange, apiKey),
    resolvePdfParts(pdfPaths, onUploadStateChange, apiKey),
  ])
  if (imageParts.length === 0 && videoParts.length === 0 && pdfParts.length === 0) {
    return text
  }
  // Deterministic ordering: images â videos â PDFs â text.
  return [...imageParts, ...videoParts, ...pdfParts, { type: 'text', text }]
}
</file>
<file path="src/tui/components/core/InputBar.tsx">
import React from 'react'
import { Box, Text, useStdout } from 'ink'
import { MultilineTextInput, type DebugKeyEvent } from './MultilineTextInput'
import { resolveIndicatorSegments } from './status-indicators-layout'
import { resolveInputBarPresentation, type InputBarMode } from './input-bar-presentation'
import type { TokenLabelLookup } from './tokenized-text'
import { OpencodeSpinner } from '../OpencodeSpinner'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
export { estimateInputBarRows } from './input-bar-layout'
export type { InputBarRowEstimateOptions } from './input-bar-layout'
const APP_CONTAINER_PADDING_X = 2
const COMMAND_SCREEN_PADDING_X = 1
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type InputBarProps = {
  value: string
  onChange: (next: string) => void
  onSubmit: (value: string) => void
  mode?: InputBarMode
  isDisabled?: boolean
  isPasteActive?: boolean
  isBusy?: boolean
  statusChips: readonly string[]
  placeholder?: string
  hint?: string | undefined
  debugLine?: string | undefined
  tokenLabel?: TokenLabelLookup | undefined
  onDebugKeyEvent?: ((event: DebugKeyEvent) => void) | undefined
}
export const InputBar: React.FC<InputBarProps> = ({
  value,
  onChange,
  onSubmit,
  mode = 'intent',
  isDisabled = false,
  isPasteActive = false,
  isBusy = false,
  statusChips,
  placeholder,
  hint,
  debugLine,
  tokenLabel,
  onDebugKeyEvent,
}) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  // `resolveInputBarPresentation` is pure but involves string/config mapping.
  // Memoizing it keeps the render path a bit more predictable.
  const presentation = React.useMemo(() => resolveInputBarPresentation(mode), [mode])
  const summary = React.useMemo(() => {
    const segments = resolveIndicatorSegments(statusChips)
    const status = segments.find((segment) => segment.label === 'Status')
    const model = segments.find((segment) => segment.label === 'Model')
    const polish = segments.find((segment) => segment.label === 'Polish')
    const target = segments.find((segment) => segment.label === 'Target')
    return { status, model, polish, target }
  }, [statusChips])
  const borderColor = presentation.borderTone === 'warning' ? theme.warning : theme.border
  const labelColor = presentation.labelTone === 'warning' ? theme.warning : theme.mutedText
  const terminalColumns = stdout?.columns ?? 80
  const barWidth = Math.max(
    0,
    terminalColumns - 2 * (APP_CONTAINER_PADDING_X + COMMAND_SCREEN_PADDING_X),
  )
  const backgroundProps = inkBackgroundColorProps(theme.panelBackground)
  const statusLineColumns = React.useMemo(() => {
    const joinerColumns = ' Âˇ '.length
    let columns = 0
    let partCount = 0
    const addPart = (value: string, extraColumns = 0): void => {
      if (partCount > 0) {
        columns += joinerColumns
      }
      columns += value.length + extraColumns
      partCount += 1
    }
    if (summary.status) {
      const spinnerColumns = isBusy ? 12 + 1 : 0
      addPart(`Status: ${summary.status.value}`, spinnerColumns)
    }
    if (summary.model) {
      addPart(`Model: ${summary.model.value}`)
    }
    if (summary.polish) {
      addPart(`Polish: ${summary.polish.value}`)
    }
    if (summary.target) {
      addPart(`Target: ${summary.target.value}`)
    }
    return columns
  }, [
    isBusy,
    summary.model?.value,
    summary.polish?.value,
    summary.status?.value,
    summary.target?.value,
  ])
  const BORDER_GLYPH = 'â'
  const renderBorderPrefix = (): React.ReactNode => (
    <>
      <Text {...backgroundProps} {...inkColorProps(borderColor)}>
        {BORDER_GLYPH}
      </Text>
      <Text {...backgroundProps}> </Text>
    </>
  )
  return (
    <Box
      flexDirection="column"
      paddingLeft={0}
      paddingRight={0}
      paddingY={0}
      width="100%"
      {...inkBackgroundColorProps(theme.panelBackground)}
    >
      <Box flexDirection="row" width="100%">
        {renderBorderPrefix()}
        <Text {...backgroundProps} {...inkColorProps(labelColor)} bold={presentation.labelBold}>
          {presentation.label}
        </Text>
        <Text {...backgroundProps}>
          {padRight('', Math.max(0, barWidth - 2 - presentation.label.length))}
        </Text>
      </Box>
      {hint ? (
        <Box flexDirection="row" width="100%">
          {renderBorderPrefix()}
          <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
            {hint}
          </Text>
          <Text {...backgroundProps}>{padRight('', Math.max(0, barWidth - 2 - hint.length))}</Text>
        </Box>
      ) : null}
      {debugLine ? (
        <Box flexDirection="row" width="100%">
          {renderBorderPrefix()}
          <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
            {debugLine}
          </Text>
          <Text {...backgroundProps}>
            {padRight('', Math.max(0, barWidth - 2 - debugLine.length))}
          </Text>
        </Box>
      ) : null}
      <MultilineTextInput
        value={value}
        onChange={onChange}
        onSubmit={onSubmit}
        placeholder={placeholder ?? 'Describe your goal or type /command'}
        focus={!isDisabled}
        isDisabled={isDisabled}
        isPasteActive={isPasteActive}
        tokenLabel={tokenLabel}
        onDebugKeyEvent={onDebugKeyEvent}
        gutter={{ glyph: BORDER_GLYPH, color: borderColor, spacer: 1 }}
        width={barWidth}
        backgroundColor={theme.panelBackground}
      />
      {summary.status || summary.model || summary.polish || summary.target ? (
        <Box flexDirection="row" width="100%">
          {renderBorderPrefix()}
          {summary.status ? (
            <>
              <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
                Status:{' '}
              </Text>
              {isBusy ? (
                <>
                  <OpencodeSpinner backgroundColor={theme.panelBackground} />
                  <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
                    {' '}
                  </Text>
                </>
              ) : null}
              <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
                {summary.status.value}
              </Text>
            </>
          ) : null}
          {summary.status && (summary.model || summary.polish || summary.target) ? (
            <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
              {' Âˇ '}
            </Text>
          ) : null}
          {summary.model ? (
            <>
              <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
                Model:{' '}
              </Text>
              <Text {...backgroundProps} {...inkColorProps(theme.text)}>
                {summary.model.value}
              </Text>
            </>
          ) : null}
          {summary.model && (summary.polish || summary.target) ? (
            <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
              {' Âˇ '}
            </Text>
          ) : null}
          {summary.polish ? (
            <>
              <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
                Polish:{' '}
              </Text>
              <Text {...backgroundProps} {...inkColorProps(theme.text)}>
                {summary.polish.value}
              </Text>
            </>
          ) : null}
          {summary.polish && summary.target ? (
            <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
              {' Âˇ '}
            </Text>
          ) : null}
          {summary.target ? (
            <>
              <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
                Target:{' '}
              </Text>
              <Text {...backgroundProps} {...inkColorProps(theme.text)}>
                {summary.target.value}
              </Text>
            </>
          ) : null}
          <Text {...backgroundProps}>
            {padRight('', Math.max(0, barWidth - 2 - statusLineColumns))}
          </Text>
        </Box>
      ) : null}
    </Box>
  )
}
</file>
<file path="src/tui/components/popups/model-popup-model.ts">
import { MODEL_PROVIDER_LABELS } from '../../../model-providers'
import { ensureLeadingHeaderVisible, resolveWindowedValues } from './list-windowing'
import type { WindowBounds } from './list-windowing'
import type { ModelOption } from '../../types'
export type ModelPopupRow =
  | { type: 'header'; title: string }
  | { type: 'spacer' }
  | { type: 'option'; option: ModelOption; optionIndex: number }
export const resolveModelPopupListRows = ({
  popupHeight,
  paddingY,
  fixedRows,
}: {
  popupHeight: number
  paddingY: number
  fixedRows: number
}): number => {
  const paddingRows = 2 * paddingY
  const availableRows = Math.max(1, popupHeight - paddingRows - fixedRows)
  return availableRows
}
export const buildModelPopupRows = (
  options: readonly ModelOption[],
  recentCount: number,
): ModelPopupRow[] => {
  if (options.length === 0) {
    return []
  }
  const rows: ModelPopupRow[] = []
  const safeRecentCount = Math.max(0, Math.min(recentCount, options.length))
  if (safeRecentCount > 0) {
    rows.push({ type: 'header', title: 'Recent' })
    for (let index = 0; index < safeRecentCount; index += 1) {
      const option = options[index]
      if (!option) {
        continue
      }
      rows.push({ type: 'option', option, optionIndex: index })
    }
    if (safeRecentCount < options.length) {
      rows.push({ type: 'spacer' })
    }
  }
  let lastProvider: string | null = null
  for (let index = safeRecentCount; index < options.length; index += 1) {
    const option = options[index]
    if (!option) {
      continue
    }
    const providerLabel = MODEL_PROVIDER_LABELS[option.provider]
    if (providerLabel !== lastProvider) {
      rows.push({ type: 'header', title: providerLabel })
      lastProvider = providerLabel
    }
    rows.push({ type: 'option', option, optionIndex: index })
  }
  return rows
}
export const resolveModelPopupSelectedRowIndex = (
  rows: readonly ModelPopupRow[],
  selectedOptionIndex: number,
): number => {
  if (rows.length === 0) {
    return 0
  }
  const index = rows.findIndex(
    (row) => row.type === 'option' && row.optionIndex === selectedOptionIndex,
  )
  return index >= 0 ? index : 0
}
export type ModelPopupVisibleRows = {
  slice: WindowBounds
  selectedRowIndex: number
  visibleRows: readonly ModelPopupRow[]
}
export const resolveModelPopupVisibleRows = ({
  rows,
  selectedOptionIndex,
  maxVisibleRows,
}: {
  rows: readonly ModelPopupRow[]
  selectedOptionIndex: number
  maxVisibleRows: number
}): ModelPopupVisibleRows => {
  const selectedRowIndex = resolveModelPopupSelectedRowIndex(rows, selectedOptionIndex)
  const window = resolveWindowedValues(rows, selectedRowIndex, maxVisibleRows)
  const slice = ensureLeadingHeaderVisible(
    rows,
    { start: window.start, end: window.end },
    maxVisibleRows,
    'header',
    'option',
  )
  const base = rows.slice(slice.start, slice.end)
  if (base.length >= maxVisibleRows) {
    return { slice, selectedRowIndex, visibleRows: base }
  }
  const padded: ModelPopupRow[] = [...base]
  while (padded.length < maxVisibleRows) {
    padded.push({ type: 'spacer' })
  }
  return { slice, selectedRowIndex, visibleRows: padded }
}
</file>
<file path="src/tui/hooks/buffered-history-writer.ts">
import type { HistoryEntry } from '../types'
export type HistoryPush = (
  content: string,
  kind?: HistoryEntry['kind'],
  format?: HistoryEntry['format'],
) => void
export type BufferedHistoryWriter = {
  pushBuffered: (
    content: string,
    kind?: HistoryEntry['kind'],
    format?: HistoryEntry['format'],
  ) => void
  pushManyBuffered: (
    entries: Array<{
      content: string
      kind?: HistoryEntry['kind']
      format?: HistoryEntry['format']
    }>,
  ) => void
  flush: () => void
}
type FlushScheduler = (flush: () => void) => void
type BufferedHistoryEntry = {
  content: string
  kind: HistoryEntry['kind'] | undefined
  format: HistoryEntry['format'] | undefined
}
const scheduleMicrotaskFlush: FlushScheduler = (flush) => {
  queueMicrotask(flush)
}
export const createBufferedHistoryWriter = (options: {
  push: HistoryPush
  scheduleFlush?: FlushScheduler
}): BufferedHistoryWriter => {
  const scheduleFlush = options.scheduleFlush ?? scheduleMicrotaskFlush
  let scheduled = false
  let queue: BufferedHistoryEntry[] = []
  const flush = () => {
    scheduled = false
    const entries = queue
    queue = []
    entries.forEach((entry) => {
      options.push(entry.content, entry.kind, entry.format)
    })
  }
  const schedule = () => {
    if (scheduled) {
      return
    }
    scheduled = true
    scheduleFlush(flush)
  }
  const pushBuffered = (
    content: string,
    kind?: HistoryEntry['kind'],
    format?: HistoryEntry['format'],
  ) => {
    queue.push({ content, kind, format })
    schedule()
  }
  const pushManyBuffered = (
    entries: Array<{
      content: string
      kind?: HistoryEntry['kind']
      format?: HistoryEntry['format']
    }>,
  ) => {
    entries.forEach((entry) => {
      queue.push({ content: entry.content, kind: entry.kind, format: entry.format })
    })
    if (entries.length > 0) {
      schedule()
    }
  }
  return {
    pushBuffered,
    pushManyBuffered,
    flush,
  }
}
</file>
<file path="src/tui/hooks/generation-history-formatters.ts">
import wrapAnsi from 'wrap-ansi'
import type { HistoryEntry } from '../types'
export type HistoryMessage = {
  content: string
  kind: HistoryEntry['kind']
  format?: HistoryEntry['format']
}
export const formatCompactTokens = (count: number): string => {
  if (count < 1000) {
    return String(count)
  }
  if (count < 10_000) {
    return `${(count / 1000).toFixed(1)}k`
  }
  if (count < 1_000_000) {
    return `${Math.round(count / 1000)}k`
  }
  return `${(count / 1_000_000).toFixed(1)}m`
}
export const extractValidationSection = (content: string): string | null => {
  const markerRegex = /^(?:#{1,6}\s*Validation\b.*|Validation\s*:.*)$/im
  const match = markerRegex.exec(content)
  if (!match) {
    return null
  }
  return content.slice(match.index).trim()
}
export const getHistoryWrapWidth = (terminalColumns: number): number => {
  return Math.max(40, terminalColumns - 6)
}
export const wrapTextForHistory = (content: string, wrapWidth: number): string[] => {
  const output: string[] = []
  content.split('\n').forEach((line) => {
    const wrapped = wrapAnsi(line, wrapWidth, { trim: false, hard: true })
    wrapped.split('\n').forEach((wrappedLine) => {
      output.push(wrappedLine)
    })
  })
  return output
}
const formatIterationTokenLabel = (tokens: number, reasoningTokens?: number): string => {
  const normalizedReasoningTokens = reasoningTokens ?? 0
  if (normalizedReasoningTokens > 0) {
    return ` (${tokens} prompt tokens Âˇ ${normalizedReasoningTokens} reasoning tokens)`
  }
  return ` (${tokens} tokens)`
}
export const buildIterationCompleteHistoryMessages = (options: {
  iteration: number
  tokens: number
  reasoningTokens?: number
  prompt: string
  wrapWidth: number
}): HistoryMessage[] => {
  const tokenLabel = formatIterationTokenLabel(options.tokens, options.reasoningTokens)
  const messages: HistoryMessage[] = [
    {
      content: `Iteration ${options.iteration} complete${tokenLabel}`,
      kind: 'progress',
    },
    {
      content: `Prompt (iteration ${options.iteration}):`,
      kind: 'system',
    },
  ]
  wrapTextForHistory(options.prompt, options.wrapWidth).forEach((line) => {
    messages.push({ content: line, kind: 'system', format: 'markdown' })
  })
  return messages
}
export const buildJsonPayloadHistoryMessages = (
  payload: unknown,
  wrapWidth: number,
): HistoryMessage[] => {
  const prettyPayload = JSON.stringify(payload, null, 2)
  const messages: HistoryMessage[] = [{ content: 'JSON payload:', kind: 'system' }]
  wrapTextForHistory(prettyPayload, wrapWidth).forEach((line) => {
    messages.push({ content: line, kind: 'system' })
  })
  return messages
}
</file>
<file path="src/tui/screens/command/hooks/popup-shortcuts/suggested-selected-list.ts">
import type { PopupState } from '../../../../types'
import { clampIndex } from './selection'
import {
  NO_EFFECT,
  guardPopupUpdater,
  type PopupKey,
  type PopupShortcutEffect,
  type PopupStateFor,
  type PopupType,
} from './types'
export type SuggestedSelectedListPopupType = Extract<PopupType, 'file' | 'image' | 'video' | 'pdf'>
export type SuggestedSelectedListPopupState =
  | PopupStateFor<'file'>
  | PopupStateFor<'image'>
  | PopupStateFor<'video'>
  | PopupStateFor<'pdf'>
export type GetSuggestedSelectedListEffectOptions = {
  popupType: SuggestedSelectedListPopupType
  popupState: SuggestedSelectedListPopupState
  itemsLength: number
  suggestions: readonly string[]
  key: PopupKey
  isBackspace: boolean
}
const guardSuggestedSelectedListUpdater = (
  popupType: SuggestedSelectedListPopupType,
  updater: (prev: SuggestedSelectedListPopupState) => SuggestedSelectedListPopupState,
): ((prev: PopupState) => PopupState) => {
  return guardPopupUpdater(
    popupType,
    updater as (prev: PopupStateFor<typeof popupType>) => PopupStateFor<typeof popupType>,
  )
}
export const getSuggestedSelectedListPopupEffect = ({
  popupType,
  popupState,
  itemsLength,
  suggestions,
  key,
  isBackspace,
}: GetSuggestedSelectedListEffectOptions): PopupShortcutEffect => {
  const hasSuggestions = suggestions.length > 0
  const maxSuggestedIndex = Math.max(suggestions.length - 1, 0)
  const draftIsEmpty = popupState.draft.trim().length === 0
  if (key.escape) {
    return { type: 'close' }
  }
  if (popupState.suggestedFocused && hasSuggestions) {
    if (key.tab) {
      return {
        type: 'set',
        updater: guardSuggestedSelectedListUpdater(popupType, (prev) => ({
          ...prev,
          suggestedFocused: false,
        })),
      }
    }
    if (key.upArrow) {
      return {
        type: 'set',
        updater: guardSuggestedSelectedListUpdater(popupType, (prev) => {
          const effectiveIndex = clampIndex(prev.suggestedSelectionIndex, suggestions.length)
          if (effectiveIndex === 0) {
            return {
              ...prev,
              suggestedFocused: false,
              selectedFocused: itemsLength > 0,
            }
          }
          return {
            ...prev,
            suggestedSelectionIndex: Math.max(prev.suggestedSelectionIndex - 1, 0),
          }
        }),
      }
    }
    if (key.downArrow) {
      return {
        type: 'set',
        updater: guardSuggestedSelectedListUpdater(popupType, (prev) => ({
          ...prev,
          suggestedSelectionIndex: Math.min(prev.suggestedSelectionIndex + 1, maxSuggestedIndex),
        })),
      }
    }
    if (key.return) {
      const effectiveIndex = clampIndex(popupState.suggestedSelectionIndex, suggestions.length)
      const selection = suggestions[effectiveIndex]
      return selection ? { type: 'selectSuggestion', value: selection } : NO_EFFECT
    }
    return NO_EFFECT
  }
  if (key.tab && !key.shift && hasSuggestions) {
    return {
      type: 'set',
      updater: guardSuggestedSelectedListUpdater(popupType, (prev) => ({
        ...prev,
        suggestedFocused: true,
        selectedFocused: false,
        suggestedSelectionIndex: 0,
      })),
    }
  }
  if (!popupState.selectedFocused && (key.upArrow || key.downArrow) && itemsLength > 0) {
    return {
      type: 'set',
      updater: guardSuggestedSelectedListUpdater(popupType, (prev) => ({
        ...prev,
        selectedFocused: true,
        selectionIndex: clampIndex(prev.selectionIndex, itemsLength),
      })),
    }
  }
  if (popupState.selectedFocused) {
    if (key.upArrow) {
      return {
        type: 'set',
        updater: guardSuggestedSelectedListUpdater(popupType, (prev) => {
          if (prev.selectionIndex === 0) {
            return { ...prev, selectedFocused: false }
          }
          return { ...prev, selectionIndex: Math.max(prev.selectionIndex - 1, 0) }
        }),
      }
    }
    if (key.downArrow) {
      return {
        type: 'set',
        updater: guardSuggestedSelectedListUpdater(popupType, (prev) => {
          if (itemsLength === 0) {
            return { ...prev, selectedFocused: false }
          }
          if (prev.selectionIndex >= itemsLength - 1) {
            return hasSuggestions
              ? {
                  ...prev,
                  suggestedFocused: true,
                  selectedFocused: false,
                  suggestedSelectionIndex: 0,
                }
              : prev
          }
          return {
            ...prev,
            selectionIndex: Math.min(prev.selectionIndex + 1, itemsLength - 1),
          }
        }),
      }
    }
    if (key.delete || isBackspace) {
      return itemsLength > 0 ? { type: 'remove', index: popupState.selectionIndex } : NO_EFFECT
    }
    return NO_EFFECT
  }
  if (key.downArrow && itemsLength === 0 && hasSuggestions) {
    return {
      type: 'set',
      updater: guardSuggestedSelectedListUpdater(popupType, (prev) => ({
        ...prev,
        suggestedFocused: true,
        selectedFocused: false,
        suggestedSelectionIndex: 0,
      })),
    }
  }
  // Backspace-remove remains available when the input is empty.
  if (draftIsEmpty && isBackspace && itemsLength > 0) {
    return { type: 'remove', index: popupState.selectionIndex }
  }
  return NO_EFFECT
}
export const applySuggestedSelectedListEffect = (
  effect: PopupShortcutEffect,
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void,
  closePopup: () => void,
  onRemove: (index: number) => void,
  onSelectSuggestion: (value: string) => void,
): void => {
  switch (effect.type) {
    case 'close':
      closePopup()
      return
    case 'set':
      setPopupState(effect.updater)
      return
    case 'remove':
      onRemove(effect.index)
      return
    case 'selectSuggestion':
      onSelectSuggestion(effect.value)
      return
    case 'none':
      return
    default: {
      const exhaustive: never = effect
      return exhaustive
    }
  }
}
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenSubmitBindings.ts">
import { useCallback } from 'react'
import type { HistoryEntry, PopupState } from '../../../types'
import { useIntentSubmitHandler } from './useIntentSubmitHandler'
export type UseCommandScreenSubmitBindingsOptions = {
  popupState: PopupState
  isAwaitingRefinement: boolean
  submitRefinement: (value: string) => void
  isCommandMenuActive: boolean
  selectedCommandId: import('../../../types').CommandDescriptor['id'] | null
  commandMenuArgsRaw: string
  isCommandMode: boolean
  intentFilePath: string
  isGenerating: boolean
  expandInputForSubmit: (value: string) => string
  setInputValue: (value: string | ((prev: string) => string)) => void
  pushHistory: (content: string, kind?: HistoryEntry['kind']) => void
  addCommandHistoryEntry: (value: string) => void
  runGeneration: (payload: {
    intent?: string
    intentFile?: string
    resume?:
      | { kind: 'history'; selector: string; mode: import('../../../types').ResumeMode }
      | { kind: 'file'; payloadPath: string; mode: import('../../../types').ResumeMode }
  }) => Promise<void>
  handleCommandSelection: (
    commandId: import('../../../types').CommandDescriptor['id'],
    argsRaw?: string,
  ) => void
  handleNewCommand: (argsRaw: string) => void
  handleReuseCommand: () => void
  lastUserIntentRef: import('react').MutableRefObject<string | null>
  handleSeriesIntentSubmit: (value: string) => void
  openHelp?: () => void
}
export type UseCommandScreenSubmitBindingsResult = {
  handleSubmit: (value: string) => void
  onSeriesSubmit: (value: string) => void
}
export const useCommandScreenSubmitBindings = ({
  popupState,
  isAwaitingRefinement,
  submitRefinement,
  isCommandMenuActive,
  selectedCommandId,
  commandMenuArgsRaw,
  isCommandMode,
  intentFilePath,
  isGenerating,
  expandInputForSubmit,
  setInputValue,
  pushHistory,
  addCommandHistoryEntry,
  runGeneration,
  handleCommandSelection,
  handleNewCommand,
  handleReuseCommand,
  lastUserIntentRef,
  handleSeriesIntentSubmit,
  openHelp,
}: UseCommandScreenSubmitBindingsOptions): UseCommandScreenSubmitBindingsResult => {
  const handleSubmit = useIntentSubmitHandler({
    popupState,
    isAwaitingRefinement,
    submitRefinement,
    isCommandMenuActive,
    selectedCommandId,
    commandMenuArgsRaw,
    isCommandMode,
    intentFilePath,
    isGenerating,
    expandInputForSubmit,
    setInputValue,
    pushHistory,
    addCommandHistoryEntry,
    runGeneration,
    handleCommandSelection,
    handleNewCommand,
    handleReuseCommand,
    lastUserIntentRef,
    ...(openHelp ? { openHelp } : {}),
  })
  const onSeriesSubmit = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      if (trimmed) {
        addCommandHistoryEntry(`/series ${trimmed}`)
      }
      handleSeriesIntentSubmit(value)
    },
    [addCommandHistoryEntry, handleSeriesIntentSubmit],
  )
  return {
    handleSubmit,
    onSeriesSubmit,
  }
}
</file>
<file path="src/tui/screens/command/hooks/useIntentSubmitHandler.ts">
import type { MutableRefObject } from 'react'
import { useCallback } from 'react'
import type { CommandDescriptor, HistoryEntry, PopupState } from '../../../types'
import { resolveSubmitPlan } from '../utils/submit-plan'
type PushHistory = (content: string, kind?: HistoryEntry['kind']) => void
type AddCommandHistoryEntry = (value: string) => void
type RunGeneration = (payload: {
  intent?: string
  intentFile?: string
  resume?:
    | { kind: 'history'; selector: string; mode: import('../../../types').ResumeMode }
    | { kind: 'file'; payloadPath: string; mode: import('../../../types').ResumeMode }
}) => Promise<void>
export type UseIntentSubmitHandlerOptions = {
  popupState: PopupState
  isAwaitingRefinement: boolean
  submitRefinement: (value: string) => void
  isCommandMenuActive: boolean
  selectedCommandId: CommandDescriptor['id'] | null
  commandMenuArgsRaw: string
  isCommandMode: boolean
  intentFilePath: string
  isGenerating: boolean
  expandInputForSubmit: (value: string) => string
  setInputValue: (value: string) => void
  pushHistory: PushHistory
  addCommandHistoryEntry: AddCommandHistoryEntry
  runGeneration: RunGeneration
  handleCommandSelection: (commandId: CommandDescriptor['id'], argsRaw?: string) => void
  handleNewCommand: (argsRaw: string) => void
  handleReuseCommand: () => void
  lastUserIntentRef: MutableRefObject<string | null>
  openHelp?: () => void
}
export const useIntentSubmitHandler = ({
  popupState,
  isAwaitingRefinement,
  submitRefinement,
  isCommandMenuActive,
  selectedCommandId,
  commandMenuArgsRaw,
  isCommandMode,
  intentFilePath,
  isGenerating,
  expandInputForSubmit,
  setInputValue,
  pushHistory,
  addCommandHistoryEntry,
  runGeneration,
  handleCommandSelection,
  handleNewCommand,
  handleReuseCommand,
  lastUserIntentRef,
  openHelp,
}: UseIntentSubmitHandlerOptions): ((value: string) => void) => {
  return useCallback(
    (value: string) => {
      const expandedValue = expandInputForSubmit(value)
      const plan = resolveSubmitPlan({
        expandedValue,
        isAwaitingRefinement,
        popupOpen: Boolean(popupState),
        isCommandMenuActive,
        selectedCommandId,
        commandMenuArgsRaw,
        isCommandMode,
        intentFilePath,
        isGenerating,
      })
      for (const action of plan) {
        switch (action.type) {
          case 'set-input': {
            setInputValue(action.value)
            break
          }
          case 'push-history': {
            pushHistory(action.content, action.kind)
            break
          }
          case 'add-command-history': {
            addCommandHistoryEntry(action.value)
            break
          }
          case 'set-last-user-intent': {
            lastUserIntentRef.current = action.value
            break
          }
          case 'submit-refinement': {
            submitRefinement(action.value)
            break
          }
          case 'run-generation': {
            const payload: {
              intent?: string
              intentFile?: string
              resume?:
                | { kind: 'history'; selector: string; mode: import('../../../types').ResumeMode }
                | { kind: 'file'; payloadPath: string; mode: import('../../../types').ResumeMode }
            } = {}
            if (action.intent) {
              payload.intent = action.intent
            }
            if (action.intentFile) {
              payload.intentFile = action.intentFile
            }
            if (action.resume) {
              payload.resume = action.resume
            }
            void runGeneration(payload)
            break
          }
          case 'run-new': {
            handleNewCommand(action.argsRaw)
            break
          }
          case 'run-reuse': {
            handleReuseCommand()
            break
          }
          case 'run-command': {
            if (action.commandId === 'help') {
              if (openHelp) {
                openHelp()
              } else {
                pushHistory('Help overlay is unavailable in this view.', 'system')
              }
              break
            }
            handleCommandSelection(action.commandId, action.argsRaw)
            break
          }
          default: {
            throw new Error('Unhandled submit plan action')
          }
        }
      }
    },
    [
      addCommandHistoryEntry,
      commandMenuArgsRaw,
      expandInputForSubmit,
      handleCommandSelection,
      handleNewCommand,
      handleReuseCommand,
      intentFilePath,
      isAwaitingRefinement,
      isCommandMenuActive,
      isCommandMode,
      isGenerating,
      lastUserIntentRef,
      openHelp,
      popupState,
      pushHistory,
      runGeneration,
      selectedCommandId,
      setInputValue,
      submitRefinement,
    ],
  )
}
</file>
<file path="src/tui/screens/command/CommandScreenImpl.tsx">
import { forwardRef, memo, useImperativeHandle } from 'react'
import { Box, Text, useStdout } from 'ink'
import { BackgroundFill } from '../../components/core/BackgroundFill'
import { CommandInput } from './components/CommandInput'
import { CommandMenuPane } from './components/CommandMenuPane'
import { HistoryPane } from './components/HistoryPane'
import { PopupArea } from './components/PopupArea'
import { useCommandScreenController } from './hooks/useCommandScreenController'
import type { NotifyOptions } from '../../notifier'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
type CommandScreenProps = {
  interactiveTransportPath?: string | undefined
  onPopupVisibilityChange?: (isOpen: boolean) => void
  commandMenuSignal?: number
  helpOpen?: boolean
  reservedRows?: number
  onOpenHelp?: () => void
  notify: (message: string, options?: NotifyOptions) => void
}
export type CommandScreenHandle = {
  suppressNextInput: () => void
}
export const CommandScreen = memo(
  forwardRef<CommandScreenHandle, CommandScreenProps>(
    (
      {
        interactiveTransportPath,
        onPopupVisibilityChange,
        commandMenuSignal,
        helpOpen = false,
        reservedRows = 0,
        onOpenHelp,
        notify,
      },
      ref,
    ) => {
      const {
        view: {
          transportMessage,
          historyPaneProps,
          popupAreaProps,
          commandMenuPaneProps,
          commandInputProps,
        },
        actions: { suppressNextInput },
      } = useCommandScreenController({
        ...(interactiveTransportPath ? { transport: { interactiveTransportPath } } : {}),
        popup: {
          ...(onPopupVisibilityChange ? { onPopupVisibilityChange } : {}),
          ...(commandMenuSignal !== undefined ? { commandMenuSignal } : {}),
          helpOpen,
          reservedRows,
          ...(onOpenHelp ? { onOpenHelp } : {}),
        },
        notify,
      })
      useImperativeHandle(ref, () => ({ suppressNextInput }), [suppressNextInput])
      const { theme } = useTheme()
      const { stdout } = useStdout()
      const terminalRows = stdout?.rows ?? 24
      const terminalColumns = stdout?.columns ?? 80
      // AppContainer applies `paddingX={2}` (left + right), which reduces the
      // actual renderable width available to CommandScreen. If we try to render
      // a full-width `BackgroundFill` at `stdout.columns`, Ink will truncate the
      // line and paint `...` in the last cells.
      const backdropColumns = Math.max(0, terminalColumns - 4)
      const showPopupOverlay = Boolean(popupAreaProps.popupState) && !popupAreaProps.helpOpen
      return (
        <Box flexGrow={1} width="100%" {...inkBackgroundColorProps(theme.background)}>
          <Box flexDirection="column" flexGrow={1} paddingX={1} paddingY={1} width="100%">
            {transportMessage ? (
              <Box flexShrink={0}>
                <Text {...inkColorProps(theme.warning)}>{transportMessage}</Text>
              </Box>
            ) : null}
            <HistoryPane {...historyPaneProps} />
            <CommandMenuPane {...commandMenuPaneProps} />
            <CommandInput {...commandInputProps} />
          </Box>
          {showPopupOverlay ? (
            <Box position="absolute" width="100%" height="100%">
              <Box position="absolute" width="100%" height="100%" overflow="hidden">
                <BackgroundFill
                  rows={terminalRows}
                  columns={backdropColumns}
                  background={theme.background}
                />
              </Box>
              <Box
                position="absolute"
                width="100%"
                height="100%"
                justifyContent="center"
                alignItems="center"
              >
                <PopupArea {...popupAreaProps} />
              </Box>
            </Box>
          ) : null}
        </Box>
      )
    },
  ),
)
CommandScreen.displayName = 'CommandScreen'
</file>
<file path="src/tui/theme/theme-validate.ts">
import {
  REQUIRED_THEME_SLOTS,
  type RequiredThemeSlot,
  type ThemeColorValue,
  type ThemeJson,
} from './theme-types'
type ThemeValidationResult = { ok: true; theme: ThemeJson } | { ok: false; message: string }
const isRecord = (value: unknown): value is Record<string, unknown> =>
  typeof value === 'object' && value !== null
const isThemeColorVariant = (value: unknown): value is { dark: unknown; light: unknown } => {
  if (!isRecord(value)) {
    return false
  }
  return 'dark' in value && 'light' in value
}
const isThemeColorValue = (value: unknown): value is ThemeColorValue => {
  if (typeof value === 'string') {
    return true
  }
  if (typeof value === 'number') {
    return Number.isFinite(value)
  }
  if (isThemeColorVariant(value)) {
    return isThemeColorValue(value.dark) && isThemeColorValue(value.light)
  }
  return false
}
export const validateThemeJson = (value: unknown): ThemeValidationResult => {
  if (!isRecord(value)) {
    return { ok: false, message: 'Theme JSON must be an object.' }
  }
  const theme = value.theme
  if (!isRecord(theme)) {
    return { ok: false, message: 'Theme JSON must include a `theme` object.' }
  }
  const defs = value.defs
  if (defs !== undefined) {
    if (!isRecord(defs)) {
      return { ok: false, message: '`defs` must be an object when present.' }
    }
    for (const [key, defValue] of Object.entries(defs)) {
      if (!isThemeColorValue(defValue)) {
        return { ok: false, message: `Invalid defs color value for key: ${key}` }
      }
    }
  }
  const missingSlots: RequiredThemeSlot[] = []
  for (const slot of REQUIRED_THEME_SLOTS) {
    const slotValue = theme[slot]
    if (slotValue === undefined) {
      missingSlots.push(slot)
      continue
    }
    if (!isThemeColorValue(slotValue)) {
      return { ok: false, message: `Invalid theme color value for slot: ${slot}` }
    }
  }
  if (missingSlots.length > 0) {
    return {
      ok: false,
      message: `Theme JSON missing required slots: ${missingSlots.join(', ')}`,
    }
  }
  for (const [key, slotValue] of Object.entries(theme)) {
    if (!isThemeColorValue(slotValue)) {
      return { ok: false, message: `Invalid theme color value for key: ${key}` }
    }
  }
  return { ok: true, theme: value as ThemeJson }
}
</file>
<file path="src/export-command.ts">
import path from 'node:path'
import yargs from 'yargs'
import type { ArgumentsCamelCase } from 'yargs'
import type { PayloadFormat } from './generate/payload-io'
import { writeGeneratePayloadExport } from './export/export-generate-payload'
import { loadGeneratePayloadFromHistory } from './history/generate-history'
type ExportArgs = {
  fromHistory: string
  format: PayloadFormat
  out: string
  quiet: boolean
  help: boolean
}
export const runExportCommand = async (argv: string[]): Promise<void> => {
  try {
    const { args, showHelp } = parseExportArgs(argv)
    if (args.help) {
      showHelp()
      return
    }
    await exportHistoryEntry(args)
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown export error.'
    console.error(message)
    process.exitCode = 1
  }
}
const HELP_FLAGS = new Set(['--help', '-h'])
const parseExportArgs = (argv: string[]): { args: ExportArgs; showHelp: () => void } => {
  const { optionArgs, helpRequested } = stripHelpFlags(argv)
  const parser = yargs(optionArgs)
    .scriptName('prompt-maker-cli export')
    .usage(
      'Usage:\n  prompt-maker-cli export [--from-history <selector>] --format json|yaml --out <path>',
    )
    .option('from-history', {
      type: 'string',
      default: 'last',
      describe: 'History selector (last, last:N, or N-th from end)',
    })
    .option('format', {
      alias: 'f',
      type: 'string',
      choices: ['json', 'yaml'] as const,
      describe: 'Output format',
    })
    .option('out', {
      alias: 'o',
      type: 'string',
      describe: 'Output file path',
    })
    .option('quiet', {
      type: 'boolean',
      default: false,
      describe: 'Suppress human-readable stderr logs',
    })
    .check((argv) => {
      if (helpRequested) {
        return true
      }
      const format = argv.format
      if (format !== 'json' && format !== 'yaml') {
        throw new Error('--format is required (json|yaml).')
      }
      const outPath = argv.out
      if (typeof outPath !== 'string' || outPath.trim().length === 0) {
        throw new Error('--out is required.')
      }
      return true
    })
    .help('help')
    .alias('help', 'h')
    .exitProcess(false)
    .showHelpOnFail(false)
    .parserConfiguration({ 'halt-at-non-option': true })
    .strict(false)
    .fail((msg, err) => {
      throw err ?? new Error(msg ?? 'Invalid CLI arguments.')
    })
  const parsed = parser.parseSync() as ArgumentsCamelCase<{
    fromHistory?: unknown
    format?: PayloadFormat
    out?: string
    quiet: boolean
    help?: boolean
  }>
  const fromHistoryValue = parsed.fromHistory
  const fromHistory =
    typeof fromHistoryValue === 'string'
      ? fromHistoryValue.trim() || 'last'
      : typeof fromHistoryValue === 'number'
        ? String(fromHistoryValue)
        : 'last'
  const help = helpRequested || Boolean(parsed.help)
  const format = parsed.format ?? 'json'
  const out = parsed.out?.trim() ?? ''
  if (!help && !out) {
    throw new Error('--out is required.')
  }
  return {
    args: {
      fromHistory,
      format,
      out,
      quiet: parsed.quiet ?? false,
      help,
    },
    showHelp: () => parser.showHelp(),
  }
}
const exportHistoryEntry = async (args: ExportArgs): Promise<void> => {
  const payload = await loadGeneratePayloadFromHistory({ selector: args.fromHistory })
  const { absolutePath } = await writeGeneratePayloadExport({
    payload,
    format: args.format,
    outPath: args.out,
  })
  if (!args.quiet) {
    console.error(
      `Wrote ${args.format.toUpperCase()} export to ${formatDisplayPath(absolutePath)}.`,
    )
  }
}
const formatDisplayPath = (absolutePath: string): string => {
  const relative = path.relative(process.cwd(), absolutePath)
  return relative && !relative.startsWith('..') ? relative : absolutePath
}
const stripHelpFlags = (tokens: string[]): { optionArgs: string[]; helpRequested: boolean } => {
  const optionArgs: string[] = []
  let helpRequested = false
  let passthrough = false
  tokens.forEach((token) => {
    if (passthrough) {
      optionArgs.push(token)
      return
    }
    if (token === '--') {
      optionArgs.push(token)
      passthrough = true
      return
    }
    if (HELP_FLAGS.has(token)) {
      helpRequested = true
      return
    }
    optionArgs.push(token)
  })
  return { optionArgs, helpRequested }
}
</file>
<file path="src/index.ts">
#!/usr/bin/env node
import { runComposeCommand } from './compose-command'
import { runExportCommand } from './export-command'
import { runGenerateCommand } from './generate-command'
import { runTestCommand } from './test-command'
type CliCommand = 'compose' | 'export' | 'generate' | 'test' | 'ui'
const { command, args } = resolveCommand(process.argv.slice(2))
switch (command) {
  case 'test':
    void runTestCommand(args)
    break
  case 'ui':
    void loadAndRunTui(args)
    break
  case 'export':
    void runExportCommand(args)
    break
  case 'compose':
    void runComposeCommand(args)
    break
  case 'generate':
  default:
    void runGenerateCommand(args)
}
async function loadAndRunTui(args: string[]): Promise<void> {
  const { runTuiCommand } = await import('./tui')
  await runTuiCommand(args)
}
function resolveCommand(args: string[]): { command: CliCommand; args: string[] } {
  if (args.length === 0) {
    return { command: 'ui', args }
  }
  const [first, ...rest] = args
  if (!first) {
    return { command: 'ui', args }
  }
  if (first === 'test') {
    return { command: 'test', args: rest }
  }
  if (first === 'ui') {
    return { command: 'ui', args: rest }
  }
  if (first === 'export') {
    return { command: 'export', args: rest }
  }
  if (first === 'compose') {
    return { command: 'compose', args: rest }
  }
  if (!first.startsWith('-') && (first === 'generate' || first === 'expand')) {
    return { command: 'generate', args: rest }
  }
  if (first.startsWith('-')) {
    return { command: 'generate', args }
  }
  return { command: 'generate', args }
}
</file>
<file path="src/media-loader.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import { GoogleAIFileManager } from '@google/generative-ai/server'
const POLL_INTERVAL_MS = 3_000
const PROCESSING_TIMEOUT_MS = 5 * 60_000
const VIDEO_MIME_TYPES: Record<string, string> = {
  '.mp4': 'video/mp4',
  '.mov': 'video/quicktime',
  '.m4v': 'video/x-m4v',
  '.webm': 'video/webm',
  '.avi': 'video/x-msvideo',
  '.mpeg': 'video/mpeg',
  '.mpg': 'video/mpeg',
  '.gif': 'image/gif',
}
const PDF_MIME_TYPE = 'application/pdf'
type GeminiFile = Awaited<ReturnType<GoogleAIFileManager['getFile']>>
type FileState = 'STATE_UNSPECIFIED' | 'PROCESSING' | 'ACTIVE' | 'FAILED'
export const uploadFileForGeminiWithMimeType = async (
  filePath: string,
  mimeType: string,
  apiKey?: string,
): Promise<string> => {
  await assertReadableFile(filePath)
  const manager = createFileManager(apiKey)
  const uploadResponse = await manager.uploadFile(filePath, {
    mimeType,
    displayName: path.basename(filePath),
  })
  const uploadedFile = uploadResponse.file
  if (!uploadedFile?.name) {
    throw new Error('Gemini Files API did not return a file name.')
  }
  const readyFile = await waitForActiveFile(manager, uploadedFile.name)
  if (!readyFile.uri) {
    throw new Error(`Gemini file ${uploadedFile.name} became active without a URI.`)
  }
  return readyFile.uri
}
export const uploadFileForGemini = async (filePath: string, apiKey?: string): Promise<string> => {
  const mimeType = inferVideoMimeType(filePath)
  return await uploadFileForGeminiWithMimeType(filePath, mimeType, apiKey)
}
const assertReadableFile = async (filePath: string): Promise<void> => {
  try {
    await fs.access(filePath)
  } catch {
    throw new Error(`Media file ${filePath} is not readable.`)
  }
}
export const inferVideoMimeType = (filePath: string): string => {
  const ext = path.extname(filePath).toLowerCase()
  const mimeType = VIDEO_MIME_TYPES[ext]
  if (!mimeType) {
    throw new Error(`Unsupported media type for ${filePath}.`)
  }
  return mimeType
}
export const inferPdfMimeType = (filePath: string): 'application/pdf' => {
  const ext = path.extname(filePath).toLowerCase()
  if (ext !== '.pdf') {
    throw new Error(`Unsupported PDF type for ${filePath}.`)
  }
  return PDF_MIME_TYPE
}
const createFileManager = (apiKey?: string): GoogleAIFileManager => {
  const resolvedApiKey = apiKey?.trim() || process.env.GEMINI_API_KEY?.trim()
  if (!resolvedApiKey) {
    throw new Error(
      'Gemini API key is required to upload media files. Pass apiKey or set GEMINI_API_KEY.',
    )
  }
  return new GoogleAIFileManager(resolvedApiKey)
}
const waitForActiveFile = async (
  manager: GoogleAIFileManager,
  fileName: string,
  timeoutMs = PROCESSING_TIMEOUT_MS,
): Promise<GeminiFile> => {
  const start = Date.now()
  // eslint-disable-next-line no-constant-condition
  while (true) {
    const file = await manager.getFile(fileName)
    const state = normalizeState(file.state)
    if (state === 'ACTIVE') {
      return file
    }
    if (state === 'FAILED') {
      const message = file.error?.message ?? 'The Gemini Files API reported a failure.'
      throw new Error(`Failed to process media file ${fileName}: ${message}`)
    }
    if (Date.now() - start > timeoutMs) {
      throw new Error(`Timed out while waiting for Gemini to process file ${fileName}.`)
    }
    await delay(POLL_INTERVAL_MS)
  }
}
const normalizeState = (state: GeminiFile['state']): FileState => {
  if (!state) {
    return 'STATE_UNSPECIFIED'
  }
  const normalized = state.toUpperCase() as FileState
  if (normalized === 'ACTIVE' || normalized === 'FAILED' || normalized === 'PROCESSING') {
    return normalized
  }
  return 'STATE_UNSPECIFIED'
}
const delay = (ms: number): Promise<void> => {
  return new Promise((resolve) => {
    setTimeout(resolve, ms)
  })
}
</file>
<file path="src/prompt-generator-service.ts">
import { callLLM, type Message, type MessageContent } from '@prompt-maker/core'
import path from 'node:path'
import {
  GEN_SYSTEM_PROMPT,
  REFINE_SYSTEM_PROMPT,
  SERIES_REPAIR_SYSTEM_PROMPT,
  SERIES_SYSTEM_PROMPT,
} from './prompt-generator/prompts'
import {
  buildInitialUserMessage,
  buildRefinementMessage,
  buildRefinementMessageText,
  buildSeriesUserMessage,
  mergeResolvedMediaWithText,
} from './prompt-generator/message-builders'
import { ensureModelCredentials, isGemini } from './prompt-generator/model-credentials'
import { parseLLMJson } from './prompt-generator/parse-llm-json'
import {
  buildSeriesRepairUserMessage,
  isRepairableSeriesValidationError,
} from './prompt-generator/series-repair'
import { validateSeriesResponse } from './prompt-generator/series-validation'
import {
  buildTargetRuntimeModelGuidance,
  sanitizePromptForTargetModelLeakage,
} from './prompt-generator/target-model-guidance'
import type {
  PromptGenerationRequest,
  PromptGenerationResult,
  SeriesResponse,
} from './prompt-generator/types'
export {
  GEN_SYSTEM_PROMPT,
  REFINE_SYSTEM_PROMPT,
  SERIES_SYSTEM_PROMPT,
  SERIES_REPAIR_SYSTEM_PROMPT,
}
export { sanitizePromptForTargetModelLeakage }
export {
  ensureModelCredentials,
  isGemini,
  resolveDefaultGenerateModel,
} from './prompt-generator/model-credentials'
export type {
  PromptGenerationRequest,
  PromptGenerationResult,
  SeriesRepairAttemptDetail,
  SeriesResponse,
  UploadDetail,
  UploadState,
  UploadStateChange,
} from './prompt-generator/types'
type CoTResponse = {
  reasoning: string
  prompt: string
}
type PdfGroundingAssessment =
  | { ok: true }
  | {
      ok: false
      reason: 'missing-pdf-filename' | 'asked-for-document' | 'missing-document-snapshot'
    }
const normalizeForHeuristic = (value: string): string => value.toLowerCase()
const getPdfBasenames = (pdfPaths: readonly string[]): string[] => {
  return pdfPaths.map((pdfPath) => path.basename(pdfPath)).filter((name) => name.length > 0)
}
const assessPdfGrounding = (
  prompt: string,
  pdfPaths: readonly string[],
): PdfGroundingAssessment => {
  if (pdfPaths.length === 0) {
    return { ok: true }
  }
  const lowered = normalizeForHeuristic(prompt)
  const forbiddenAsks = [
    'paste the',
    'paste the document',
    'paste document',
    'provide the document',
    'provide document',
    'upload the document',
    'upload the pdf',
    'provide as a file',
    'provide as file',
  ]
  if (forbiddenAsks.some((needle) => lowered.includes(needle))) {
    return { ok: false, reason: 'asked-for-document' }
  }
  const basenames = getPdfBasenames(pdfPaths)
  const mentionsAnyBasename = basenames.some((name) =>
    normalizeForHeuristic(prompt).includes(name.toLowerCase()),
  )
  if (!mentionsAnyBasename) {
    return { ok: false, reason: 'missing-pdf-filename' }
  }
  const hasSnapshot = lowered.includes('document snapshot') || lowered.includes('document outline')
  if (!hasSnapshot) {
    return { ok: false, reason: 'missing-document-snapshot' }
  }
  return { ok: true }
}
const buildPdfGroundingRepairInstruction = (pdfPaths: readonly string[]): string => {
  const basenames = getPdfBasenames(pdfPaths)
  return [
    'Make this prompt contract non-generic and explicitly grounded in the attached PDF(s).',
    '',
    'Hard requirements:',
    `- Mention the attached PDF filename(s) verbatim somewhere in the contract: ${
      basenames.join(', ') || '(unknown filename)'
    }.`,
    '- In "Inputs", do NOT ask the user to paste/upload/provide the PDF/path. The PDF is already attached and must be used directly.',
    '- Add a new section (in the contract) titled "Document Snapshot" that proves you read the attached PDF:',
    '  - 5-10 bullet points capturing the documentâs specific topics/sections.',
    '  - 3 short verbatim quotes (10-25 words each) from the PDF (wrap each quote in double quotes).',
    '  - If the PDF appears image-only/scanned and you cannot quote text, state that explicitly and request OCR as the only missing input.',
    '',
    'Do not change the overall prompt-contract format requirements; just make it grounded and actionable.',
  ].join('\n')
}
export class PromptGeneratorService {
  async generatePromptDetailed(request: PromptGenerationRequest): Promise<PromptGenerationResult> {
    await ensureModelCredentials(request.model)
    const isRefinement = Boolean(request.previousPrompt && request.refinementInstruction)
    const systemContent = isRefinement ? REFINE_SYSTEM_PROMPT : GEN_SYSTEM_PROMPT
    const geminiApiKey = isGemini(request.model) ? process.env.GEMINI_API_KEY?.trim() : undefined
    let userContent: MessageContent
    if (isRefinement) {
      const previousPrompt = request.previousPrompt
      const refinementInstruction = request.refinementInstruction
      if (!previousPrompt || !refinementInstruction) {
        throw new Error('Refinement requests require previousPrompt and refinementInstruction.')
      }
      userContent = await buildRefinementMessage(
        previousPrompt,
        refinementInstruction,
        request.intent,
        request.fileContext,
        request.images,
        request.videos,
        request.pdfs ?? [],
        request.metaInstructions,
        request.onUploadStateChange,
        geminiApiKey,
      )
    } else {
      userContent = await buildInitialUserMessage(
        request.intent,
        request.fileContext,
        request.images,
        request.videos,
        request.pdfs ?? [],
        request.metaInstructions,
        request.onUploadStateChange,
        geminiApiKey,
      )
    }
    const targetGuidance = buildTargetRuntimeModelGuidance(request.targetModel)
    const messages: Message[] = [
      { role: 'system', content: systemContent },
      ...(targetGuidance ? [{ role: 'system' as const, content: targetGuidance }] : []),
      { role: 'user', content: userContent },
    ]
    const rawResponse = await callLLM(messages, request.model)
    const attemptRepair = async (
      prompt: string,
      reasoning?: string,
    ): Promise<PromptGenerationResult> => {
      const pdfPaths = request.pdfs ?? []
      const assessment = assessPdfGrounding(prompt, pdfPaths)
      if (assessment.ok) {
        return { prompt, ...(reasoning ? { reasoning } : {}) }
      }
      // Avoid infinite repair loops.
      const MAX_PDF_GROUNDING_REPAIR_ATTEMPTS = 1
      for (let attempt = 0; attempt < MAX_PDF_GROUNDING_REPAIR_ATTEMPTS; attempt += 1) {
        const refinementInstruction = buildPdfGroundingRepairInstruction(pdfPaths)
        request.onPromptAutoRepairAttempt?.({
          kind: 'pdf-grounding',
          reason: assessment.reason,
          attempt: 1,
          maxAttempts: MAX_PDF_GROUNDING_REPAIR_ATTEMPTS,
          pdfs: [...pdfPaths],
        })
        const repairText = buildRefinementMessageText(
          prompt,
          refinementInstruction,
          request.intent,
          request.fileContext,
          pdfPaths,
          request.metaInstructions,
        )
        const repairUserContent = mergeResolvedMediaWithText(userContent, repairText)
        const repairMessages: Message[] = [
          { role: 'system', content: REFINE_SYSTEM_PROMPT },
          ...(targetGuidance ? [{ role: 'system' as const, content: targetGuidance }] : []),
          { role: 'user', content: repairUserContent },
        ]
        const repairedRaw = await callLLM(repairMessages, request.model)
        try {
          const repaired = parseLLMJson<CoTResponse>(repairedRaw)
          const repairedPrompt = sanitizePromptForTargetModelLeakage({
            prompt: repaired.prompt,
            intent: request.intent,
            targetModel: request.targetModel,
          })
          const repairedAssessment = assessPdfGrounding(repairedPrompt, pdfPaths)
          if (repairedAssessment.ok) {
            return {
              prompt: repairedPrompt,
              ...(repaired.reasoning ? { reasoning: repaired.reasoning } : {}),
            }
          }
          // If still failing, return the repaired prompt anyway (better than original).
          return {
            prompt: repairedPrompt,
            ...(repaired.reasoning ? { reasoning: repaired.reasoning } : {}),
          }
        } catch {
          // If JSON parse fails, fall back to raw repaired text.
          const repairedPrompt = sanitizePromptForTargetModelLeakage({
            prompt: repairedRaw,
            intent: request.intent,
            targetModel: request.targetModel,
          })
          return { prompt: repairedPrompt }
        }
      }
      return { prompt, ...(reasoning ? { reasoning } : {}) }
    }
    try {
      const result = parseLLMJson<CoTResponse>(rawResponse)
      if (process.env.DEBUG || process.env.VERBOSE) {
        console.error('\n--- AI Reasoning ---')
        console.error(result.reasoning)
        console.error('--------------------\n')
      }
      const sanitizedPrompt = sanitizePromptForTargetModelLeakage({
        prompt: result.prompt,
        intent: request.intent,
        targetModel: request.targetModel,
      })
      return await attemptRepair(sanitizedPrompt, result.reasoning)
    } catch {
      const sanitizedPrompt = sanitizePromptForTargetModelLeakage({
        prompt: rawResponse,
        intent: request.intent,
        targetModel: request.targetModel,
      })
      return await attemptRepair(sanitizedPrompt)
    }
  }
  async generatePrompt(request: PromptGenerationRequest): Promise<string> {
    const result = await this.generatePromptDetailed(request)
    return result.prompt
  }
  async generatePromptSeries(request: PromptGenerationRequest): Promise<SeriesResponse> {
    await ensureModelCredentials(request.model)
    const geminiApiKey = isGemini(request.model) ? process.env.GEMINI_API_KEY?.trim() : undefined
    const userContent = await buildSeriesUserMessage(
      request.intent,
      request.fileContext,
      request.images,
      request.videos,
      request.pdfs ?? [],
      request.metaInstructions,
      request.onUploadStateChange,
      geminiApiKey,
    )
    const targetGuidance = buildTargetRuntimeModelGuidance(request.targetModel)
    const messages: Message[] = [
      { role: 'system', content: SERIES_SYSTEM_PROMPT },
      ...(targetGuidance ? [{ role: 'system' as const, content: targetGuidance }] : []),
      { role: 'user', content: userContent },
    ]
    const MAX_SERIES_REPAIR_ATTEMPTS = 2
    let rawResponse = await callLLM(messages, request.model)
    let series: SeriesResponse
    try {
      series = parseLLMJson<SeriesResponse>(rawResponse)
    } catch {
      throw new Error('LLM did not return valid SeriesResponse JSON.')
    }
    for (let attempt = 0; attempt <= MAX_SERIES_REPAIR_ATTEMPTS; attempt++) {
      try {
        validateSeriesResponse(series)
        break
      } catch (error) {
        if (attempt === MAX_SERIES_REPAIR_ATTEMPTS || !isRepairableSeriesValidationError(error)) {
          throw error
        }
        const validationError = error instanceof Error ? error.message : String(error)
        request.onSeriesRepairAttempt?.({
          attempt: attempt + 1,
          maxAttempts: MAX_SERIES_REPAIR_ATTEMPTS,
          validationError,
        })
        const repairUserContent = buildSeriesRepairUserMessage({
          intent: request.intent,
          validationError,
          previousSeries: series,
        })
        const repairMessages: Message[] = [
          { role: 'system', content: SERIES_REPAIR_SYSTEM_PROMPT },
          ...(targetGuidance ? [{ role: 'system' as const, content: targetGuidance }] : []),
          { role: 'user', content: repairUserContent },
        ]
        rawResponse = await callLLM(repairMessages, request.model)
        try {
          series = parseLLMJson<SeriesResponse>(rawResponse)
        } catch {
          throw new Error('LLM did not return valid SeriesResponse JSON.')
        }
      }
    }
    const sanitizedOverviewPrompt = sanitizePromptForTargetModelLeakage({
      prompt: series.overviewPrompt,
      intent: request.intent,
      targetModel: request.targetModel,
    })
    const sanitizedAtomicPrompts = series.atomicPrompts.map((step) => ({
      ...step,
      content: sanitizePromptForTargetModelLeakage({
        prompt: step.content,
        intent: request.intent,
        targetModel: request.targetModel,
      }),
    }))
    if (process.env.DEBUG || process.env.VERBOSE) {
      console.error('\n--- Series Reasoning ---')
      console.error(series.reasoning)
      console.error('------------------------\n')
    }
    return {
      ...series,
      overviewPrompt: sanitizedOverviewPrompt,
      atomicPrompts: sanitizedAtomicPrompts,
    }
  }
}
export const createPromptGeneratorService = async (): Promise<PromptGeneratorService> => {
  return new PromptGeneratorService()
}
export const generatePromptSeries = async (
  request: PromptGenerationRequest,
): Promise<SeriesResponse> => {
  const service = await createPromptGeneratorService()
  return await service.generatePromptSeries(request)
}
</file>
<file path="package.json">
{
  "name": "@perceptron/prompt-maker-cli",
  "version": "0.1.0",
  "description": "Terminal workflow for diagnosing and improving prompts via prompt-maker.",
  "license": "MIT",
  "type": "module",
  "bin": {
    "prompt-maker-cli": "./dist/index.js"
  },
  "files": [
    "dist"
  ],
  "scripts": {
    "build": "node scripts/build.mjs",
    "dev": "node scripts/dev.mjs",
    "start": "node dist/index.js",
    "typecheck": "tsc -p tsconfig.json --noEmit",
    "test": "jest",
    "format": "prettier -w .",
    "lint": "prettier -c \"{src,packages,scripts,tests}/**/*.{ts,tsx,js,jsx,cjs,mjs,json,md}\"",
    "refresh": "npm run lint && npm run typecheck && npm test && npm run build && npm install -g .",
    "prepare": "npm run build",
    "prepublishOnly": "npm run build"
  },
  "keywords": [
    "prompt",
    "cli",
    "prompt-maker",
    "neovim"
  ],
  "engines": {
    "node": ">=18"
  },
  "dependencies": {
    "@google/generative-ai": "^0.24.1",
    "boxen": "^8.0.1",
    "chalk": "^5.3.0",
    "cli-cursor": "^4.0.0",
    "cli-table3": "^0.6.5",
    "clipboardy": "^5.0.1",
    "enquirer": "^2.4.1",
    "fast-glob": "^3.3.3",
    "fzf": "^0.5.2",
    "html-to-text": "^9.0.5",
    "ink": "^4.4.1",
    "js-tiktoken": "^1.0.21",
    "js-yaml": "^4.1.0",
    "open": "^11.0.0",
    "ora": "^8.1.0",
    "pdfjs-dist": "^4.10.38",
    "react": "^18.3.1",
    "react-dom": "^18.3.1",
    "wrap-ansi": "^8.1.0",
    "yargs": "^18.0.0",
    "zod": "^3.23.8"
  },
  "devDependencies": {
    "@testing-library/react": "^16.1.0",
    "@types/html-to-text": "^9.0.4",
    "@types/jest": "^29.5.12",
    "@types/js-yaml": "^4.0.9",
    "@types/node": "^20.12.12",
    "@types/react": "^18.3.11",
    "@types/react-dom": "^18.3.0",
    "@types/yargs": "^17.0.33",
    "@typescript-eslint/eslint-plugin": "^8.51.0",
    "@typescript-eslint/parser": "^8.51.0",
    "esbuild": "^0.24.2",
    "eslint": "^8.57.1",
    "jest": "^29.7.0",
    "jsdom": "^24.0.0",
    "prettier": "^3.3.3",
    "ts-jest": "^29.2.5",
    "typescript": "^5.6.3"
  }
}
</file>
<file path="docs/neovim-plugin-integration.md">
# prompt-maker-cli Ă NeoVim Integration Guide
_Comprehensive reference for building a NeoVim plugin that orchestrates prompt-maker-cli runs, context capture, iterative refinements, and artifact delivery inside the editor._
## Quick Checklist
- Ensure `prompt-maker-cli` binary is built/installed and discoverable in `$PATH`.
- Confirm `OPENAI_API_KEY`/`GEMINI_API_KEY` (plus optional `GITHUB_TOKEN`) are set or present in `~/.config/prompt-maker-cli/config.json`.
- Capture intent via inline text, temp files, or buffer exportsânever launch without validated intent input.
- Attach context through `--context`, `--url`, `--image`/`--video`, and `--smart-context-root` as needed; guard against oversized or duplicate files.
- Prefer `--json --quiet --stream jsonl` for editor integrations; parse `generation.final`, `context.telemetry`, and `context.overflow` to drive UI.
- Set token budgets via `--max-input-tokens` and/or `--max-context-tokens`, and choose an overflow strategy via `--context-overflow`.
- Use `telemetry.totalTokens` and `telemetry.fileTokens` from the `context.telemetry` event as guardrails (warn early, and keep a safety margin).
- Use `--interactive-transport <socket>` for refinement loops; send `{"type":"refine"}` / `{"type":"finish"}` messages and mirror streamed events.
- Apply `--context-template nvim` (or user template) before writing to buffers; fall back to `polishedPrompt`/`prompt` if `renderedPrompt` absent.
- Surface warnings/errors from stderr immediately (credentials, context fetch failures, upload issues) and offer corrective prompts.
- Append runs to history or expose pickers by tailing `~/.config/prompt-maker-cli/history.jsonl`.
- Clean up sockets/temp files when jobs end; cancel outstanding refinements on disconnect.
## 1. Purpose & Audience
- **Audience**: developers of a NeoVim plugin (Lua or TypeScript via `deno-nvim`/`node-host`) and accompanying coding agents collaborating on that plugin.
- **Goal**: expose `prompt-maker-cli` from inside NeoVim without reimplementing its business logicâuse the CLI as the single source of truth for prompt contracts, telemetry, history logging, and transports.
- **Scope**: everything from installation and configuration through synchronous command execution, streaming refinement loops, media/context ingestion, diagnostics, and fallback handling.
## 2. Build & Runtime Prerequisites
- **Repository build**: from repo root run `npm ci` then `npm run build`. Artifacts land in `dist/`.
- **Global install**: `npm install -g .` (or `npm link` for iterative local development). The binary name is `prompt-maker-cli` (aliasable to `pmc`).
- **Direct execution**: during development you can run `node dist/index.js ...` (or `npm start -- ...`) without reinstalling.
- **Node tooling**: plugin should locate the binary via `which prompt-maker-cli` or respect user-configured aliases.
- **Config directory**: `$HOME/.config/prompt-maker-cli/` hosts `config.json`, `history.jsonl`, and the embeddings cache; the plugin must never write arbitrary files here unless mirroring CLI behavior (e.g., context templates, transport sockets under `/tmp`).
- **Environment**: CLI relies on `OPENAI_API_KEY`, `GEMINI_API_KEY`, optional `*_BASE_URL` overrides, and `PROMPT_MAKER_*` vars. Plugin settings UI should surface these but avoid storing secrets in git.
## 3. Command Surfaces & Flag Matrix
- **Primary command**: `prompt-maker-cli [intent] [options]` maps to the âgenerateâ workflow; `prompt-maker-cli test ...` is unrelated to prompt generation.
- **Key positional/flag inputs**: summarized below (full list in Appendix A).
  - Intent: inline argument, `--intent-file`, or stdin.
  - Context: repeated `--context` globs, `--url`, `--image`, `--video`, `--smart-context` (+ `--smart-context-root`).
  - Output controls: `--json`, `--copy`, `--open-chatgpt`, `--context-template`, `--context-file`, `--context-format`.
  - Token budgets: `--max-input-tokens`, `--max-context-tokens`, `--context-overflow`.
  - Models: `--model <name>` (generation) and `--target <name>` (runtime optimization; recorded in JSON/history, not included in the generated prompt text by default).
  - Interaction: `-i/--interactive`, `--stream jsonl`, `--interactive-transport <path>`, `--quiet`, `--no-progress`.
  - Post-processing: `--polish`, `--polish-model`.
## 4. Intent & Context Ingestion
- **Intent sources**: exactly one of inline argument, `--intent-file`, or stdin must provide content. Files are capped at 512 KB, must be UTFâ8 text, and cannot contain NUL bytes (`readIntentFile`).
- **Interactive intent disambiguation**: placing a path immediately after `-i/--interactive` is treated as an implicit intent file with a warningâplugin should avoid this ambiguity by emitting explicit flags.
- **Context files**: `resolveFileContext` expands globs with `fast-glob` (`dot: true`). Missing matches trigger warnings but are not fatal. Each file becomes `<file path="âŚ">âŚ</file>` inside prompts.
- **Context dedupe**: plugin should keep track of already attached buffers/files to avoid redundant globbing, especially when also using smart context.
- **Context exports**: use `--show-context`, `--context-file <path>`, and `--context-format text|json` to preview or persist the blocks that feed the LLMâhandy for preview windows inside NeoVim.
## 5. Remote Context Resolution
- **HTTP/HTTPS URLs**: `resolveUrlContext` downloads each URL (â¤1 MB). HTML is converted to text via `html-to-text`, stripping scripts/styles. Failures log warnings but continue.
- **Duplicate suppression**: identical URLs are skipped. Only http(s) protocols allowed.
- **GitHub URLs**: `github-context.ts` recognizes `blob`, `tree`, and repo root URLs. Safeguards include file caps (â¤64 KB per file, â¤60 files), ignore lists (`node_modules`, `dist`, lockfiles, archives), and automatic path prefix filtering. Responses stream progress via the `onProgress` callbackâplugins can map these to NeoVim notifications.
- **Authentication**: honors `GITHUB_TOKEN` if set; otherwise uses unauthenticated requests with strict rate limits.
## 6. Smart Context (Local RAG)
- **Trigger**: `--smart-context` optionally paired with `--smart-context-root <dir>`.
- **Scan**: `fast-glob` searches `**/*.{ts,tsx,js,jsx,py,md,json}` excluding `node_modules`, build outputs, lockfiles, git metadata. Files >25 KB are skipped.
- **Embeddings**: `src/rag/vector-store.ts` stores SHA256 hashes + embeddings in `$HOME/.config/prompt-maker-cli/embeddings_cache.json` via `@prompt-maker/core#getEmbedding`.
- **Workflow**: index (with caching) â topâk search (default 5) against intent string â read + append unique files not already in the user-provided context list.
- **Progress reporting**: `resolveSmartContextFiles` emits messages (âScanningâŚâ, âIndexedâŚâ, âSmart context readyâ) that surface through progress spinners and `--stream jsonl` events; the plugin should surface these as status lines.
## 7. Media Attachments
- **Images** (`--image`): Accepts PNG/JPG/JPEG/WEBP/GIF up to 20 MB. Files are Base64 encoded into `@prompt-maker/core` image parts. Unsupported extensions or oversize files generate warnings and are skipped.
- **Videos** (`--video`): Requires Gemini models. If any `--video` flag is present and the requested model is not Gemini, the CLI auto-switches to `gemini-3-pro-preview` (or configured default). Uploads use Googleâs Files API via `GoogleAIFileManager`, polling until `ACTIVE` or failing with a detailed error.
- **Upload telemetry**: `upload.state` events emit `start/finish` for each file; when spinners are visible the label swaps to âUploadingâŚâ until transfers finish. Plugin integrations should reflect this to prevent users from closing buffers mid-upload.
## 8. Model Resolution & Credentials
- **Default model**: `resolveDefaultGenerateModel` picks `promptGenerator.defaultModel`, `PROMPT_MAKER_GENERATE_MODEL`, or `gpt-4o-mini`.
- **Gemini fallback**: `resolveGeminiVideoModel` prefers `promptGenerator.defaultGeminiModel`, otherwise `gemini-3-pro-preview`.
- **Credential loading**: `ensureModelCredentials` pulls env vars first, then falls back to `~/.config/prompt-maker-cli/config.json`. Missing keys throw descriptive errors before API calls, so the plugin should capture stderr and surface the message.
- **Polish model**: defaults to the generation model unless `--polish-model` or `PROMPT_MAKER_POLISH_MODEL` overrides it.
- **Target model**: set via `--target <name>` (or `/target` in the TUI). If omitted, it defaults to the resolved default generation model. The CLI uses it as internal optimization guidance (and emits it as `targetModel` in payloads/history), but it must not be echoed into the user-facing prompt text unless the intent explicitly asks for it.
## 9. Generation & Refinement Workflow
1. **Telemetry prep**: count tokens for intent + context (using `js-tiktoken`). Results print in a cyan box/table unless `--quiet` or `--json` suppresses UI. Plugin can parse `generation.iteration.start` events for token counts when running quiet.
2. **Initial iteration**: `PromptGeneratorService.generatePrompt` builds CoT JSON prompts with `GEN_SYSTEM_PROMPT` and `buildInitialUserMessage`. Responses must be JSON containing `reasoning` and `prompt`; if parsing fails, raw text is passed through after logging a warning.
3. **Interactive mode**:
   - **TTY loop**: uses Enquirer to ask âRefine?â and prompt for instructions. Plugin integrations typically avoid this mode and prefer transports.
   - **Transport loop**: `--interactive-transport /tmp/pmc.sock` opens a Unix socket (or Windows pipe) that accepts newline-delimited JSON commands: `{"type":"refine","instruction":"..."}` or `{"type":"finish"}`. The CLI streams events back over the same socket (mirroring `--stream jsonl` output) so the plugin can update UI without parsing stdout.
4. **Refinements**: each instruction triggers a new iteration, passing the prior prompt, original intent, context, and the latest instruction into `buildRefinementMessage` (system prompt switches to `REFINE_SYSTEM_PROMPT`).
5. **Completion**: when the user stops refining, the CLI emits `interactive.state`=`complete` and returns the final prompt plus iteration count.
6. **Polish pass** (optional): `--polish` invokes `polishPrompt` with a fixed system instruction emphasizing formatting fidelity.
## 10. Streaming Events & IPC
- **Modes**: `--stream none|jsonl` controls whether events print to stdout. Even with `none`, taps can receive events (e.g., interactive transport attaches as a tap).
- **Schema** (see `StreamEvent` in `src/generate/types.ts`):
  - All stream events include `event` and `timestamp`.
  - `context.telemetry` â `{ telemetry:{ files:[{path,tokens}], intentTokens, fileTokens, systemTokens, totalTokens } }`
  - `context.overflow` â `{ strategy, before, after, droppedPaths:[{path, source}] }`
  - `resume.loaded` â `{ source:'history'|'file', reusedContextPaths:[{path,source}], missingContextPaths:[{path,source}] }`
  - `progress.update` â `{ label, state:'start|update|stop', scope?:'url|smart|generate|polish|generic' }`
  - `upload.state` â `{ state:'start|finish', detail:{kind:'image|video', filePath} }`
  - `generation.iteration.start` â `{ iteration, intent, model, interactive, inputTokens, refinements[], latestRefinement? }`
  - `generation.iteration.complete` â `{ iteration, prompt, tokens }`
  - `interactive.state` â `{ phase:'start|prompt|refine|complete', iteration }`
  - `interactive.awaiting` â `{ mode:'transport|tty|none' }`
  - `transport.listening|transport.client.connected|transport.client.disconnected`
  - `generation.final` â `{ result: GenerateJsonPayload }`
- **Transport lifecycle**: server cleans up Unix sockets on exit (`SIGINT`, `SIGTERM`, `process.exit`). Plugin must handle reconnects and `transport.error` messages.
## 11. Output Assembly & Delivery
- **GenerateJsonPayload**: includes `schemaVersion`, `intent`, `model` (generation model), `targetModel` (runtime model), `prompt`, `refinements`, `iterations`, `interactive`, `timestamp`, `contextPaths`, optional `outputPath`, `polishedPrompt`, `polishModel`, `contextTemplate`, `renderedPrompt`.
- **Context templates**: built-in `nvim` template injects a header and instructions before inserting `{{prompt}}`. User-defined templates live under `contextTemplates` in `config.json`. Plugin should let users select templates per run or default to `nvim` when targeting scratch buffers.
- **Clipboard/browser**: `--copy` uses `clipboardy`; `--open-chatgpt` opens `https://chatgpt.com/?q=...`. In headless editor environments these options should default to false unless explicitly enabled.
- **History logging**: every run (JSON payload) appends to `$HOME/.config/prompt-maker-cli/history.jsonl`. Plugins can tail this file to show recent prompts or rehydrate drafts.
## 12. Telemetry & Status Presentation
- **Token summaries**: highlight total, intent, and file tokens plus top 10 files with counts. When running `--quiet` the plugin should parse the `telemetry` object from the `context.telemetry` event and render it in NeoVim (virtual text, floating window, etc.).
- **Spinners**: `ora` spinners appear only when `--progress` is true and stdout is a TTY. Plugins using `--quiet --stream jsonl` get deterministic `progress.update` events instead of spinners.
- **Reasoning**: setting `DEBUG=1` or `VERBOSE=1` prints `reasoning` text to stderr after each LLM call. Plugins can capture stderr to display reasoning logs or route them to a diagnostics pane.
## 13. Error Handling & Edge Cases
- **Missing intent**: CLI throws `Intent text is requiredâŚ`. Plugin should prompt the user to provide text before spawning the process.
- **Invalid path/URL**: warnings are emitted but runs continue. Capture stderr so the user sees skipped files.
- **Oversized/binary intent files**: fail fast with descriptive errors (size in KB, binary detection). Provide guardrails in the plugin UI (file picker + size hint).
- **Credential errors**: `ensureModelCredentials` throws if API keys are absentâsurface these immediately with actionable messaging (e.g., open plugin settings).
- **Schema mismatch (history exports)**: `prompt-maker-cli export --from-history ...` validates `schemaVersion` and fails if the selected entry is unsupported. Surface the error and suggest upgrading/downgrading prompt-maker-cli or regenerating/exporting with a matching version (`src/history/generate-history.ts`).
- **Transport disconnects**: CLI emits `transport.client.disconnected` and drains pending refinement promises; the plugin should reconnect or end the session gracefully.
- **Upload failures**: warnings identify the file and root cause. Consider showing a quickfix entry pointing at the asset path.
- **Smart context indexing failure**: CLI logs warnings but continues. Plugin may retry with reduced scope or inform the user to run without smart context.
## 14. Integration Playbooks
1. **One-shot prompt into buffer**
   - Command: `prompt-maker-cli <intent> --json --quiet --context-template nvim --context ...`
   - Steps: run job â capture stdout JSON â insert `renderedPrompt` (fallback to `polishedPrompt`/`prompt`).
2. **Interactive refinement split**
   - Start CLI with `--quiet --stream jsonl --interactive-transport /tmp/pmc.sock` inside a background job.
   - Plugin tails stdout for JSONL events, displays prompts in a buffer, and sends `refine`/`finish` commands over the socket based on user input.
3. **Context preview workflow**
   - Use `--context-file /tmp/pmc-context.json --context-format json --show-context`.
   - Plugin parses the JSON for an expandable tree view and optionally caches context for auditing.
4. **History-driven picker**
   - Tail `$HOME/.config/prompt-maker-cli/history.jsonl` (or periodically parse) to feed a Telescope picker listing recent intents/prompts.
   - Selecting an entry can repopulate buffers or re-run with `--intent-file` referencing saved drafts.
5. **Media-assisted runs**
   - Validate selected images/videos before invoking CLI (check extension + size).
   - Display upload progress based on `upload.state` events and prevent user from interrupting until finish.
6. **Automated guardrails**
   - Monitor `telemetry.totalTokens` from the `context.telemetry` event; if above a threshold (e.g., 30k), warn the user or prompt to trim context.
   - Set budgets proactively: start with `--max-context-tokens` (for predictable trimming), and add `--max-input-tokens` once you also account for system/intent overhead.
   - Prefer `--context-overflow drop-smart` when using `--smart-context` so auto-attached files are dropped before user-selected context.
   - Budgets apply only to text context entries (file/url/smart); images/videos are not trimmed by these strategies.
   - React to `context.overflow` by showing a notice that lists `droppedPaths` (and consider offering a one-click re-run with a larger budget).
   - Enforce linear command queueing when multiple runs share the same socket path to avoid collisions.
## 15. Appendix
### A. High-value Flags
| Flag                                       | Purpose                                                                                  |
| ------------------------------------------ | ---------------------------------------------------------------------------------------- |
| `<intent>` / `--intent-file` / stdin       | Provide rough intent text.                                                               |
| `-c, --context <glob>`                     | Attach local files. Repeatable.                                                          |
| `--url <https://âŚ>`                        | Pull remote docs or GitHub repos.                                                        |
| `--image <path>` / `--video <path>`        | Inline media (image Base64, video uploads).                                              |
| `--smart-context` / `--smart-context-root` | Enable embedding-based file selection.                                                   |
| `-i, --interactive`                        | Enable TTY refinement loop.                                                              |
| `--interactive-transport <path>`           | Socket/pipe commands + streaming events.                                                 |
| `--stream jsonl`                           | Emit newline-delimited structured events.                                                |
| `--max-input-tokens <n>`                   | Cap total input tokens (intent + system + text context).                                 |
| `--max-context-tokens <n>`                 | Cap tokens reserved for text context entries.                                            |
| `--context-overflow <strategy>`            | Handle budget overflow: `fail`, `drop-smart`, `drop-url`, `drop-largest`, `drop-oldest`. |
| `--json`                                   | Print final payload for programmatic consumption.                                        |
| `--context-template <name>`                | Wrap final prompt (built-in `nvim`).                                                     |
| `--model <name>`                           | Generation model used by the CLI.                                                        |
| `--target <name>`                          | Runtime model the prompt is optimized for.                                               |
| `--copy`, `--open-chatgpt`                 | Clipboard/browser handoff.                                                               |
| `--polish`, `--polish-model`               | Post-generation refinement.                                                              |
| `--quiet`, `--no-progress`                 | Suppress UI spinners/banners.                                                            |
### B. Environment Variables & Config Keys
| Key                                  | Meaning                                    |
| ------------------------------------ | ------------------------------------------ |
| `OPENAI_API_KEY` / `OPENAI_BASE_URL` | OpenAI credentials (env overrides config). |
| `GEMINI_API_KEY` / `GEMINI_BASE_URL` | Gemini credentials + custom endpoints.     |
| `GITHUB_TOKEN`                       | Optional token for GitHub API rate limits. |
| `PROMPT_MAKER_GENERATE_MODEL`        | Default generation model override.         |
| `PROMPT_MAKER_POLISH_MODEL`          | Default polishing model.                   |
| `PROMPT_MAKER_CLI_CONFIG`            | Custom config file path.                   |
| `DEBUG` / `VERBOSE`                  | Enable reasoning logs to stderr.           |
Config file example (`~/.config/prompt-maker-cli/config.json`):
```json
{
  "openaiApiKey": "sk-âŚ",
  "geminiApiKey": "gk-âŚ",
  "promptGenerator": {
    "defaultModel": "gemini-1.5-flash",
    "defaultGeminiModel": "gemini-3-pro-preview"
  },
  "contextTemplates": {
    "nvim": "## NeoVim Prompt Buffer\n\n{{prompt}}",
    "scratch": "# Prompt Vault\n\n{{prompt}}"
  }
}
```
### C. File & Socket Locations
- CLI binary: global npm prefix (e.g., `~/.nvm/versions/node/v22.15.0/bin/prompt-maker-cli`).
- Config/history: `$HOME/.config/prompt-maker-cli/` (`config.json`, `history.jsonl`, `embeddings_cache.json`).
- Transport sockets: plugin chooses path (e.g., `/tmp/pmc.nvim.sock`). Clean up stale sockets before binding.
- Context exports: plugin-controlled temp dir (respect user preferences, clean up automatically).
### D. Event Handling Checklist
1. Always parse stdout as JSONL when `--stream jsonl` is active.
2. Ignore unknown `event` values (forward-compatible); treat them as informational unless your integration opts in.
3. Treat stderr as human-readable diagnostics (warnings, reasoning, spinner fallbacks).
4. Handle `context.overflow` by notifying the user and reconciling any UI âattached contextâ lists with `droppedPaths`.
5. Handle `transport.error` messages by notifying the user and prompting for corrected commands.
6. When the CLI exits, stop sending commands, close socket handles, and clean up temp files.
---
Use this guide as the single reference for coding agents and humans collaborating on the NeoVim plugin. The CLI already provides robust orchestration primitives; the pluginâs job is to present them elegantly inside NeoVim, protect users from misconfiguration, and surface rich telemetry in real time.
</file>
<file path="docs/pmc-tutorial.md">
# Comprehensive, Progressive Lesson Plan for Using prompt-maker-cli (PMC)
This tutorial is a progressive, lab-first curriculum that teaches a developer to use **prompt-maker-cli (PMC)** from beginner to advanced.
Itâs grounded in the repositoryâs docs and source-of-truth behavior:
- `docs/cookbook.md`
- `docs/tui-styling-guide.md`
- `docs/prompt-maker-cli-tui-encyclopedia.md`
- `docs/neovim-plugin-integration.md`
- `src/prompt-generator/message-builders.ts`
> Note: there is also a legacy tutorial at `legacy docs/tutorial.md`. It contains older path references, but its command patterns are still broadly consistent with the modern CLI.
---
## Assumptions
- You are in the repo root (`prompt-maker/`) with Node.js **18+**.
- You have at least one provider configured:
  - OpenAI: `OPENAI_API_KEY`
  - Gemini: `GEMINI_API_KEY` (required for video)
- Optional: `GITHUB_TOKEN` for GitHub URL context (improves API rate limits).
- OS defaults: macOS or Linux. Windows works for most CLI features, but interactive transport uses Windows named pipes (see Lesson 8).
---
## Syllabus (Progressive Lessons)
1. **Setup & Mental Model** (Beginner)
2. **TUI Basics: Generate View + Command Palette** (Beginner)
3. **TUI Power Features: Context, Series, History, Tokens, Themes** (Intermediate)
4. **Generate Mode Fundamentals: Intent + Context + Output** (Beginner â Intermediate)
5. **Remote Context: URLs + GitHub URLs** (Intermediate)
6. **Smart Context (Local RAG / Embeddings)** (Intermediate)
7. **Media Attachments: Images + Video (Gemini)** (Intermediate)
8. **Refinement Workflows: Interactive TTY + Interactive Transport** (Advanced)
9. **Automation Outputs: `--json`, `--stream jsonl`, quiet/progress controls** (Advanced)
10. **Prompt Testing: `prompt-maker-cli test` + YAML suites** (Intermediate)
11. **NeoVim Integration Playbooks (including sidekick.nvim terminal workflows)** (Advanced)
Capstones:
- **Capstone A**: Non-interactive automation run â JSON â extract prompt
- **Capstone B**: JSONL streaming run â parse events
- **Capstone C**: NeoVim workflow â one-shot + interactive transport loop
---
## Shared Reference: Run Methods
Use whichever matches your situation:
### From source (recommended while working in this repo)
```bash
npm ci
npm run build
# TUI (default when no args)
npm start
# Generate
npm start -- "Draft a confident onboarding-bot spec" --model gpt-4o-mini
# Test runner
npm start -- test prompt-tests.yaml
```
### Installed CLI binary
```bash
# TUI
prompt-maker-cli
# Generate
prompt-maker-cli "Summarize src/tui/ in 5 bullets" --context "src/tui/**/*.ts*" --polish
# Tests
prompt-maker-cli test
```
---
# Lesson 1 â Setup & Mental Model (Beginner)
## Objectives
- Build and run PMC from this repo.
- Understand the three top-level modes: `ui`, `generate`, `test`.
- Configure credentials via env vars or config file.
## Prerequisites
- Node.js 18+
- Repo clone
## Concepts
- **Routing rules** (`docs/prompt-maker-cli-tui-encyclopedia.md`):
  - No args â `ui`
  - `ui` â TUI
  - `test` â prompt test runner
  - `generate` / `expand` â generate pipeline
  - Anything else (including flags like `--json`) â generate pipeline
- **Config resolution order** (`docs/prompt-maker-cli-tui-encyclopedia.md`, `docs/cookbook.md`):
  1. `PROMPT_MAKER_CLI_CONFIG=/path/to/config.json`
  2. `~/.config/prompt-maker-cli/config.json`
  3. `~/.prompt-maker-cli.json`
- **Credential precedence** (`docs/cookbook.md`, `docs/neovim-plugin-integration.md`):
  - Env vars override config keys:
    - `OPENAI_API_KEY` (optional `OPENAI_BASE_URL`)
    - `GEMINI_API_KEY` (optional `GEMINI_BASE_URL`)
## Walkthrough (Commands)
```bash
# 1) Install deps + build
npm ci
npm run build
# 2) Confirm the CLI runs (TUI)
npm start
```
If you want to use the `prompt-maker-cli` binary directly:
```bash
# From the repo root
npm install -g .
# Sanity check
prompt-maker-cli --help
```
## Lab
1. Start the TUI via `npm start`.
2. Exit cleanly with `Ctrl+C`.
3. Create a config file (choose one):
   - `~/.config/prompt-maker-cli/config.json` (recommended)
   - or set `PROMPT_MAKER_CLI_CONFIG` to point at a custom path.
Example config skeleton (do not commit secrets):
```json
{
  "openaiApiKey": "sk-...",
  "geminiApiKey": "gk-...",
  "promptGenerator": {
    "defaultModel": "gpt-4o-mini",
    "defaultGeminiModel": "gemini-3-pro-preview"
  },
  "contextTemplates": {
    "scratch": "# Scratch\n\n{{prompt}}"
  },
  "theme": "ocean",
  "themeMode": "system"
}
```
## Validation
- `npm start` launches an Ink UI (not a stack trace).
- `prompt-maker-cli` is on your `$PATH` if you installed globally.
- With credentials configured, later lessons wonât fail with âMissing OpenAI credentialsâ / âMissing Gemini credentialsâ.
## Troubleshooting
- **Ink raw mode error**: youâre not in a real TTY (e.g. piping output); use generate mode instead.
- **Credential errors**: set env vars or config keys (`openaiApiKey` / `geminiApiKey`).
## Video Script (Lesson 1)
- On screen: repo root â run `npm ci` â run `npm run build`.
- Show: `npm start` opens the TUI and exits with `Ctrl+C`.
- Narration beats:
  - âThree modes: `ui`, `generate`, `test`.â
  - âConfig precedence and env var override.â
  - âWeâre using repo build first; global install is optional.â
---
# Lesson 2 â TUI Basics: Generate View + Command Palette (Beginner)
## Objectives
- Navigate the TUI and use core global keys.
- Use the command palette to discover TUI commands.
- Generate a basic prompt from typed intent.
## Prerequisites
- Lesson 1
## Concepts
- TUI global keys (`docs/cookbook.md`, `docs/prompt-maker-cli-tui-encyclopedia.md`):
  - `Ctrl+G`: open command palette
  - `Ctrl+T`: switch to Test Runner view
  - `/help`: open help overlay (definitive keybind list)
  - `Esc`: dismiss popups
- The TUI is âkeyboard-firstâ and follows strict input routing:
  1. Help overlay
  2. Popup input
  3. Screen input
  4. Global keys
## Walkthrough (Commands)
```bash
# Start the TUI
prompt-maker-cli
# (or: npm start)
```
In the UI:
1. Type `/help` to open help.
2. Press `Ctrl+G` to open the command palette.
3. Type `/` (or keep palette open) and browse commands.
## Lab
1. In the Generate view, type a short intent (example):
   - âSummarize this repository in 5 bullets.â
2. Press `Enter` to generate.
3. Open the command palette (`Ctrl+G`) and run:
   - `/settings`
   - `/history`
## Validation
- You can open help (`/help`) and the command palette (`Ctrl+G`).
- A prompt is generated and appears in the history pane.
- `/history` shows prior commands/runs.
## Troubleshooting
- **Keys donât work**: a popup or help overlay likely owns inputâpress `Esc`.
- **Generation fails**: check credential setup (Lesson 1).
## Video Script (Lesson 2)
- Show `prompt-maker-cli` launch.
- Demo: `/help`, `Ctrl+G`, run `/settings`, run `/history`.
- Generate one intent and point out history updates.
---
# Lesson 3 â TUI Power Features: Context, Series, History, Tokens, Themes (Intermediate)
## Objectives
- Add context (files, URLs, images, video, smart context) via the TUI.
- Generate a **series** (atomic prompt plan) and find the saved files.
- Understand token telemetry and history in the UI.
- Change themes and theme mode, and understand where themes are loaded from.
## Prerequisites
- Lessons 1â2
## Concepts
### Adding context via TUI commands
From `docs/cookbook.md` and `docs/prompt-maker-cli-tui-encyclopedia.md`:
- `/file`: add file globs
- `/url`: add URL context
- `/image`: attach image paths
- `/video`: attach video paths (forces Gemini at generation time)
- `/smart`: toggle smart context
- `/smart-root`: set/clear smart-context root
### Series generation and its constraints
- TUI generates a âseriesâ and writes markdown files under:
  - `generated/series/<timestamped-folder>/`
  - Includes `00-overview.md` plus one file per atomic prompt (`docs/cookbook.md`).
- Under the hood, series generation asks the model to return **strict JSON** matching:
```json
{
  "reasoning": "...",
  "overviewPrompt": "...",
  "atomicPrompts": [{ "title": "...", "content": "..." }]
}
```
(See `src/prompt-generator/message-builders.ts`.)
- Series and regular prompt generation share a constraint: **the model must not execute tasks**, only craft instruction artifacts.
### Themes and theme discovery
From `docs/prompt-maker-cli-tui-encyclopedia.md` and `docs/tui-styling-guide.md`:
- Built-in themes live under `src/tui/theme/builtins/`.
- Custom themes are discovered from:
  - `~/.config/prompt-maker-cli/themes/*.json`
  - Project: `.prompt-maker-cli/themes/*.json` (searches ancestor directories)
- Theme selection persists back to your CLI config as `theme` and `themeMode`.
## Walkthrough (Commands)
```bash
prompt-maker-cli
```
In the UI:
1. Add file context: open palette â `/file` â enter `docs/*.md`.
2. Add URL context: `/url` â paste `https://github.com/<owner>/<repo>` (any repo URL is fine).
3. Open token breakdown popup: `/tokens`.
4. Create a series:
   - type an intent, then press `Tab`, or
   - run `/series`.
5. Change theme:
   - `/theme` â choose a theme
   - `/theme-mode` â pick `dark`, `light`, or `system`
## Lab
1. Run `/series` for an intent like:
   - âCreate an implementation plan to add a new command to the TUI.â
2. Find the generated folder:
   - `generated/series/<timestamped-folder>/`
3. Open `/tokens` after a run and note:
   - total tokens
   - which files contributed the most tokens
4. (Optional) Install a custom theme by copying the example theme:
```bash
mkdir -p .prompt-maker-cli/themes
cp src/tui/theme/examples/ocean-example.json .prompt-maker-cli/themes/ocean.json
```
Restart the TUI and select `ocean` via `/theme`.
## Validation
- A new folder exists under `generated/series/` after `/series`.
- `/tokens` shows a breakdown (even if values differ).
- Theme selection persists across TUI restarts.
## Troubleshooting
- **Series folder not written**: filesystem permissions issue; the TUI should still generate but will report write errors.
- **Theme not found**: confirm theme JSON is in one of the discovery directories and restart the TUI.
## Video Script (Lesson 3)
- Demo: `/file` with `docs/*.md` and `/tokens`.
- Demo: `/series` â show `generated/series/...` folder.
- Demo: copy `ocean-example.json` into `.prompt-maker-cli/themes/` and select it.
---
# Lesson 4 â Generate Mode Fundamentals: Intent + Context + Output (Beginner â Intermediate)
## Objectives
- Use generate mode via `prompt-maker-cli "..."` and `prompt-maker-cli generate ...`.
- Provide intent via positional arg, `--intent-file`, or stdin.
- Attach file context with `--context` globs (including `!` excludes).
- Use `--show-context` / `--context-file` / `--context-format`.
## Prerequisites
- Lesson 1
## Concepts
### Intent ingestion methods
From `docs/prompt-maker-cli-tui-encyclopedia.md`:
- Positional intent: `prompt-maker-cli "..."`
- Intent file: `--intent-file path/to/intent.md` (alias `-f`)
- stdin: pipe into the command
Rule of thumb:
- Provide intent exactly one way (avoid ambiguity).
### Local context ingestion
From `docs/prompt-maker-cli-tui-encyclopedia.md` and `docs/cookbook.md`:
- `-c, --context <glob>` is repeatable.
- Globs use `fast-glob` and support excludes via `!` prefixes.
### Context preview/export
- `--show-context` prints resolved context before generation.
- `--context-file <path>` writes resolved context to a file.
- `--context-format text|json` controls rendering.
## Walkthrough (Commands)
```bash
# Positional intent
prompt-maker-cli "Summarize the TUI architecture in 5 bullets" \
  --context "docs/prompt-maker-cli-tui-encyclopedia.md"
# Intent from file (keep it small; intent files are validated and size-capped)
cat > /tmp/pmc-intent.md <<'EOF'
Summarize the key PMC run modes and how routing works.
EOF
prompt-maker-cli --intent-file /tmp/pmc-intent.md \
  --context "docs/prompt-maker-cli-tui-encyclopedia.md" \
  --show-context
# Intent from stdin
printf "%s" "List key CLI flags and what they do" | prompt-maker-cli \
  --context docs/prompt-maker-cli-tui-encyclopedia.md
```
## Lab
1. Run a context preview export:
```bash
prompt-maker-cli "Explain PMC context ingestion" \
  --context "docs/prompt-maker-cli-tui-encyclopedia.md" \
  --context-file /tmp/pmc-context.json \
  --context-format json
```
2. Inspect that file and confirm itâs structured JSON.
## Validation
- `--show-context` prints `<file ...>` blocks (text mode).
- `/tmp/pmc-context.json` is a JSON array (json mode).
## Troubleshooting
- **No files matched**: your glob didnât match any files; PMC warns and continues.
- **Unreadable files**: PMC warns and skips those files.
## Video Script (Lesson 4)
- Show 3 intent ingestion methods (positional, `--intent-file`, stdin).
- Show `--context` excludes using `!`.
- Show `--context-file --context-format json` and open the output.
---
# Lesson 5 â Remote Context: URLs + GitHub URLs (Intermediate)
## Objectives
- Attach remote docs using `--url`.
- Use GitHub URLs (`blob`, `tree`, repo root) as context.
- Understand size/file limits and authentication behaviors.
## Prerequisites
- Lesson 4
## Concepts
From `docs/prompt-maker-cli-tui-encyclopedia.md` and `docs/neovim-plugin-integration.md`:
- `--url` only accepts `http:` / `https:`.
- HTML pages are downloaded (â¤ 1MB) and converted to text.
- GitHub URLs are special-cased:
  - Repo root: `https://github.com/<owner>/<repo>`
  - Tree: `https://github.com/<owner>/<repo>/tree/<ref>/<path?>`
  - Blob: `https://github.com/<owner>/<repo>/blob/<ref>/<path>`
- GitHub safety limits:
  - max files: 60
  - max size per file: 64KB
  - common large/binary paths skipped (`node_modules`, `dist`, lockfiles, archives, etc.)
- Optional auth: set `GITHUB_TOKEN` to reduce rate-limit pain.
## Walkthrough (Commands)
```bash
# Basic web URL context
prompt-maker-cli "Summarize this page into prompt instructions" \
  --url https://example.com
# GitHub repo root as context
prompt-maker-cli "Summarize this repo" \
  --url https://github.com/folke/sidekick.nvim
# GitHub tree path as context
prompt-maker-cli "Summarize the docs folder" \
  --url https://github.com/folke/sidekick.nvim/tree/main/doc
```
## Lab
1. Run a GitHub URL ingest and confirm it keeps running even if some files are skipped.
2. Repeat with `GITHUB_TOKEN` set (if you have one) and observe improved reliability under rate limits.
## Validation
- Output includes content that clearly came from the URL.
- If any URL fails, PMC emits warnings but continues.
## Troubleshooting
- **HTTP errors**: confirm URL is reachable and uses http(s).
- **GitHub rate limiting**: set `GITHUB_TOKEN`.
## Video Script (Lesson 5)
- Demo `--url` against a normal page and a GitHub repo tree.
- Point out safety limits and why PMC skips heavy paths.
---
# Lesson 6 â Smart Context (Local RAG / Embeddings) (Intermediate)
## Objectives
- Turn on smart context with `--smart-context`.
- Scope scanning with `--smart-context-root`.
- Understand what gets indexed, where itâs cached, and common failure modes.
## Prerequisites
- Lesson 4
## Concepts
From `docs/prompt-maker-cli-tui-encyclopedia.md` and `docs/neovim-plugin-integration.md`:
- Smart context:
  - scans local files using `fast-glob` patterns like `**/*.{ts,tsx,js,jsx,py,md,json}`
  - skips `node_modules`, `dist`, `.git`, lockfiles, etc.
  - skips files > 25KB
  - caches embeddings in `~/.config/prompt-maker-cli/embeddings_cache.json`
  - searches top-k results (k=5) based on the intent
## Walkthrough (Commands)
```bash
# Smart context with default scan root (CWD)
prompt-maker-cli "Explain how the TUI theme system works" \
  --smart-context
# Smart context with an explicit root
prompt-maker-cli "Explain how token telemetry is computed" \
  --smart-context \
  --smart-context-root src
```
## Lab
1. Run smart context once.
2. Confirm the cache file exists:
```bash
ls -la ~/.config/prompt-maker-cli/embeddings_cache.json
```
3. Run again with the same intent and root; it should be faster due to caching.
## Validation
- `~/.config/prompt-maker-cli/embeddings_cache.json` exists after the first run.
- Output references local files you didnât explicitly pass via `--context`.
## Troubleshooting
- **Indexing errors**: PMC logs warnings but continues; reduce scope with `--smart-context-root`.
- **Unexpected file choices**: tighten your intent (embeddings search is intent-driven).
## Video Script (Lesson 6)
- Run one command without smart context, one with smart context.
- Show embeddings cache file creation.
- Explain when to use `--smart-context-root`.
---
# Lesson 7 â Media Attachments: Images + Video (Gemini) (Intermediate)
## Objectives
- Attach images via `--image` and understand supported formats and size caps.
- Attach video via `--video` and understand Gemini-only requirements.
- Understand model auto-switch behavior when `--video` is present.
## Prerequisites
- Lesson 4
- Gemini credentials for video: `GEMINI_API_KEY`
## Concepts
From `docs/prompt-maker-cli-tui-encyclopedia.md` and `docs/neovim-plugin-integration.md`:
- Images (`--image`):
  - supported: PNG/JPG/JPEG/WEBP/GIF
  - max 20MB each
  - unsupported/oversize â warning and skipped
- Video (`--video`):
  - requires Gemini
  - uploads via Google Files API and polls until `ACTIVE`
  - if you selected a non-Gemini model, PMC auto-switches to a Gemini video-capable model
## Walkthrough (Commands)
```bash
# Image attach (repeatable)
prompt-maker-cli "Critique this UI for accessibility" \
  --image path/to/screenshot.png \
  --polish
# Video attach (Gemini required; model may auto-switch)
prompt-maker-cli "Analyze this demo recording and list usability issues" \
  --video path/to/demo.mp4 \
  --progress=false
```
## Lab
1. Run an image-based prompt generation.
2. Run a video-based prompt generation and observe:
   - an upload phase
   - `--progress=false` reduces spinner noise (or use `--no-progress`)
## Validation
- For image: the run succeeds without warnings about unsupported extensions.
- For video: the run only succeeds when Gemini credentials are configured.
## Troubleshooting
- **Image skipped**: verify extension and file size.
- **Video fails immediately**: missing `GEMINI_API_KEY` or unreadable file.
- **Upload appears stuck**: keep the process alive; uploads poll until `ACTIVE`.
## Video Script (Lesson 7)
- Show one run with `--image` and one run with `--video`.
- Call out: âVideo forces Gemini; PMC may auto-switch the model.â
---
# Lesson 8 â Refinement Workflows: Interactive TTY + Interactive Transport (Advanced)
## Objectives
- Use `--interactive` for a TTY-based refinement loop.
- Use `--interactive-transport` for programmatic refinement (editor/plugin workflows).
- Understand what commands transport accepts and what events you can observe.
## Prerequisites
- Lesson 4
## Concepts
### Interactive TTY mode
From `docs/prompt-maker-cli-tui-encyclopedia.md`:
- `--interactive` enables a refinement loop **only when a TTY is available**.
- If no TTY is detected, PMC warns and proceeds non-interactively.
### Interactive transport mode
From `docs/prompt-maker-cli-tui-encyclopedia.md` and `docs/neovim-plugin-integration.md`:
- `--interactive-transport <path>` opens a local server:
  - Unix socket path like `/tmp/pmc.sock` (macOS/Linux)
  - Windows named pipe path must start with `\\.\pipe\...`
- Client sends **newline-delimited JSON** commands:
  - `{"type":"refine","instruction":"..."}`
  - `{"type":"finish"}`
- Transport also receives JSONL events emitted by the pipeline.
### Hard constraints
- `--json` cannot be combined with `--interactive` or `--interactive-transport`.
## Walkthrough (Commands)
### A) Interactive TTY
```bash
prompt-maker-cli "Draft a code review prompt for this repo" \
  --context "docs/prompt-maker-cli-tui-encyclopedia.md" \
  --interactive
```
Follow the prompts to refine; finish by choosing not to refine.
### B) Interactive transport (two terminals)
Terminal A (start PMC):
```bash
prompt-maker-cli "Draft a refactor plan for the token telemetry system" \
  --context "src/generate/token-telemetry.ts" \
  --stream jsonl \
  --quiet \
  --interactive-transport /tmp/pmc.sock
```
Terminal B (send commands using Node.js):
```bash
node -e "
const net = require('node:net');
const socketPath = '/tmp/pmc.sock';
const c = net.createConnection(socketPath);
c.on('data', (d) => process.stdout.write(d));
c.on('connect', () => {
  c.write(JSON.stringify({ type: 'refine', instruction: 'Add a Validation section and keep steps testable.' }) + '\n');
  setTimeout(() => c.write(JSON.stringify({ type: 'finish' }) + '\n'), 500);
});
"
```
## Lab
1. Run the transport workflow above.
2. Observe the JSONL stream in Terminal A.
3. Confirm you see at least:
   - `interactive.awaiting` events
   - `generation.iteration.start` / `generation.iteration.complete`
   - `generation.final`
## Validation
- Refinement changes the output (you should see at least two iterations).
- The process exits after `finish`.
## Troubleshooting
- **Socket path already exists**: remove the stale file and retry.
- **No `interactive.awaiting`**: ensure you started with `--interactive-transport`.
- **Mixed output**: use `--quiet` so stdout is JSONL-only (plus any final prompt text depending on mode).
## Video Script (Lesson 8)
- Split-screen recording (two terminals):
  - Left: PMC running with `--interactive-transport`.
  - Right: Node one-liner sending `refine` then `finish`.
- Narrate: âTransport is how editor plugins drive refinement loops.â
---
# Lesson 9 â Automation Outputs: `--json`, `--stream jsonl`, quiet/progress controls (Advanced)
## Objectives
- Produce machine-readable JSON output with `--json`.
- Stream JSONL events with `--stream jsonl`.
- Use `--quiet` and `--progress=false` / `--no-progress` to keep stdout parseable.
- Understand context templates and `renderedPrompt` fallback logic.
- Learn where history is stored and how to use it.
## Prerequisites
- Lesson 4
## Concepts
### JSON output
From `docs/prompt-maker-cli-tui-encyclopedia.md` and `docs/cookbook.md`:
- `--json` prints a pretty JSON payload to **stdout**.
- `--show-context` + `--json` prints context to **stderr** (to protect stdout JSON).
- `--json` cannot be combined with interactive modes.
### Streaming JSONL
From `docs/neovim-plugin-integration.md`:
- `--stream jsonl` writes newline-delimited JSON events to stdout.
- Use `--quiet` to suppress human-oriented output so stdout is parseable.
Event names you can rely on include (each JSONL line includes an `event` field plus event-specific payload fields):
- `context.telemetry`
- `progress.update`
- `upload.state`
- `generation.iteration.start`
- `generation.iteration.complete`
- `interactive.state`
- `interactive.awaiting`
- `transport.listening`, `transport.client.connected`, `transport.client.disconnected`
- `generation.final`
### `--quiet`, progress flags, and telemetry presentation
- `--quiet` suppresses banners/telemetry UI.
- Progress spinners are controlled via:
  - `--progress=false` (boolean form)
  - or `--no-progress` (CLI alias)
### Context templates and fallback fields
From `docs/neovim-plugin-integration.md`:
- `--context-template nvim` wraps output into an editor-friendly template.
- JSON payload may include:
  - `renderedPrompt` (if template applied)
  - `polishedPrompt` (if `--polish`)
  - otherwise `prompt`
Fallback logic for consumers:
1. Use `renderedPrompt` if present.
2. Else use `polishedPrompt` if present.
3. Else use `prompt`.
### History
- Every generate run appends JSONL to:
  - `~/.config/prompt-maker-cli/history.jsonl`
## Walkthrough (Commands)
### A) JSON output (recommended for scripting)
```bash
prompt-maker-cli "Summarize the PMC generate flags" \
  --context docs/prompt-maker-cli-tui-encyclopedia.md \
  --json > /tmp/pmc-run.json
# Extract the generated prompt
node -e "
const fs = require('node:fs');
const run = JSON.parse(fs.readFileSync('/tmp/pmc-run.json','utf8'));
console.log(run.renderedPrompt ?? run.polishedPrompt ?? run.prompt);
"
```
### B) JSONL stream output (recommended for live UIs)
```bash
prompt-maker-cli "Explain smart context and where it caches embeddings" \
  --context docs/neovim-plugin-integration.md \
  --stream jsonl \
  --quiet \
  --progress=false > /tmp/pmc-events.jsonl
```
## Lab
1. Produce JSON output and extract the `prompt` using the Node snippet above.
2. Produce JSONL stream output and confirm you have newline-delimited JSON objects.
3. Confirm history appended:
```bash
ls -la ~/.config/prompt-maker-cli/history.jsonl
```
## Validation
- `/tmp/pmc-run.json` parses as JSON and contains keys like `schemaVersion`, `intent`, `model`, `targetModel`, and `prompt`.
- `/tmp/pmc-events.jsonl` contains multiple JSON objects separated by newlines.
- `~/.config/prompt-maker-cli/history.jsonl` grows over time.
## Troubleshooting
- **Stdout not parseable**: add `--quiet` and disable progress (`--progress=false` or `--no-progress`).
- **Need both JSONL and final JSON**: be carefulâcombining `--stream jsonl` with `--json` mixes formats on stdout; prefer reading the final payload from `~/.config/prompt-maker-cli/history.jsonl` instead.
## Video Script (Lesson 9)
- Show `--json > run.json` and extract via `node -e`.
- Show `--stream jsonl --quiet` and open the JSONL file.
- Explain the `renderedPrompt â polishedPrompt â prompt` fallback order.
---
# Lesson 10 â Prompt Testing: `prompt-maker-cli test` + YAML suites (Intermediate)
## Objectives
- Run prompt tests with `prompt-maker-cli test`.
- Understand the YAML suite concept and how failures are reported.
- Run tests in the TUI Test Runner view.
## Prerequisites
- Lesson 1
## Concepts
From `docs/prompt-maker-cli-tui-encyclopedia.md` and `docs/cookbook.md`:
- CLI test runner:
  - `prompt-maker-cli test` uses `prompt-tests.yaml` by default.
  - `prompt-maker-cli test path/to/suite.yaml` runs a specific file.
- The suite is validated (Zod schema) and tests result in a non-zero exit code on failures.
## Walkthrough (Commands)
```bash
# Default suite
prompt-maker-cli test
# Explicit suite
prompt-maker-cli test prompt-tests.yaml
```
TUI:
- `Ctrl+T` â Test Runner view
- Select file (default `prompt-tests.yaml`) â Run
## Lab
1. Run `prompt-maker-cli test`.
2. Observe PASS/FAIL results and ensure exit code reflects failures.
```bash
prompt-maker-cli test
echo $?
```
## Validation
- Tests print PASS/FAIL per test.
- Exit code is non-zero if any test fails.
## Troubleshooting
- **`test --help` still runs tests**: known quirk (`docs/prompt-maker-cli-tui-encyclopedia.md`).
- **Credential errors**: tests still require provider access.
## Video Script (Lesson 10)
- Run `prompt-maker-cli test`.
- Show a failing test case (if present) and the exit code.
- Open the TUI test runner (`Ctrl+T`) and run the same suite.
---
# Lesson 11 â NeoVim Integration Playbooks (including sidekick.nvim) (Advanced)
## Objectives
- Use the docs-backed integration patterns for:
  - one-shot prompt into a buffer
  - interactive transport refinement loop
  - context preview workflow
  - history-driven picker concept
  - media-assisted runs
- Run PMC from inside NeoVim terminals.
- Use **sidekick.nvim** as a convenient âintegrated AI CLI terminalâ to host PMC sessions.
## Prerequisites
- Lessons 8â9
- NeoVim installed
- Optional: sidekick.nvim installed (see https://github.com/folke/sidekick.nvim)
## Concepts
From `docs/neovim-plugin-integration.md`:
- For editor automation, prefer:
  - `--json --quiet --stream jsonl` (for programmatic consumption)
  - `--interactive-transport` for refinement loops
  - `--context-template nvim` for scratch-buffer-friendly formatting
- Template fallback logic for buffer insertion:
  - `renderedPrompt` â `polishedPrompt` â `prompt`
From sidekick.nvimâs README:
- sidekick.nvim provides an integrated terminal wrapper for arbitrary AI CLIs.
- It can âselectâ or âtoggleâ a tool session and send text to it.
## Walkthrough (Commands)
### Playbook 1: One-shot prompt into a buffer (CLI-first)
```bash
prompt-maker-cli "Write a refactor prompt for this file" \
  --context src/index.ts \
  --context-template nvim \
  --json --quiet > /tmp/pmc.json
```
Then in NeoVim, paste the output you want (`renderedPrompt` preferred).
### Playbook 2: Interactive transport loop (CLI + editor)
Start PMC:
```bash
prompt-maker-cli "Draft a refactor plan for the active file" \
  --context src/index.ts \
  --quiet \
  --stream jsonl \
  --interactive-transport /tmp/pmc.nvim.sock
```
Send `refine` / `finish` commands from another process (e.g. via Lua plugin code or the Node one-liner from Lesson 8).
### Playbook 3: Context preview (editor preview windows)
```bash
prompt-maker-cli "Explain what this context contains" \
  --context "src/**/*.ts" \
  --context "!src/**/__tests__/**" \
  --context-file /tmp/pmc-context.json \
  --context-format json \
  --show-context
```
### Playbook 4: History-driven picker (concept)
- Read and parse:
  - `~/.config/prompt-maker-cli/history.jsonl`
- Show recent entries by `intent`, `timestamp`, and a snippet of `prompt`.
### Running the PMC TUI inside NeoVim
- In NeoVim, use a builtin terminal:
```vim
:terminal prompt-maker-cli ui
```
- For transport-enabled sessions:
```vim
:terminal prompt-maker-cli ui --interactive-transport /tmp/pmc.nvim.sock
```
### Using sidekick.nvim to host PMC sessions
sidekick.nvimâs âAI CLI Integrationâ is a general terminal wrapper; you can add a custom tool for PMC.
Example `lazy.nvim` snippet (adapted from sidekick.nvim README):
```lua
{
  "folke/sidekick.nvim",
  opts = {
    cli = {
      tools = {
        pmc = {
          cmd = { "prompt-maker-cli" },
        },
      },
    },
  },
  keys = {
    {
      "<leader>ap",
      function() require("sidekick.cli").toggle({ name = "pmc", focus = true }) end,
      desc = "Sidekick: PMC",
    },
  },
}
```
From NeoVim:
- `:Sidekick cli show name=pmc focus=true` (start or attach)
- Run the TUI or generate commands inside the sidekick terminal:
  - `prompt-maker-cli ui`
  - `prompt-maker-cli "..." --json --quiet --stream jsonl`
## Lab
1. Do a one-shot run from NeoVim that produces `--json --quiet` output.
2. Start a transport session and send a refine command.
3. If using sidekick.nvim:
   - start the `pmc` tool terminal
   - run `prompt-maker-cli ui` inside it
## Validation
- NeoVim terminal can run `prompt-maker-cli ui` without Ink raw-mode errors.
- Transport socket workflow works end-to-end (refine then finish).
- sidekick.nvim can open a terminal session and keep it accessible.
## Troubleshooting
- **Ink raw mode errors inside NeoVim**: your terminal buffer might not support raw mode (or youâre in a headless environment); use generate mode instead.
- **Socket conflicts**: ensure `/tmp/pmc.nvim.sock` is unique per session.
## Video Script (Lesson 11)
- Record NeoVim:
  - open `:terminal prompt-maker-cli ui`
  - show `--interactive-transport` variant
- If sidekick.nvim installed:
  - show `:Sidekick cli show name=pmc focus=true`
  - run a generate command in that terminal
---
# Capstone A â Automation Run: JSON Output â Extract Prompt (Advanced)
## Goal
Produce a non-interactive JSON artifact and programmatically extract the final prompt text.
## Steps
```bash
prompt-maker-cli "Create a release checklist for this repo" \
  --context README.md \
  --context docs/cookbook.md \
  --context-template nvim \
  --polish \
  --json --quiet > /tmp/release-checklist.json
node -e "
const fs = require('node:fs');
const payload = JSON.parse(fs.readFileSync('/tmp/release-checklist.json','utf8'));
const text = payload.renderedPrompt ?? payload.polishedPrompt ?? payload.prompt;
console.log(text);
"
```
## Validation
- `/tmp/release-checklist.json` parses as JSON.
- Output is the template-wrapped prompt (when `renderedPrompt` is present).
## Common failure modes
- Missing credentials.
- Accidentally using `--interactive` with `--json` (invalid combination).
---
# Capstone B â Streaming Run: JSONL Events â Parse & React (Advanced)
## Goal
Capture a JSONL event stream and parse key milestones like `context.telemetry` and `generation.final`.
## Steps
```bash
prompt-maker-cli "Summarize how the TUI theme system loads custom themes" \
  --context docs/tui-styling-guide.md \
  --stream jsonl \
  --quiet \
  --progress=false > /tmp/pmc-theme-events.jsonl
node -e "
const fs = require('node:fs');
const lines = fs.readFileSync('/tmp/pmc-theme-events.jsonl','utf8').trim().split(/\n+/);
for (const line of lines) {
  const evt = JSON.parse(line);
  if (evt.event === 'context.telemetry') {
    console.error('telemetry totalTokens=', evt.telemetry.totalTokens);
  }
  if (evt.event === 'generation.final') {
    const result = evt.result;
    console.log(result.renderedPrompt ?? result.polishedPrompt ?? result.prompt);
  }
}
"
```
## Validation
- The JSONL file contains multiple lines of JSON.
- You observe a `generation.final` event and print the final prompt.
## Common failure modes
- Missing `--quiet` causes non-JSON output to mix into stdout.
---
# Capstone C â NeoVim Scenario: One-shot + Interactive Transport Loop (Advanced)
## Goal
Inside NeoVim, run a one-shot prompt build for a file, then run an interactive transport refinement session.
## Steps
1. NeoVim one-shot:
```vim
:terminal prompt-maker-cli "Review this file and propose tests" --context % --context-template nvim --json --quiet
```
2. NeoVim transport session:
```vim
:terminal prompt-maker-cli "Draft a refactor plan for this file" --context % --quiet --stream jsonl --interactive-transport /tmp/pmc.nvim.sock
```
3. Send commands using a second terminal or a small Lua helper (conceptually the same as Lesson 8âs Node client):
   - send `{"type":"refine","instruction":"..."}`
   - then `{"type":"finish"}`
## Validation
- NeoVim terminal displays a prompt output (one-shot).
- Transport session performs at least one refinement iteration before finishing.
---
# Feature Coverage Checklist (Docs-backed)
| Feature / Capability                                                                                                                  | Where covered            |
| :------------------------------------------------------------------------------------------------------------------------------------ | :----------------------- |
| Install/build from source (`npm ci`, `npm run build`, `npm start`)                                                                    | Lessons 1â2              |
| Global install + binary usage (`prompt-maker-cli`)                                                                                    | Lesson 1                 |
| Three modes: `ui`, `generate` (default), `test`                                                                                       | Lesson 1                 |
| TUI basics: help, command palette, navigation                                                                                         | Lessons 2â3              |
| TUI commands: `/file`, `/url`, `/image`, `/video`, `/smart`, `/smart-root`, `/tokens`, `/history`, `/theme`, `/theme-mode`, `/series` | Lesson 3                 |
| Intent ingestion: positional, `--intent-file/-f`, stdin                                                                               | Lesson 4                 |
| Local context: `--context` globs + excludes (`!`)                                                                                     | Lesson 4                 |
| Context preview/export: `--show-context`, `--context-file`, `--context-format text\|json`                                             | Lessons 4, 11            |
| URL context (`--url`) + GitHub URLs                                                                                                   | Lesson 5                 |
| GitHub token usage (`GITHUB_TOKEN`)                                                                                                   | Lesson 5                 |
| Smart context: `--smart-context`, `--smart-context-root`, embeddings cache                                                            | Lesson 6                 |
| Media: `--image` constraints                                                                                                          | Lesson 7                 |
| Media: `--video` Gemini requirement + upload behavior + model auto-switch                                                             | Lesson 7                 |
| Output: `--json` and non-combinability with interactive modes                                                                         | Lessons 8â9, Capstone A  |
| Output: `--stream jsonl` events and parsing                                                                                           | Lesson 9, Capstone B     |
| `--quiet`, `--progress=false`, `--no-progress` (clean stdout)                                                                         | Lessons 7â9              |
| Context templates: `--context-template nvim`, `renderedPrompt` fallback                                                               | Lessons 9, 11            |
| Refinement: `--interactive` (TTY)                                                                                                     | Lesson 8                 |
| Refinement: `--interactive-transport` (IPC)                                                                                           | Lesson 8, Capstone C     |
| Polishing: `--polish`, `--polish-model`                                                                                               | Lessons 7, 9             |
| `--model` vs `--target` semantics (target not echoed in prompt text by default)                                                       | Lessons 9, 11            |
| History: `~/.config/prompt-maker-cli/history.jsonl`                                                                                   | Lesson 9, 11             |
| Token telemetry (`/tokens` and `context.telemetry` events)                                                                            | Lessons 3, 9, Capstone B |
| Prompt tests: `prompt-maker-cli test`, `prompt-tests.yaml` default                                                                    | Lesson 10                |
| Run tests in the TUI test runner view                                                                                                 | Lessons 2, 10            |
| NeoVim integration playbooks (one-shot, transport loop, context preview, history)                                                     | Lesson 11                |
| sidekick.nvim integration (run PMC in integrated terminal)                                                                            | Lesson 11                |
---
## Next steps
- If you want, extend this curriculum by creating a personal âprompt suiteâ in `prompt-tests.yaml` and running it via:
  - `prompt-maker-cli test`
  - TUI Test Runner (`Ctrl+T`)
</file>
<file path="docs/sidekick.nvim-prompt-maker-tui-integration.md">
# Running Prompt Maker CLI TUI inside Neovim via `sidekick.nvim` (Streaming + Transport)
## 1. Title + Purpose
This document is a **design + implementation hand-off guide** for building a Neovim integration that runs **Prompt Maker CLIâs Ink TUI** (`prompt-maker-cli ui`) inside a **sidekick.nvim-managed terminal**, while also using Prompt Makerâs **interactive transport** and **JSONL streaming events** for a richer UX outside the terminal.
The target result is a plugin/module in your Neovim repo that:
- Opens the Prompt Maker Ink TUI in a Sidekick terminal window.
- Creates (and owns) a per-session `--interactive-transport` socket/pipe.
- Connects a Neovim-side transport client to:
  - parse streaming JSONL events for progress/telemetry/results
  - send newline-delimited JSON commands (`refine` / `finish`) to drive refinement loops
- Surfaces structured status and the final prompt into buffers/windows (without scraping terminal output).
**Authoritative Prompt Maker references in this repo**:
- `docs/neovim-plugin-integration.md` (integration flags and workflow guidance)
- `docs/prompt-maker-cli-tui-encyclopedia.md` (TUI behavior + transport notes)
- `docs/tui-design.md` (TUI input routing invariants)
- `src/generate/types.ts` (exact JSONL event schema)
- `src/generate/interactive-transport.ts` (exact transport command schema + lifecycle)
**Relevant Sidekick references (upstream)**:
- `folke/sidekick.nvim/lua/sidekick/cli/terminal.lua` (terminal job lifecycle + keymaps)
- `folke/sidekick.nvim/lua/sidekick/cli/session/init.lua` (session backend model)
- `folke/sidekick.nvim/lua/sidekick/cli/watch.lua` (file watching for external changes)
- `folke/sidekick.nvim/lua/sidekick/cli/actions.lua` (how terminal keymaps dispatch actions)
---
## 2. Glossary
- **Prompt Maker / PMC**: `prompt-maker-cli`, the Node-based CLI.
- **Ink TUI**: Prompt Makerâs terminal UI (`prompt-maker-cli ui`).
- **Sidekick Terminal**: Neovim terminal window created/managed by sidekick.nvim.
- **JSONL stream**: newline-delimited JSON âeventsâ emitted by Prompt Maker (`--stream jsonl`) or mirrored over the transport tap.
- **Interactive transport**: local IPC channel created by PMC server (`--interactive-transport <path>`) that accepts newline-delimited JSON commands and mirrors events.
- **Session**: one running instance of Prompt Maker + its transport connection.
- **Rendered prompt**: the final prompt string after applying a context template.
---
## 3. Requirements & Non-Goals
### Requirements
- Run **Ink TUI** inside Neovim using Sidekickâs terminal/session abstractions.
- Use **interactive transport** to avoid scraping the terminal buffer for state/results.
- Use **JSONL events** to drive Neovim UI updates.
- Must respect Prompt Maker constraints from `docs/neovim-plugin-integration.md`:
  - Prefer `--quiet --stream jsonl` for editor integrations.
  - Set token budgets (`--max-input-tokens`/`--max-context-tokens`) and handle `context.overflow` for predictable behavior with large contexts.
  - `--json` **cannot** be used with interactive (`--interactive` or `--interactive-transport`).
  - `--interactive-transport` accepts newline-delimited JSON commands `{ "type": "refine", "instruction": "..." }` and `{ "type": "finish" }`.
  - When extracting the final prompt, use fallback: `renderedPrompt` â `polishedPrompt` â `prompt`.
- Cross-platform support:
  - Unix socket path on macOS/Linux
  - Windows named pipes (`\\.\pipe\...`)
  - Cleanup of stale socket files before launch (non-Windows only).
### Non-Goals
- Reimplement Prompt Makerâs internal TUI UX in Lua.
- Scrape the Ink terminal UI for prompts/state.
- Make network calls from tests (use fixtures/stubs).
- Store provider API keys in plugin state; rely on environment variables or Prompt Makerâs config file (`~/.config/prompt-maker-cli/config.json`).
---
## 4. UX Overview (user stories + keymaps)
### Core user stories
1. **Open the TUI**
   - As a user, I run `:Sidekick` / keymap and Prompt Makerâs TUI opens in a Sidekick terminal split.
2. **See progress and telemetry outside the terminal**
   - As a user, I see current phase (URL fetch / smart context / generate / polish), token totals, and upload progress in Neovim notifications / statusline.
3. **Refine from Neovim**
   - As a user, I can send a refine instruction from a Neovim prompt (floating input), without focusing the terminal.
4. **Finish and insert**
   - As a user, I can press a key to send `finish`, then the plugin opens a scratch buffer containing the final prompt and optionally inserts it into my current buffer.
5. **Close cleanly**
   - As a user, closing the terminal detaches the transport client, stops timers, and deletes any transport socket file.
### Suggested keymaps (defaults)
These are **suggestions**; integrate with your repoâs conventions.
- `:Sidekick cli toggle name=pmc` (or a custom command) â open/focus the TUI terminal
- `<leader>pr` â âRefineâŚâ (`vim.ui.input`), send transport `refine`
- `<leader>pf` â âFinishâ (`finish`), focus the âPrompt Resultâ buffer
- `<leader>po` â open a âPrompt Maker Statusâ floating window
- `<leader>pc` â copy final prompt to clipboard (Neovim-side; do not rely on `--copy`)
---
## 5. Architecture (diagram + components)
### High-level diagram
```text
+-------------------+         +------------------------------+
|     Neovim        |         |   sidekick.nvim terminal      |
|                   |         |   (term=true job)             |
|  - Commands       |  open   |                              |
|  - Status UI      +-------->+  prompt-maker-cli ui ...      |
|  - Transport      |         |  (Ink raw-mode TUI)           |
|    client         |         +------------------------------+
|        ^          |
|        | JSONL    |
|        | events   |
|        |          |
|  connect/send     |
|   over socket/pipe|
+--------+----------+
         |
         v
+------------------------------+
| prompt-maker-cli transport   |
| server (--interactive-transport)
|  - accepts NDJSON commands   |
|  - writes JSONL events back  |
+------------------------------+
```
### Components you will implement
1. **Session manager**
   - Owns per-session identifiers, transport path selection, and cleanup.
2. **Sidekick launcher**
   - Starts/attaches a Sidekick terminal session running `prompt-maker-cli ui --interactive-transport <path>`.
3. **Transport client**
   - Connects to the transport server (socket/pipe), reads JSONL events, and sends NDJSON commands.
4. **Event router**
   - Parses JSON lines into Lua tables and dispatches handlers.
5. **Neovim UI surfaces**
   - Statusline integration hooks, notifications, optional floating window, and a scratch buffer for final output.
---
## 6. CLI Invocation & Modes (exact commands)
### Mode A (Primary): Ink TUI in Sidekick terminal + transport tap for events
Run **Ink TUI** in a terminal, but drive out-of-band UX using the transport:
```bash
prompt-maker-cli ui --interactive-transport <transport_path>
```
Notes:
- `ui` only parses `--interactive-transport` (see `docs/prompt-maker-cli-tui-encyclopedia.md`). Do not depend on `ui --help`.
- Token budgets (`--max-input-tokens`, `--max-context-tokens`, `--context-overflow`) are **generate-mode flags** and are not parsed by `ui`.
  - To use budgets with the TUI, set defaults via config under `promptGenerator` (or set them interactively with the TUI `/budgets` popup, which persists back to config):
    ```json
    {
      "promptGenerator": {
        "maxInputTokens": 12000,
        "maxContextTokens": 8000,
        "contextOverflowStrategy": "drop-smart"
      }
    }
    ```
- PR #15 TUI parity workflows (popup-only; no inline args parsing):
  - Help overlay includes a âWorkflowsâ section (`src/tui/help-config.ts`).
  - Command palette includes:
    - `/budgets` (persists `promptGenerator.maxInputTokens`, `promptGenerator.maxContextTokens`, `promptGenerator.contextOverflowStrategy`)
    - `/resume` (persists `resumeMode`, `resumeSourceKind`)
    - `/export` (persists `exportFormat`, `exportOutDir`)
  - These defaults are written back into CLI config (`src/config.ts`), so Sidekick sessions can âlearnâ settings from prior TUI usage.
- The TUI requires a real TTY; sidekick.nvimâs terminal uses `term=true` (`folke/sidekick.nvim/lua/sidekick/cli/terminal.lua`).
### Mode B (Optional): Headless generate loop (no Ink), still inside Sidekick terminal
If you want a non-TUI fallback that still streams progress deterministically:
```bash
prompt-maker-cli "<intent>" \
  --quiet \
  --stream jsonl \
  --interactive-transport <transport_path> \
  --context-template nvim \
  --max-context-tokens 8000 \
  --context-overflow drop-smart
```
Important constraints from Prompt Maker:
- `--json` cannot be combined with interactive mode (`--interactive` or `--interactive-transport`) (see `docs/neovim-plugin-integration.md` and `docs/prompt-maker-cli-tui-encyclopedia.md`).
- Therefore, your plugin must treat `generation.final` (JSONL) as the âfinal payloadâ source.
### Environment and binary resolution
- Binary is `prompt-maker-cli` (see `README.md`).
- Credentials are loaded from env vars or config file; your plugin should not try to pass keys directly.
---
## 7. Streaming (JSONL) Event Handling
### Wire format (exact)
Prompt Maker stream events are JSON objects **one per line**:
- Always include:
  - `event` (string)
  - `timestamp` (ISO string)
- Plus event-specific payload fields.
Schema source: `src/generate/types.ts` and serialization in `src/generate/stream.ts`.
Example line:
```json
{
  "event": "progress.update",
  "timestamp": "2026-01-03T12:00:00.000Z",
  "label": "Generating...",
  "state": "start",
  "scope": "generate"
}
```
### Relevant event types and expected plugin behavior
Below is the minimal mapping you should implement; expand as needed.
#### `transport.listening`
Payload: `{ path: string }`
- Mark session transport server as ready.
- If the client isnât connected yet, attempt `connect()` with retry/backoff.
#### `transport.client.connected` / `transport.client.disconnected`
Payload: `{ status: 'connected'|'disconnected' }`
- Update statusline state.
- On disconnect: disable refine/finish UI and stop reading loop.
#### `transport.error`
Not part of the typed stream union, but written by the transport server as:
```json
{ "event": "transport.error", "message": "..." }
```
- Surface with `vim.notify(..., vim.log.levels.ERROR)`.
- Consider showing a âlast transport errorâ in your status window.
#### `progress.update`
Payload:
- `label: string`
- `state: 'start'|'update'|'stop'`
- `scope?: 'url'|'smart'|'generate'|'polish'|'generic'`
Plugin actions:
- Maintain a simple âcurrent phaseâ state machine:
  - `start`: show spinner/indicator
  - `update`: update label
  - `stop`: clear spinner
- UI options:
  - Notifications on `start`/`stop` only
  - Statusline/virtual text updates on every `update`
#### `context.telemetry`
Payload: `{ telemetry: { totalTokens, intentTokens, fileTokens, systemTokens, files:[{path,tokens}...] } }`
Plugin actions:
- Persist latest telemetry in session state.
- If token counts exceed a threshold, show warning (see âSecurity & Safetyâ).
- Optional: render top-N file token contributors in a floating window.
#### `context.overflow`
Emitted when token budgets are enabled and the CLI drops one or more **text** context entries to satisfy the budget.
Payload includes:
- `strategy`: `fail | drop-smart | drop-url | drop-largest | drop-oldest`
- `before`: token telemetry before trimming
- `after`: token telemetry after trimming
- `droppedPaths`: `[{ path, source }]` describing removed context entries
Plugin actions:
- Notify the user that some context was dropped (this can materially change results).
- Reconcile any âattached contextâ UI with `droppedPaths`.
- Consider offering a one-click re-run with a larger budget or a different overflow strategy.
#### `resume.loaded`
Emitted when resuming from history or a payload file and the CLI attempts to reuse prior `contextPaths`.
Payload: `{ source:'history'|'file', reusedContextPaths:[{path,source}], missingContextPaths:[{path,source}] }`
Plugin actions:
- Surface missing file paths clearly.
- If you run with strict mode, treat missing `source:"file"` paths as fatal (the CLI will throw).
#### `upload.state`
Payload: `{ state: 'start'|'finish', detail: { kind: 'image'|'video', filePath: string } }`
Plugin actions:
- Indicate uploads in statusline; discourage closing session while uploads active.
#### `generation.iteration.start`
Payload includes: `{ iteration, intent, model, interactive, inputTokens, refinements, latestRefinement? }`
Plugin actions:
- Update âiteration Nâ status.
- If `latestRefinement` exists, append it to a ârefinement logâ buffer.
#### `generation.iteration.complete`
Payload includes: `{ iteration, prompt, tokens, reasoningTokens? }`
Plugin actions:
- Optionally show âprompt previewâ (not the whole prompt by default; it can be huge).
#### `interactive.state`
Payload: `{ phase:'start'|'prompt'|'refine'|'complete', iteration:number }`
Plugin actions:
- Use `phase` to enable/disable refine/finish controls:
  - `start`/`prompt`: allow refine + finish
  - `refine`: disable input until next `prompt`
  - `complete`: finalization UI (open prompt buffer)
#### `interactive.awaiting`
Payload: `{ mode:'transport'|'tty'|'none' }`
Plugin actions:
- If mode is `transport`, show âawaiting refine/finishâ indicator.
#### `generation.final`
Payload: `{ result: GenerateJsonPayload }`
Plugin actions:
- Extract the final prompt text using:
  1. `result.renderedPrompt` (preferred)
  2. else `result.polishedPrompt`
  3. else `result.prompt`
- Open a scratch buffer and populate it with the extracted text.
- Optionally:
  - set filetype `markdown`
  - set buffer name like `Prompt Maker: <timestamp>`
  - provide actions: copy, insert into current buffer, save to file
---
## 8. Interactive Transport Protocol (client/server lifecycle)
### Server behavior (Prompt Maker)
Prompt Maker creates a local `net.Server` on the given `--interactive-transport` path (see `src/generate/interactive-transport.ts`).
- On Unix: PMC deletes any existing socket file at that path before binding.
- On Windows: the path must start with `\\.\pipe\`.
Once a client connects:
- PMC reads newline-delimited JSON commands.
- PMC writes JSONL events back to the same socket.
### Client responsibilities (your plugin)
- Connect to the transport server and keep reading lines until disconnect.
- Provide a `send_command(cmd)` API that serializes JSON + appends `\n`.
- Reconnect strategy:
  - If you connect before the server is listening, retry for up to `N` seconds.
  - If you disconnect mid-session, stop retries unless the terminal process is still running.
### Command schema (exact)
Send **one JSON object per line**:
- Refine:
```json
{ "type": "refine", "instruction": "Make it shorter" }
```
- Finish:
```json
{ "type": "finish" }
```
Rules from server implementation:
- `refine.instruction` must be a non-empty string after trimming.
- Unknown payloads yield `transport.error` event.
### Lifecycle sequencing (recommended)
1. Compute transport path.
2. Launch Sidekick terminal running `prompt-maker-cli ui --interactive-transport <path>`.
3. Wait for `transport.listening` OR retry-connect until it succeeds.
4. When connected, start JSONL reader loop.
5. On Neovim exit, terminal close, or user âstop sessionâ:
   - Close client socket.
   - Delete stale Unix socket file (best-effort).
---
## 9. Integration with sidekick.nvim (what to reuse, what to extend)
### What to reuse
- **Terminal lifecycle + window management**
  - Sidekickâs terminal wrapper (`folke/sidekick.nvim/lua/sidekick/cli/terminal.lua`) already:
    - creates a scratch terminal buffer/window
    - starts jobs with `term=true` (needed for Ink)
    - manages focus/hide/toggle
    - has a send queue (`Terminal:send`) and keymap wiring
- **Session model**
  - Sidekickâs session layer (`folke/sidekick.nvim/lua/sidekick/cli/session/init.lua`) gives you:
    - stable session IDs per tool+cwd
    - attach/detach semantics
    - mux backends (tmux/zellij) if users enable them
- **File watching (optional)**
  - If the TUI writes files (e.g., `/series` output folders), you can enable sidekick watcher (`folke/sidekick.nvim/lua/sidekick/cli/watch.lua`) to refresh buffers automatically.
### What to extend (recommended)
Sidekickâs terminal is not designed to parse job stdout when `term=true`. For this integration:
- Treat the terminal as **display + interactive input only**.
- Treat the transport socket as your **machine-readable event stream**.
Recommended extension points:
1. **Add a dedicated tool entry**
   - In the user config: `cli.tools.pmc = { cmd = { "prompt-maker-cli", "ui" } }`.
2. **Add a âPMC adapterâ module** (in your repo)
   - Wrap `sidekick.cli` to start/attach a session with a **modified cmd** that injects `--interactive-transport <path>`.
   - Keep the rest of Sidekick untouched.
3. **Emit User events**
   - Sidekick uses `Util.emit(...)` internally (see session attach/detach); your integration can emit `User` autocmd events like:
     - `User SidekickPmcEvent` (with payload in `vim.g`/in module state)
     - or direct `vim.api.nvim_exec_autocmds("User", { pattern = "SidekickPmcEvent", data = ... })`
---
## 10. Neovim UI Plan (buffers, windows, statusline, notifications)
### Recommended UI surfaces
1. **Statusline component**
   - Expose `require("pmc").status()` returning a short string like:
     - `PMC: idle`
     - `PMC: generating (2)`
     - `PMC: awaiting refine`
     - `PMC: uploads 1`
2. **Notifications**
   - Use `vim.notify` sparingly:
     - on `transport.error`
     - on `generation.final`
     - on fatal disconnects
3. **Prompt result buffer**
   - Create a scratch buffer (unlisted or listed depending on your preference) containing the final prompt.
   - Provide buffer-local mappings:
     - `y` copy to clipboard
     - `p` paste/insert into original buffer
     - `q` close
4. **Telemetry window (optional)**
   - A floating window rendering token totals and top-N files.
### Interaction design rules
- Never block Neovim UI while waiting for transport events.
  - Use `vim.uv` + `vim.schedule` to apply UI updates.
- Avoid flooding with progress updates.
  - Debounce `progress.update` rendering.
---
## 11. Configuration Spec (Lua table schema + defaults)
Define a dedicated config table (separate from sidekickâs own config) so the integration can be used without forking sidekick.nvim.
```lua
---@class PmcSidekickConfig
---@field enabled boolean
---@field tool_name string           -- sidekick cli tool name (default: "pmc")
---@field bin string|string[]?       -- override cmd[1] (binary) or full cmd
---@field cwd fun():string?          -- resolve cwd for session
---@field env table<string,string|false>? -- extra env vars; false clears
---@field transport
---@field transport.dir fun():string -- unix socket dir selection
---@field transport.name fun(ctx:{pid:number,cwd:string}):string -- filename/pipe name
---@field transport.connect_timeout_ms number
---@field transport.retry_interval_ms number
---@field ui
---@field ui.open_result_on_final boolean
---@field ui.result_filetype string
---@field ui.notify_level integer
---@field ui.token_warn_threshold number
---@field ui.token_error_threshold number
---@field ui.progress_debounce_ms number
local defaults = {
  enabled = true,
  tool_name = "pmc",
  bin = nil,
  cwd = function() return vim.fn.getcwd(0) end,
  env = {},
  transport = {
    dir = function()
      -- Prefer a per-user runtime dir if available.
      -- stdpath('run') exists on newer Neovim; otherwise fall back to tempname().
      return (vim.fn.has("win32") == 1) and "" or (vim.fn.stdpath("run") or vim.loop.os_tmpdir())
    end,
    name = function(ctx)
      return ("pmc-nvim-%d-%s.sock"):format(ctx.pid, vim.fn.sha256(ctx.cwd):sub(1, 8))
    end,
    connect_timeout_ms = 8000,
    retry_interval_ms = 100,
  },
  ui = {
    open_result_on_final = true,
    result_filetype = "markdown",
    notify_level = vim.log.levels.INFO,
    token_warn_threshold = 30000,
    token_error_threshold = 60000,
    progress_debounce_ms = 50,
  },
}
```
Notes:
- For Windows, use a named pipe path:
  - `\\.\pipe\pmc-nvim-<pid>-<hash>`
- For Unix, compute a socket path under `stdpath('run')` or `os_tmpdir()`.
- Always avoid collisions by including `vim.fn.getpid()` and a cwd hash.
---
## 12. Implementation Plan (files/modules + pseudocode)
This plan assumes you implement this in your Neovim repo as a small plugin/module that depends on sidekick.nvim.
### Suggested module layout
- `lua/pmc_sidekick/init.lua`
  - `setup(opts)`
  - user commands + keymaps
- `lua/pmc_sidekick/session.lua`
  - create/start/stop a session
  - store state: terminal session id, transport path, telemetry, etc.
- `lua/pmc_sidekick/transport.lua`
  - connect/read/write
  - jsonl decoder
- `lua/pmc_sidekick/events.lua`
  - event dispatch table: `handlers[event](session, payload)`
- `lua/pmc_sidekick/ui.lua`
  - open result buffer
  - status window
  - notifications
- `lua/pmc_sidekick/util.lua`
  - path helpers
  - debounce
  - safe json decode
### Session state machine
Keep a single Lua table per running session:
```lua
---@class PmcSession
---@field id string
---@field cwd string
---@field transport_path string
---@field terminal any?              -- sidekick terminal/session handle
---@field client any?                -- uv pipe/tcp handle
---@field status {phase?:string, label?:string, iteration?:number, connected?:boolean}
---@field telemetry table?           -- from context.telemetry
---@field final_payload table?       -- from generation.final
---@field closed boolean
```
### Starting the TUI session
Key behavior: start Sidekick terminal with a cmd including `--interactive-transport`.
Pseudocode:
```lua
local Sidekick = require("sidekick.cli")
local Session = require("sidekick.cli.session")
function M.start()
  local cwd = config.cwd()
  local transport_path = Transport.make_path(cwd)
  -- Ensure the tool exists in sidekick config: cli.tools[tool_name]
  -- Then clone/override cmd to append args.
  local base_tool = require("sidekick.config").get_tool(config.tool_name)
  local tool = base_tool:clone({
    cmd = { "prompt-maker-cli", "ui", "--interactive-transport", transport_path },
    env = config.env,
  })
  local session = Session.new({ tool = tool, cwd = cwd, backend = "terminal" })
  require("sidekick.cli.session").attach(session)
  -- Now connect transport client.
  Transport.connect(session, transport_path)
end
```
Implementation detail:
- Sidekickâs `Session.new` normalizes cwd and manages IDs (`folke/sidekick.nvim/lua/sidekick/cli/session/init.lua`).
- Sidekickâs terminal backend uses `jobstart(..., { term = true })` to provide a real TTY (`folke/sidekick.nvim/lua/sidekick/cli/terminal.lua`).
### Transport client: reading JSONL
Use `vim.uv`:
- On Unix: `uv.new_pipe(false)` and `pipe:connect(path, cb)`
- On Windows named pipe: `uv.new_pipe(true)` is typically required; validate with Neovimâs libuv.
Pseudocode:
```lua
local function decode_lines(chunk, state)
  state.buf = state.buf .. chunk
  while true do
    local i = state.buf:find("\n", 1, true)
    if not i then break end
    local line = vim.trim(state.buf:sub(1, i - 1))
    state.buf = state.buf:sub(i + 1)
    if line ~= "" then
      local ok, obj = pcall(vim.json.decode, line)
      if ok and type(obj) == "table" then
        Events.dispatch(session, obj)
      else
        -- optionally log parse failure
      end
    end
  end
end
function Transport.connect(session, path)
  -- retry connect until timeout
  -- once connected:
  client:read_start(function(err, chunk)
    if err then return Transport.on_error(session, err) end
    if not chunk then return Transport.on_eof(session) end
    decode_lines(chunk, session._decoder)
  end)
end
function Transport.send(session, obj)
  session.client:write(vim.json.encode(obj) .. "\n")
end
```
### Sending refine/finish from Neovim UI
- `refine`: prompt user via `vim.ui.input`, send `{ type="refine", instruction=input }`
- `finish`: send `{ type="finish" }`
Guardrails:
- Only allow if `session.status.connected == true`.
- Debounce refine: if `interactive.state.phase == "refine"`, block additional refine sends until next `prompt`.
### Extracting the final prompt
From `generation.final` event payload (`src/generate/types.ts`):
```lua
local result = event.result
local final = result.renderedPrompt or result.polishedPrompt or result.prompt
```
Create a buffer and store the full `result` as session.final_payload.
---
## 13. Testing Plan
Align with sidekick.nvim conventions (mini.test, fixtures, no network):
### Unit tests
1. **JSONL decoder**
   - Given arbitrary chunk boundaries, ensure the line splitter:
     - emits full decoded objects
     - tolerates empty lines
     - tolerates trailing partial lines
2. **Event mapping**
   - Feed sample events for each type and assert:
     - status transitions
     - telemetry storage
     - final prompt extraction
3. **Transport command serialization**
   - Ensure `refine`/`finish` commands are encoded as NDJSON lines.
### Integration tests (headless Neovim)
- Use `nvim --headless` and mini.test style similar to sidekick.nvim.
- Avoid running `prompt-maker-cli` in CI.
  - Instead, spawn a tiny Lua âfake serverâ using `vim.uv` that:
    - accepts a connection
    - sends JSONL events
    - asserts it received expected `refine`/`finish`
Fixtures
- Provide JSONL fixture files for:
  - a full successful run including `generation.final`
  - transport error line
  - token telemetry overflow case
---
## 14. Edge Cases & Failure Modes
- **Transport path collision (Unix)**
  - If the socket file exists from a previous crash, the server may fail to bind.
  - Best practice: delete stale socket file before launching.
- **Client connects too early**
  - Implement retry loop up to `connect_timeout_ms`.
- **Terminal closed while transport connected**
  - Detect terminal `TermClose` via sidekick terminal lifecycle and stop transport client.
- **Huge `generation.iteration.complete.prompt`**
  - Do not render full prompt in notifications.
  - Keep it in memory only; render on demand.
- **Uploads in progress**
  - Use `upload.state` counters; warn before stopping session if uploads are active.
- **Interactive mode mismatch**
  - If the TUI is used normally, it may not be âawaiting transportâ at all times.
  - Only enable refine/finish UI when you observe `interactive.awaiting.mode == "transport"`.
---
## 15. Security/Privacy Considerations
- Never persist API keys; do not ask users to put secrets in config committed to git.
- Do not auto-read unbounded files.
  - If your plugin provides âattach buffer/fileâ helpers, enforce size caps (Prompt Makerâs own caps are described in `docs/neovim-plugin-integration.md`).
- Treat prompt output as potentially sensitive:
  - default to scratch buffers, not file writes
  - provide explicit âsave to fileâ command
- Token telemetry thresholds:
  - If `telemetry.totalTokens` exceeds `token_warn_threshold`, show a warning.
  - If it exceeds `token_error_threshold`, consider prompting user to trim context.
  - If you observe `context.overflow`, show which files were dropped (`droppedPaths`) and consider offering a re-run with a higher budget.
---
## 16. Roadmap (MVP vs Phase 2)
### MVP
- Launch `prompt-maker-cli ui --interactive-transport ...` in Sidekick terminal.
- Connect transport client and parse JSONL events.
- Implement refine/finish commands from Neovim.
- Extract `generation.final.result` and open a result buffer.
- Basic statusline string + notifications.
### Phase 2
- Telemetry window with top-N token files and quickfix population.
- Rich progress UI (floating progress, virtual text in current buffer).
- History picker (tail `~/.config/prompt-maker-cli/history.jsonl`) using Sidekick pickers.
- Multi-session support + session picker.
- âInsert prompt at cursorâ with preview + confirm.
---
## 17. Open Questions
1. In Prompt Makerâs **Ink TUI** flow, is `interactive.awaiting.mode` reliably `transport` when `--interactive-transport` is passed, or only during refinement loops?
2. Does the TUI always emit `generation.final` over the transport tap, or only in generate-mode interactive sessions?
3. Should the integration prefer launching **generate mode** (non-Ink) for deterministic transport-driven refinement, with Ink TUI offered as an optional âmonitor UIâ?
4. How should the plugin resolve a âdefault context templateâ when using the TUI (since `ui` doesnât accept `--context-template`)? Rely on Prompt Maker config, or post-wrap in Neovim?
5. For Windows: what is the most reliable libuv pipe connect mode in Neovim (named pipe quirks vary)?
6. Should we expose âattach context from selection/bufferâ via Sidekickâs context system, or keep the first version transport-only?
</file>
<file path="docs/tui-styling-guide.md">
# TUI Styling Guide (Ink) â prompt-maker
This guide teaches **Ink styling fundamentals** and then explains **exactly how styling and theming work in this repo**, with **screen-by-screen** and **component-by-component** pointers to the files you should change.
It is grounded in these repo docs and code:
- UX goals + input routing invariants: `docs/tui-design.md`
- Architecture + performance notes: `src/tui/DEVELOPER_NOTE.md`
- Boot sequence + module map: `docs/prompt-maker-cli-tui-encyclopedia.md`
- Worked example popup: `src/tui/components/popups/InstructionsPopup.tsx`
> Note on paths: some older docs may reference `apps/prompt-maker-cli/...`, but this repoâs authoritative TUI implementation is under `src/tui/**`.
---
## 1) Overview
Youâll learn:
1. **Ink basics**: how `<Box>` and `<Text>` render in a terminal, and which props matter.
2. **This repoâs theme system**: where theme tokens live, how a theme JSON becomes Ink props, and how theme selection is persisted.
3. **A styling map** of:
   - Screens (`Generate` and `Test Runner`)
   - Shared layout components (`core/*`)
   - Popup components (`popups/*`)
4. Practical recipes:
   - Change popup background/border
   - Change accent vs muted text
   - Make a layout responsive to terminal width
   - Add a new theme token (slot)
   - Add a new theme JSON
Constraints / philosophy (mirrors repo intent):
- Prefer **theme tokens** over hard-coded colors.
- Keep **input routing invariants** intact (help overlay and popups must âownâ keyboard input).
- Avoid costly work in render paths.
---
## 2) Ink Basics (Beginner Tutorial)
### 2.1 What Ink is
Ink is **React for command-line interfaces**:
- You write React components (`function Component() { return <Box>...</Box> }`).
- Ink renders them to the terminal using a layout engine.
- Keyboard input is handled via Ink hooks (not browser events).
In this repo, the TUI mounts from `src/tui/index.tsx` via Inkâs `render()`.
### 2.2 The two primitives youâll style 90% of the time
#### `<Text>`
`<Text>` prints text to the terminal.
Common styling props:
- `color`: foreground color (e.g. `"#ff0000"` or `"cyan"` depending on terminal)
- `backgroundColor`: background fill behind the text
- `bold`, `italic`, `underline`
- `wrap`: how to handle long lines (`"wrap"`, `"truncate"`, etc.)
- `inverse`: swaps foreground/background for that cell (used for cursors and selections)
Example:
```tsx
import { Text } from 'ink'
export const Example = () => (
  <Text color="#58a6ff" bold>
    Accent headline
  </Text>
)
```
#### `<Box>`
`<Box>` is Inkâs layout container.
Key layout props:
- `flexDirection`: `"row"` or `"column"` (most screens use column)
- `width`, `height`: numbers or strings like `"100%"`
- `flexGrow`, `flexShrink`
- `justifyContent`, `alignItems`
- `paddingX`, `paddingY`, `marginTop`, etc.
- `borderStyle`: `"round"`, `"single"`, etc.
- `borderColor`: border color
- `backgroundColor`: background fill
- `position="absolute"`: overlay layers (popups and toasts use this)
Example:
```tsx
import { Box, Text } from 'ink'
export const Card = () => (
  <Box borderStyle="round" paddingX={1} flexDirection="column">
    <Text>Title</Text>
    <Text>Body</Text>
  </Box>
)
```
### 2.3 Terminals impose constraints you must design around
#### Monospace + âcolumnsâ
Terminals measure width in **columns**. When you want a boxed UI that looks solid, you usually must:
- Compute the available width (via `useStdout().stdout.columns`).
- Clamp it to reasonable bounds.
- Pad each rendered line with spaces so background colors fill the entire box.
This âpad to full widthâ pattern is used throughout popups (see `src/tui/components/popups/InstructionsPopup.tsx` and many others).
#### Wrapping vs truncation
If your content may exceed the available width, you must decide:
- `wrap="wrap"`: can reflow but may make the UI jumpy.
- `wrap="truncate"`: stable layout but hides overflow.
The repo often prefers stable layouts in constrained panes (e.g. popups) by computing `contentWidth` and padding/truncating manually.
#### Unicode width and borders
Box borders (like `borderStyle="round"`) use Unicode line-drawing characters. Some terminals/fonts render these differently. When changing border styles, sanity-check in multiple terminals.
---
## 3) How Styling Works in This Repo
### 3.1 The ThemeProvider and `useTheme()`
The TUI is wrapped by `ThemeProvider` in `src/tui/AppContainer.tsx`.
- Provider: `src/tui/theme/theme-provider.tsx`
- Consumer hook: `useTheme()` exported from the same file
`useTheme()` gives you:
- `theme`: a **ResolvedTheme** (a map of named slots â Ink-compatible colors)
- `mode`: current mode (`'light' | 'dark' | 'system'`)
- `themes`: loaded theme descriptors (built-in + custom)
- `activeThemeName`: the current selected theme name
- `setTheme(name)`, `previewTheme(name)`
- `setMode(mode)`
- `error` and `warnings`
### 3.2 Theme tokens (slots)
Theme slot definitions live in `src/tui/theme/theme-types.ts`.
Required slots (current list):
- `background`
- `text`
- `mutedText`
- `border`
- `accent`
- `accentText`
- `warning`
- `error`
- `success`
- `panelBackground`
- `popupBackground`
- `selectionBackground`
- `selectionText`
- `chipBackground`
- `chipText`
- `chipMutedText`
These are enforced by:
- `REQUIRED_THEME_SLOTS` in `src/tui/theme/theme-types.ts`
- Theme JSON validation in `src/tui/theme/theme-loader.ts`
### 3.3 Turning theme tokens into Ink props
The repo uses tiny helpers in `src/tui/theme/theme-types.ts`:
- `inkColorProps(theme.text)` â `{ color?: string }`
- `inkBackgroundColorProps(theme.panelBackground)` â `{ backgroundColor?: string }`
- `inkBorderColorProps(theme.border)` â `{ borderColor?: string }`
They deliberately return `{}` when the value is `undefined`, which keeps JSX clean:
```tsx
<Text {...inkColorProps(theme.mutedText)}>Muted text</Text>
```
This pattern is everywhere (screens, core components, popups).
### 3.4 Theme JSON â ResolvedTheme
Theme JSON is **not** âInk propsâ. Itâs a higher-level schema that supports:
- Hex colors (`"#RRGGBB"`)
- 8-digit hex (`"#RRGGBBAA"`), where `AA == "00"` resolves to âtransparentâ (treated as `undefined`)
- ANSI color numbers (`0..255`) (converted to a hex value)
- References (strings that point to `defs.<name>` or `theme.<slot>` keys)
- Variants: `{ "dark": ..., "light": ... }`
Resolution is implemented in `src/tui/theme/theme-resolver.ts`.
### 3.5 Built-in themes
Built-in themes live in:
- `src/tui/theme/builtins/pm-dark.ts` (default)
- `src/tui/theme/builtins/pm-light.ts`
They are registered in `src/tui/theme/theme-registry.ts`.
### 3.6 Theme loading locations (custom themes)
Theme discovery and validation are implemented in `src/tui/theme/theme-loader.ts`.
It loads theme JSON files from:
1. **Built-in themes** (from `theme-registry.ts`)
2. **Global user themes**:
   - `~/.config/prompt-maker-cli/themes/*.json`
3. **Project themes** (walks upward from `cwd`):
   - `<project or parent>/.prompt-maker-cli/themes/*.json`
Project themes are discovered by walking up directories until an optional `stopAt` boundary.
The file name (minus `.json`) becomes the theme name.
### 3.7 Persistence: where theme selection is stored
Theme selection persists into the CLI config JSON via `src/config.ts`:
- Config fields: `theme?: string`, `themeMode?: ThemeMode`
- Update function: `updateCliThemeSettings(...)`
The bridge between the TUI and CLI config is in `src/tui/theme/theme-settings-service.ts`:
- `loadThemeSelection()` loads config + themes and resolves fallbacks.
- `saveThemeSelection()` writes `theme` and/or `themeMode`.
### 3.8 âSystemâ mode and terminal appearance detection
`ThemeMode = 'light' | 'dark' | 'system'` in `src/tui/theme/theme-types.ts`.
When mode is `system`, `ThemeProvider` resolves it using `detectTerminalAppearanceMode()` from `src/tui/theme/terminal-appearance.ts`:
- Uses `TERM_BACKGROUND` if set (`light`/`dark`)
- Else uses `COLORFGBG` heuristics
- Else falls back to `dark`
---
## 4) Where Styles Live (Path Map)
### 4.1 Entry points and wrappers
| Concern                                                   | File(s)                                                     |
| --------------------------------------------------------- | ----------------------------------------------------------- |
| CLI â TUI mount                                           | `src/tui/index.tsx`                                         |
| Root shell (layout + help + global keys + theme provider) | `src/tui/AppContainer.tsx`                                  |
| Screen re-export stubs                                    | `src/tui/CommandScreen.tsx`, `src/tui/TestRunnerScreen.tsx` |
### 4.2 Theme system
| Concern                               | File(s)                                                                   |
| ------------------------------------- | ------------------------------------------------------------------------- |
| Theme provider + `useTheme()`         | `src/tui/theme/theme-provider.tsx`                                        |
| Theme token types + Ink prop helpers  | `src/tui/theme/theme-types.ts`                                            |
| Theme JSON resolution                 | `src/tui/theme/theme-resolver.ts`                                         |
| Theme discovery + validation          | `src/tui/theme/theme-loader.ts`                                           |
| Theme persistence bridge (CLI config) | `src/tui/theme/theme-settings-service.ts`                                 |
| Built-in theme registry               | `src/tui/theme/theme-registry.ts`                                         |
| Built-in themes                       | `src/tui/theme/builtins/pm-dark.ts`, `src/tui/theme/builtins/pm-light.ts` |
| Example theme JSON                    | `src/tui/theme/examples/ocean-example.json`                               |
### 4.3 Screens
| Screen               | Entry                          | Implementation directory        |
| -------------------- | ------------------------------ | ------------------------------- |
| Generate (âCommandâ) | `src/tui/CommandScreen.tsx`    | `src/tui/screens/command/*`     |
| Test Runner          | `src/tui/TestRunnerScreen.tsx` | `src/tui/screens/test-runner/*` |
### 4.4 Shared components
| Category                              | Directory                                |
| ------------------------------------- | ---------------------------------------- |
| Core presentational components        | `src/tui/components/core/*`              |
| Popup components                      | `src/tui/components/popups/*`            |
| Spinner (currently hard-coded colors) | `src/tui/components/OpencodeSpinner.tsx` |
---
## 5) Screen-by-Screen Styling Guide
This section is intentionally practical: _âI want to change X â where do I go?â_
### 5.1 App shell: `AppContainer`
**Primary file:** `src/tui/AppContainer.tsx`
What it styles:
- Global background via `inkBackgroundColorProps(theme.background)`.
- Header text colors (`theme.accent`, `theme.mutedText`).
- Help overlay and toast overlay positioning.
Safe styling changes:
- Change overall app background using theme token `background`.
- Change the header look by adjusting `accent`/`mutedText` tokens.
- Be careful changing `paddingX`/`paddingY`: multiple components assume `AppContainer` has `paddingX={2}` when computing widths.
Related invariants:
- Input routing order from `docs/tui-design.md` and `docs/prompt-maker-cli-tui-encyclopedia.md`:
  1. Help overlay
  2. Popup input
  3. Screen input
  4. Global keys
### 5.2 Generate screen (âCommandScreenâ)
**Entry:** `src/tui/CommandScreen.tsx`
**Main implementation:** `src/tui/screens/command/CommandScreenImpl.tsx`
(There is also a local re-export at `src/tui/screens/command/CommandScreen.tsx`; the TUI shell imports the screen via `src/tui/CommandScreen.tsx`.)
Styled regions:
1. **Main background**: `theme.background`.
2. **History area**: `HistoryPane` paints `theme.background` and passes `backgroundColor={theme.background}` into `ScrollableOutput`.
3. **Command palette**: `CommandMenuPane` â `CommandMenu` uses `theme.panelBackground` + `theme.border`, with selection via `selectionBackground`/`selectionText`.
4. **Input bar**: `CommandInput` â `InputBar` uses `theme.panelBackground`; the left gutter uses `theme.border` or `theme.warning` depending on mode.
5. **Popup overlay**:
   - A full-screen background paint using `BackgroundFill` with `theme.background` (prevents âholesâ/bleed-through in absolute overlays).
   - A centered popup (one of `src/tui/components/popups/*`), typically built on `PopupSheet` with `theme.popupBackground`.
Key files (styling focus):
- History area wrapper: `src/tui/screens/command/components/HistoryPane.tsx`
- Command palette wrapper: `src/tui/screens/command/components/CommandMenuPane.tsx`
- Input wrapper: `src/tui/screens/command/components/CommandInput.tsx`
- Popup selection switch: `src/tui/screens/command/components/PopupArea.tsx`
Safe modifications:
- To change the history âpanelâ look: there isnât a dedicated token; it currently uses `background` (adjust tokens or edit `src/tui/components/core/ScrollableOutput.tsx`).
- To change selection styling in command palette/popups: use `selectionBackground`/`selectionText`.
- To change the popup overlay backdrop: today it uses `background`; for a dim overlay, add a dedicated theme slot and use it in `src/tui/screens/command/CommandScreenImpl.tsx`.
Terminal-width pitfalls (from `CommandScreenImpl.tsx`):
- `AppContainer` uses `paddingX={2}`; the command screen compensates when filling backgrounds to avoid Ink truncating with `...`.
### 5.3 Test Runner screen
**Entry:** `src/tui/TestRunnerScreen.tsx`
**Implementation:** `src/tui/screens/test-runner/TestRunnerScreen.tsx`
Styled regions:
- Headings (`Tests`, `Summary`, `Recent Logs`) use `theme.accent`.
- Summary uses `theme.success` and `theme.error`.
- Logs use `theme.mutedText`/`warning`/`error`.
- File input uses a bordered card with `theme.panelBackground` and a focus border using `theme.accent`.
Key styled files:
- Screen orchestration: `src/tui/screens/test-runner/TestRunnerScreen.tsx` (re-exported by `src/tui/TestRunnerScreen.tsx`)
- File input: `src/tui/screens/test-runner/components/TestRunnerFileInput.tsx`
- Summary: `src/tui/screens/test-runner/components/TestRunnerSummary.tsx`
- Logs: `src/tui/screens/test-runner/components/TestRunnerLogs.tsx`
- Test list: `src/tui/screens/test-runner/components/TestList.tsx`
- Errors: `src/tui/screens/test-runner/components/TestRunnerError.tsx`
Safe modifications:
- Change card background via `panelBackground`.
- Change focus border via `accent`.
- Change fail/pass colors via `error`/`success`.
---
## 6) Shared Components Styling Guide
### 6.1 Core components (`src/tui/components/core/*`)
These are used across screens and popups.
#### `BackgroundFill`
- File: `src/tui/components/core/BackgroundFill.tsx`
- Purpose: paint a solid background for absolute overlays (popups).
- Styling inputs: `background` prop (passed from theme in callers).
Tip: this component uses NBSP padding to force Ink to paint background cells.
#### `ScrollableOutput`
- File: `src/tui/components/core/ScrollableOutput.tsx`
- Purpose: windowed rendering of history/log lines.
- Styling:
  - Colors are chosen by entry kind (`user` â `accent`, `progress` â `warning`, default â `text`).
  - Uses `backgroundColor` prop (often `background`, `panelBackground`, or `popupBackground`).
Safe changes:
- Adjust kind â token mapping here if you want different âspeakerâ coloring.
#### `InputBar`
- File: `src/tui/components/core/InputBar.tsx`
- Purpose: the main multi-line input with status summary.
- Styling:
  - Card background: `panelBackground`
  - Border: `border` or `warning` depending on mode
  - Label and hint text: `mutedText` or `warning`
  - Spinner and accent text: `accent`
Note: the input bar tries to keep render work predictable via memoization.
#### `MultilineTextInput`
- File: `src/tui/components/core/MultilineTextInput.tsx`
- Purpose: editable multiline input with a custom cursor.
- Styling:
  - Prompt prefix uses `theme.accent`.
  - Placeholder uses `theme.mutedText`.
  - Cursor is rendered using `<Text inverse>`.
Pitfall: because the cursor is cell-based, extra styling (padding, emoji, wide unicode) can easily shift perceived cursor position.
#### `SingleLineTextInput`
- File: `src/tui/components/core/SingleLineTextInput.tsx`
- Purpose: single-line input used in popups and panels.
- Styling:
  - Placeholder uses `theme.mutedText`.
  - Cursor uses `<Text inverse>`.
  - Supports `width` and `backgroundColor` props so popups can paint full-width.
#### `CommandMenu`
- File: `src/tui/components/core/CommandMenu.tsx`
- Purpose: command palette list.
- Styling:
  - Background: `panelBackground`
  - Border: `border`
  - Header: `accent`
  - Selection: `selectionText` + `selectionBackground`
#### `HelpOverlay`
- File: `src/tui/components/core/HelpOverlay.tsx`
- Purpose: the highest-priority overlay that suppresses other input.
- Styling:
  - Background: `popupBackground`
  - Border: `border`
  - Titles: `accent`
  - Content: `mutedText`
Because itâs a key input layer, be careful making it too transparent or visually subtle.
#### `Toast` + `ToastOverlay`
- Files: `src/tui/components/core/Toast.tsx`, `src/tui/components/core/ToastOverlay.tsx`
- Purpose: transient messages.
- Styling:
  - Background: `popupBackground` (opaque)
  - Border: `border` / `warning` / `error` depending on toast kind
  - Title: `mutedText` / `warning` / `error`
Important pattern: `Toast` pads each line to `contentWidth` so Ink paints background cells in absolute overlays.
#### Status indicator segments
- File: `src/tui/components/core/status-indicators-layout.ts`
- Purpose: parse `statusChips` into display segments (used by `InputBar` and `SettingsPopup`).
- Styling:
  - Segments are mapped to theme tokens (`success`, `warning`, `danger` â `error`, etc.).
Note: the old `StatusIndicators.tsx` and `PastedSnippetCard.tsx` components were removed during cleanup; status/paste UI now lives in the screen + popup components that need it.
### 6.2 Top-level entrypoints (`src/tui/*.tsx`)
After the screen/component cleanup, `src/tui/*.tsx` is mostly entrypoints and glue (`AppContainer`, `CommandScreen`, `TestRunnerScreen`, `index`, `context`). Most styling work now lives under:
- `src/tui/screens/*` (screen layouts)
- `src/tui/components/*` (reusable building blocks)
### 6.3 Popups (`src/tui/components/popups/*`)
Popups share a consistent âsheetâ style:
- Most popups render inside `PopupSheet` (`src/tui/components/popups/PopupSheet.tsx`).
- `PopupSheet` uses `BackgroundFill` to paint a solid `popupBackground` rectangle (no bleed-through).
- `width` is derived from terminal columns using `clamp(terminalColumns - 10, 40, 72)`.
- `contentWidth` is computed by subtracting **padding only** (there is no popup border anymore).
- `backgroundProps = inkBackgroundColorProps(theme.popupBackground)` is applied to each `<Text>` line.
- Header uses `theme.accent`, footer uses `theme.mutedText`.
- Selection uses `selectionText` + `selectionBackground`.
- Suggestion/unfocused selection styling uses `chipBackground` + `chipText`.
Note: popups are intentionally borderless after the cleanup; the `border` token is still used by bordered UI like `HelpOverlay`, `CommandMenu`, and `Toast`.
Popups are selected/rendered in `src/tui/screens/command/components/PopupArea.tsx`.
Below is a quick inventory with styling notes.
| Popup                      | File                                              | Main tokens                                                                                        |
| -------------------------- | ------------------------------------------------- | -------------------------------------------------------------------------------------------------- |
| Model selection            | `src/tui/components/popups/ModelPopup.tsx`        | `popupBackground`, `accent`, `mutedText`, `selection*`, plus `warning`/`error` for provider status |
| Generic list + suggestions | `src/tui/components/popups/ListPopup.tsx`         | `popupBackground`, `mutedText`, `text`, `selection*`, `chip*` for âunfocused selectionâ            |
| Intent file chooser        | `src/tui/components/popups/IntentFilePopup.tsx`   | `popupBackground`, `mutedText`, `selection*`, `chip*`                                              |
| Instructions               | `src/tui/components/popups/InstructionsPopup.tsx` | `popupBackground`, `accent`, `mutedText`                                                           |
| Toggle on/off              | `src/tui/components/popups/TogglePopup.tsx`       | `popupBackground`, `accent`, `mutedText`, `selection*`                                             |
| Series intent              | `src/tui/components/popups/SeriesIntentPopup.tsx` | `popupBackground`, `accent`, `mutedText`                                                           |
| Prompt test run            | `src/tui/components/popups/TestPopup.tsx`         | `popupBackground`, `accent`, `mutedText`                                                           |
| Token usage                | `src/tui/components/popups/TokenUsagePopup.tsx`   | `popupBackground`, `accent`, `text`, `mutedText`                                                   |
| Settings display           | `src/tui/components/popups/SettingsPopup.tsx`     | `popupBackground`, `accent`, `mutedText`, `success`/`warning`/`error`                              |
| Theme picker               | `src/tui/components/popups/ThemePickerPopup.tsx`  | `popupBackground`, `accent`, `mutedText`, `selection*`, `error`                                    |
| Theme mode picker          | `src/tui/components/popups/ThemeModePopup.tsx`    | `popupBackground`, `accent`, `mutedText`, `selection*`, `error`                                    |
| Reasoning view             | `src/tui/components/popups/ReasoningPopup.tsx`    | `popupBackground`, `accent`, `mutedText`, and `ScrollableOutput`                                   |
| Smart root chooser         | `src/tui/components/popups/SmartPopup.tsx`        | `popupBackground`, `accent`, `mutedText`, `selection*`, `chip*`                                    |
---
## 7) Worked Example: `InstructionsPopup.tsx`
**File:** `src/tui/components/popups/InstructionsPopup.tsx`
This popup is a great âmicrocosmâ of how the repo styles popups.
### 7.1 Responsive sizing with `useStdout()` + clamp
```ts
const { stdout } = useStdout()
const terminalColumns = stdout?.columns ?? 80
const popupWidth = clamp(terminalColumns - 10, 40, 72)
```
Why:
- `stdout.columns` is the runtime terminal width.
- `-10` leaves a margin so the popup is not edge-to-edge.
- `clamp(..., 40, 72)` keeps the popup readable and stable.
### 7.2 Correct content width accounting
```ts
const paddingColumns = 2 * POPUP_PADDING_X
const contentWidth = Math.max(0, popupWidth - paddingColumns)
```
Why:
- These popups are borderless; padding is the main width cost.
- Without subtracting padding, your padded lines will overflow and Ink may truncate.
### 7.3 âOpaque backgroundâ pattern
Most popups render inside `PopupSheet` (`src/tui/components/popups/PopupSheet.tsx`), which uses `BackgroundFill` to paint an opaque rectangle behind the content.
Inside the popup, we still compute:
```ts
const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
```
âŚand apply `backgroundProps` to each `<Text>` line, padding to `contentWidth`:
```ts
{
  padRight('Meta Instructions', contentWidth)
}
```
This keeps the popup fully opaque even when rendered in an absolute overlay.
### 7.4 Theme helpers vs direct props
The popup uses:
- `inkBackgroundColorProps(theme.popupBackground)`
- `inkColorProps(theme.accent)`
- `inkColorProps(theme.mutedText)`
This is the preferred approach.
### 7.5 Example changes (documentation-only)
#### Change popup background
You usually do this by changing theme tokens, not the component.
Example: create a custom theme JSON that changes popup background (and the shared `border` token used by bordered UI):
```json
{
  "defs": {
    "bg": "#0b0f14",
    "popup": "#1a2029",
    "border": "#6cb6ff",
    "text": "#e6edf3",
    "muted": "#9aa4b2",
    "accent": "#6cb6ff",
    "warning": "#f2cc60",
    "error": "#ff6b6b",
    "success": "#3fb950",
    "panel": "#111820",
    "selectionBg": 60
  },
  "theme": {
    "background": "bg",
    "text": "text",
    "mutedText": "muted",
    "border": "border",
    "accent": "accent",
    "accentText": "bg",
    "warning": "warning",
    "error": "error",
    "success": "success",
    "panelBackground": "panel",
    "popupBackground": "popup",
    "selectionBackground": "selectionBg",
    "selectionText": "text",
    "chipBackground": "panelBackground",
    "chipText": "text",
    "chipMutedText": "mutedText"
  }
}
```
Put it in either:
- `~/.config/prompt-maker-cli/themes/<name>.json`
- `<repo>/.prompt-maker-cli/themes/<name>.json`
Then select it via the theme popup (see `ThemePickerPopup`).
---
## 8) Common Customizations (Recipes)
### 8.1 Change popup background
Goal: all popups use a new background.
Best practice: change tokens, not components.
- Background: `popupBackground`
Where to do it:
- Built-ins: `src/tui/theme/builtins/pm-dark.ts` / `src/tui/theme/builtins/pm-light.ts`
- Or create a custom theme JSON in the user/project theme directories.
Note: popups are borderless after the cleanup. The `border` token still controls bordered UI like `src/tui/components/core/HelpOverlay.tsx`, `src/tui/components/core/Toast.tsx`, and `src/tui/components/core/CommandMenu.tsx`.
### 8.2 Change âaccentâ and âmutedâ text styling
Tokens:
- âAccentâ: `accent`
- âMutedâ text: `mutedText`
- Default text: `text`
Common places:
- Headings across screens/panels: `theme.accent`
  - `src/tui/AppContainer.tsx`
  - `src/tui/screens/test-runner/TestRunnerScreen.tsx` (re-exported by `src/tui/TestRunnerScreen.tsx`)
- Instructional/help text: `theme.mutedText`
  - `src/tui/components/core/HelpOverlay.tsx`
  - most popup footers
### 8.3 Adjust widths responsively based on terminal columns
This repo now has two common width-math patterns.
**Borderless sheets (most popups via `PopupSheet`)**
```ts
const { stdout } = useStdout()
const terminalColumns = stdout?.columns ?? 80
const popupWidth = clamp(terminalColumns - 10, 40, 72)
const paddingColumns = 2 * POPUP_PADDING_X
const contentWidth = Math.max(0, popupWidth - paddingColumns)
```
**Bordered boxes (help overlay, command menu, toasts)**
```ts
const borderColumns = 2
const paddingColumns = 2
const contentWidth = Math.max(0, boxWidth - borderColumns - paddingColumns)
```
Guidelines:
- Subtract padding for `PopupSheet` popups; subtract **border + padding** for bordered boxes.
- Clamp widths to keep UX stable.
- Prefer manual padding/truncation (stable layout) over uncontrolled wrapping in overlays.
### 8.4 Add a new theme token (slot)
You only need this when you want a new semantic role that cannot be expressed with existing tokens.
Example goal: introduce a dedicated `dimOverlayBackground` token (instead of reusing `background`).
Steps (repo-accurate locations):
1. Add the slot to the union in `src/tui/theme/theme-types.ts`:
```ts
export type ThemeSlot =
  | 'background'
  // ...
  | 'chipMutedText'
  | 'dimOverlayBackground'
```
2. Add it to `REQUIRED_THEME_SLOTS` in `src/tui/theme/theme-types.ts`.
3. Update built-in themes to include it:
- `src/tui/theme/builtins/pm-dark.ts`
- `src/tui/theme/builtins/pm-light.ts`
4. Update any theme examples you ship (optional but recommended):
- `src/tui/theme/examples/ocean-example.json`
5. Use it in UI code (example, popup overlay in `src/tui/screens/command/CommandScreenImpl.tsx`).
Why this matters:
- Theme JSON validation in `src/tui/theme/theme-loader.ts` enforces required slots based on `REQUIRED_THEME_SLOTS`.
- Theme resolution in `src/tui/theme/theme-resolver.ts` resolves required slots.
### 8.5 Create a new theme JSON
Fast path:
1. Copy `src/tui/theme/examples/ocean-example.json`.
2. Save as:
   - `~/.config/prompt-maker-cli/themes/my-theme.json`
   - or `<repo>/.prompt-maker-cli/themes/my-theme.json`
3. Restart the TUI and select it in the theme picker popup.
Tips:
- Use `defs` for reusable named colors.
- Use references (`"popupBackground": "panelBackground"`) to keep things consistent.
- Use variants to support `system` mode:
```json
"bg": { "dark": "#10141a", "light": "#fafcff" }
```
---
## 9) Troubleshooting & Pitfalls
### 9.1 Input routing: styling can hide focus cues
From `docs/tui-design.md` and `docs/prompt-maker-cli-tui-encyclopedia.md`:
- Help overlay should visually read as âtop layerâ.
- Popups should clearly indicate they own input.
If you make backgrounds too similar (e.g., `popupBackground` == `background` and borders muted), users may not understand where focus is.
### 9.2 Overlays need explicit background painting
Ink wonât always repaint âempty cellsâ in absolute overlays.
Symptoms:
- Toasts or popups appear with âholesâ or underlying text bleeding through.
Fix pattern:
- Apply `backgroundColor` to each `<Text>`.
- Pad each line to full width.
Examples:
- `src/tui/components/core/Toast.tsx`
- `src/tui/components/popups/InstructionsPopup.tsx`
### 9.3 Performance: avoid heavy work in render
From `src/tui/DEVELOPER_NOTE.md`:
- Avoid recreating arrays/objects as props.
- Memoize derived arrays when it prevents meaningful work.
- Use reducers for state transitions.
Styling-specific advice:
- Donât build large padded string arrays in render unless memoized.
- Prefer `useMemo` for expensive formatting of lines (see `ScrollableOutput`, `CommandMenu`, `ListPopup`).
### 9.4 Cross-terminal differences
- Not all terminals support true color the same way.
- Unicode border glyphs may vary.
- Width calculations can break with wide characters.
When changing themes:
- Test on a light and dark terminal.
- Test with different font/renderers if possible.
### 9.5 Hard-coded colors exist in `OpencodeSpinner`
Most of the TUI uses `useTheme()` and theme tokens. One exception:
- `src/tui/components/OpencodeSpinner.tsx` uses hard-coded hex colors.
If you want fully themeable visuals, consider migrating this to theme tokens.
---
## 10) Safe Styling Change Checklist
Before you submit a styling change:
- Confirm youâre changing **tokens** (theme JSON / built-ins) rather than sprinkling hard-coded colors.
- If you touched popups or overlays:
  - Verify full opacity: backgrounds paint correctly (no âholesâ).
  - Verify width math: `contentWidth` subtracts padding (and border columns when the component draws a border).
- If you touched layout/padding:
  - Re-check any components that assume `AppContainer` padding when computing widths.
- Verify input routing cues still make sense:
  - Help overlay obviously looks like a modal.
  - Popups look like the active layer.
- Keep render paths light:
  - Use `useMemo` for expensive padded line arrays.
---
### Appendix: Quick âwhere do I changeâŚâ index
- Global app background: `background` token (themes) â used in `src/tui/AppContainer.tsx` and `src/tui/screens/command/CommandScreenImpl.tsx`
- Panel background (command menu + input bar): `panelBackground` token (history uses `background`)
- Popup background: `popupBackground` token
- Border colors: `border` token (or `warning`/`error` for state)
- Selected row styling: `selectionBackground` + `selectionText`
- Unfocused selection / chip styling: `chipBackground` + `chipText` (+ `chipMutedText`)
</file>
<file path="packages/core/src/__tests__/llm.test.ts">
/// <reference types="jest" />
import type { Message } from '../lib/llm'
jest.mock('node:fs/promises', () => ({
  readFile: jest.fn(),
}))
declare const global: typeof globalThis & { fetch: jest.Mock }
describe('prompt-maker-core llm wrapper', () => {
  const fetchMock = jest.fn()
  let callLLM: typeof import('../lib/llm').callLLM
  let getEmbedding: typeof import('../lib/llm').getEmbedding
  const fsPromises = jest.requireMock('node:fs/promises') as { readFile: jest.Mock }
  beforeAll(async () => {
    const llm = await import('../lib/llm')
    callLLM = llm.callLLM
    getEmbedding = llm.getEmbedding
  })
  beforeEach(() => {
    fetchMock.mockReset()
    global.fetch = fetchMock
    fsPromises.readFile.mockReset()
    process.env.OPENAI_API_KEY = 'openai-key'
    process.env.GEMINI_API_KEY = 'gemini-key'
    process.env.OPENAI_BASE_URL = 'https://api.openai.com/v1'
    process.env.GEMINI_BASE_URL = 'https://generativelanguage.googleapis.com'
  })
  it('routes callLLM through OpenAI by default', async () => {
    fetchMock.mockResolvedValue({
      ok: true,
      json: async () => ({ choices: [{ message: { content: 'result text' } }] }),
    })
    const result = await callLLM([{ role: 'user', content: 'Hello' }])
    expect(result).toBe('result text')
    expect(fetchMock).toHaveBeenCalledWith(
      expect.stringContaining('/chat/completions'),
      expect.objectContaining({ method: 'POST' }),
    )
  })
  it('routes GPT-5 reasoning models to OpenAI Responses API', async () => {
    fetchMock.mockResolvedValue({
      ok: true,
      json: async () => ({ output_text: 'responses text' }),
    })
    const result = await callLLM(
      [
        { role: 'system', content: 'rules' },
        { role: 'user', content: 'Hi' },
      ],
      'gpt-5.2-pro',
    )
    expect(result).toBe('responses text')
    expect(fetchMock).toHaveBeenCalledWith(
      expect.stringContaining('/responses'),
      expect.objectContaining({ method: 'POST' }),
    )
    const [, options] = fetchMock.mock.calls[0]
    const body = JSON.parse((options as { body: string }).body)
    expect(body).toMatchObject({
      model: 'gpt-5.2-pro',
      input: [
        { role: 'developer', content: 'rules' },
        { role: 'user', content: 'Hi' },
      ],
    })
    expect(body.temperature).toBeUndefined()
  })
  it('retries via Responses API when Chat endpoint rejects model', async () => {
    fetchMock
      .mockResolvedValueOnce({
        ok: false,
        status: 400,
        text: async () =>
          'The model `gpt-5.2-pro-chat` does not support the /v1/chat/completions endpoint. Please use /v1/responses.',
      })
      .mockResolvedValueOnce({
        ok: true,
        json: async () => ({ output_text: 'retried text' }),
      })
    const result = await callLLM([{ role: 'user', content: 'Hello' }], 'gpt-5.2-pro-chat')
    expect(result).toBe('retried text')
    expect(fetchMock).toHaveBeenCalledTimes(2)
    expect(fetchMock.mock.calls[0][0]).toEqual(expect.stringContaining('/chat/completions'))
    expect(fetchMock.mock.calls[1][0]).toEqual(expect.stringContaining('/responses'))
    const [, secondOptions] = fetchMock.mock.calls[1]
    const secondBody = JSON.parse((secondOptions as { body: string }).body)
    expect(secondBody).toMatchObject({
      model: 'gpt-5.2-pro-chat',
      input: [{ role: 'user', content: 'Hello' }],
    })
  })
  it('throws when OpenAI API key is missing', async () => {
    delete process.env.OPENAI_API_KEY
    await expect(callLLM([{ role: 'user', content: 'Hi' }], 'gpt-4o')).rejects.toThrow(
      'OPENAI_API_KEY env var is not set.',
    )
  })
  it('supports OpenAI array content responses', async () => {
    fetchMock.mockResolvedValue({
      ok: true,
      json: async () => ({
        choices: [
          {
            message: {
              content: [
                { type: 'text', text: 'first' },
                { type: 'text', text: 'second' },
              ],
            },
          },
        ],
      }),
    })
    const result = await callLLM([{ role: 'user', content: 'Hello' }], 'gpt-4o')
    expect(result).toBe('firstsecond')
    expect(fetchMock).toHaveBeenCalledWith(
      expect.stringContaining('/chat/completions'),
      expect.objectContaining({ method: 'POST' }),
    )
  })
  it('extracts PDF text for OpenAI requests', async () => {
    fsPromises.readFile.mockResolvedValue(
      Buffer.from(
        '%PDF-1.4\n1 0 obj\n<<>>\nstream\nBT\n(Hello PDF) Tj\nET\nendstream\n%%EOF',
        'latin1',
      ),
    )
    fetchMock.mockResolvedValue({
      ok: true,
      json: async () => ({ choices: [{ message: { content: 'result text' } }] }),
    })
    const result = await callLLM(
      [
        {
          role: 'user',
          content: [{ type: 'pdf', mimeType: 'application/pdf', filePath: 'doc.pdf' }],
        } as Message,
      ],
      'gpt-4o',
    )
    expect(result).toBe('result text')
    const [, options] = fetchMock.mock.calls[0]
    const body = JSON.parse((options as { body: string }).body)
    expect(JSON.stringify(body.messages[0].content)).toContain('Hello PDF')
  })
  it('routes Gemini models to Gemini endpoint', async () => {
    fetchMock.mockResolvedValue({
      ok: true,
      json: async () => ({ candidates: [{ content: { parts: [{ text: 'gemini result' }] } }] }),
    })
    const result = await callLLM([{ role: 'user', content: 'Hi' }], 'gemini-1.5-pro')
    expect(result).toBe('gemini result')
    expect(fetchMock).toHaveBeenCalledWith(
      expect.stringContaining('/v1beta/models/gemini-1.5-pro:generateContent'),
      expect.objectContaining({ method: 'POST' }),
    )
  })
  it('throws when Gemini API key missing', async () => {
    delete process.env.GEMINI_API_KEY
    await expect(callLLM([{ role: 'user', content: 'Hi' }], 'gemini-1.5-pro')).rejects.toThrow(
      'GEMINI_API_KEY env var is not set.',
    )
  })
  it('includes systemInstruction for Gemini payloads', async () => {
    fetchMock.mockResolvedValue({
      ok: true,
      json: async () => ({ candidates: [{ content: { parts: [{ text: 'gemini result' }] } }] }),
    })
    await callLLM(
      [
        { role: 'system', content: 'rules' },
        { role: 'user', content: 'Do work' },
      ],
      'gemini-1.5-pro',
    )
    const [, options] = fetchMock.mock.calls[0]
    const body = JSON.parse((options as { body: string }).body)
    expect(body.systemInstruction.parts[0]).toEqual({ text: 'rules' })
  })
  it('callLLM rejects OpenAI video inputs', async () => {
    await expect(
      callLLM(
        [
          {
            role: 'user',
            content: [{ type: 'video_uri', mimeType: 'video/mp4', fileUri: 'gs://video' }],
          } as Message,
        ],
        'gpt-4o',
      ),
    ).rejects.toThrow('Video inputs are only supported when using Gemini models.')
  })
  it('getEmbedding uses OpenAI by default', async () => {
    fetchMock.mockResolvedValue({
      ok: true,
      json: async () => ({ data: [{ embedding: [0.1, 0.2] }] }),
    })
    const vector = await getEmbedding('text to embed')
    expect(vector).toEqual([0.1, 0.2])
    expect(fetchMock).toHaveBeenCalledWith(
      expect.stringContaining('/embeddings'),
      expect.objectContaining({ method: 'POST' }),
    )
  })
  it('getEmbedding routes to Gemini models when requested', async () => {
    fetchMock.mockResolvedValue({
      ok: true,
      json: async () => ({ embedding: { value: [0.9, 0.8] } }),
    })
    const vector = await getEmbedding('embed me', 'text-embedding-004')
    expect(vector).toEqual([0.9, 0.8])
    expect(fetchMock).toHaveBeenCalledWith(
      expect.stringContaining(':embedContent'),
      expect.objectContaining({ method: 'POST' }),
    )
  })
  it('getEmbedding rejects empty input', async () => {
    await expect(getEmbedding('  ')).rejects.toThrow('Text to embed must not be empty.')
  })
})
</file>
<file path="packages/core/src/lib/llm.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import { inflateSync } from 'node:zlib'
export type TextPart = { type: 'text'; text: string }
export type ImagePart = { type: 'image'; mimeType: string; data: string }
export type VideoPart = { type: 'video_uri'; mimeType: string; fileUri: string }
export type PdfPart = {
  type: 'pdf'
  mimeType: 'application/pdf'
  filePath: string
  fileUri?: string
}
export type MessageContent = string | (TextPart | ImagePart | VideoPart | PdfPart)[]
export type Message = {
  role: 'system' | 'user' | 'assistant'
  content: MessageContent
}
type OpenAIChatMessageContent =
  | string
  | Array<{ type: 'text'; text: string } | { type: 'image_url'; image_url: { url: string } }>
type OpenAIChatCompletionMessage = {
  role: 'system' | 'user' | 'assistant'
  content: OpenAIChatMessageContent
}
type OpenAIResponseContentPart = { type: 'text'; text: string }
type ChatCompletionChoice = {
  index: number
  message: { role: 'assistant'; content: string | OpenAIResponseContentPart[] }
}
type ChatCompletionResponse = {
  choices: ChatCompletionChoice[]
}
type GeminiApiVersion = 'v1' | 'v1beta'
type GeminiContentPart =
  | { text: string }
  | { inlineData: { mimeType: string; data: string } }
  | { fileData: { mimeType: string; fileUri: string } }
type GeminiContent = {
  role: 'user' | 'model' | 'system'
  parts: GeminiContentPart[]
}
type GeminiResponse = {
  candidates?: Array<{ content?: { parts?: GeminiContentPart[] } }>
}
type GeminiRequestBody = {
  contents: GeminiContent[]
  systemInstruction?: GeminiContent
  generationConfig: { temperature: number }
}
type OpenAIEmbeddingResponse = {
  data?: Array<{ embedding: number[] }>
}
type GeminiEmbeddingResponse = {
  embedding?: { value?: number[] }
}
/**
 * OpenAI Responses API types (partial, just what we need).
 * Docs: https://platform.openai.com/docs/api-reference/responses
 */
type OpenAIResponsesInputText = { type: 'input_text'; text: string }
type OpenAIResponsesInputImage = { type: 'input_image'; image_url: string }
type OpenAIResponsesInputContent =
  | string
  | Array<OpenAIResponsesInputText | OpenAIResponsesInputImage>
type OpenAIResponsesInputMessage = {
  role: 'developer' | 'user' | 'assistant'
  content: OpenAIResponsesInputContent
}
type OpenAIResponsesOutputText = { type: 'output_text'; text?: string }
type OpenAIResponsesOutputMessage = {
  type: 'message'
  role?: 'assistant' | 'user' | 'developer'
  content?: OpenAIResponsesOutputText[]
}
type OpenAIResponsesOutputOther = { type?: string; role?: unknown } & Record<string, unknown>
type OpenAIResponsesOutputItem = OpenAIResponsesOutputMessage | OpenAIResponsesOutputOther
type OpenAIResponsesResponse = {
  output_text?: string
  output?: OpenAIResponsesOutputItem[]
}
const DEFAULT_MODEL = process.env.OPENAI_MODEL ?? process.env.GEMINI_MODEL ?? 'gpt-5.1-codex'
const DEFAULT_OPENAI_EMBEDDING_MODEL = 'text-embedding-3-small'
const DEFAULT_GEMINI_EMBEDDING_MODEL = 'text-embedding-004'
const rawOpenAiBase = process.env.OPENAI_BASE_URL ?? 'https://api.openai.com/v1'
const OPENAI_BASE_URL = rawOpenAiBase.replace(/\/$/, '')
// Keep backwards-compatible behavior if OPENAI_BASE_URL was set to a nested path.
const OPENAI_CHAT_ENDPOINT = `${OPENAI_BASE_URL.replace(/\/chat\/completions$/, '')}/chat/completions`
const OPENAI_RESPONSES_ENDPOINT = `${OPENAI_BASE_URL.replace(/\/chat\/completions$/, '')}/responses`
const OPENAI_EMBEDDING_ENDPOINT = `${OPENAI_BASE_URL.replace(/\/chat\/completions$/, '')}/embeddings`
const DEFAULT_GEMINI_BASE_URL = 'https://generativelanguage.googleapis.com'
const normalizeGeminiBaseUrl = (value: string | undefined): string => {
  const trimmed = value?.trim()
  const candidate = trimmed && trimmed.length > 0 ? trimmed : DEFAULT_GEMINI_BASE_URL
  const withoutTrailingSlash = candidate.replace(/\/$/, '')
  const suffixes = ['/v1beta/models', '/v1/models', '/v1beta', '/v1']
  const stripped = suffixes.reduce((current, suffix) => {
    return current.endsWith(suffix) ? current.slice(0, -suffix.length) : current
  }, withoutTrailingSlash)
  return stripped || DEFAULT_GEMINI_BASE_URL
}
const GEMINI_BASE_URL = normalizeGeminiBaseUrl(process.env.GEMINI_BASE_URL)
const GEMINI_API_VERSION = process.env.GEMINI_API_VERSION?.trim() || 'v1beta'
const normalizeGeminiApiVersion = (value: string): GeminiApiVersion => {
  const trimmed = value.trim().toLowerCase()
  return trimmed === 'v1beta' ? 'v1beta' : 'v1'
}
export const callLLM = async (
  messages: Message[],
  model: string = DEFAULT_MODEL,
): Promise<string> => {
  const provider = resolveProvider(model)
  if (provider === 'gemini') {
    return callGemini(messages, model)
  }
  return callOpenAI(messages, model)
}
export const getEmbedding = async (text: string, model?: string): Promise<number[]> => {
  if (!text || !text.trim()) {
    throw new Error('Text to embed must not be empty.')
  }
  const requestedModel = model?.trim()
  const targetModel =
    requestedModel && requestedModel.length > 0 ? requestedModel : DEFAULT_OPENAI_EMBEDDING_MODEL
  const provider = resolveProvider(targetModel)
  if (provider === 'gemini') {
    const geminiModel =
      requestedModel && requestedModel.length > 0 ? requestedModel : DEFAULT_GEMINI_EMBEDDING_MODEL
    return callGeminiEmbedding(text, geminiModel)
  }
  return callOpenAIEmbedding(text, targetModel)
}
const resolveProvider = (model: string): 'openai' | 'gemini' => {
  const normalized = model.trim().toLowerCase()
  if (
    normalized.startsWith('gemini') ||
    normalized.startsWith('gemma') ||
    normalized === 'text-embedding-004'
  ) {
    return 'gemini'
  }
  return 'openai'
}
/**
 * Routing rule:
 * - Chat Completions endpoint is for chat-tuned models.
 * - GPT-5.x reasoning models (e.g., gpt-5.2-pro) should go to Responses API.
 */
const shouldUseChatCompletions = (model: string): boolean => {
  const m = model.trim().toLowerCase()
  // GPT-5.x reasoning models (and other "o" series reasoning models) are not supported on chat.
  if (m.startsWith('gpt-5') && !m.includes('chat') && !m.includes('codex')) {
    return false
  }
  if (m.startsWith('o1') || m.startsWith('o3') || m.startsWith('o4')) {
    return false
  }
  return true
}
const toErrorMessage = (error: unknown): string => {
  if (typeof error === 'string') return error
  if (error && typeof error === 'object' && 'message' in error) {
    const maybeMessage = (error as { message?: unknown }).message
    if (typeof maybeMessage === 'string') return maybeMessage
  }
  return ''
}
const isOpenAIEndpointMismatchError = (error: unknown): boolean => {
  const text = toErrorMessage(error).toLowerCase()
  if (!text) return false
  const mentionsChat = text.includes('/chat/completions') || text.includes('chat/completions')
  const mentionsResponses =
    text.includes('/responses') || text.includes('v1/responses') || text.includes('responses api')
  // Typical error: "does not support the /v1/chat/completions endpoint. Please use /v1/responses"
  return mentionsChat && mentionsResponses
}
const callOpenAI = async (messages: Message[], model: string): Promise<string> => {
  const apiKey = process.env.OPENAI_API_KEY
  if (!apiKey) {
    throw new Error('OPENAI_API_KEY env var is not set.')
  }
  const preferChat = shouldUseChatCompletions(model)
  try {
    return preferChat
      ? await callOpenAIChatCompletions(messages, model, apiKey)
      : await callOpenAIResponses(messages, model, apiKey)
  } catch (error: unknown) {
    if (!isOpenAIEndpointMismatchError(error)) {
      throw error
    }
    return preferChat
      ? await callOpenAIResponses(messages, model, apiKey)
      : await callOpenAIChatCompletions(messages, model, apiKey)
  }
}
const callOpenAIChatCompletions = async (
  messages: Message[],
  model: string,
  apiKey: string,
): Promise<string> => {
  const payloadMessages = await Promise.all(messages.map(toOpenAIMessageAsync))
  const response = await fetch(OPENAI_CHAT_ENDPOINT, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model,
      temperature: 0.2,
      messages: payloadMessages,
    }),
  })
  if (!response.ok) {
    const details = await response.text()
    throw new Error(`OpenAI request failed with status ${response.status}: ${details}`)
  }
  const data = (await response.json()) as ChatCompletionResponse
  const rawContent = data.choices?.[0]?.message?.content
  const content =
    typeof rawContent === 'string'
      ? rawContent.trim()
      : rawContent
        ? rawContent
            .map((part) => part.text ?? '')
            .join('')
            .trim()
        : ''
  if (!content) {
    throw new Error('OpenAI response did not include assistant content.')
  }
  return content
}
const callOpenAIResponses = async (
  messages: Message[],
  model: string,
  apiKey: string,
): Promise<string> => {
  const input = await Promise.all(messages.map(toOpenAIResponsesInputMessageAsync))
  // Note: reasoning models can have parameter compatibility constraints.
  // Keep this payload minimal to avoid invalid requests.
  const response = await fetch(OPENAI_RESPONSES_ENDPOINT, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model,
      input,
    }),
  })
  if (!response.ok) {
    const details = await response.text()
    throw new Error(`OpenAI request failed with status ${response.status}: ${details}`)
  }
  const data = (await response.json()) as OpenAIResponsesResponse
  const content = extractOpenAIResponsesText(data)
  if (!content) {
    throw new Error('OpenAI response did not include assistant content.')
  }
  return content
}
const extractOpenAIResponsesText = (response: OpenAIResponsesResponse): string | null => {
  const direct = response.output_text?.trim()
  if (direct && direct.length > 0) return direct
  const output = response.output ?? []
  const assistantMessages = output.filter(
    (item): item is OpenAIResponsesOutputMessage =>
      item.type === 'message' && item.role === 'assistant',
  )
  const text = assistantMessages
    .flatMap((msg) => msg.content ?? [])
    .filter((part): part is OpenAIResponsesOutputText => part.type === 'output_text')
    .map((part) => (part.text ?? '').toString())
    .join('')
    .trim()
  return text.length > 0 ? text : null
}
type GeminiCallFailure = {
  ok: false
  status: number
  details: string
  apiVersion: GeminiApiVersion
}
type GeminiCallResult = { ok: true; content: string } | GeminiCallFailure
const callGeminiOnce = async (
  messages: Message[],
  model: string,
  apiKey: string,
  apiVersion: GeminiApiVersion,
): Promise<GeminiCallResult> => {
  const endpointBase = GEMINI_BASE_URL
  const normalizedVersion = normalizeGeminiApiVersion(apiVersion)
  const url = `${endpointBase}/${normalizedVersion}/models/${model}:generateContent?key=${apiKey}`
  const body = buildGeminiRequestBody(messages)
  const response = await fetch(url, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify(body),
  })
  if (!response.ok) {
    const details = await response.text()
    return { ok: false, status: response.status, details, apiVersion: normalizedVersion }
  }
  const data = (await response.json()) as GeminiResponse
  const content = extractGeminiText(data)
  if (!content) {
    throw new Error('Gemini response did not include text content.')
  }
  return { ok: true, content }
}
const messageHasGeminiFileParts = (content: MessageContent): boolean => {
  return typeof content !== 'string' && content.some((part) => 'fileUri' in part)
}
const requestHasGeminiFileParts = (messages: Message[]): boolean => {
  return messages.some((message) => messageHasGeminiFileParts(message.content))
}
const shouldRetryGeminiApiVersion = (status: number): boolean => {
  // Gemini models sometimes move between API versions.
  return status === 404
}
const callGemini = async (messages: Message[], model: string): Promise<string> => {
  const apiKey = process.env.GEMINI_API_KEY
  if (!apiKey) {
    throw new Error('GEMINI_API_KEY env var is not set.')
  }
  const wantsFileParts = requestHasGeminiFileParts(messages)
  const envVersion = normalizeGeminiApiVersion(GEMINI_API_VERSION)
  // `fileData`/`file_data` parts are only supported on some Gemini API versions.
  const primaryVersion: GeminiApiVersion = wantsFileParts ? 'v1beta' : envVersion
  const primary = await callGeminiOnce(messages, model, apiKey, primaryVersion)
  if (primary.ok) {
    return primary.content
  }
  if (shouldRetryGeminiApiVersion(primary.status)) {
    const fallbackVersion: GeminiApiVersion = primaryVersion === 'v1beta' ? 'v1' : 'v1beta'
    const fallback = await callGeminiOnce(messages, model, apiKey, fallbackVersion)
    if (fallback.ok) {
      return fallback.content
    }
    throw new Error(
      `Gemini (${fallback.apiVersion}) request failed with status ${fallback.status}: ${fallback.details}\n` +
        `Tried ${primary.apiVersion} first: ${primary.details}`,
    )
  }
  throw new Error(
    `Gemini (${primary.apiVersion}) request failed with status ${primary.status}: ${primary.details}`,
  )
}
const buildGeminiRequestBody = (messages: Message[]): GeminiRequestBody => {
  const systemMessages = messages.filter((message) => message.role === 'system')
  const contents: GeminiContent[] = messages
    .filter((message) => message.role !== 'system')
    .map((message) => {
      const role = message.role === 'user' ? 'user' : 'model'
      const parts = toGeminiParts(message.content)
      if (parts.length === 0) {
        parts.push({ text: '' })
      }
      return {
        role,
        parts,
      }
    })
  if (contents.length === 0) {
    throw new Error('Gemini requests require at least one user message.')
  }
  const payload: GeminiRequestBody = {
    contents,
    generationConfig: { temperature: 0.2 },
  }
  const systemParts = systemMessages.flatMap((message) => toGeminiParts(message.content))
  if (systemParts.length > 0) {
    payload.systemInstruction = {
      role: 'system',
      parts: systemParts,
    }
  }
  return payload
}
const extractGeminiText = (response: GeminiResponse): string | null => {
  const firstCandidate = response.candidates?.[0]
  const parts = firstCandidate?.content?.parts ?? []
  const text = parts
    .map((part) => ('text' in part ? (part.text ?? '') : ''))
    .join('')
    .trim()
  return text || null
}
const callOpenAIEmbedding = async (text: string, model: string): Promise<number[]> => {
  const apiKey = process.env.OPENAI_API_KEY
  if (!apiKey) {
    throw new Error('OPENAI_API_KEY env var is not set.')
  }
  const response = await fetch(OPENAI_EMBEDDING_ENDPOINT, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${apiKey}`,
    },
    body: JSON.stringify({ model, input: text }),
  })
  if (!response.ok) {
    const details = await response.text()
    throw new Error(`OpenAI embedding request failed with status ${response.status}: ${details}`)
  }
  const data = (await response.json()) as OpenAIEmbeddingResponse
  const embedding = data.data?.[0]?.embedding
  if (!embedding) {
    throw new Error('OpenAI embedding response did not include embedding values.')
  }
  return embedding
}
const callGeminiEmbedding = async (text: string, model: string): Promise<number[]> => {
  const apiKey = process.env.GEMINI_API_KEY
  if (!apiKey) {
    throw new Error('GEMINI_API_KEY env var is not set.')
  }
  const endpointBase = GEMINI_BASE_URL
  const url = `${endpointBase}/${GEMINI_API_VERSION}/models/${model}:embedContent?key=${apiKey}`
  const body = {
    content: {
      parts: [{ text }],
    },
  }
  const response = await fetch(url, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify(body),
  })
  if (!response.ok) {
    const details = await response.text()
    throw new Error(`Gemini embedding request failed with status ${response.status}: ${details}`)
  }
  const data = (await response.json()) as GeminiEmbeddingResponse
  const embedding = data.embedding?.value
  if (!embedding) {
    throw new Error('Gemini embedding response did not include embedding values.')
  }
  return embedding
}
const PDF_MAX_PAGES_ENV = 'PROMPT_MAKER_PDF_MAX_PAGES'
const PDF_MAX_TEXT_CHARS_ENV = 'PROMPT_MAKER_PDF_MAX_TEXT_CHARS'
const PDF_MAX_STREAMS_ENV = 'PROMPT_MAKER_PDF_MAX_STREAMS'
const DEFAULT_PDF_MAX_PAGES = 30
const DEFAULT_PDF_MAX_TEXT_CHARS = 200_000
const DEFAULT_PDF_MAX_STREAMS = 200
const parsePositiveIntegerEnv = (key: string, fallback: number): number => {
  const raw = process.env[key]
  if (!raw) return fallback
  const parsed = Number.parseInt(raw, 10)
  return Number.isFinite(parsed) && parsed > 0 ? parsed : fallback
}
type PdfExtractResult = { ok: true; text: string } | { ok: false; message: string }
type ParsedPdfString = { bytes: Uint8Array; nextIndex: number }
type PdfJsLoadingTask = { promise: Promise<unknown> }
type PdfJsModule = {
  getDocument: (options: unknown) => PdfJsLoadingTask
}
type PdfJsDocument = {
  numPages: number
  getPage: (pageNumber: number) => Promise<unknown>
}
type PdfJsPage = {
  getTextContent: () => Promise<unknown>
}
type PdfJsTextItem = { str?: unknown } & Record<string, unknown>
type PdfJsTextContent = {
  items?: unknown
}
const isPdfJsModule = (value: unknown): value is PdfJsModule => {
  if (!value || typeof value !== 'object') {
    return false
  }
  const maybe = value as { getDocument?: unknown }
  return typeof maybe.getDocument === 'function'
}
const isPdfJsDocument = (value: unknown): value is PdfJsDocument => {
  if (!value || typeof value !== 'object') return false
  const maybe = value as { numPages?: unknown; getPage?: unknown }
  return typeof maybe.numPages === 'number' && typeof maybe.getPage === 'function'
}
const isPdfJsPage = (value: unknown): value is PdfJsPage => {
  if (!value || typeof value !== 'object') return false
  return typeof (value as { getTextContent?: unknown }).getTextContent === 'function'
}
const isPdfJsTextContent = (value: unknown): value is PdfJsTextContent => {
  return Boolean(value) && typeof value === 'object'
}
const loadPdfJs = async (): Promise<PdfJsModule | null> => {
  try {
    const mod = (await import('pdfjs-dist/legacy/build/pdf.mjs')) as unknown
    return isPdfJsModule(mod) ? mod : null
  } catch {
    return null
  }
}
const normalizeExtractedText = (value: string): string => {
  return value
    .replace(/[\u0000-\u0008\u000b\u000c\u000e-\u001f]/g, ' ')
    .replace(/\s+/g, ' ')
    .trim()
}
const extractPdfTextWithPdfJs = async (filePath: string): Promise<PdfExtractResult> => {
  const pdfjs = await loadPdfJs()
  if (!pdfjs) {
    return { ok: false, message: 'PDF text extraction library is unavailable.' }
  }
  const maxPages = parsePositiveIntegerEnv(PDF_MAX_PAGES_ENV, DEFAULT_PDF_MAX_PAGES)
  const maxChars = parsePositiveIntegerEnv(PDF_MAX_TEXT_CHARS_ENV, DEFAULT_PDF_MAX_TEXT_CHARS)
  let buffer: Buffer
  try {
    buffer = await fs.readFile(filePath)
  } catch (error) {
    const message = error instanceof Error ? error.message : String(error)
    return { ok: false, message: `Unable to read PDF ${filePath}: ${message}` }
  }
  try {
    const bytes = new Uint8Array(buffer)
    const task = pdfjs.getDocument({ data: bytes, disableWorker: true })
    const doc = await task.promise
    if (!isPdfJsDocument(doc)) {
      return { ok: false, message: `PDF parser returned an unexpected document for ${filePath}.` }
    }
    const pageCount = Math.max(0, Math.min(doc.numPages, maxPages))
    const chunks: string[] = []
    let charsSoFar = 0
    for (let pageNumber = 1; pageNumber <= pageCount; pageNumber += 1) {
      const page = await doc.getPage(pageNumber)
      if (!isPdfJsPage(page)) {
        continue
      }
      const textContent = await page.getTextContent()
      if (!isPdfJsTextContent(textContent)) {
        continue
      }
      const items = (textContent as PdfJsTextContent).items
      if (!Array.isArray(items)) {
        continue
      }
      const pageStrings = items
        .map((item) => {
          const str = (item as PdfJsTextItem).str
          return typeof str === 'string' ? str : ''
        })
        .filter((value) => value.length > 0)
      const pageText = normalizeExtractedText(pageStrings.join(' '))
      if (!pageText) {
        continue
      }
      chunks.push(pageText)
      charsSoFar += pageText.length + 1
      if (charsSoFar >= maxChars) {
        break
      }
    }
    const merged = normalizeExtractedText(chunks.join('\n'))
    if (!merged) {
      return {
        ok: false,
        message:
          'Unable to extract text from PDF. If this is a scanned PDF, convert it to searchable text (OCR) or use a Gemini model for native PDF input.',
      }
    }
    return { ok: true, text: merged.slice(0, maxChars) }
  } catch (error) {
    const message = error instanceof Error ? error.message : String(error)
    return { ok: false, message: `Failed to parse PDF ${filePath}: ${message}` }
  }
}
const isWhitespaceByte = (byte: number): boolean => {
  return byte === 0x09 || byte === 0x0a || byte === 0x0d || byte === 0x20
}
const decodeUtf16Be = (bytes: Uint8Array): string => {
  const length = bytes.length
  const start = length >= 2 && bytes[0] === 0xfe && bytes[1] === 0xff ? 2 : 0
  const codeUnits: number[] = []
  for (let i = start; i + 1 < length; i += 2) {
    const high = bytes[i] ?? 0
    const low = bytes[i + 1] ?? 0
    codeUnits.push((high << 8) | low)
  }
  return String.fromCharCode(...codeUnits)
}
const decodePdfStringBytes = (bytes: Uint8Array): string => {
  if (bytes.length >= 2 && bytes[0] === 0xfe && bytes[1] === 0xff) {
    return decodeUtf16Be(bytes)
  }
  // PDFDocEncoding is not implemented here; latin1 is a pragmatic fallback.
  return Buffer.from(bytes).toString('latin1')
}
const readPdfLiteralString = (data: string, startIndex: number): ParsedPdfString | null => {
  if (data[startIndex] !== '(') return null
  const bytes: number[] = []
  let depth = 1
  let i = startIndex + 1
  while (i < data.length) {
    const ch = data[i]
    if (ch === undefined) break
    if (ch === '\\') {
      const next = data[i + 1]
      if (next === undefined) {
        i += 1
        continue
      }
      // Line continuation
      if (next === '\n') {
        i += 2
        continue
      }
      if (next === '\r') {
        if (data[i + 2] === '\n') {
          i += 3
        } else {
          i += 2
        }
        continue
      }
      const octalMatch = data.slice(i + 1, i + 4).match(/^[0-7]{1,3}/)
      if (octalMatch) {
        bytes.push(Number.parseInt(octalMatch[0], 8))
        i += 1 + octalMatch[0].length
        continue
      }
      const mapped =
        next === 'n'
          ? 0x0a
          : next === 'r'
            ? 0x0d
            : next === 't'
              ? 0x09
              : next === 'b'
                ? 0x08
                : next === 'f'
                  ? 0x0c
                  : next.charCodeAt(0)
      bytes.push(mapped)
      i += 2
      continue
    }
    if (ch === '(') {
      depth += 1
      bytes.push(ch.charCodeAt(0))
      i += 1
      continue
    }
    if (ch === ')') {
      depth -= 1
      if (depth === 0) {
        return { bytes: Uint8Array.from(bytes), nextIndex: i + 1 }
      }
      bytes.push(ch.charCodeAt(0))
      i += 1
      continue
    }
    bytes.push(ch.charCodeAt(0))
    i += 1
  }
  return null
}
const readPdfHexString = (data: string, startIndex: number): ParsedPdfString | null => {
  if (data[startIndex] !== '<') return null
  if (data[startIndex + 1] === '<') return null
  let i = startIndex + 1
  const hexChars: string[] = []
  while (i < data.length) {
    const ch = data[i]
    if (ch === undefined) break
    if (ch === '>') {
      i += 1
      break
    }
    if (ch === ' ' || ch === '\n' || ch === '\r' || ch === '\t' || ch === '\f') {
      i += 1
      continue
    }
    hexChars.push(ch)
    i += 1
  }
  const hex = hexChars.join('')
  if (hex.length === 0) return null
  const normalized = hex.length % 2 === 1 ? `${hex}0` : hex
  const bytes: number[] = []
  for (let idx = 0; idx + 1 < normalized.length; idx += 2) {
    const byte = Number.parseInt(normalized.slice(idx, idx + 2), 16)
    if (!Number.isFinite(byte)) {
      return null
    }
    bytes.push(byte)
  }
  return { bytes: Uint8Array.from(bytes), nextIndex: i }
}
const readOperator = (data: string, startIndex: number): { op: string; nextIndex: number } => {
  let i = startIndex
  while (i < data.length) {
    const byte = data.charCodeAt(i)
    if (!isWhitespaceByte(byte)) break
    i += 1
  }
  const ch = data[i]
  if (ch === undefined) {
    return { op: '', nextIndex: i }
  }
  if (ch === "'" || ch === '"') {
    return { op: ch, nextIndex: i + 1 }
  }
  let end = i
  while (end < data.length) {
    const b = data.charCodeAt(end)
    const isAlpha = (b >= 0x41 && b <= 0x5a) || (b >= 0x61 && b <= 0x7a)
    if (!isAlpha) break
    end += 1
  }
  return { op: data.slice(i, end), nextIndex: end }
}
const extractTextFromContentStream = (data: string): string[] => {
  const out: string[] = []
  let i = 0
  while (i < data.length) {
    const ch = data[i]
    if (ch === '(') {
      const parsed = readPdfLiteralString(data, i)
      if (!parsed) {
        i += 1
        continue
      }
      const { op, nextIndex } = readOperator(data, parsed.nextIndex)
      if (op === 'Tj' || op === "'" || op === '"') {
        const text = normalizeExtractedText(decodePdfStringBytes(parsed.bytes))
        if (text) out.push(text)
        i = nextIndex
        continue
      }
      i = parsed.nextIndex
      continue
    }
    if (ch === '[') {
      const pieces: string[] = []
      i += 1
      while (i < data.length && data[i] !== ']') {
        const inner = data[i]
        if (inner === '(') {
          const parsed = readPdfLiteralString(data, i)
          if (!parsed) {
            i += 1
            continue
          }
          const text = normalizeExtractedText(decodePdfStringBytes(parsed.bytes))
          if (text) pieces.push(text)
          i = parsed.nextIndex
          continue
        }
        if (inner === '<') {
          const parsed = readPdfHexString(data, i)
          if (!parsed) {
            i += 1
            continue
          }
          const text = normalizeExtractedText(decodePdfStringBytes(parsed.bytes))
          if (text) pieces.push(text)
          i = parsed.nextIndex
          continue
        }
        i += 1
      }
      if (data[i] === ']') {
        const { op, nextIndex } = readOperator(data, i + 1)
        if (op === 'TJ') {
          const combined = normalizeExtractedText(pieces.join(' '))
          if (combined) out.push(combined)
          i = nextIndex
          continue
        }
      }
      i += 1
      continue
    }
    i += 1
  }
  return out
}
const extractPdfTextFromBuffer = (buffer: Buffer): PdfExtractResult => {
  const maxStreams = parsePositiveIntegerEnv(PDF_MAX_STREAMS_ENV, DEFAULT_PDF_MAX_STREAMS)
  const maxChars = parsePositiveIntegerEnv(PDF_MAX_TEXT_CHARS_ENV, DEFAULT_PDF_MAX_TEXT_CHARS)
  const pieces: string[] = []
  let searchIndex = 0
  let streamsSeen = 0
  while (streamsSeen < maxStreams) {
    const streamIndex = buffer.indexOf('stream', searchIndex)
    if (streamIndex < 0) break
    streamsSeen += 1
    const dictStart = Math.max(0, streamIndex - 2048)
    const dictText = buffer.subarray(dictStart, streamIndex).toString('latin1')
    const isFlate = /\/FlateDecode\b/.test(dictText)
    let dataStart = streamIndex + 'stream'.length
    while (dataStart < buffer.length) {
      const byte = buffer[dataStart]
      if (byte === 0x0a) {
        dataStart += 1
        break
      }
      if (byte === 0x0d) {
        if (buffer[dataStart + 1] === 0x0a) {
          dataStart += 2
        } else {
          dataStart += 1
        }
        break
      }
      if (byte === 0x20 || byte === 0x09) {
        dataStart += 1
        continue
      }
      // Unexpected; bail out of this stream.
      break
    }
    const endIndex = buffer.indexOf('endstream', dataStart)
    if (endIndex < 0) break
    const rawStream = buffer.subarray(dataStart, endIndex)
    let contentBytes: Buffer = rawStream
    if (isFlate) {
      try {
        contentBytes = inflateSync(rawStream)
      } catch {
        contentBytes = rawStream
      }
    }
    const streamText = contentBytes.toString('latin1')
    let lengthSoFar = pieces.reduce((sum, part) => sum + part.length + 1, 0)
    for (const extracted of extractTextFromContentStream(streamText)) {
      if (!extracted) {
        continue
      }
      pieces.push(extracted)
      lengthSoFar += extracted.length + 1
      if (lengthSoFar >= maxChars) {
        break
      }
    }
    if (lengthSoFar >= maxChars) {
      break
    }
    searchIndex = endIndex + 'endstream'.length
  }
  const merged = normalizeExtractedText(pieces.join('\n'))
  if (!merged) {
    return {
      ok: false,
      message:
        'Unable to extract text from PDF. If this is a scanned PDF, convert it to searchable text (OCR) or use a Gemini model for native PDF input.',
    }
  }
  return { ok: true, text: merged.slice(0, maxChars) }
}
const extractPdfTextFromFile = async (filePath: string): Promise<PdfExtractResult> => {
  const pdfJsResult = await extractPdfTextWithPdfJs(filePath)
  if (pdfJsResult.ok) {
    return pdfJsResult
  }
  try {
    const buffer = await fs.readFile(filePath)
    return extractPdfTextFromBuffer(buffer)
  } catch (error) {
    const message = error instanceof Error ? error.message : String(error)
    return { ok: false, message: `Unable to read PDF ${filePath}: ${message}` }
  }
}
const isVideoPart = (part: TextPart | ImagePart | VideoPart | PdfPart): part is VideoPart => {
  return part.type === 'video_uri'
}
const isPdfPart = (part: TextPart | ImagePart | VideoPart | PdfPart): part is PdfPart => {
  return part.type === 'pdf'
}
const toOpenAIMessageAsync = async (message: Message): Promise<OpenAIChatCompletionMessage> => ({
  role: message.role,
  content: await toOpenAIContentAsync(message.content),
})
const toOpenAIContentAsync = async (content: MessageContent): Promise<OpenAIChatMessageContent> => {
  if (typeof content === 'string') {
    return content
  }
  const hasVideo = content.some((part) => isVideoPart(part))
  if (hasVideo) {
    throw new Error(
      'Video inputs are only supported when using Gemini models. Remove --video or switch to a Gemini model.',
    )
  }
  const isAllText = content.every((part) => part.type === 'text')
  if (isAllText) {
    return content.map((part) => ('text' in part ? part.text : '')).join('')
  }
  const parts: Array<
    { type: 'text'; text: string } | { type: 'image_url'; image_url: { url: string } }
  > = []
  for (const part of content) {
    if (part.type === 'text') {
      parts.push({ type: 'text', text: part.text })
      continue
    }
    if (part.type === 'image') {
      parts.push({
        type: 'image_url',
        image_url: { url: `data:${part.mimeType};base64,${part.data}` },
      })
      continue
    }
    if (isPdfPart(part)) {
      const extracted = await extractPdfTextFromFile(part.filePath)
      if (extracted.ok) {
        const label = path.basename(part.filePath)
        parts.push({ type: 'text', text: `PDF (${label}):\n${extracted.text}` })
      } else {
        parts.push({ type: 'text', text: extracted.message })
      }
      continue
    }
    parts.push({ type: 'text', text: '' })
  }
  return parts
}
const toOpenAIResponsesInputMessageAsync = async (
  message: Message,
): Promise<OpenAIResponsesInputMessage> => ({
  role: message.role === 'system' ? 'developer' : message.role,
  content: await toOpenAIResponsesContentAsync(message.content),
})
const toOpenAIResponsesContentAsync = async (
  content: MessageContent,
): Promise<OpenAIResponsesInputContent> => {
  if (typeof content === 'string') {
    return content
  }
  const hasVideo = content.some((part) => isVideoPart(part))
  if (hasVideo) {
    throw new Error(
      'Video inputs are only supported when using Gemini models. Remove --video or switch to a Gemini model.',
    )
  }
  const isAllText = content.every((part) => part.type === 'text')
  if (isAllText) {
    return content.map((part) => ('text' in part ? part.text : '')).join('')
  }
  const parts: Array<OpenAIResponsesInputText | OpenAIResponsesInputImage> = []
  for (const part of content) {
    if (part.type === 'text') {
      parts.push({ type: 'input_text', text: part.text })
      continue
    }
    if (part.type === 'image') {
      parts.push({
        type: 'input_image',
        image_url: `data:${part.mimeType};base64,${part.data}`,
      })
      continue
    }
    if (isPdfPart(part)) {
      const extracted = await extractPdfTextFromFile(part.filePath)
      if (extracted.ok) {
        const label = path.basename(part.filePath)
        parts.push({ type: 'input_text', text: `PDF (${label}):\n${extracted.text}` })
      } else {
        parts.push({ type: 'input_text', text: extracted.message })
      }
      continue
    }
    parts.push({ type: 'input_text', text: '' })
  }
  return parts
}
const toGeminiParts = (content: MessageContent): GeminiContentPart[] => {
  if (typeof content === 'string') {
    return content ? [{ text: content }] : []
  }
  return content.map((part) => {
    if (part.type === 'text') {
      return { text: part.text }
    }
    if (part.type === 'image') {
      return { inlineData: { mimeType: part.mimeType, data: part.data } }
    }
    if (part.type === 'video_uri') {
      return { fileData: { mimeType: part.mimeType, fileUri: part.fileUri } }
    }
    if (part.type === 'pdf') {
      if (!part.fileUri) {
        throw new Error(
          `PDF attachment ${part.filePath} is missing a Gemini fileUri. ` +
            'Upload the PDF via the Gemini Files API before calling Gemini models.',
        )
      }
      return { fileData: { mimeType: part.mimeType, fileUri: part.fileUri } }
    }
    return { text: '' }
  })
}
</file>
<file path="src/__tests__/popup-reducer.test.ts">
import type { PopupState } from '../tui/types'
import { popupReducer, type PopupAction, type PopupManagerState } from '../tui/popup-reducer'
describe('popupReducer', () => {
  const reduce = (state: PopupManagerState, action: PopupAction): PopupManagerState =>
    popupReducer(state, action)
  const initialState = (): PopupManagerState => ({ popupState: null, activeScan: null })
  it('opens and closes popups explicitly', () => {
    const opened = reduce(initialState(), { type: 'open-tokens' })
    expect(opened.popupState).toEqual({ type: 'tokens' })
    const closed = reduce(opened, { type: 'close' })
    expect(closed).toEqual({ popupState: null, activeScan: null })
  })
  it('opens theme popup with initial name', () => {
    const opened = reduce(initialState(), {
      type: 'open-theme',
      selectionIndex: 2,
      initialThemeName: 'pm-dark',
    })
    expect(opened.popupState).toEqual({
      type: 'theme',
      selectionIndex: 2,
      initialThemeName: 'pm-dark',
    })
    expect(opened.activeScan).toBeNull()
  })
  it('opens theme mode popup with initial mode', () => {
    const opened = reduce(initialState(), {
      type: 'open-theme-mode',
      selectionIndex: 1,
      initialMode: 'dark',
    })
    expect(opened.popupState).toEqual({
      type: 'themeMode',
      selectionIndex: 1,
      initialMode: 'dark',
    })
    expect(opened.activeScan).toBeNull()
  })
  it('opens URL popup', () => {
    const opened = reduce(initialState(), { type: 'open-url' })
    expect(opened.popupState).toEqual({
      type: 'url',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      editingIndex: null,
    })
    expect(opened.activeScan).toBeNull()
  })
  it('opens image/video popups with scan state', () => {
    const afterImage = reduce(initialState(), { type: 'open-image', scanId: 1 })
    expect(afterImage.popupState).toEqual({
      type: 'image',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(afterImage.activeScan).toEqual({ kind: 'image', id: 1 })
    const afterVideo = reduce(afterImage, { type: 'open-video', scanId: 2 })
    expect(afterVideo.popupState).toEqual({
      type: 'video',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(afterVideo.activeScan).toEqual({ kind: 'video', id: 2 })
  })
  it('switching popups clears scan state', () => {
    const afterImage = reduce(initialState(), { type: 'open-image', scanId: 1 })
    expect(afterImage.popupState).toEqual({
      type: 'image',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(afterImage.activeScan).toEqual({ kind: 'image', id: 1 })
    const afterVideo = reduce(afterImage, { type: 'open-video', scanId: 2 })
    expect(afterVideo.popupState).toEqual({
      type: 'video',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(afterVideo.activeScan).toEqual({ kind: 'video', id: 2 })
  })
  it('switching popups clears scan state', () => {
    const afterFile = reduce(initialState(), { type: 'open-file', scanId: 1 })
    expect(afterFile.popupState).toEqual({
      type: 'file',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(afterFile.activeScan).toEqual({ kind: 'file', id: 1 })
    const afterSmart = reduce(afterFile, { type: 'open-smart', scanId: 2, draft: 'src' })
    expect(afterSmart.popupState).toEqual({
      type: 'smart',
      draft: 'src',
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(afterSmart.activeScan).toEqual({ kind: 'smart', id: 2 })
  })
  it('ignores stale async scan results', () => {
    const afterFile = reduce(initialState(), { type: 'open-file', scanId: 1 })
    const afterSmart = reduce(afterFile, { type: 'open-smart', scanId: 2, draft: '' })
    // A file scan resolving after the user switched popups must be ignored.
    const staleApplied = reduce(afterSmart, {
      type: 'scan-suggestions-success',
      kind: 'file',
      scanId: 1,
      suggestions: ['README.md'],
    })
    expect(staleApplied.popupState).toEqual(afterSmart.popupState)
    expect(staleApplied.activeScan).toEqual(afterSmart.activeScan)
  })
  it('applies scan results when popup and scanId match', () => {
    const afterFile = reduce(initialState(), { type: 'open-file', scanId: 1 })
    const applied = reduce(afterFile, {
      type: 'scan-suggestions-success',
      kind: 'file',
      scanId: 1,
      suggestions: ['src/index.ts', 'README.md'],
    })
    expect(applied.popupState).toEqual({
      type: 'file',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: ['src/index.ts', 'README.md'],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(applied.activeScan).toBeNull()
  })
  it('preserves active scan across same-type set updates', () => {
    const afterFile = reduce(initialState(), { type: 'open-file', scanId: 1 })
    const updated = reduce(afterFile, {
      type: 'set',
      next: (prev: PopupState) => (prev?.type === 'file' ? { ...prev, draft: 'x' } : prev),
    })
    expect(updated.popupState).toEqual({
      type: 'file',
      draft: 'x',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(updated.activeScan).toEqual({ kind: 'file', id: 1 })
  })
})
</file>
<file path="src/generate/payload-io.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import yaml from 'js-yaml'
import { GENERATE_JSON_PAYLOAD_SCHEMA_VERSION, type GenerateJsonPayload } from './types'
export type PayloadFormat = 'json' | 'yaml'
type JsonRecord = Record<string, unknown>
const isRecord = (value: unknown): value is JsonRecord =>
  typeof value === 'object' && value !== null && !Array.isArray(value)
const isStringArray = (value: unknown): value is string[] =>
  Array.isArray(value) && value.every((entry) => typeof entry === 'string')
const isContextPaths = (value: unknown): value is GenerateJsonPayload['contextPaths'] =>
  Array.isArray(value) &&
  value.every(
    (entry) =>
      isRecord(entry) && typeof entry.path === 'string' && typeof entry.source === 'string',
  )
const isGenerateJsonPayload = (value: unknown): value is GenerateJsonPayload => {
  if (!isRecord(value)) {
    return false
  }
  const hasMediaLists =
    (value.images === undefined || isStringArray(value.images)) &&
    (value.videos === undefined || isStringArray(value.videos)) &&
    (value.pdfs === undefined || isStringArray(value.pdfs))
  return (
    value.schemaVersion === GENERATE_JSON_PAYLOAD_SCHEMA_VERSION &&
    typeof value.intent === 'string' &&
    typeof value.model === 'string' &&
    typeof value.targetModel === 'string' &&
    typeof value.prompt === 'string' &&
    isStringArray(value.refinements) &&
    typeof value.iterations === 'number' &&
    Number.isFinite(value.iterations) &&
    typeof value.interactive === 'boolean' &&
    typeof value.timestamp === 'string' &&
    isContextPaths(value.contextPaths) &&
    hasMediaLists
  )
}
const getPayloadFormatForPath = (filePath: string): PayloadFormat => {
  const ext = path.extname(filePath).toLowerCase()
  if (ext === '.json') {
    return 'json'
  }
  if (ext === '.yaml' || ext === '.yml') {
    return 'yaml'
  }
  throw new Error(
    `Unsupported payload file extension for ${formatDisplayPath(filePath)}. Expected .json, .yaml, or .yml.`,
  )
}
const isMissingFileError = (error: unknown): error is { code: string } =>
  typeof error === 'object' &&
  error !== null &&
  'code' in error &&
  (error as { code?: unknown }).code === 'ENOENT'
const formatDisplayPath = (filePath: string): string => {
  const relative = path.relative(process.cwd(), filePath)
  return relative && !relative.startsWith('..') ? relative : filePath
}
const parseGeneratePayload = (raw: string, format: PayloadFormat, filePath: string): unknown => {
  if (format === 'json') {
    try {
      return JSON.parse(raw) as unknown
    } catch (error) {
      const message = error instanceof Error ? error.message : 'Unknown JSON error.'
      throw new Error(`Failed to parse JSON in ${formatDisplayPath(filePath)}: ${message}`)
    }
  }
  try {
    return yaml.load(raw)
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown YAML error.'
    throw new Error(`Failed to parse YAML in ${formatDisplayPath(filePath)}: ${message}`)
  }
}
export const validateGeneratePayloadObject = (
  value: unknown,
  sourceLabel: string,
): GenerateJsonPayload => {
  if (!isGenerateJsonPayload(value)) {
    throw new Error(
      `Invalid generate payload in ${sourceLabel}. Expected schemaVersion=${GENERATE_JSON_PAYLOAD_SCHEMA_VERSION} and required fields (intent, model, targetModel, prompt, refinements, iterations, interactive, timestamp, contextPaths).`,
    )
  }
  return value
}
export const loadGeneratePayloadFromFile = async (
  filePath: string,
): Promise<GenerateJsonPayload> => {
  let raw: string
  try {
    raw = await fs.readFile(filePath, 'utf8')
  } catch (error) {
    if (isMissingFileError(error)) {
      throw new Error(`Payload file not found: ${formatDisplayPath(filePath)}`)
    }
    const message = error instanceof Error ? error.message : 'Unknown file read error.'
    throw new Error(`Failed to read payload file ${formatDisplayPath(filePath)}: ${message}`)
  }
  const format = getPayloadFormatForPath(filePath)
  const parsed = parseGeneratePayload(raw, format, filePath)
  return validateGeneratePayloadObject(parsed, formatDisplayPath(filePath))
}
export const serializeGeneratePayload = (
  payload: GenerateJsonPayload,
  format: PayloadFormat,
): string => {
  if (format === 'json') {
    return `${JSON.stringify(payload, null, 2)}\n`
  }
  const dumped = yaml.dump(payload, {
    sortKeys: true,
    noRefs: true,
  })
  return dumped.endsWith('\n') ? dumped : `${dumped}\n`
}
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenContextPopupBindings.ts">
import type { PopupState } from '../../../types'
import { useContextPopupGlue } from './useContextPopupGlue'
import { useDroppedFileDetection } from './useDroppedFileDetection'
export type UseCommandScreenContextPopupBindingsOptions = {
  inputValue: string
  popupState: PopupState
  helpOpen: boolean
  isPopupOpen: boolean
  isCommandMode: boolean
  isCommandMenuActive: boolean
  isGenerating: boolean
  droppedFilePath: string | null
  files: string[]
  urls: string[]
  images: string[]
  videos: string[]
  pdfs: string[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  addFile: (value: string) => void
  removeFile: (index: number) => void
  addUrl: (value: string) => void
  removeUrl: (index: number) => void
  updateUrl: (index: number, value: string) => void
  addImage: (value: string) => void
  removeImage: (index: number) => void
  addVideo: (value: string) => void
  removeVideo: (index: number) => void
  addPdf: (value: string) => void
  removePdf: (index: number) => void
  toggleSmartContext: () => void
  setSmartRoot: (value: string) => void
  setInputValue: (value: string | ((prev: string) => string)) => void
  setPopupState: import('react').Dispatch<import('react').SetStateAction<PopupState>>
  suppressNextInput: () => void
  notify: (message: string) => void
  pushHistory: (content: string, kind?: 'system' | 'user' | 'progress') => void
  addCommandHistoryEntry: (value: string) => void
  handleCommandSelection: (
    commandId: import('../../../types').CommandDescriptor['id'],
    argsRaw?: string,
  ) => void
  consumeSuppressedTextInputChange: () => boolean
}
export type UseCommandScreenContextPopupBindingsResult = ReturnType<typeof useContextPopupGlue>
export const useCommandScreenContextPopupBindings = ({
  inputValue,
  popupState,
  helpOpen,
  isPopupOpen,
  isCommandMode,
  isCommandMenuActive,
  isGenerating,
  droppedFilePath,
  files,
  urls,
  images,
  videos,
  pdfs,
  smartContextEnabled,
  smartContextRoot,
  addFile,
  removeFile,
  addUrl,
  removeUrl,
  updateUrl,
  addImage,
  removeImage,
  addVideo,
  removeVideo,
  addPdf,
  removePdf,
  toggleSmartContext,
  setSmartRoot,
  setInputValue,
  setPopupState,
  suppressNextInput,
  notify,
  pushHistory,
  addCommandHistoryEntry,
  handleCommandSelection,
  consumeSuppressedTextInputChange,
}: UseCommandScreenContextPopupBindingsOptions): UseCommandScreenContextPopupBindingsResult => {
  const popupDraftInput =
    popupState?.type === 'file' ||
    popupState?.type === 'image' ||
    popupState?.type === 'video' ||
    popupState?.type === 'pdf'
      ? popupState.draft
      : ''
  const { isFilePath } = useDroppedFileDetection(popupDraftInput)
  return useContextPopupGlue({
    inputValue,
    popupState,
    helpOpen,
    isPopupOpen,
    isCommandMode,
    isCommandMenuActive,
    isGenerating,
    droppedFilePath,
    files,
    urls,
    images,
    videos,
    pdfs,
    smartContextEnabled,
    smartContextRoot,
    addFile,
    removeFile,
    addUrl,
    removeUrl,
    updateUrl,
    addImage,
    removeImage,
    addVideo,
    removeVideo,
    addPdf,
    removePdf,
    toggleSmartContext,
    setSmartRoot,
    setInputValue,
    setPopupState,
    suppressNextInput,
    notify,
    pushHistory,
    addCommandHistoryEntry,
    handleCommandSelection,
    consumeSuppressedTextInputChange,
    isFilePath,
  })
}
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenInputState.ts">
import type { MutableRefObject } from 'react'
import { useCallback, useMemo, useRef } from 'react'
import type { DebugKeyEvent } from '../../../components/core/MultilineTextInput'
import { isCommandInput } from '../../../drag-drop-path'
import type { HistoryEntry } from '../../../types'
import { useDroppedFileDetection } from './useDroppedFileDetection'
import { useCommandScreen } from '../useCommandScreen'
import { formatDebugKeyEvent } from '../utils/debug-keys'
export type UseCommandScreenInputStateOptions = {
  pushHistoryProxy: (
    content: string,
    kind?: HistoryEntry['kind'],
    format?: HistoryEntry['format'],
  ) => void
}
export type UseCommandScreenInputStateResult = {
  // Screen state
  terminalRows: number
  terminalColumns: number
  inputValue: string
  isPasteActive: boolean
  commandSelectionIndex: number
  debugKeyLine: string | null
  // Setters
  setTerminalSize: (rows: number, columns: number) => void
  setInputValue: (value: string | ((prev: string) => string)) => void
  setPasteActive: (active: boolean) => void
  setCommandSelectionIndex: (next: number | ((prev: number) => number)) => void
  setDebugKeyLine: (value: string) => void
  // Local toggles/state
  intentFilePath: string
  setIntentFilePath: (value: string) => void
  copyEnabled: boolean
  setCopyEnabled: (value: boolean) => void
  chatGptEnabled: boolean
  setChatGptEnabled: (value: boolean) => void
  jsonOutputEnabled: boolean
  setJsonOutputEnabled: (value: boolean) => void
  // Intent refs
  lastUserIntentRef: MutableRefObject<string | null>
  lastTypedIntentRef: MutableRefObject<string>
  // Text input suppression
  consumeSuppressedTextInputChange: () => boolean
  suppressNextInput: () => void
  // Debug
  debugKeysEnabled: boolean
  onDebugKeyEvent: (event: DebugKeyEvent) => void
  // Helpers
  updateLastTypedIntent: (next: string) => void
}
export const useCommandScreenInputState = ({
  pushHistoryProxy: _pushHistoryProxy,
}: UseCommandScreenInputStateOptions): UseCommandScreenInputStateResult => {
  const {
    state: screenState,
    setTerminalSize,
    setInputValue,
    setPasteActive,
    setCommandSelectionIndex,
    setDebugKeyLine,
    setIntentFilePath: setIntentFilePathState,
    setCopyEnabled: setCopyEnabledState,
    setChatGptEnabled: setChatGptEnabledState,
    setJsonOutputEnabled: setJsonOutputEnabledState,
  } = useCommandScreen()
  const terminalRows = screenState.terminalRows
  const terminalColumns = screenState.terminalColumns
  const debugKeyLine = screenState.debugKeyLine
  const inputValue = screenState.inputValue
  const isPasteActive = screenState.isPasteActive
  const commandSelectionIndex = screenState.commandSelectionIndex
  const lastUserIntentRef = useRef<string | null>(null)
  const lastTypedIntentRef = useRef<string>('')
  const debugKeysEnabled = useMemo(() => {
    const value = process.env.PROMPT_MAKER_DEBUG_KEYS
    if (!value) {
      return false
    }
    const normalized = value.trim().toLowerCase()
    return normalized !== '0' && normalized !== 'false'
  }, [])
  const onDebugKeyEvent = useCallback(
    (event: DebugKeyEvent): void => {
      if (!debugKeysEnabled) {
        return
      }
      setDebugKeyLine(formatDebugKeyEvent(event))
    },
    [debugKeysEnabled, setDebugKeyLine],
  )
  const intentFilePath = screenState.intentFilePath
  const copyEnabled = screenState.copyEnabled
  const chatGptEnabled = screenState.chatGptEnabled
  const jsonOutputEnabled = screenState.jsonOutputEnabled
  const setIntentFilePath = useCallback(
    (value: string) => {
      setIntentFilePathState(value)
    },
    [setIntentFilePathState],
  )
  const setCopyEnabled = useCallback(
    (value: boolean) => {
      setCopyEnabledState(value)
    },
    [setCopyEnabledState],
  )
  const setChatGptEnabled = useCallback(
    (value: boolean) => {
      setChatGptEnabledState(value)
    },
    [setChatGptEnabledState],
  )
  const setJsonOutputEnabled = useCallback(
    (value: boolean) => {
      setJsonOutputEnabledState(value)
    },
    [setJsonOutputEnabledState],
  )
  const suppressNextInputRef = useRef(false)
  const consumeSuppressedTextInputChange = useCallback((): boolean => {
    if (!suppressNextInputRef.current) {
      return false
    }
    suppressNextInputRef.current = false
    return true
  }, [])
  const suppressNextInput = useCallback(() => {
    suppressNextInputRef.current = true
  }, [])
  const { existsSync } = useDroppedFileDetection(inputValue)
  const updateLastTypedIntent = useCallback(
    (next: string): void => {
      if (isCommandInput(next, existsSync)) {
        return
      }
      lastTypedIntentRef.current = next
    },
    [existsSync],
  )
  return {
    terminalRows,
    terminalColumns,
    inputValue,
    isPasteActive,
    commandSelectionIndex,
    debugKeyLine,
    setTerminalSize,
    setInputValue,
    setPasteActive,
    setCommandSelectionIndex,
    setDebugKeyLine,
    intentFilePath,
    setIntentFilePath,
    copyEnabled,
    setCopyEnabled,
    chatGptEnabled,
    setChatGptEnabled,
    jsonOutputEnabled,
    setJsonOutputEnabled,
    lastUserIntentRef,
    lastTypedIntentRef,
    consumeSuppressedTextInputChange,
    suppressNextInput,
    debugKeysEnabled,
    onDebugKeyEvent,
    updateLastTypedIntent,
  }
}
</file>
<file path="src/tui/screens/command/hooks/useMiscPopupDraftHandlers.ts">
import { useCallback } from 'react'
import type { PopupState } from '../../../types'
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
type ConsumeSuppressedTextInputChange = () => boolean
export type UseMiscPopupDraftHandlersOptions = {
  setPopupState: SetPopupState
  consumeSuppressedTextInputChange: ConsumeSuppressedTextInputChange
}
export type UseMiscPopupDraftHandlersResult = {
  onModelPopupQueryChange: (next: string) => void
  onSeriesDraftChange: (next: string) => void
  onInstructionsDraftChange: (next: string) => void
  onTestDraftChange: (next: string) => void
  onBudgetsMaxContextTokensDraftChange: (next: string) => void
  onBudgetsMaxInputTokensDraftChange: (next: string) => void
  onResumePayloadPathDraftChange: (next: string) => void
  onExportOutPathDraftChange: (next: string) => void
}
export const useMiscPopupDraftHandlers = ({
  setPopupState,
  consumeSuppressedTextInputChange,
}: UseMiscPopupDraftHandlersOptions): UseMiscPopupDraftHandlersResult => {
  const onModelPopupQueryChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) =>
        prev?.type === 'model' ? { ...prev, query: next, selectionIndex: 0 } : prev,
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  const onSeriesDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) => (prev?.type === 'series' ? { ...prev, draft: next } : prev))
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  const onInstructionsDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) => (prev?.type === 'instructions' ? { ...prev, draft: next } : prev))
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  const onTestDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) => (prev?.type === 'test' ? { ...prev, draft: next } : prev))
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  const onBudgetsMaxContextTokensDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) =>
        prev?.type === 'budgets'
          ? { ...prev, maxContextTokensDraft: next, errorMessage: null }
          : prev,
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  const onBudgetsMaxInputTokensDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) =>
        prev?.type === 'budgets'
          ? { ...prev, maxInputTokensDraft: next, errorMessage: null }
          : prev,
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  const onResumePayloadPathDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) =>
        prev?.type === 'resume'
          ? {
              ...prev,
              payloadPathDraft: next,
              suggestedFocused: false,
              suggestedSelectionIndex: 0,
            }
          : prev,
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  const onExportOutPathDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) => (prev?.type === 'export' ? { ...prev, outPathDraft: next } : prev))
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  return {
    onModelPopupQueryChange,
    onSeriesDraftChange,
    onInstructionsDraftChange,
    onTestDraftChange,
    onBudgetsMaxContextTokensDraftChange,
    onBudgetsMaxInputTokensDraftChange,
    onResumePayloadPathDraftChange,
    onExportOutPathDraftChange,
  }
}
</file>
<file path="src/tui/context-store.ts">
import { createContext, useContext } from 'react'
import type { ContextOverflowStrategy } from '../config'
export type ContextSourceState = {
  files: string[]
  urls: string[]
  images: string[]
  videos: string[]
  pdfs: string[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  metaInstructions: string
  maxContextTokens: number | null
  maxInputTokens: number | null
  contextOverflowStrategy: ContextOverflowStrategy | null
  lastReasoning: string | null
  lastGeneratedPrompt: string | null
}
export type ContextDispatch = {
  addFile: (value: string) => void
  removeFile: (index: number) => void
  addUrl: (value: string) => void
  removeUrl: (index: number) => void
  updateUrl: (index: number, value: string) => void
  addImage: (value: string) => void
  removeImage: (index: number) => void
  addVideo: (value: string) => void
  removeVideo: (index: number) => void
  addPdf: (value: string) => void
  removePdf: (index: number) => void
  toggleSmartContext: () => void
  setSmartRoot: (value: string) => void
  setMetaInstructions: (value: string) => void
  setBudgets: (value: {
    maxContextTokens: number | null
    maxInputTokens: number | null
    contextOverflowStrategy: ContextOverflowStrategy | null
  }) => void
  setLastReasoning: (value: string | null) => void
  setLastGeneratedPrompt: (value: string | null) => void
  resetContext: () => void
}
export const ContextStateContext = createContext<ContextSourceState | null>(null)
export const ContextDispatchContext = createContext<ContextDispatch | null>(null)
export const useContextState = (): ContextSourceState => {
  const context = useContext(ContextStateContext)
  if (!context) {
    throw new Error('useContextState must be used within ContextProvider')
  }
  return context
}
export const useContextDispatch = (): ContextDispatch => {
  const context = useContext(ContextDispatchContext)
  if (!context) {
    throw new Error('useContextDispatch must be used within ContextProvider')
  }
  return context
}
</file>
<file path="src/tui/context.tsx">
import React, { useCallback, useEffect, useState } from 'react'
import { loadCliConfig } from '../config'
import { ContextDispatchContext, ContextStateContext } from './context-store'
export const ContextProvider: React.FC<{ children: React.ReactNode }> = ({ children }) => {
  const [files, setFiles] = useState<string[]>([])
  const [urls, setUrls] = useState<string[]>([])
  const [images, setImages] = useState<string[]>([])
  const [videos, setVideos] = useState<string[]>([])
  const [pdfs, setPdfs] = useState<string[]>([])
  const [smartContextEnabled, setSmartContextEnabled] = useState(false)
  const [smartContextRoot, setSmartContextRoot] = useState<string | null>(null)
  const [metaInstructions, setMetaInstructions] = useState('')
  const [maxContextTokens, setMaxContextTokens] = useState<number | null>(null)
  const [maxInputTokens, setMaxInputTokens] = useState<number | null>(null)
  const [contextOverflowStrategy, setContextOverflowStrategy] = useState<
    import('../config').ContextOverflowStrategy | null
  >(null)
  const [lastReasoning, setLastReasoning] = useState<string | null>(null)
  const [lastGeneratedPrompt, setLastGeneratedPrompt] = useState<string | null>(null)
  const addEntry = useCallback(
    (value: string, setter: React.Dispatch<React.SetStateAction<string[]>>) => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      setter((prev) => [...prev, trimmed])
    },
    [],
  )
  const removeEntry = useCallback(
    (index: number, setter: React.Dispatch<React.SetStateAction<string[]>>) => {
      setter((prev) => prev.filter((_, idx) => idx !== index))
    },
    [],
  )
  const updateEntry = useCallback(
    (index: number, value: string, setter: React.Dispatch<React.SetStateAction<string[]>>) => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      setter((prev) => {
        if (index < 0 || index >= prev.length) {
          return prev
        }
        const next = [...prev]
        next[index] = trimmed
        return next
      })
    },
    [],
  )
  const addFile = useCallback((value: string) => addEntry(value, setFiles), [addEntry])
  const removeFile = useCallback((index: number) => removeEntry(index, setFiles), [removeEntry])
  const addUrl = useCallback((value: string) => addEntry(value, setUrls), [addEntry])
  const removeUrl = useCallback((index: number) => removeEntry(index, setUrls), [removeEntry])
  const updateUrl = useCallback(
    (index: number, value: string) => updateEntry(index, value, setUrls),
    [updateEntry],
  )
  const addImage = useCallback((value: string) => addEntry(value, setImages), [addEntry])
  const removeImage = useCallback((index: number) => removeEntry(index, setImages), [removeEntry])
  const addVideo = useCallback((value: string) => addEntry(value, setVideos), [addEntry])
  const removeVideo = useCallback((index: number) => removeEntry(index, setVideos), [removeEntry])
  const addPdf = useCallback((value: string) => addEntry(value, setPdfs), [addEntry])
  const removePdf = useCallback((index: number) => removeEntry(index, setPdfs), [removeEntry])
  const toggleSmartContext = useCallback(() => {
    setSmartContextEnabled((prev) => !prev)
  }, [])
  const setSmartRoot = useCallback((value: string) => {
    const trimmed = value.trim()
    setSmartContextRoot(trimmed.length > 0 ? trimmed : null)
  }, [])
  const resetContext = useCallback(() => {
    setFiles([])
    setUrls([])
    setImages([])
    setVideos([])
    setPdfs([])
    setSmartContextEnabled(false)
    setSmartContextRoot(null)
    setMetaInstructions('')
    setLastReasoning(null)
  }, [])
  const setBudgets = useCallback(
    (value: {
      maxContextTokens: number | null
      maxInputTokens: number | null
      contextOverflowStrategy: import('../config').ContextOverflowStrategy | null
    }) => {
      setMaxContextTokens(value.maxContextTokens)
      setMaxInputTokens(value.maxInputTokens)
      setContextOverflowStrategy(value.contextOverflowStrategy)
    },
    [],
  )
  useEffect(() => {
    let cancelled = false
    const loadBudgetsFromConfig = async (): Promise<void> => {
      const config = await loadCliConfig().catch(() => null)
      if (cancelled) {
        return
      }
      const promptGenerator = config?.promptGenerator
      setMaxInputTokens(promptGenerator?.maxInputTokens ?? null)
      setMaxContextTokens(promptGenerator?.maxContextTokens ?? null)
      setContextOverflowStrategy(promptGenerator?.contextOverflowStrategy ?? null)
    }
    void loadBudgetsFromConfig()
    return () => {
      cancelled = true
    }
  }, [])
  return (
    <ContextStateContext.Provider
      value={{
        files,
        urls,
        images,
        videos,
        pdfs,
        smartContextEnabled,
        smartContextRoot,
        metaInstructions,
        maxContextTokens,
        maxInputTokens,
        contextOverflowStrategy,
        lastReasoning,
        lastGeneratedPrompt,
      }}
    >
      <ContextDispatchContext.Provider
        value={{
          addFile,
          removeFile,
          addUrl,
          removeUrl,
          updateUrl,
          addImage,
          removeImage,
          addVideo,
          removeVideo,
          addPdf,
          removePdf,
          toggleSmartContext,
          setSmartRoot,
          setMetaInstructions,
          setBudgets,
          setLastReasoning,
          setLastGeneratedPrompt,
          resetContext,
        }}
      >
        {children}
      </ContextDispatchContext.Provider>
    </ContextStateContext.Provider>
  )
}
</file>
<file path="src/tui/help-config.ts">
import type { CommandDescriptor } from './types'
export type HelpSection = {
  title: string
  lines: string[]
}
const chunk = <T>(items: readonly T[], size: number): T[][] => {
  if (size <= 0) {
    return [Array.from(items)]
  }
  const result: T[][] = []
  for (let index = 0; index < items.length; index += size) {
    result.push(items.slice(index, index + size))
  }
  return result
}
const formatCommandLines = (commandDescriptors: readonly CommandDescriptor[]): string[] => {
  const commands = commandDescriptors.map((descriptor) => `/${descriptor.id}`)
  const chunks = chunk(commands, 7)
  return chunks.map((group, index) => {
    const prefix = index === 0 ? 'Commands: ' : '         '
    return `${prefix}${group.join(' ')}`
  })
}
export type HelpConfigOptions = {
  commandDescriptors: readonly CommandDescriptor[]
}
export const estimateHelpOverlayHeight = (sections: readonly HelpSection[]): number => {
  const titleRows = 1
  const sectionRows = sections.reduce(
    (accumulator, section) => accumulator + 1 + 1 + section.lines.length,
    0,
  )
  const borderRows = 2
  return titleRows + sectionRows + borderRows
}
export const createHelpSections = ({ commandDescriptors }: HelpConfigOptions): HelpSection[] => {
  return [
    {
      title: 'Global',
      lines: [
        'Ctrl+G: Generate + open command palette',
        'Ctrl+T: Switch to Test Runner',
        '/help: Show this help overlay',
        'Ctrl+C: Exit (or /exit)',
        'Esc: Dismiss UI (never exits)',
      ],
    },
    {
      title: 'Quick Start',
      lines: [
        'Type natural language requests or start a command with /.',
        'Press Enter to log input; arrow keys scroll history.',
        'Type /help anytime to view keyboard shortcuts.',
        'Series: /series generates standalone atomic prompts (no cross-references); it prefills from typed/last intent (or /intent file).',
        'Tests: /test prompt-tests.yaml runs the prompt test suite.',
        'Tokens: /tokens shows token usage breakdown.',
        'Reasoning: /reasoning (or /why) shows last model reasoning.',
        'JSON: /json on|off toggles prompt payload in history.',
        'Tip: Drag & drop a file path, then press Tab to add it to context.',
        'Tip: Press Tab to open the Series intent popup.',
      ],
    },
    {
      title: 'Workflows',
      lines: [
        'Budgets: /budgets sets token limits + overflow strategy.',
        'Resume: /resume picks history or an exported payload file.',
        'Resume modes: strict (refuse missing files) Âˇ best-effort (skip missing).',
        'Export: /export writes a selected history payload to JSON/YAML.',
        'CLI-only: compose subcommand exists (shell/editor integration; not in TUI).',
      ],
    },
    {
      title: 'Generate',
      lines: ['History: â/â scroll Âˇ PgUp/PgDn page', ...formatCommandLines(commandDescriptors)],
    },
    {
      title: 'Test Runner',
      lines: [
        'Tab / Shift+Tab: Move focus',
        'Enter (File): Move to Actions',
        'Enter (Actions): Run tests',
      ],
    },
    {
      title: 'Popups',
      lines: ['Esc: Close Âˇ â/â: Navigate Âˇ Enter: Confirm', 'Del/Backspace: Remove selected item'],
    },
  ]
}
</file>
<file path="docs/sidekick.nvim-prompt-maker-jsonl-fixtures.md">
# Prompt Maker JSONL + Transport Fixture Samples (for Neovim/Sidekick tests)
This file contains **copy/pasteable JSONL** streams for testing a Neovim-side client that parses Prompt Maker stream events and the interactive transport tap.
Sources of truth for schema:
- Stream event union: `src/generate/types.ts`
- Serialization: `src/generate/stream.ts`
- Transport server + `transport.error`: `src/generate/interactive-transport.ts`
Notes:
- Stream events emitted by `createStreamDispatcher(...).emit(...)` always include `timestamp`.
- The transport serverâs `transport.error` is written as:
  - `{"event":"transport.error","message":"..."}\n`
  - It currently **does not include** `timestamp`.
  - Clients should tolerate this and treat it as a valid event line.
---
## Fixture A: âHappy Pathâ interactive transport run
This simulates:
- transport lifecycle (listening â connected)
- progress updates
- telemetry
- 2 iterations (initial + 1 refine)
- interactive loop completion
- final payload with `renderedPrompt`
```jsonl
{"event":"transport.listening","timestamp":"2026-01-03T12:00:00.000Z","path":"/tmp/pmc-nvim-12345-acde1234.sock"}
{"event":"transport.client.connected","timestamp":"2026-01-03T12:00:00.050Z","status":"connected"}
{"event":"progress.update","timestamp":"2026-01-03T12:00:00.100Z","label":"Resolving URL context","state":"start","scope":"url"}
{"event":"progress.update","timestamp":"2026-01-03T12:00:00.800Z","label":"Resolved 2 URLs","state":"stop","scope":"url"}
{"event":"progress.update","timestamp":"2026-01-03T12:00:00.900Z","label":"Scanning smart context","state":"start","scope":"smart"}
{"event":"progress.update","timestamp":"2026-01-03T12:00:01.400Z","label":"Smart context ready","state":"stop","scope":"smart"}
{"event":"context.telemetry","timestamp":"2026-01-03T12:00:01.500Z","telemetry":{"files":[{"path":"src/main.ts","tokens":820},{"path":"docs/spec.md","tokens":1410}],"intentTokens":140,"fileTokens":2230,"systemTokens":650,"totalTokens":3020}}
{"event":"generation.iteration.start","timestamp":"2026-01-03T12:00:01.600Z","iteration":1,"intent":"Draft a prompt for a code review agent","model":"gpt-4o-mini","interactive":true,"inputTokens":3020,"refinements":[]}
{"event":"generation.iteration.complete","timestamp":"2026-01-03T12:00:02.200Z","iteration":1,"prompt":"# Prompt\n\nYou are a code review agent...","tokens":620}
{"event":"interactive.state","timestamp":"2026-01-03T12:00:02.210Z","phase":"start","iteration":1}
{"event":"interactive.state","timestamp":"2026-01-03T12:00:02.220Z","phase":"prompt","iteration":1}
{"event":"interactive.awaiting","timestamp":"2026-01-03T12:00:02.230Z","mode":"transport"}
{"event":"interactive.state","timestamp":"2026-01-03T12:00:05.000Z","phase":"refine","iteration":1}
{"event":"generation.iteration.start","timestamp":"2026-01-03T12:00:05.010Z","iteration":2,"intent":"Draft a prompt for a code review agent","model":"gpt-4o-mini","interactive":true,"inputTokens":3020,"refinements":["Make it shorter and add a checklist"],"latestRefinement":"Make it shorter and add a checklist"}
{"event":"generation.iteration.complete","timestamp":"2026-01-03T12:00:05.600Z","iteration":2,"prompt":"# Prompt\n\nYou are a code review agent. Follow this checklist...","tokens":540,"reasoningTokens":120}
{"event":"interactive.state","timestamp":"2026-01-03T12:00:05.610Z","phase":"prompt","iteration":2}
{"event":"interactive.awaiting","timestamp":"2026-01-03T12:00:05.620Z","mode":"transport"}
{"event":"interactive.state","timestamp":"2026-01-03T12:00:06.200Z","phase":"complete","iteration":2}
{"event":"generation.final","timestamp":"2026-01-03T12:00:06.250Z","result":{"schemaVersion":"1","intent":"Draft a prompt for a code review agent","model":"gpt-4o-mini","targetModel":"gpt-4o-mini","prompt":"# Prompt\n\nYou are a code review agent...","refinements":["Make it shorter and add a checklist"],"iterations":2,"interactive":true,"timestamp":"2026-01-03T12:00:06.250Z","contextPaths":[{"path":"src/main.ts","source":"file"},{"path":"docs/spec.md","source":"file"}],"contextTemplate":"nvim","renderedPrompt":"## NeoVim Prompt Buffer\n\n# Prompt\n\nYou are a code review agent. Follow this checklist..."}}
{"event":"transport.client.disconnected","timestamp":"2026-01-03T12:00:06.300Z","status":"disconnected"}
```
---
## Fixture B: Transport error + recover
This simulates the server sending `transport.error` due to a bad refine instruction.
```jsonl
{"event":"transport.listening","timestamp":"2026-01-03T12:10:00.000Z","path":"/tmp/pmc-nvim-12345-acde9999.sock"}
{"event":"transport.client.connected","timestamp":"2026-01-03T12:10:00.050Z","status":"connected"}
{"event":"interactive.awaiting","timestamp":"2026-01-03T12:10:00.100Z","mode":"transport"}
{"event":"transport.error","message":"Refinement instruction must be non-empty."}
{"event":"interactive.awaiting","timestamp":"2026-01-03T12:10:02.000Z","mode":"transport"}
{"event":"generation.iteration.start","timestamp":"2026-01-03T12:10:03.000Z","iteration":2,"intent":"Draft a prompt","model":"gpt-4o-mini","interactive":true,"inputTokens":1200,"refinements":["Add explicit output format"],"latestRefinement":"Add explicit output format"}
{"event":"generation.iteration.complete","timestamp":"2026-01-03T12:10:03.500Z","iteration":2,"prompt":"# Prompt\n\nOutput format: ...","tokens":400}
{"event":"generation.final","timestamp":"2026-01-03T12:10:04.000Z","result":{"schemaVersion":"1","intent":"Draft a prompt","model":"gpt-4o-mini","targetModel":"gpt-4o-mini","prompt":"# Prompt\n\nOutput format: ...","refinements":["Add explicit output format"],"iterations":2,"interactive":true,"timestamp":"2026-01-03T12:10:04.000Z","contextPaths":[],"polishedPrompt":"# Prompt (Polished)\n\nOutput format: ..."}}
```
---
## Fixture C: Token telemetry warning/overflow scenario
This is for testing your Neovim-side guardrails when total tokens exceed thresholds.
```jsonl
{"event":"context.telemetry","timestamp":"2026-01-03T12:20:00.000Z","telemetry":{"files":[{"path":"src/bigfile.ts","tokens":41000},{"path":"docs/huge.md","tokens":22000}],"intentTokens":600,"fileTokens":63000,"systemTokens":1800,"totalTokens":65400}}
{"event":"progress.update","timestamp":"2026-01-03T12:20:00.100Z","label":"Generating...","state":"start","scope":"generate"}
```
---
## Fixture D: Upload progress (image + video)
Use this to test counters and UI gating while uploads are active.
```jsonl
{"event":"upload.state","timestamp":"2026-01-03T12:30:00.000Z","state":"start","detail":{"kind":"image","filePath":"assets/mock.png"}}
{"event":"upload.state","timestamp":"2026-01-03T12:30:00.800Z","state":"finish","detail":{"kind":"image","filePath":"assets/mock.png"}}
{"event":"upload.state","timestamp":"2026-01-03T12:30:01.000Z","state":"start","detail":{"kind":"video","filePath":"media/demo.mp4"}}
{"event":"progress.update","timestamp":"2026-01-03T12:30:01.100Z","label":"Uploading video...","state":"update","scope":"generic"}
{"event":"upload.state","timestamp":"2026-01-03T12:30:03.500Z","state":"finish","detail":{"kind":"video","filePath":"media/demo.mp4"}}
```
---
## Fixture E: Context budget trimming (`context.overflow`)
This simulates budgets being enabled (via CLI flags `--max-input-tokens`/`--max-context-tokens` or config defaults) and the CLI dropping one or more **text** context entries to satisfy the budget.
Important ordering note:
- When trimming occurs, the CLI emits `context.overflow` and then emits `context.telemetry` for the post-trim telemetry.
- `context.overflow` happens **before** any `generation.iteration.*` events.
```jsonl
{"event":"progress.update","timestamp":"2026-01-03T12:40:00.000Z","label":"Resolving context","state":"start","scope":"generic"}
{"event":"progress.update","timestamp":"2026-01-03T12:40:00.450Z","label":"Resolving context","state":"stop","scope":"generic"}
{"event":"context.overflow","timestamp":"2026-01-03T12:40:00.500Z","strategy":"drop-largest","before":{"files":[{"path":"src/core.ts","tokens":1200},{"path":"docs/huge.md","tokens":4800},{"path":"url:https://example.com","tokens":2600}],"intentTokens":220,"fileTokens":8600,"systemTokens":800,"totalTokens":9620},"after":{"files":[{"path":"src/core.ts","tokens":1200},{"path":"docs/brief.md","tokens":900}],"intentTokens":220,"fileTokens":2100,"systemTokens":800,"totalTokens":3120},"droppedPaths":[{"path":"docs/huge.md","source":"file"},{"path":"url:https://example.com","source":"url"}]}
{"event":"context.telemetry","timestamp":"2026-01-03T12:40:00.510Z","telemetry":{"files":[{"path":"src/core.ts","tokens":1200},{"path":"docs/brief.md","tokens":900}],"intentTokens":220,"fileTokens":2100,"systemTokens":800,"totalTokens":3120}}
{"event":"progress.update","timestamp":"2026-01-03T12:40:00.600Z","label":"Generating prompt","state":"start","scope":"generate"}
{"event":"generation.iteration.start","timestamp":"2026-01-03T12:40:00.610Z","iteration":1,"intent":"Draft a prompt with strict context budget","model":"gpt-4o-mini","interactive":false,"inputTokens":3120,"refinements":[]}
{"event":"generation.iteration.complete","timestamp":"2026-01-03T12:40:01.100Z","iteration":1,"prompt":"# Prompt\n\nUse only the provided context.","tokens":220}
{"event":"progress.update","timestamp":"2026-01-03T12:40:01.120Z","label":"Generating prompt","state":"stop","scope":"generate"}
{"event":"generation.final","timestamp":"2026-01-03T12:40:01.150Z","result":{"schemaVersion":"1","intent":"Draft a prompt with strict context budget","model":"gpt-4o-mini","targetModel":"gpt-4o-mini","prompt":"# Prompt\n\nUse only the provided context.","refinements":[],"iterations":1,"interactive":false,"timestamp":"2026-01-03T12:40:01.150Z","contextPaths":[{"path":"inline-intent","source":"intent"},{"path":"src/core.ts","source":"file"},{"path":"docs/brief.md","source":"file"}]}}
```
---
## Fixture F: Resume early in run (`resume.loaded`)
This simulates a run that loads a previous payload from history, reuses one context path successfully, and reports a missing context file. The `resume.loaded` event is emitted before context resolution telemetry and generation.
```jsonl
{"event":"resume.loaded","timestamp":"2026-01-03T12:50:00.000Z","source":"history","reusedContextPaths":[{"path":"notes/existing.md","source":"file"}],"missingContextPaths":[{"path":"notes/missing.md","source":"file"}]}
{"event":"progress.update","timestamp":"2026-01-03T12:50:00.050Z","label":"Resolving context","state":"start","scope":"generic"}
{"event":"progress.update","timestamp":"2026-01-03T12:50:00.250Z","label":"Resolving context","state":"stop","scope":"generic"}
{"event":"context.telemetry","timestamp":"2026-01-03T12:50:00.300Z","telemetry":{"files":[{"path":"notes/existing.md","tokens":640}],"intentTokens":160,"fileTokens":640,"systemTokens":780,"totalTokens":1580}}
{"event":"generation.iteration.start","timestamp":"2026-01-03T12:50:00.350Z","iteration":3,"intent":"Resumed intent","model":"gpt-4o-mini","interactive":false,"inputTokens":1580,"refinements":["prior refinement"]}
{"event":"generation.iteration.complete","timestamp":"2026-01-03T12:50:00.900Z","iteration":3,"prompt":"# Prompt\n\nContinue from the previous run.","tokens":260}
{"event":"generation.final","timestamp":"2026-01-03T12:50:00.950Z","result":{"schemaVersion":"1","intent":"Resumed intent","model":"gpt-4o-mini","targetModel":"gpt-4o-mini","prompt":"# Prompt\n\nContinue from the previous run.","refinements":["prior refinement"],"iterations":3,"interactive":false,"timestamp":"2026-01-03T12:50:00.950Z","contextPaths":[{"path":"inline-intent","source":"intent"},{"path":"notes/existing.md","source":"file"}]}}
```
</file>
<file path="src/__tests__/tui/popup-reducer.test.ts">
import type { PopupState } from '../../tui/types'
import { INITIAL_POPUP_MANAGER_STATE, popupReducer } from '../../tui/popup-reducer'
describe('popupReducer', () => {
  it('opens budgets popup with seeded drafts', () => {
    const next = popupReducer(INITIAL_POPUP_MANAGER_STATE, {
      type: 'open-budgets',
      maxContextTokens: 120,
      maxInputTokens: null,
      contextOverflowStrategy: 'drop-oldest',
    })
    expect(next.popupState).toEqual({
      type: 'budgets',
      selectionIndex: 0,
      maxContextTokensDraft: '120',
      maxInputTokensDraft: '',
      contextOverflowStrategyDraft: 'drop-oldest',
      errorMessage: null,
    })
    expect(next.activeScan).toBeNull()
  })
  it('opens resume popup with seeded defaults', () => {
    const next = popupReducer(INITIAL_POPUP_MANAGER_STATE, {
      type: 'open-resume',
      scanId: 123,
      sourceKind: 'history',
      mode: 'best-effort',
      payloadPathDraft: '',
      historyItems: [{ selector: 'last', title: 't', detail: 'd' }],
      historySelectionIndex: 0,
      historyErrorMessage: null,
    })
    expect(next.popupState).toEqual({
      type: 'resume',
      selectionIndex: 0,
      sourceKind: 'history',
      mode: 'best-effort',
      historyItems: [{ selector: 'last', title: 't', detail: 'd' }],
      historySelectionIndex: 0,
      historyErrorMessage: null,
      payloadPathDraft: '',
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(next.activeScan).toEqual({ kind: 'resume', id: 123 })
  })
  it('opens export popup with seeded defaults', () => {
    const next = popupReducer(INITIAL_POPUP_MANAGER_STATE, {
      type: 'open-export',
      format: 'json',
      outPathDraft: 'prompt-export.json',
      historyItems: [
        {
          selector: 'last',
          title: 't',
          detail: 'd',
          schemaVersion: 'v1',
          supported: false,
        },
      ],
      historySelectionIndex: 0,
      historyErrorMessage: null,
    })
    expect(next.popupState).toEqual({
      type: 'export',
      selectionIndex: 0,
      historyItems: [
        {
          selector: 'last',
          title: 't',
          detail: 'd',
          schemaVersion: 'v1',
          supported: false,
        },
      ],
      historySelectionIndex: 0,
      historyErrorMessage: null,
      format: 'json',
      outPathDraft: 'prompt-export.json',
    })
    expect(next.activeScan).toBeNull()
  })
  describe('scan-suggestions-success staleness gating', () => {
    it('does not apply suggestions when kind mismatches', () => {
      const state = popupReducer(INITIAL_POPUP_MANAGER_STATE, { type: 'open-file', scanId: 123 })
      const next = popupReducer(state, {
        type: 'scan-suggestions-success',
        kind: 'image',
        scanId: 123,
        suggestions: ['a'],
      })
      expect(next).toBe(state)
    })
    it('does not apply suggestions when scanId mismatches', () => {
      const state = popupReducer(INITIAL_POPUP_MANAGER_STATE, { type: 'open-file', scanId: 123 })
      const next = popupReducer(state, {
        type: 'scan-suggestions-success',
        kind: 'file',
        scanId: 999,
        suggestions: ['a'],
      })
      expect(next).toBe(state)
    })
    it('does not apply suggestions when activeScan is null', () => {
      const popupState = {
        type: 'file',
        draft: '',
        selectionIndex: 0,
        selectedFocused: false,
        suggestedItems: [],
        suggestedSelectionIndex: 0,
        suggestedFocused: false,
      } satisfies PopupState
      const state = { popupState, activeScan: null }
      const next = popupReducer(state, {
        type: 'scan-suggestions-success',
        kind: 'file',
        scanId: 123,
        suggestions: ['a'],
      })
      expect(next).toBe(state)
    })
  })
  describe('scan-suggestions-success applies suggestions and clears activeScan', () => {
    it('applies file suggestions, resets suggestion selection/focus, and clears activeScan', () => {
      const opened = popupReducer(INITIAL_POPUP_MANAGER_STATE, { type: 'open-file', scanId: 123 })
      const prepared = popupReducer(opened, {
        type: 'set',
        next: (prev) => {
          if (prev?.type !== 'file') {
            throw new Error('Expected file popup')
          }
          return {
            ...prev,
            selectionIndex: 1,
            selectedFocused: true,
            suggestedItems: ['old'],
            suggestedSelectionIndex: 2,
            suggestedFocused: true,
          }
        },
      })
      const next = popupReducer(prepared, {
        type: 'scan-suggestions-success',
        kind: 'file',
        scanId: 123,
        suggestions: ['a', 'b'],
      })
      expect(next.activeScan).toBeNull()
      expect(next.popupState?.type).toBe('file')
      if (next.popupState?.type !== 'file') {
        throw new Error('Expected file popup')
      }
      expect(next.popupState.suggestedItems).toEqual(['a', 'b'])
      expect(next.popupState.suggestedSelectionIndex).toBe(0)
      expect(next.popupState.suggestedFocused).toBe(false)
      // Ensure unrelated fields are preserved.
      expect(next.popupState.selectionIndex).toBe(1)
      expect(next.popupState.selectedFocused).toBe(true)
    })
  })
  describe("'set' preserves scan only when popup type does not change", () => {
    it('preserves activeScan when popup type stays the same', () => {
      const opened = popupReducer(INITIAL_POPUP_MANAGER_STATE, { type: 'open-file', scanId: 123 })
      const next = popupReducer(opened, {
        type: 'set',
        next: (prev) => {
          if (prev?.type !== 'file') {
            throw new Error('Expected file popup')
          }
          return { ...prev, draft: 'abc' }
        },
      })
      expect(next.activeScan).toEqual({ kind: 'file', id: 123 })
      expect(next.popupState?.type).toBe('file')
      if (next.popupState?.type !== 'file') {
        throw new Error('Expected file popup')
      }
      expect(next.popupState.draft).toBe('abc')
    })
    it('clears activeScan when popup type changes', () => {
      const opened = popupReducer(INITIAL_POPUP_MANAGER_STATE, { type: 'open-file', scanId: 123 })
      const next = popupReducer(opened, {
        type: 'set',
        next: {
          type: 'smart',
          draft: 'hi',
          suggestedItems: [],
          suggestedSelectionIndex: 0,
          suggestedFocused: false,
        },
      })
      expect(next.activeScan).toBeNull()
      expect(next.popupState?.type).toBe('smart')
    })
  })
})
</file>
<file path="src/__tests__/file-suggestions.test.ts">
import fg from 'fast-glob'
import {
  discoverDirectorySuggestions,
  discoverFileSuggestions,
  discoverIntentFileSuggestions,
  FILE_SUGGESTION_IGNORE_PATTERNS,
  filterDirectorySuggestions,
  filterFileSuggestions,
  filterIntentFileSuggestions,
} from '../tui/file-suggestions'
jest.mock('fast-glob')
const globMock = fg as unknown as jest.MockedFunction<typeof fg>
describe('file-suggestions', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    globMock.mockResolvedValue([])
  })
  it('discovers workspace files, normalizes, sorts, and limits', async () => {
    globMock.mockResolvedValue(['/repo/z.ts', '/repo/a.ts', '/other/outside.md'])
    const results = await discoverFileSuggestions({ cwd: '/repo', limit: 2 })
    expect(globMock).toHaveBeenCalledWith(
      expect.any(Array),
      expect.objectContaining({
        cwd: '/repo',
        ignore: FILE_SUGGESTION_IGNORE_PATTERNS,
        onlyFiles: true,
      }),
    )
    expect(results).toEqual(['a.ts', 'z.ts'])
  })
  it('discovers workspace directories, normalizes, sorts, and limits', async () => {
    globMock.mockResolvedValue(['/repo/z', '/repo/a', '/other/outside'])
    const results = await discoverDirectorySuggestions({ cwd: '/repo', limit: 2 })
    expect(globMock).toHaveBeenCalledWith(
      expect.any(Array),
      expect.objectContaining({
        cwd: '/repo',
        ignore: FILE_SUGGESTION_IGNORE_PATTERNS,
        onlyDirectories: true,
      }),
    )
    expect(results).toEqual(['a', 'z'])
  })
  it('filters directory suggestions using fuzzy query and exclusion', () => {
    const results = filterDirectorySuggestions({
      suggestions: ['apps/prompt-maker-cli', 'src/app', 'docs'],
      // "spm" is not a substring of "prompt-maker", but should match fuzzily.
      query: 'spm',
      exclude: ['docs'],
    })
    expect(results).toEqual(['apps/prompt-maker-cli'])
  })
  it('filters suggestions by query and excludes existing entries', () => {
    const results = filterFileSuggestions({
      suggestions: ['src/app.ts', 'docs/Guide.md', 'README.md', 'src/utils/helpers.ts'],
      query: 'src',
      exclude: ['README.md'],
    })
    expect(results).toEqual(['src/app.ts', 'src/utils/helpers.ts'])
  })
  it('supports non-contiguous fuzzy matches', () => {
    const results = filterFileSuggestions({
      suggestions: ['src/app.ts', 'docs/Guide.md', 'src/utils/helpers.ts'],
      // "suh" is not a substring; it matches "src/utils/helpers.ts" fuzzily.
      query: 'suh',
    })
    expect(results).toEqual(['src/utils/helpers.ts'])
  })
  it('normalizes absolute-path queries to workspace-relative matches', () => {
    const cwdSpy = jest.spyOn(process, 'cwd').mockReturnValue('/repo')
    try {
      const results = filterFileSuggestions({
        suggestions: ['src/tui/theme/terminal-appearance.ts'],
        query: '/repo/src/tui/theme/terminal-appearance.ts',
      })
      expect(results).toEqual(['src/tui/theme/terminal-appearance.ts'])
    } finally {
      cwdSpy.mockRestore()
    }
  })
  it('uses smart-case matching like fzf', () => {
    const lowerResults = filterFileSuggestions({
      suggestions: ['README.md', 'readme.md'],
      query: 'read',
    })
    expect(lowerResults).toEqual(['README.md', 'readme.md'])
    const upperResults = filterFileSuggestions({
      suggestions: ['README.md', 'readme.md'],
      query: 'README',
    })
    expect(upperResults).toEqual(['README.md'])
  })
  it('prefers forward matching for path-shaped queries', () => {
    const results = filterFileSuggestions({
      suggestions: ['src/theme/terminal-appearance.ts', 'src/tui/theme/terminal-appearance.ts'],
      query: 'src/tui',
    })
    expect(results[0]).toBe('src/tui/theme/terminal-appearance.ts')
  })
  it('supports fzf prefix/suffix operators', () => {
    const results = filterFileSuggestions({
      suggestions: [
        'src/tui/theme/terminal-appearance.ts',
        'xsrc/tui/theme/terminal-appearance.ts',
      ],
      query: '^src/tui/theme/terminal-appearance.ts$',
    })
    expect(results).toEqual(['src/tui/theme/terminal-appearance.ts'])
  })
  it('supports fzf exact match operator with absolute paths', () => {
    const cwdSpy = jest.spyOn(process, 'cwd').mockReturnValue('/repo')
    try {
      const results = filterFileSuggestions({
        suggestions: ['src/tui/theme/terminal-appearance.ts'],
        query: "'/repo/src/tui/theme/terminal-appearance.ts",
      })
      expect(results).toEqual(['src/tui/theme/terminal-appearance.ts'])
    } finally {
      cwdSpy.mockRestore()
    }
  })
  it('discovers intent file suggestions by scanning markdown/text files', async () => {
    globMock.mockResolvedValue(['/repo/intents/a.md', '/repo/intents/b.txt'])
    const results = await discoverIntentFileSuggestions({ cwd: '/repo', limit: 5 })
    expect(globMock).toHaveBeenCalledWith(
      expect.any(Array),
      expect.objectContaining({
        cwd: '/repo',
        ignore: FILE_SUGGESTION_IGNORE_PATTERNS,
        onlyFiles: true,
      }),
    )
    expect(results).toEqual(['intents/a.md', 'intents/b.txt'])
  })
  it('filters intent files using fuzzy token matching', () => {
    const results = filterIntentFileSuggestions({
      suggestions: [
        'tmp-uat/intent-basic.md',
        'intents/travel-app-notes.md',
        'intents/onboarding-bot.md',
      ],
      query: 'itb',
    })
    expect(results[0]).toBe('tmp-uat/intent-basic.md')
  })
  it('supports multi-token intent fuzzy search', () => {
    const results = filterIntentFileSuggestions({
      suggestions: ['intents/travel-app-notes.md', 'intents/onboarding-bot.md'],
      query: 'trav app',
    })
    expect(results).toEqual(['intents/travel-app-notes.md'])
  })
})
</file>
<file path="src/generate/args.ts">
import yargs from 'yargs'
import type { ArgumentsCamelCase } from 'yargs'
import type { ContextOverflowStrategy, GenerateArgs, StreamMode } from './types'
const VALUE_FLAGS = new Set([
  '--intent-file',
  '-f',
  '--model',
  '--target',
  '--polish-model',
  '--context',
  '-c',
  '--image',
  '--video',
  '--pdf',
  '--url',
  '--context-file',
  '--context-format',
  '--context-template',
  '--smart-context-root',
  '--max-input-tokens',
  '--max-context-tokens',
  '--context-overflow',
  '--interactive-transport',
  '--resume',
  '--resume-from',
  '--resume-mode',
  '--stream',
])
const HELP_FLAGS = new Set(['--help', '-h'])
const CONTEXT_OVERFLOW_STRATEGIES = [
  'fail',
  'drop-smart',
  'drop-url',
  'drop-largest',
  'drop-oldest',
] as const satisfies ReadonlyArray<ContextOverflowStrategy>
const RESUME_MODES = ['strict', 'best-effort'] as const
type ParsedArgs = {
  args: GenerateArgs
  showHelp: () => void
}
export const parseGenerateArgs = (argv: string[]): ParsedArgs => {
  const {
    optionArgs: rawOptionArgs,
    positionalIntent,
    positionalIntentAfterInteractive,
  } = extractIntentArg(argv)
  const { optionArgs, helpRequested } = stripHelpFlags(rawOptionArgs)
  const parser = yargs(optionArgs)
    .scriptName('prompt-maker-cli')
    .usage('Prompt Maker CLI (generate-only)\n\nUsage:\n  prompt-maker-cli [intent] [options]')
    .option('intent-file', {
      alias: 'f',
      type: 'string',
      describe: 'Read intent from file',
    })
    .option('model', {
      type: 'string',
      describe: 'Override model for generation',
    })
    .option('target', {
      type: 'string',
      describe:
        'Target/runtime model used for optimization (not included in the generated prompt text)',
    })
    .option('polish-model', {
      type: 'string',
      describe: 'Override the model used for polishing',
    })
    .option('interactive', {
      alias: 'i',
      type: 'boolean',
      default: false,
      describe: 'Enable interactive refinement loop',
    })
    .option('copy', {
      type: 'boolean',
      default: false,
      describe: 'Copy the final prompt to the clipboard',
    })
    .option('open-chatgpt', {
      type: 'boolean',
      default: false,
      describe: 'Open ChatGPT with the final prompt',
    })
    .option('polish', {
      type: 'boolean',
      default: false,
      describe: 'Run the polish pass after generation',
    })
    .option('json', {
      type: 'boolean',
      default: false,
      describe: 'Emit machine-readable JSON (non-interactive only)',
    })
    .option('quiet', {
      type: 'boolean',
      default: false,
      describe: 'Suppress interactive UI output (telemetry, banners)',
    })
    .option('progress', {
      type: 'boolean',
      default: true,
      describe: 'Show progress indicator',
    })
    .option('stream', {
      type: 'string',
      choices: ['none', 'jsonl'] as const,
      default: 'none',
      describe: 'Emit structured events via stdout',
    })
    .option('resume-last', {
      type: 'boolean',
      default: false,
      describe: 'Resume from the last history entry',
    })
    .option('resume', {
      type: 'string',
      describe: 'Resume from history selector (last, last:N, or N-th from end)',
    })
    .option('resume-from', {
      type: 'string',
      describe: 'Resume from a JSONL payload file path',
    })
    .option('resume-mode', {
      type: 'string',
      choices: RESUME_MODES,
      default: 'best-effort',
      describe: 'Resume validation mode for history/file inputs',
    })
    .option('context-template', {
      type: 'string',
      describe: 'Wrap the final prompt using a named template',
    })
    .option('interactive-transport', {
      type: 'string',
      describe: 'Listen on a local socket/pipe for interactive commands',
    })
    .option('show-context', {
      type: 'boolean',
      default: false,
      describe: 'Print resolved context files before generation',
    })
    .option('context', {
      alias: 'c',
      type: 'string',
      array: true,
      default: [],
      describe: 'Add file context via glob (repeatable)',
    })
    .option('url', {
      type: 'string',
      array: true,
      default: [],
      describe: 'Add URL context (repeatable)',
    })
    .option('image', {
      type: 'string',
      array: true,
      default: [],
      describe: 'Attach an image (repeatable)',
    })
    .option('video', {
      type: 'string',
      array: true,
      default: [],
      describe: 'Attach a video file (repeatable)',
    })
    .option('pdf', {
      type: 'string',
      array: true,
      default: [],
      describe: 'Attach a PDF file (repeatable)',
    })
    .option('context-file', {
      type: 'string',
      describe: 'Write resolved context to the specified file',
    })
    .option('context-format', {
      type: 'string',
      choices: ['text', 'json'] as const,
      default: 'text',
      describe: 'Format for --show-context or --context-file output',
    })
    .option('smart-context', {
      type: 'boolean',
      default: false,
      describe: 'Automatically attach relevant files via local embeddings',
    })
    .option('smart-context-root', {
      type: 'string',
      describe: 'Override the base directory scanned when --smart-context is enabled',
    })
    .option('max-input-tokens', {
      type: 'number',
      describe: 'Maximum allowed input tokens (intent + system + context)',
      coerce: (value: unknown) => parsePositiveIntegerFlag('--max-input-tokens', value),
    })
    .option('max-context-tokens', {
      type: 'number',
      describe: 'Maximum allowed tokens reserved for context attachments',
      coerce: (value: unknown) => parsePositiveIntegerFlag('--max-context-tokens', value),
    })
    .option('context-overflow', {
      type: 'string',
      choices: CONTEXT_OVERFLOW_STRATEGIES,
      describe: 'Strategy for resolving context token overflows',
    })
    .check((argv) => {
      if (argv.maxInputTokens !== undefined) {
        argv.maxInputTokens = parsePositiveIntegerFlag('--max-input-tokens', argv.maxInputTokens)
      }
      if (argv.maxContextTokens !== undefined) {
        argv.maxContextTokens = parsePositiveIntegerFlag(
          '--max-context-tokens',
          argv.maxContextTokens,
        )
      }
      if (argv.contextOverflow !== undefined && !isContextOverflowStrategy(argv.contextOverflow)) {
        throw new Error(
          `--context-overflow must be one of: ${CONTEXT_OVERFLOW_STRATEGIES.join(', ')}.`,
        )
      }
      if (argv.resume !== undefined) {
        argv.resume = normalizeHistorySelector('--resume', argv.resume)
      }
      if (argv.resumeFrom !== undefined) {
        argv.resumeFrom = normalizeNonEmptyStringFlag('--resume-from', argv.resumeFrom)
      }
      if (argv.resume !== undefined && argv.resumeLast) {
        throw new Error('--resume and --resume-last cannot be combined.')
      }
      if (argv.resumeFrom !== undefined && (argv.resume !== undefined || argv.resumeLast)) {
        throw new Error('--resume-from cannot be combined with --resume or --resume-last.')
      }
      if (argv.resumeMode !== undefined && !isResumeMode(argv.resumeMode)) {
        throw new Error(`--resume-mode must be one of: ${RESUME_MODES.join(', ')}.`)
      }
      return true
    })
    .help('help')
    .alias('help', 'h')
    .exitProcess(false)
    .showHelpOnFail(false)
    .parserConfiguration({ 'halt-at-non-option': true })
    .strict(false)
    .fail((msg, err) => {
      throw err ?? new Error(msg ?? 'Invalid CLI arguments.')
    })
  const parsed = parser.parseSync() as ArgumentsCamelCase<{
    intentFile?: string
    model?: string
    target?: string
    polishModel?: string
    interactive: boolean
    copy: boolean
    openChatgpt: boolean
    polish: boolean
    json: boolean
    quiet: boolean
    progress: boolean
    help?: boolean
    context: string | string[]
    contextFile?: string
    contextFormat?: 'text' | 'json'
    url: string | string[]
    image: string | string[]
    video: string | string[]
    smartContext: boolean
    smartContextRoot?: string
    showContext: boolean
    contextTemplate?: string
    interactiveTransport?: string
    stream?: StreamMode
    resumeLast: boolean
    resume?: string
    resumeFrom?: string
    resumeMode?: (typeof RESUME_MODES)[number]
    maxInputTokens?: number
    maxContextTokens?: number
    contextOverflow?: ContextOverflowStrategy
    _?: (string | number)[]
  }>
  const intent = positionalIntent ?? (typeof parsed._?.[0] === 'string' ? parsed._?.[0] : undefined)
  const resumeModeExplicit = optionArgs.some(
    (token) => token === '--resume-mode' || token.startsWith('--resume-mode='),
  )
  const resumeRequested =
    Boolean(parsed.resumeLast) ||
    Boolean(parsed.resume) ||
    Boolean(parsed.resumeFrom) ||
    resumeModeExplicit
  const args: GenerateArgs = {
    interactive: parsed.interactive ?? false,
    copy: parsed.copy ?? false,
    openChatGpt: parsed.openChatgpt ?? false,
    polish: parsed.polish ?? false,
    json: parsed.json ?? false,
    quiet: parsed.quiet ?? false,
    progress: parsed.progress ?? true,
    stream: parsed.stream ?? 'none',
    showContext: parsed.showContext ?? false,
    contextFormat: parsed.contextFormat ?? 'text',
    help: helpRequested || Boolean(parsed.help),
    ...(resumeRequested ? { resumeMode: parsed.resumeMode ?? 'best-effort' } : {}),
    ...(parsed.resume ? { resume: parsed.resume } : {}),
    ...(parsed.resumeFrom ? { resumeFrom: parsed.resumeFrom } : {}),
    ...(parsed.resumeLast ? { resumeLast: true } : {}),
    ...(parsed.maxInputTokens !== undefined ? { maxInputTokens: parsed.maxInputTokens } : {}),
    ...(parsed.maxContextTokens !== undefined ? { maxContextTokens: parsed.maxContextTokens } : {}),
    ...(parsed.contextOverflow !== undefined ? { contextOverflow: parsed.contextOverflow } : {}),
    context: normalizeListArg(parsed.context),
    urls: normalizeListArg(parsed.url),
    images: normalizeListArg(parsed.image),
    video: normalizeListArg(parsed.video),
    pdf: normalizeListArg(parsed.pdf),
    smartContext: parsed.smartContext ?? false,
    ...(parsed.contextTemplate ? { contextTemplate: parsed.contextTemplate } : {}),
    ...(parsed.interactiveTransport ? { interactiveTransport: parsed.interactiveTransport } : {}),
    ...(parsed.contextFile ? { contextFile: parsed.contextFile } : {}),
    ...(parsed.smartContextRoot ? { smartContextRoot: parsed.smartContextRoot } : {}),
  }
  if (intent) {
    args.intent = intent
    if (positionalIntentAfterInteractive) {
      args.inlineIntentAfterInteractive = true
    }
  }
  if (parsed.intentFile) {
    args.intentFile = parsed.intentFile
  }
  if (parsed.model) {
    args.model = parsed.model
  }
  if (parsed.target) {
    args.target = parsed.target
  }
  if (parsed.polishModel) {
    args.polishModel = parsed.polishModel
  }
  return {
    args,
    showHelp: () => parser.showHelp(),
  }
}
export const extractIntentArg = (
  argv: string[],
): {
  optionArgs: string[]
  positionalIntent?: string
  positionalIntentAfterInteractive?: boolean
} => {
  const optionArgs: string[] = []
  let positionalIntent: string | undefined
  let positionalIntentAfterInteractive = false
  let awaitingInteractiveIntent = false
  for (let i = 0; i < argv.length; i += 1) {
    const token = argv[i]
    if (token === undefined) {
      continue
    }
    if (token === '--') {
      optionArgs.push(...argv.slice(i))
      break
    }
    if (token.startsWith('-')) {
      optionArgs.push(token)
      awaitingInteractiveIntent = token === '-i' || token === '--interactive'
      if (VALUE_FLAGS.has(token)) {
        const next = argv[i + 1]
        if (next !== undefined) {
          optionArgs.push(next)
          i += 1
        }
        awaitingInteractiveIntent = false
      }
      continue
    }
    if (positionalIntent === undefined) {
      positionalIntent = token
      positionalIntentAfterInteractive = awaitingInteractiveIntent
      awaitingInteractiveIntent = false
      continue
    }
    awaitingInteractiveIntent = false
    optionArgs.push(token)
  }
  return positionalIntent !== undefined
    ? { optionArgs, positionalIntent, positionalIntentAfterInteractive }
    : { optionArgs }
}
export const stripHelpFlags = (
  tokens: string[],
): { optionArgs: string[]; helpRequested: boolean } => {
  if (tokens.length === 0) {
    return { optionArgs: tokens, helpRequested: false }
  }
  const sanitized: string[] = []
  let helpRequested = false
  let passthrough = false
  tokens.forEach((token) => {
    if (passthrough) {
      sanitized.push(token)
      return
    }
    if (token === '--') {
      sanitized.push(token)
      passthrough = true
      return
    }
    if (HELP_FLAGS.has(token)) {
      helpRequested = true
      return
    }
    sanitized.push(token)
  })
  return { optionArgs: sanitized, helpRequested }
}
const normalizeListArg = (value: unknown): string[] => {
  if (Array.isArray(value)) {
    return value.map((entry) => entry.toString())
  }
  if (value === undefined || value === null) {
    return []
  }
  return [value.toString()]
}
const isContextOverflowStrategy = (value: unknown): value is ContextOverflowStrategy =>
  typeof value === 'string' &&
  CONTEXT_OVERFLOW_STRATEGIES.includes(value as ContextOverflowStrategy)
const parsePositiveIntegerFlag = (flagName: string, value: unknown): number => {
  const numeric =
    typeof value === 'number'
      ? value
      : typeof value === 'string' && value.trim() !== ''
        ? Number(value)
        : Number.NaN
  if (!Number.isFinite(numeric) || !Number.isInteger(numeric) || numeric <= 0) {
    throw new Error(`${flagName} must be a positive integer.`)
  }
  return numeric
}
const normalizeNonEmptyStringFlag = (flagName: string, value: unknown): string => {
  const raw = typeof value === 'number' ? String(value) : value
  if (typeof raw !== 'string') {
    throw new Error(`${flagName} requires a string value.`)
  }
  const trimmed = raw.trim()
  if (!trimmed) {
    throw new Error(`${flagName} requires a non-empty value.`)
  }
  return trimmed
}
const normalizeHistorySelector = (flagName: string, value: unknown): string => {
  const selector = normalizeNonEmptyStringFlag(flagName, value)
  const parseOffset = (rawOffset: string | undefined): number => {
    if (!rawOffset) {
      throw new Error(
        `Invalid ${flagName} selector "${selector}". Offset must be a positive integer.`,
      )
    }
    const numeric = Number(rawOffset)
    if (!Number.isInteger(numeric) || numeric <= 0) {
      throw new Error(
        `Invalid ${flagName} selector "${selector}". Offset must be a positive integer.`,
      )
    }
    return numeric
  }
  if (selector === 'last') {
    return selector
  }
  const lastMatch = selector.match(/^last:(\d+)$/)
  if (lastMatch) {
    parseOffset(lastMatch[1])
    return selector
  }
  const numericMatch = selector.match(/^(\d+)$/)
  if (numericMatch) {
    parseOffset(numericMatch[1])
    return selector
  }
  throw new Error(
    `Invalid ${flagName} selector "${selector}". Use "last", "last:N", or "N" (N-th from end).`,
  )
}
const isResumeMode = (value: unknown): value is (typeof RESUME_MODES)[number] =>
  typeof value === 'string' && RESUME_MODES.includes(value as (typeof RESUME_MODES)[number])
</file>
<file path="src/tui/components/popups/ListPopup.tsx">
import { useMemo } from 'react'
import { Box, Text, useStdout } from 'ink'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps, type InkColorValue } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
import {
  buildListPopupModel,
  type ListPopupBlockModel,
  type ListPopupRowModel,
  type ListPopupSectionModel,
} from './list-popup-model'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const POPUP_MIN_HEIGHT = 10
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type ListPopupProps = {
  title: string
  placeholder: string
  draft: string
  items: readonly string[]
  selectedIndex: number
  emptyLabel: string
  instructions: string
  suggestedItems?: readonly string[]
  suggestedSelectionIndex?: number
  suggestedFocused?: boolean
  selectedFocused?: boolean
  layout?: 'input-first' | 'selected-first'
  maxHeight?: number
  onDraftChange: (value: string) => void
  onSubmitDraft: (value: string) => void
}
const resolveRowTextProps = (
  row: ListPopupRowModel,
  backgroundProps: ReturnType<typeof inkBackgroundColorProps>,
  theme: {
    text: InkColorValue
    mutedText: InkColorValue
  },
  focusedSelectionProps: ReturnType<typeof inkColorProps> &
    ReturnType<typeof inkBackgroundColorProps>,
  unfocusedSelectionProps: ReturnType<typeof inkColorProps> &
    ReturnType<typeof inkBackgroundColorProps>,
): ReturnType<typeof inkColorProps> & ReturnType<typeof inkBackgroundColorProps> => {
  if (row.selection === 'focused') {
    return focusedSelectionProps
  }
  if (row.selection === 'unfocused') {
    return unfocusedSelectionProps
  }
  if (row.tone === 'muted') {
    return { ...backgroundProps, ...inkColorProps(theme.mutedText) }
  }
  return { ...backgroundProps, ...inkColorProps(theme.text) }
}
export const ListPopup = ({
  title,
  placeholder,
  draft,
  items,
  selectedIndex,
  emptyLabel,
  instructions,
  suggestedItems,
  suggestedSelectionIndex,
  suggestedFocused,
  selectedFocused,
  layout = 'input-first',
  maxHeight,
  onDraftChange,
  onSubmitDraft,
}: ListPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const fallbackHeight = 16
  const popupHeight = Math.max(POPUP_MIN_HEIGHT, Math.floor(maxHeight ?? fallbackHeight))
  const focusedSelectionProps = {
    ...inkColorProps(theme.selectionText),
    ...inkBackgroundColorProps(theme.selectionBackground),
  }
  const unfocusedSelectionProps = {
    ...inkColorProps(theme.chipText),
    ...inkBackgroundColorProps(theme.chipBackground),
  }
  // Hooks must run consistently across renders (suggestions can arrive async).
  const model = useMemo(() => {
    const modelOptions: Parameters<typeof buildListPopupModel>[0] = {
      items,
      selectedIndex,
      emptyLabel,
      instructions,
      layout,
      popupHeight,
      ...(suggestedItems === undefined ? {} : { suggestedItems }),
      ...(suggestedSelectionIndex === undefined ? {} : { suggestedSelectionIndex }),
      ...(suggestedFocused === undefined ? {} : { suggestedFocused }),
      ...(selectedFocused === undefined ? {} : { selectedFocused }),
    }
    return buildListPopupModel(modelOptions)
  }, [
    emptyLabel,
    instructions,
    items,
    layout,
    popupHeight,
    selectedFocused,
    selectedIndex,
    suggestedFocused,
    suggestedItems,
    suggestedSelectionIndex,
  ])
  const renderSpacer = (key: string) => (
    <Text key={key} {...backgroundProps}>
      {padRight('', contentWidth)}
    </Text>
  )
  const renderSection = (section: ListPopupSectionModel) => {
    const containerProps =
      section.fixedRowCount === undefined
        ? { flexDirection: 'column' as const }
        : {
            flexDirection: 'column' as const,
            height: 1 + section.fixedRowCount,
            flexShrink: 0,
            overflow: 'hidden' as const,
          }
    return (
      <Box key={`section-${section.id}`} {...containerProps}>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight(section.header, contentWidth)}
        </Text>
        {section.rows.map((row) => (
          <Text
            key={row.key}
            {...resolveRowTextProps(
              row,
              backgroundProps,
              { text: theme.text, mutedText: theme.mutedText },
              focusedSelectionProps,
              unfocusedSelectionProps,
            )}
          >
            {padRight(row.label, contentWidth)}
          </Text>
        ))}
      </Box>
    )
  }
  const renderInput = (block: Extract<ListPopupBlockModel, { type: 'input' }>) => {
    if (block.input.variant === 'inline') {
      return (
        <Box key="input" flexDirection="row">
          <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
            {block.input.label}
          </Text>
          <SingleLineTextInput
            value={draft}
            onChange={onDraftChange}
            placeholder={placeholder}
            onSubmit={() => onSubmitDraft(draft)}
            focus={block.input.focus}
            width={Math.max(1, contentWidth - 'Add: '.length)}
            backgroundColor={theme.popupBackground}
          />
        </Box>
      )
    }
    return (
      <Box key="input" flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight(block.input.title, contentWidth)}
        </Text>
        <SingleLineTextInput
          value={draft}
          onChange={onDraftChange}
          placeholder={placeholder}
          onSubmit={() => onSubmitDraft(draft)}
          focus={block.input.focus}
          width={contentWidth}
          backgroundColor={theme.popupBackground}
        />
      </Box>
    )
  }
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight(title, contentWidth)}
      </Text>
      {model.blocks.map((block, index) => {
        switch (block.type) {
          case 'spacer':
            return renderSpacer(block.key)
          case 'input':
            return renderInput(block)
          case 'section':
            return renderSection(block.section)
          case 'instructions':
            return (
              <Box key={`instructions-${index}`} flexShrink={0} flexDirection="column">
                {block.lines.map((line, lineIndex) => (
                  <Text
                    key={`${lineIndex}-${line}`}
                    {...backgroundProps}
                    {...inkColorProps(theme.mutedText)}
                  >
                    {padRight(line, contentWidth)}
                  </Text>
                ))}
              </Box>
            )
        }
      })}
    </PopupSheet>
  )
}
</file>
<file path="src/tui/screens/command/hooks/useCommandGenerationPipeline.ts">
import { useRef } from 'react'
import type { BudgetSettings } from '../../../budget-settings'
import { useGenerationPipeline } from '../../../hooks/useGenerationPipeline'
import type { NotifyOptions } from '../../../notifier'
import { createTokenUsageStore } from '../../../token-usage-store'
import type { HistoryEntry, ProviderStatus } from '../../../types'
import type { TokenUsageBreakdown, TokenUsageRun } from '../../../token-usage-store'
export type UseCommandGenerationPipelineOptions = {
  pushHistory: (
    content: string,
    kind?: HistoryEntry['kind'],
    format?: HistoryEntry['format'],
  ) => void
  notify?: (message: string, options?: NotifyOptions) => void
  files: string[]
  urls: string[]
  images: string[]
  videos: string[]
  pdfs: string[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  metaInstructions: string
  budgets: BudgetSettings
  currentModel: string
  targetModel: string
  interactiveTransportPath?: string | undefined
  terminalColumns: number
  polishModelId: string | null
  jsonOutputEnabled: boolean
  copyEnabled: boolean
  chatGptEnabled: boolean
  isTestCommandRunning: boolean
  onProviderStatusUpdate: (status: ProviderStatus) => void
  onReasoningUpdate: (reasoning: string | null) => void
  onLastGeneratedPromptUpdate: (prompt: string) => void
}
export type UseCommandGenerationPipelineResult = {
  isGenerating: boolean
  runGeneration: ReturnType<typeof useGenerationPipeline>['runGeneration']
  runSeriesGeneration: ReturnType<typeof useGenerationPipeline>['runSeriesGeneration']
  statusChips: string[]
  latestContextOverflow: ReturnType<typeof useGenerationPipeline>['latestContextOverflow']
  isAwaitingRefinement: boolean
  submitRefinement: ReturnType<typeof useGenerationPipeline>['submitRefinement']
  awaitingInteractiveMode: ReturnType<typeof useGenerationPipeline>['awaitingInteractiveMode']
  tokenUsageRun: TokenUsageRun | null
  tokenUsageBreakdown: TokenUsageBreakdown | null
}
export const useCommandGenerationPipeline = ({
  pushHistory,
  notify,
  files,
  urls,
  images,
  videos,
  pdfs,
  smartContextEnabled,
  smartContextRoot,
  metaInstructions,
  budgets,
  currentModel,
  targetModel,
  interactiveTransportPath,
  terminalColumns,
  polishModelId,
  jsonOutputEnabled,
  copyEnabled,
  chatGptEnabled,
  isTestCommandRunning,
  onProviderStatusUpdate,
  onReasoningUpdate,
  onLastGeneratedPromptUpdate,
}: UseCommandGenerationPipelineOptions): UseCommandGenerationPipelineResult => {
  const tokenUsageStoreRef = useRef<ReturnType<typeof createTokenUsageStore> | null>(null)
  if (!tokenUsageStoreRef.current) {
    tokenUsageStoreRef.current = createTokenUsageStore()
  }
  const trimmedMetaInstructions = metaInstructions.trim()
  const pipeline = useGenerationPipeline({
    pushHistory,
    ...(notify ? { notify } : {}),
    files,
    urls,
    images,
    videos,
    pdfs,
    smartContextEnabled,
    smartContextRoot,
    currentModel,
    targetModel,
    ...(interactiveTransportPath ? { interactiveTransportPath } : {}),
    terminalColumns,
    metaInstructions: trimmedMetaInstructions,
    budgets,
    polishModelId,
    jsonOutputEnabled,
    copyEnabled,
    chatGptEnabled,
    isTestCommandRunning,
    tokenUsageStore: tokenUsageStoreRef.current,
    onProviderStatusUpdate,
    onReasoningUpdate,
    onLastGeneratedPromptUpdate,
  })
  const tokenUsageRun = tokenUsageStoreRef.current?.getLatestRun() ?? null
  const tokenUsageBreakdown = tokenUsageStoreRef.current?.getLatestBreakdown() ?? null
  return {
    isGenerating: pipeline.isGenerating,
    runGeneration: pipeline.runGeneration,
    runSeriesGeneration: pipeline.runSeriesGeneration,
    statusChips: pipeline.statusChips,
    latestContextOverflow: pipeline.latestContextOverflow,
    isAwaitingRefinement: pipeline.isAwaitingRefinement,
    submitRefinement: pipeline.submitRefinement,
    awaitingInteractiveMode: pipeline.awaitingInteractiveMode,
    tokenUsageRun,
    tokenUsageBreakdown,
  }
}
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts">
import type { NotifyOptions } from '../../../notifier'
import { useCallback, useEffect, useRef, useState } from 'react'
import type { BudgetSettings } from '../../../budget-settings'
import type { HistoryEntry, ModelOption } from '../../../types'
import { DEFAULT_MODEL_ID, getPreferredModelId } from '../../../model-options'
import { resolveDefaultGenerateModel } from '../../../../prompt-generator-service'
import { useModelProviderState } from './useModelProviderState'
import { useCommandGenerationPipeline } from './useCommandGenerationPipeline'
export type UseCommandScreenModelGenerationOptions = {
  pushHistoryProxy: (
    content: string,
    kind?: HistoryEntry['kind'],
    format?: HistoryEntry['format'],
  ) => void
  notify: (message: string, options?: NotifyOptions) => void
  files: string[]
  urls: string[]
  images: string[]
  videos: string[]
  pdfs: string[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  metaInstructions: string
  budgets: BudgetSettings
  interactiveTransportPath?: string | undefined
  terminalColumns: number
  copyEnabled: boolean
  chatGptEnabled: boolean
  jsonOutputEnabled: boolean
  isTestCommandRunning: boolean
  setLastReasoning: (value: string | null) => void
  setLastGeneratedPrompt: (value: string | null) => void
}
export type UseCommandScreenModelGenerationResult = {
  modelOptions: ReturnType<typeof useModelProviderState>['modelOptions']
  currentModel: ReturnType<typeof useModelProviderState>['currentModel']
  selectModel: ReturnType<typeof useModelProviderState>['selectModel']
  polishModelId: ModelOption['id'] | null
  selectPolishModel: (nextId: ModelOption['id'] | null) => void
  currentTargetModel: ModelOption['id']
  selectTargetModel: (nextId: ModelOption['id']) => void
  providerStatuses: ReturnType<typeof useModelProviderState>['providerStatuses']
  updateProviderStatus: ReturnType<typeof useModelProviderState>['updateProviderStatus']
  pipeline: ReturnType<typeof useCommandGenerationPipeline>
}
export const useCommandScreenModelGeneration = ({
  pushHistoryProxy,
  notify,
  files,
  urls,
  images,
  videos,
  pdfs,
  smartContextEnabled,
  smartContextRoot,
  metaInstructions,
  budgets,
  interactiveTransportPath,
  terminalColumns,
  copyEnabled,
  chatGptEnabled,
  jsonOutputEnabled,
  isTestCommandRunning,
  setLastReasoning,
  setLastGeneratedPrompt,
}: UseCommandScreenModelGenerationOptions): UseCommandScreenModelGenerationResult => {
  const { modelOptions, currentModel, selectModel, providerStatuses, updateProviderStatus } =
    useModelProviderState({ pushHistory: pushHistoryProxy })
  const [polishModelId, setPolishModelIdState] = useState<ModelOption['id'] | null>(null)
  const selectPolishModel = useCallback((nextId: ModelOption['id'] | null) => {
    setPolishModelIdState((prev) => (prev === nextId ? prev : nextId))
  }, [])
  const [currentTargetModel, setCurrentTargetModelState] =
    useState<ModelOption['id']>(DEFAULT_MODEL_ID)
  const userSelectedTargetModelRef = useRef(false)
  const selectTargetModel = useCallback((nextId: ModelOption['id']) => {
    userSelectedTargetModelRef.current = true
    setCurrentTargetModelState((prev) => (prev === nextId ? prev : nextId))
  }, [])
  useEffect(() => {
    let cancelled = false
    const syncDefaultTargetModel = async (): Promise<void> => {
      if (userSelectedTargetModelRef.current) {
        return
      }
      const resolvedDefault = await resolveDefaultGenerateModel().catch(() => null)
      if (cancelled || userSelectedTargetModelRef.current) {
        return
      }
      const preferred = getPreferredModelId(modelOptions, resolvedDefault)
      setCurrentTargetModelState((prev) => (prev === preferred ? prev : preferred))
    }
    void syncDefaultTargetModel()
    return () => {
      cancelled = true
    }
  }, [modelOptions])
  const pipeline = useCommandGenerationPipeline({
    pushHistory: pushHistoryProxy,
    notify,
    files,
    urls,
    images,
    videos,
    pdfs,
    smartContextEnabled,
    smartContextRoot,
    metaInstructions,
    budgets,
    currentModel,
    targetModel: currentTargetModel,
    interactiveTransportPath,
    terminalColumns,
    polishModelId,
    jsonOutputEnabled,
    copyEnabled,
    chatGptEnabled,
    isTestCommandRunning,
    onProviderStatusUpdate: updateProviderStatus,
    onReasoningUpdate: setLastReasoning,
    onLastGeneratedPromptUpdate: setLastGeneratedPrompt,
  })
  return {
    modelOptions,
    currentModel,
    selectModel,
    polishModelId,
    selectPolishModel,
    currentTargetModel,
    selectTargetModel,
    providerStatuses,
    updateProviderStatus,
    pipeline,
  }
}
</file>
<file path="src/config.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
import type { ModelDefinition, ModelProvider } from './model-providers'
import type { ThemeMode } from './tui/theme/theme-types'
export type ContextOverflowStrategy =
  | 'fail'
  | 'drop-smart'
  | 'drop-url'
  | 'drop-largest'
  | 'drop-oldest'
export type PromptGeneratorConfig = {
  defaultModel?: string
  defaultGeminiModel?: string
  models?: ModelDefinition[]
  maxInputTokens?: number
  maxContextTokens?: number
  contextOverflowStrategy?: ContextOverflowStrategy
}
export type TuiResumeMode = 'best-effort' | 'strict'
export type TuiResumeSourceKind = 'history' | 'file'
export type PromptMakerCliConfig = {
  openaiApiKey?: string
  openaiBaseUrl?: string
  geminiApiKey?: string
  geminiBaseUrl?: string
  promptGenerator?: PromptGeneratorConfig
  contextTemplates?: Record<string, string>
  // TUI theme settings (persisted).
  theme?: string
  themeMode?: ThemeMode
  // TUI resume defaults (persisted).
  resumeMode?: TuiResumeMode
  resumeSourceKind?: TuiResumeSourceKind
  // TUI export defaults (persisted).
  exportFormat?: 'json' | 'yaml'
  exportOutDir?: string
}
let cachedConfig: PromptMakerCliConfig | null | undefined
let cachedConfigPath: string | null | undefined
const getCandidateConfigPaths = (): string[] => {
  const explicit = process.env.PROMPT_MAKER_CLI_CONFIG?.trim()
  const home = os.homedir()
  const defaults = [
    path.join(home, '.config', 'prompt-maker-cli', 'config.json'),
    path.join(home, '.prompt-maker-cli.json'),
  ]
  return [explicit, ...defaults].filter((value): value is string => Boolean(value))
}
const getDefaultConfigPath = (): string => {
  const home = os.homedir()
  return path.join(home, '.config', 'prompt-maker-cli', 'config.json')
}
const resolveConfigPathForWrite = async (): Promise<string> => {
  const explicit = process.env.PROMPT_MAKER_CLI_CONFIG?.trim()
  if (explicit) {
    return explicit
  }
  if (cachedConfigPath) {
    return cachedConfigPath
  }
  for (const candidate of getCandidateConfigPaths()) {
    try {
      await fs.stat(candidate)
      return candidate
    } catch (error) {
      if (isFileMissingError(error)) {
        continue
      }
      const message = error instanceof Error ? error.message : 'Unknown config error.'
      throw new Error(`Failed to access config at ${candidate}: ${message}`)
    }
  }
  return getDefaultConfigPath()
}
export const loadCliConfig = async (): Promise<PromptMakerCliConfig | null> => {
  if (cachedConfig !== undefined) {
    return cachedConfig
  }
  for (const filePath of getCandidateConfigPaths()) {
    try {
      const contents = await fs.readFile(filePath, 'utf8')
      const parsed = JSON.parse(contents) as unknown
      const config = parseConfig(parsed)
      cachedConfig = config
      cachedConfigPath = filePath
      return config
    } catch (error) {
      if (isFileMissingError(error)) {
        continue
      }
      const message = error instanceof Error ? error.message : 'Unknown config error.'
      throw new Error(`Failed to load config at ${filePath}: ${message}`)
    }
  }
  cachedConfig = null
  cachedConfigPath = null
  return null
}
export const resolveOpenAiCredentials = async (): Promise<{
  apiKey: string
  baseUrl?: string
}> => {
  const envKey = process.env.OPENAI_API_KEY?.trim()
  const envBaseUrl = process.env.OPENAI_BASE_URL?.trim()
  if (envKey) {
    const credentials: { apiKey: string; baseUrl?: string } = { apiKey: envKey }
    if (envBaseUrl) {
      credentials.baseUrl = envBaseUrl
    }
    return credentials
  }
  const config = await loadCliConfig()
  const apiKey = config?.openaiApiKey?.trim()
  if (apiKey) {
    const baseUrl = config?.openaiBaseUrl?.trim()
    const credentials: { apiKey: string; baseUrl?: string } = { apiKey }
    if (baseUrl) {
      credentials.baseUrl = baseUrl
    }
    return credentials
  }
  throw new Error(
    'Missing OpenAI credentials. Set OPENAI_API_KEY or add "openaiApiKey" to ~/.config/prompt-maker-cli/config.json.',
  )
}
export const resolveGeminiCredentials = async (): Promise<{
  apiKey: string
  baseUrl?: string
}> => {
  const envKey = process.env.GEMINI_API_KEY?.trim()
  const envBaseUrl = process.env.GEMINI_BASE_URL?.trim()
  if (envKey) {
    const credentials: { apiKey: string; baseUrl?: string } = { apiKey: envKey }
    if (envBaseUrl) {
      credentials.baseUrl = envBaseUrl
    }
    return credentials
  }
  const config = await loadCliConfig()
  const apiKey = config?.geminiApiKey?.trim()
  if (apiKey) {
    const baseUrl = config?.geminiBaseUrl?.trim()
    const credentials: { apiKey: string; baseUrl?: string } = { apiKey }
    if (baseUrl) {
      credentials.baseUrl = baseUrl
    }
    return credentials
  }
  throw new Error(
    'Missing Gemini credentials. Set GEMINI_API_KEY or add "geminiApiKey" to ~/.config/prompt-maker-cli/config.json.',
  )
}
export type ThemeSettingsPatch = {
  theme?: string | null
  themeMode?: ThemeMode | null
}
export type ResumeSettingsPatch = {
  resumeMode?: TuiResumeMode | null
  resumeSourceKind?: TuiResumeSourceKind | null
}
export type ExportSettingsPatch = {
  exportFormat?: 'json' | 'yaml' | null
  exportOutDir?: string | null
}
export const updateCliThemeSettings = async (
  patch: ThemeSettingsPatch,
  options?: { configPath?: string },
): Promise<void> => {
  const configPath = options?.configPath ?? (await resolveConfigPathForWrite())
  const directory = path.dirname(configPath)
  await fs.mkdir(directory, { recursive: true })
  let raw: unknown = {}
  try {
    const contents = await fs.readFile(configPath, 'utf8')
    raw = JSON.parse(contents) as unknown
  } catch (error) {
    if (!isFileMissingError(error)) {
      const message = error instanceof Error ? error.message : 'Unknown config error.'
      throw new Error(`Failed to read config at ${configPath}: ${message}`)
    }
  }
  if (!isRecord(raw)) {
    throw new Error(`Failed to update config at ${configPath}: root must be a JSON object.`)
  }
  const next: Record<string, unknown> = { ...raw }
  if ('theme' in patch) {
    if (patch.theme === null || patch.theme === undefined || patch.theme.trim() === '') {
      delete next.theme
    } else {
      next.theme = patch.theme.trim()
    }
  }
  if ('themeMode' in patch) {
    if (patch.themeMode === null || patch.themeMode === undefined) {
      delete next.themeMode
    } else {
      next.themeMode = patch.themeMode
    }
  }
  const contents = JSON.stringify(next, null, 2)
  const tempFile = `${configPath}.${process.pid}.tmp`
  await fs.writeFile(tempFile, `${contents}\n`, 'utf8')
  try {
    await fs.rename(tempFile, configPath)
  } catch {
    await fs.writeFile(configPath, `${contents}\n`, 'utf8')
  }
  cachedConfig = parseConfig(next)
  cachedConfigPath = configPath
}
export const updateCliResumeSettings = async (
  patch: ResumeSettingsPatch,
  options?: { configPath?: string },
): Promise<void> => {
  const configPath = options?.configPath ?? (await resolveConfigPathForWrite())
  const directory = path.dirname(configPath)
  await fs.mkdir(directory, { recursive: true })
  let raw: unknown = {}
  try {
    const contents = await fs.readFile(configPath, 'utf8')
    raw = JSON.parse(contents) as unknown
  } catch (error) {
    if (!isFileMissingError(error)) {
      const message = error instanceof Error ? error.message : 'Unknown config error.'
      throw new Error(`Failed to read config at ${configPath}: ${message}`)
    }
  }
  if (!isRecord(raw)) {
    throw new Error(`Failed to update config at ${configPath}: root must be a JSON object.`)
  }
  const next: Record<string, unknown> = { ...raw }
  if ('resumeMode' in patch) {
    if (patch.resumeMode === null || patch.resumeMode === undefined) {
      delete next.resumeMode
    } else {
      next.resumeMode = expectResumeMode(patch.resumeMode, 'resumeMode')
    }
  }
  if ('resumeSourceKind' in patch) {
    if (patch.resumeSourceKind === null || patch.resumeSourceKind === undefined) {
      delete next.resumeSourceKind
    } else {
      next.resumeSourceKind = expectResumeSourceKind(patch.resumeSourceKind, 'resumeSourceKind')
    }
  }
  const contents = JSON.stringify(next, null, 2)
  const tempFile = `${configPath}.${process.pid}.tmp`
  await fs.writeFile(tempFile, `${contents}\n`, 'utf8')
  try {
    await fs.rename(tempFile, configPath)
  } catch {
    await fs.writeFile(configPath, `${contents}\n`, 'utf8')
  }
  cachedConfig = parseConfig(next)
  cachedConfigPath = configPath
}
export const updateCliExportSettings = async (
  patch: ExportSettingsPatch,
  options?: { configPath?: string },
): Promise<void> => {
  const configPath = options?.configPath ?? (await resolveConfigPathForWrite())
  const directory = path.dirname(configPath)
  await fs.mkdir(directory, { recursive: true })
  let raw: unknown = {}
  try {
    const contents = await fs.readFile(configPath, 'utf8')
    raw = JSON.parse(contents) as unknown
  } catch (error) {
    if (!isFileMissingError(error)) {
      const message = error instanceof Error ? error.message : 'Unknown config error.'
      throw new Error(`Failed to read config at ${configPath}: ${message}`)
    }
  }
  if (!isRecord(raw)) {
    throw new Error(`Failed to update config at ${configPath}: root must be a JSON object.`)
  }
  const next: Record<string, unknown> = { ...raw }
  if ('exportFormat' in patch) {
    if (patch.exportFormat === null || patch.exportFormat === undefined) {
      delete next.exportFormat
    } else {
      next.exportFormat = expectExportFormat(patch.exportFormat, 'exportFormat')
    }
  }
  if ('exportOutDir' in patch) {
    const outDir = patch.exportOutDir
    if (outDir === null || outDir === undefined || outDir.trim() === '') {
      delete next.exportOutDir
    } else {
      next.exportOutDir = outDir.trim()
    }
  }
  const contents = JSON.stringify(next, null, 2)
  const tempFile = `${configPath}.${process.pid}.tmp`
  await fs.writeFile(tempFile, `${contents}\n`, 'utf8')
  try {
    await fs.rename(tempFile, configPath)
  } catch {
    await fs.writeFile(configPath, `${contents}\n`, 'utf8')
  }
  cachedConfig = parseConfig(next)
  cachedConfigPath = configPath
}
export type PromptGeneratorSettingsPatch = {
  maxInputTokens?: number | null
  maxContextTokens?: number | null
  contextOverflowStrategy?: ContextOverflowStrategy | null
}
export const updateCliPromptGeneratorSettings = async (
  patch: PromptGeneratorSettingsPatch,
  options?: { configPath?: string },
): Promise<void> => {
  const configPath = options?.configPath ?? (await resolveConfigPathForWrite())
  const directory = path.dirname(configPath)
  await fs.mkdir(directory, { recursive: true })
  let raw: unknown = {}
  try {
    const contents = await fs.readFile(configPath, 'utf8')
    raw = JSON.parse(contents) as unknown
  } catch (error) {
    if (!isFileMissingError(error)) {
      const message = error instanceof Error ? error.message : 'Unknown config error.'
      throw new Error(`Failed to read config at ${configPath}: ${message}`)
    }
  }
  if (!isRecord(raw)) {
    throw new Error(`Failed to update config at ${configPath}: root must be a JSON object.`)
  }
  const next: Record<string, unknown> = { ...raw }
  const existingPromptGenerator = next.promptGenerator
  const promptGenerator = isRecord(existingPromptGenerator)
    ? { ...existingPromptGenerator }
    : ({} satisfies Record<string, unknown>)
  if ('maxInputTokens' in patch) {
    if (patch.maxInputTokens === null || patch.maxInputTokens === undefined) {
      delete promptGenerator.maxInputTokens
    } else {
      promptGenerator.maxInputTokens = expectPositiveInteger(
        patch.maxInputTokens,
        'promptGenerator.maxInputTokens',
      )
    }
  }
  if ('maxContextTokens' in patch) {
    if (patch.maxContextTokens === null || patch.maxContextTokens === undefined) {
      delete promptGenerator.maxContextTokens
    } else {
      promptGenerator.maxContextTokens = expectPositiveInteger(
        patch.maxContextTokens,
        'promptGenerator.maxContextTokens',
      )
    }
  }
  if ('contextOverflowStrategy' in patch) {
    if (patch.contextOverflowStrategy === null || patch.contextOverflowStrategy === undefined) {
      delete promptGenerator.contextOverflowStrategy
    } else {
      promptGenerator.contextOverflowStrategy = expectContextOverflowStrategy(
        patch.contextOverflowStrategy,
        'promptGenerator.contextOverflowStrategy',
      )
    }
  }
  if (Object.keys(promptGenerator).length === 0) {
    delete next.promptGenerator
  } else {
    next.promptGenerator = promptGenerator
  }
  const contents = JSON.stringify(next, null, 2)
  const tempFile = `${configPath}.${process.pid}.tmp`
  await fs.writeFile(tempFile, `${contents}\n`, 'utf8')
  try {
    await fs.rename(tempFile, configPath)
  } catch {
    await fs.writeFile(configPath, `${contents}\n`, 'utf8')
  }
  cachedConfig = parseConfig(next)
  cachedConfigPath = configPath
}
const parseConfig = (raw: unknown): PromptMakerCliConfig => {
  if (!isRecord(raw)) {
    throw new Error('CLI config must be a JSON object.')
  }
  const config: PromptMakerCliConfig = {}
  if (raw.openaiApiKey !== undefined) {
    config.openaiApiKey = expectString(raw.openaiApiKey, 'openaiApiKey')
  }
  if (raw.openaiBaseUrl !== undefined) {
    config.openaiBaseUrl = expectString(raw.openaiBaseUrl, 'openaiBaseUrl')
  }
  if (raw.geminiApiKey !== undefined) {
    config.geminiApiKey = expectString(raw.geminiApiKey, 'geminiApiKey')
  }
  if (raw.geminiBaseUrl !== undefined) {
    config.geminiBaseUrl = expectString(raw.geminiBaseUrl, 'geminiBaseUrl')
  }
  if (raw.promptGenerator !== undefined) {
    if (!isRecord(raw.promptGenerator)) {
      throw new Error('"promptGenerator" must be an object if provided.')
    }
    const promptGenerator: PromptGeneratorConfig = {}
    if (raw.promptGenerator.defaultModel !== undefined) {
      promptGenerator.defaultModel = expectString(
        raw.promptGenerator.defaultModel,
        'promptGenerator.defaultModel',
      )
    }
    if (raw.promptGenerator.defaultGeminiModel !== undefined) {
      promptGenerator.defaultGeminiModel = expectString(
        raw.promptGenerator.defaultGeminiModel,
        'promptGenerator.defaultGeminiModel',
      )
    }
    if (raw.promptGenerator.models !== undefined) {
      promptGenerator.models = parsePromptGeneratorModels(raw.promptGenerator.models)
    }
    if (raw.promptGenerator.maxInputTokens !== undefined) {
      promptGenerator.maxInputTokens = expectPositiveInteger(
        raw.promptGenerator.maxInputTokens,
        'promptGenerator.maxInputTokens',
      )
    }
    if (raw.promptGenerator.maxContextTokens !== undefined) {
      promptGenerator.maxContextTokens = expectPositiveInteger(
        raw.promptGenerator.maxContextTokens,
        'promptGenerator.maxContextTokens',
      )
    }
    if (raw.promptGenerator.contextOverflowStrategy !== undefined) {
      promptGenerator.contextOverflowStrategy = expectContextOverflowStrategy(
        raw.promptGenerator.contextOverflowStrategy,
        'promptGenerator.contextOverflowStrategy',
      )
    }
    config.promptGenerator = promptGenerator
  }
  if (raw.contextTemplates !== undefined) {
    if (!isRecord(raw.contextTemplates)) {
      throw new Error('"contextTemplates" must be an object if provided.')
    }
    const templates: Record<string, string> = {}
    for (const [key, value] of Object.entries(raw.contextTemplates)) {
      templates[key] = expectString(value, `contextTemplates.${key}`)
    }
    config.contextTemplates = templates
  }
  if (raw.theme !== undefined) {
    const theme = expectString(raw.theme, 'theme').trim()
    if (theme) {
      config.theme = theme
    }
  }
  if (raw.themeMode !== undefined) {
    config.themeMode = expectThemeMode(raw.themeMode, 'themeMode')
  }
  if (raw.resumeMode !== undefined) {
    config.resumeMode = expectResumeMode(raw.resumeMode, 'resumeMode')
  }
  if (raw.resumeSourceKind !== undefined) {
    config.resumeSourceKind = expectResumeSourceKind(raw.resumeSourceKind, 'resumeSourceKind')
  }
  if (raw.exportFormat !== undefined) {
    config.exportFormat = expectExportFormat(raw.exportFormat, 'exportFormat')
  }
  if (raw.exportOutDir !== undefined) {
    const exportOutDir = expectString(raw.exportOutDir, 'exportOutDir').trim()
    if (exportOutDir) {
      config.exportOutDir = exportOutDir
    }
  }
  return config
}
const parsePromptGeneratorModels = (value: unknown): ModelDefinition[] => {
  if (!Array.isArray(value)) {
    throw new Error('"promptGenerator.models" must be an array when provided.')
  }
  return value.map((entry, index) => parsePromptGeneratorModel(entry, index))
}
const parsePromptGeneratorModel = (value: unknown, index: number): ModelDefinition => {
  if (!isRecord(value)) {
    throw new Error(`promptGenerator.models[${index}] must be an object.`)
  }
  const id = expectString(value.id, `promptGenerator.models[${index}].id`).trim()
  if (!id) {
    throw new Error(`promptGenerator.models[${index}].id must not be empty.`)
  }
  const model: ModelDefinition = { id }
  if (value.label !== undefined) {
    const label = expectString(value.label, `promptGenerator.models[${index}].label`).trim()
    if (label) {
      model.label = label
    }
  }
  if (value.provider !== undefined) {
    model.provider = expectProvider(value.provider, `promptGenerator.models[${index}].provider`)
  }
  if (value.description !== undefined) {
    const description = expectString(
      value.description,
      `promptGenerator.models[${index}].description`,
    ).trim()
    if (description) {
      model.description = description
    }
  }
  if (value.notes !== undefined) {
    const notes = expectString(value.notes, `promptGenerator.models[${index}].notes`).trim()
    if (notes) {
      model.notes = notes
    }
  }
  if (value.capabilities !== undefined) {
    const capabilities = parseCapabilitiesField(
      value.capabilities,
      `promptGenerator.models[${index}].capabilities`,
    )
    if (capabilities.length > 0) {
      model.capabilities = capabilities
    }
  }
  if (value.default !== undefined) {
    model.default = expectBoolean(value.default, `promptGenerator.models[${index}].default`)
  }
  return model
}
const parseCapabilitiesField = (value: unknown, label: string): string[] => {
  if (typeof value === 'string') {
    const normalized = value.trim()
    return normalized ? [normalized] : []
  }
  if (Array.isArray(value)) {
    return value
      .map((entry, idx) => expectString(entry, `${label}[${idx}]`).trim())
      .filter((entry) => entry.length > 0)
  }
  throw new Error(`${label} must be a string or array of strings.`)
}
const CONTEXT_OVERFLOW_STRATEGIES = [
  'fail',
  'drop-smart',
  'drop-url',
  'drop-largest',
  'drop-oldest',
] as const satisfies ReadonlyArray<ContextOverflowStrategy>
function expectPositiveInteger(value: unknown, label: string): number {
  if (
    typeof value !== 'number' ||
    !Number.isFinite(value) ||
    !Number.isInteger(value) ||
    value <= 0
  ) {
    throw new Error(`${label} must be a positive integer.`)
  }
  return value
}
function expectContextOverflowStrategy(value: unknown, label: string): ContextOverflowStrategy {
  if (typeof value !== 'string') {
    throw new Error(`${label} must be one of: ${CONTEXT_OVERFLOW_STRATEGIES.join(', ')}.`)
  }
  const normalized = value.trim().toLowerCase()
  if (isContextOverflowStrategy(normalized)) {
    return normalized
  }
  throw new Error(`${label} must be one of: ${CONTEXT_OVERFLOW_STRATEGIES.join(', ')}.`)
}
function isContextOverflowStrategy(value: string): value is ContextOverflowStrategy {
  return CONTEXT_OVERFLOW_STRATEGIES.includes(value as ContextOverflowStrategy)
}
const expectBoolean = (value: unknown, label: string): boolean => {
  if (typeof value !== 'boolean') {
    throw new Error(`${label} must be a boolean.`)
  }
  return value
}
const RESUME_MODES = ['best-effort', 'strict'] as const satisfies ReadonlyArray<TuiResumeMode>
const expectResumeMode = (value: unknown, label: string): TuiResumeMode => {
  if (typeof value !== 'string') {
    throw new Error(`${label} must be one of: ${RESUME_MODES.join(', ')}.`)
  }
  const normalized = value.trim().toLowerCase()
  if ((RESUME_MODES as readonly string[]).includes(normalized)) {
    return normalized as TuiResumeMode
  }
  throw new Error(`${label} must be one of: ${RESUME_MODES.join(', ')}.`)
}
const RESUME_SOURCE_KINDS = [
  'history',
  'file',
] as const satisfies ReadonlyArray<TuiResumeSourceKind>
const expectResumeSourceKind = (value: unknown, label: string): TuiResumeSourceKind => {
  if (typeof value !== 'string') {
    throw new Error(`${label} must be one of: ${RESUME_SOURCE_KINDS.join(', ')}.`)
  }
  const normalized = value.trim().toLowerCase()
  if ((RESUME_SOURCE_KINDS as readonly string[]).includes(normalized)) {
    return normalized as TuiResumeSourceKind
  }
  throw new Error(`${label} must be one of: ${RESUME_SOURCE_KINDS.join(', ')}.`)
}
const EXPORT_FORMATS = ['json', 'yaml'] as const satisfies ReadonlyArray<'json' | 'yaml'>
const expectExportFormat = (value: unknown, label: string): 'json' | 'yaml' => {
  if (typeof value !== 'string') {
    throw new Error(`${label} must be one of: ${EXPORT_FORMATS.join(', ')}.`)
  }
  const normalized = value.trim().toLowerCase()
  if ((EXPORT_FORMATS as readonly string[]).includes(normalized)) {
    return normalized as 'json' | 'yaml'
  }
  throw new Error(`${label} must be one of: ${EXPORT_FORMATS.join(', ')}.`)
}
const expectProvider = (value: unknown, label: string): ModelProvider => {
  if (typeof value !== 'string') {
    throw new Error(`${label} must be one of openai, gemini, or other.`)
  }
  const normalized = value.trim().toLowerCase()
  if (normalized === 'openai' || normalized === 'gemini' || normalized === 'other') {
    return normalized as ModelProvider
  }
  throw new Error(`${label} must be one of openai, gemini, or other.`)
}
const expectString = (value: unknown, label: string): string => {
  if (typeof value !== 'string') {
    throw new Error(`${label} must be a string.`)
  }
  return value
}
const expectThemeMode = (value: unknown, label: string): ThemeMode => {
  if (typeof value !== 'string') {
    throw new Error(`${label} must be one of light, dark, system, or auto.`)
  }
  const normalized = value.trim().toLowerCase()
  if (normalized === 'auto') {
    return 'system'
  }
  if (normalized === 'light' || normalized === 'dark' || normalized === 'system') {
    return normalized as ThemeMode
  }
  throw new Error(`${label} must be one of light, dark, system, or auto.`)
}
function isRecord(value: unknown): value is Record<string, unknown> {
  return typeof value === 'object' && value !== null && !Array.isArray(value)
}
function hasErrnoCode(value: unknown): value is { code: string } {
  return (
    typeof value === 'object' &&
    value !== null &&
    'code' in value &&
    typeof (value as { code: unknown }).code === 'string'
  )
}
function isFileMissingError(error: unknown): boolean {
  return hasErrnoCode(error) && error.code === 'ENOENT'
}
</file>
<file path="src/__tests__/tui/command-screen-popup-bindings-contract.test.ts">
import { renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import {
  useCommandScreenPopupBindings,
  type UseCommandScreenPopupBindingsOptions,
} from '../../tui/screens/command/hooks/useCommandScreenPopupBindings'
const dom = new JSDOM('<!doctype html><html><body></body></html>')
type GlobalDom = { window: Window; document: Document }
beforeAll(() => {
  const target = globalThis as unknown as GlobalDom
  target.window = dom.window as unknown as Window
  target.document = dom.window.document
})
afterAll(() => {
  const target = globalThis as unknown as Partial<GlobalDom>
  delete target.window
  delete target.document
})
jest.mock('../../tui/screens/command/hooks/useCommandScreenPasteBindings', () => {
  const tokenLabel = jest.fn(() => null)
  const handleInputChange = jest.fn(() => undefined)
  const expandInputForSubmit = jest.fn((value: string) => value)
  return {
    useCommandScreenPasteBindings: jest.fn(() => ({
      tokenLabel,
      handleInputChange,
      expandInputForSubmit,
    })),
  }
})
jest.mock('../../tui/screens/command/hooks/useCommandScreenContextPopupBindings', () => {
  const onFilePopupDraftChange = jest.fn(() => undefined)
  const onAddFile = jest.fn(() => undefined)
  const onRemoveFile = jest.fn(() => undefined)
  const onUrlPopupDraftChange = jest.fn(() => undefined)
  const onAddUrl = jest.fn(() => undefined)
  const onRemoveUrl = jest.fn(() => undefined)
  const onImagePopupDraftChange = jest.fn(() => undefined)
  const onAddImage = jest.fn(() => undefined)
  const onRemoveImage = jest.fn(() => undefined)
  const onVideoPopupDraftChange = jest.fn(() => undefined)
  const onAddVideo = jest.fn(() => undefined)
  const onRemoveVideo = jest.fn(() => undefined)
  const onSmartPopupDraftChange = jest.fn(() => undefined)
  const onSmartRootSubmit = jest.fn(() => undefined)
  return {
    useCommandScreenContextPopupBindings: jest.fn(() => ({
      filePopupSuggestions: [],
      filePopupSuggestionSelectionIndex: 0,
      filePopupSuggestionsFocused: false,
      onFilePopupDraftChange,
      onAddFile,
      onRemoveFile,
      onUrlPopupDraftChange,
      onAddUrl,
      onRemoveUrl,
      imagePopupSuggestions: [],
      imagePopupSuggestionSelectionIndex: 0,
      imagePopupSuggestionsFocused: false,
      onImagePopupDraftChange,
      onAddImage,
      onRemoveImage,
      videoPopupSuggestions: [],
      videoPopupSuggestionSelectionIndex: 0,
      videoPopupSuggestionsFocused: false,
      onVideoPopupDraftChange,
      onAddVideo,
      onRemoveVideo,
      smartPopupSuggestions: [],
      smartPopupSuggestionSelectionIndex: 0,
      smartPopupSuggestionsFocused: false,
      onSmartPopupDraftChange,
      onSmartRootSubmit,
    })),
  }
})
jest.mock('../../tui/screens/command/hooks/useCommandScreenHistoryIntentPopupBindings', () => {
  const onHistoryPopupDraftChange = jest.fn(() => undefined)
  const onHistoryPopupSubmit = jest.fn(() => undefined)
  const onIntentPopupDraftChange = jest.fn(() => undefined)
  return {
    useCommandScreenHistoryIntentPopupBindings: jest.fn(() => ({
      history: {
        historyPopupItems: [],
        onHistoryPopupDraftChange,
        onHistoryPopupSubmit,
      },
      intent: {
        intentPopupSuggestions: [],
        intentPopupSuggestionSelectionIndex: 0,
        intentPopupSuggestionsFocused: false,
        onIntentPopupDraftChange,
      },
    })),
  }
})
jest.mock('../../tui/screens/command/hooks/useModelPopupData', () => ({
  useModelPopupData: jest.fn(() => ({
    modelPopupOptions: [],
    modelPopupRecentCount: 0,
    modelPopupSelection: 0,
  })),
}))
jest.mock('../../tui/screens/command/hooks/useReasoningPopup', () => ({
  useReasoningPopup: jest.fn(() => ({
    reasoningPopupVisibleRows: 0,
    reasoningPopupLines: [],
  })),
}))
jest.mock('../../tui/screens/command/hooks/useThemePopupGlue', () => ({
  useThemePopupGlue: jest.fn(() => ({
    themeCount: 0,
    onThemeConfirm: jest.fn(),
    onThemeCancel: jest.fn(),
  })),
}))
jest.mock('../../tui/screens/command/hooks/useThemeModePopupGlue', () => ({
  useThemeModePopupGlue: jest.fn(() => ({
    optionCount: 0,
    onConfirm: jest.fn(),
    onCancel: jest.fn(),
  })),
}))
jest.mock('../../tui/screens/command/hooks/usePopupKeyboardShortcuts', () => ({
  usePopupKeyboardShortcuts: jest.fn(),
}))
jest.mock('../../tui/screens/command/hooks/useCommandScreenSubmitBindings', () => {
  const handleSubmit = jest.fn(() => undefined)
  const onSeriesSubmit = jest.fn(() => undefined)
  return {
    useCommandScreenSubmitBindings: jest.fn(() => ({
      handleSubmit,
      onSeriesSubmit,
    })),
  }
})
jest.mock('../../tui/screens/command/hooks/useMiscPopupDraftHandlers', () => {
  const onModelPopupQueryChange = jest.fn(() => undefined)
  const onSeriesDraftChange = jest.fn(() => undefined)
  const onInstructionsDraftChange = jest.fn(() => undefined)
  const onTestDraftChange = jest.fn(() => undefined)
  const onBudgetsMaxContextTokensDraftChange = jest.fn(() => undefined)
  const onBudgetsMaxInputTokensDraftChange = jest.fn(() => undefined)
  const onResumePayloadPathDraftChange = jest.fn(() => undefined)
  const onExportOutPathDraftChange = jest.fn(() => undefined)
  return {
    useMiscPopupDraftHandlers: jest.fn(() => ({
      onModelPopupQueryChange,
      onSeriesDraftChange,
      onInstructionsDraftChange,
      onTestDraftChange,
      onBudgetsMaxContextTokensDraftChange,
      onBudgetsMaxInputTokensDraftChange,
      onResumePayloadPathDraftChange,
      onExportOutPathDraftChange,
    })),
  }
})
describe('useCommandScreenPopupBindings (shape contract)', () => {
  it('returns grouped handlers that are safe to call', () => {
    const setState =
      jest.fn() as unknown as UseCommandScreenPopupBindingsOptions['popup']['setState']
    const options: UseCommandScreenPopupBindingsOptions = {
      input: {
        value: '',
        setValue: jest.fn(),
        setPasteActive: jest.fn(),
        consumeSuppressedTextInputChange: jest.fn(() => false),
        suppressNextInput: jest.fn(),
        updateLastTypedIntent: jest.fn(),
        intentFilePath: '',
        lastUserIntentRef: { current: null },
      },
      popup: {
        state: null,
        setState,
        isOpen: false,
        helpOpen: false,
        close: jest.fn(),
        actions: {
          handleCommandSelection: jest.fn(),
          handleModelPopupSubmit: jest.fn(),
          applyToggleSelection: jest.fn(),
          handleIntentFileSubmit: jest.fn(),
          handleResumeSubmit: jest.fn(),
          handleExportSubmit: jest.fn(),
          handleBudgetsSubmit: jest.fn(),
          handleSeriesIntentSubmit: jest.fn(),
        },
      },
      menu: {
        isActive: false,
        selectedCommandId: null,
        argsRaw: '',
        isCommandMode: false,
        actions: {
          handleNewCommand: jest.fn(),
          handleReuseCommand: jest.fn(),
        },
      },
      generation: {
        isGenerating: false,
        isAwaitingRefinement: false,
        submitRefinement: jest.fn(),
        runGeneration: jest.fn(async () => undefined),
      },
      history: {
        pushHistory: jest.fn(),
        addCommandHistoryEntry: jest.fn(),
        commandHistoryValues: [],
      },
      context: {
        droppedFilePath: null,
        files: [],
        urls: [],
        images: [],
        videos: [],
        pdfs: [],
        smartContextEnabled: false,
        smartContextRoot: null,
        addFile: jest.fn(),
        removeFile: jest.fn(),
        addUrl: jest.fn(),
        removeUrl: jest.fn(),
        updateUrl: jest.fn(),
        addImage: jest.fn(),
        removeImage: jest.fn(),
        addVideo: jest.fn(),
        removeVideo: jest.fn(),
        addPdf: jest.fn(),
        removePdf: jest.fn(),
        toggleSmartContext: jest.fn(),
        setSmartRoot: jest.fn(),
        notify: jest.fn(),
        modelOptions: [],
        lastReasoning: null,
        terminalColumns: 80,
        reasoningPopupHeight: 10,
      },
    }
    const { result, rerender } = renderHook((props) => useCommandScreenPopupBindings(props), {
      initialProps: options,
    })
    expect(result.current).toEqual(
      expect.objectContaining({
        input: expect.objectContaining({
          tokenLabel: expect.any(Function),
          onChange: expect.any(Function),
        }),
        submit: expect.objectContaining({
          onSubmit: expect.any(Function),
          onSeriesSubmit: expect.any(Function),
        }),
        popup: expect.any(Object),
      }),
    )
    expect(() => result.current.input.onChange('next')).not.toThrow()
    expect(() => result.current.submit.onSubmit('run')).not.toThrow()
    expect(() => result.current.popup.misc.onInstructionsDraftChange('draft')).not.toThrow()
    expect(() => result.current.popup.context.file.onAdd('README.md')).not.toThrow()
    const firstInputChange = result.current.input.onChange
    const firstSubmit = result.current.submit.onSubmit
    const firstInstructionsDraftChange = result.current.popup.misc.onInstructionsDraftChange
    rerender(options)
    expect(result.current.input.onChange).toBe(firstInputChange)
    expect(result.current.submit.onSubmit).toBe(firstSubmit)
    expect(result.current.popup.misc.onInstructionsDraftChange).toBe(firstInstructionsDraftChange)
  })
})
</file>
<file path="src/__tests__/tui/popup-manager-command-mapping.test.ts">
import {
  JSON_INTERACTIVE_ERROR,
  mapPopupCommandSelection,
  type CommandMappingContext,
} from '../../tui/hooks/popup-manager/command-mapping'
describe('popup manager command mapping', () => {
  const baseContext: CommandMappingContext = {
    copyEnabled: false,
    chatGptEnabled: false,
    jsonOutputEnabled: false,
    interactiveTransportPath: undefined,
    urls: [],
    images: [],
    videos: [],
    pdfs: [],
    smartContextEnabled: false,
    smartContextRoot: null,
  }
  it('blocks /json when interactive transport is active', () => {
    const result = mapPopupCommandSelection({
      commandId: 'json',
      argsRaw: undefined,
      context: {
        ...baseContext,
        interactiveTransportPath: '/tmp/socket',
      },
    })
    expect(result).toEqual({
      kind: 'steps',
      steps: [
        { type: 'push-history', message: JSON_INTERACTIVE_ERROR, kind: 'system' },
        { type: 'set-input', value: '' },
      ],
    })
  })
  it('maps /url args into add/warn steps with dedupe', () => {
    const result = mapPopupCommandSelection({
      commandId: 'url',
      argsRaw: 'http://a.example, example.com http://a.example',
      context: baseContext,
    })
    if (result.kind !== 'steps') {
      throw new Error('Expected steps result')
    }
    expect(result.steps).toEqual([
      { type: 'add-url', value: 'http://a.example' },
      { type: 'push-history', message: 'Context URL added: http://a.example', kind: 'system' },
      { type: 'push-history', message: 'Warning: Invalid URL: example.com', kind: 'system' },
      { type: 'set-input', value: '' },
      { type: 'close-popup' },
    ])
  })
  it('opens budgets popup from /budgets', () => {
    const result = mapPopupCommandSelection({
      commandId: 'budgets',
      argsRaw: undefined,
      context: baseContext,
    })
    expect(result).toEqual({
      kind: 'steps',
      steps: [
        { type: 'open-popup', popup: 'budgets' },
        { type: 'set-input', value: '' },
      ],
    })
  })
  it('opens resume popup from /resume', () => {
    const result = mapPopupCommandSelection({
      commandId: 'resume',
      argsRaw: undefined,
      context: baseContext,
    })
    expect(result).toEqual({
      kind: 'steps',
      steps: [
        { type: 'open-popup', popup: 'resume' },
        { type: 'set-input', value: '' },
      ],
    })
  })
  it('opens export popup from /export', () => {
    const result = mapPopupCommandSelection({
      commandId: 'export',
      argsRaw: undefined,
      context: baseContext,
    })
    expect(result).toEqual({
      kind: 'steps',
      steps: [
        { type: 'open-popup', popup: 'export' },
        { type: 'set-input', value: '' },
      ],
    })
  })
  it('clears smart root when disabling smart context', () => {
    const result = mapPopupCommandSelection({
      commandId: 'smart',
      argsRaw: 'off',
      context: {
        ...baseContext,
        smartContextEnabled: true,
        smartContextRoot: 'src',
      },
    })
    if (result.kind !== 'steps') {
      throw new Error('Expected steps result')
    }
    expect(result.steps).toEqual([
      { type: 'set-smart-root', value: '' },
      { type: 'toggle-smart-context' },
      {
        type: 'notify',
        message: 'Smart context disabled; root cleared',
        kind: 'warning',
      },
      { type: 'set-input', value: '' },
      { type: 'close-popup' },
    ])
  })
})
</file>
<file path="src/__tests__/generate-command.test.ts">
// eslint-disable unnecessary-semicolon
import clipboard from 'clipboardy'
import open from 'open'
import { callLLM } from '@prompt-maker/core'
import { runGenerateCommand, InteractiveTransport } from '../generate-command'
import { appendToHistory } from '../history-logger'
import { readFromStdin } from '../io'
import { resolveFileContext } from '../file-context'
import { resolveSmartContextFiles } from '../smart-context-service'
import {
  createPromptGeneratorService,
  resolveDefaultGenerateModel,
  isGemini,
} from '../prompt-generator-service'
import { resolveUrlContext } from '../url-context'
import { countTokens } from '../token-counter'
jest.mock('enquirer', () => {
  const prompt = jest.fn()
  return {
    __esModule: true,
    default: { prompt },
    prompt,
  }
})
const promptMock = (jest.requireMock('enquirer') as { prompt: jest.Mock }).prompt
jest.mock('../config', () => ({
  loadCliConfig: jest.fn().mockResolvedValue({
    promptGenerator: { defaultGeminiModel: 'gemini-1.5-pro' },
  }),
}))
const mockLoadCliConfig = (jest.requireMock('../config') as { loadCliConfig: jest.Mock })
  .loadCliConfig
jest.mock('clipboardy', () => ({ write: jest.fn() }))
jest.mock('open', () => jest.fn())
jest.mock('@prompt-maker/core', () => ({ callLLM: jest.fn() }))
jest.mock('../prompt-generator-service', () => {
  const actual = jest.requireActual(
    '../prompt-generator-service',
  ) as typeof import('../prompt-generator-service')
  return {
    createPromptGeneratorService: jest.fn(),
    ensureModelCredentials: jest.fn(),
    isGemini: jest.fn((model: string) => model.startsWith('gemini')),
    resolveDefaultGenerateModel: jest.fn().mockResolvedValue('gpt-4o-mini'),
    sanitizePromptForTargetModelLeakage: actual.sanitizePromptForTargetModelLeakage,
  }
})
jest.mock('../file-context', () => ({
  resolveFileContext: jest.fn().mockResolvedValue([{ path: 'ctx.md', content: '# ctx' }]),
  formatContextForPrompt: jest.requireActual('../file-context').formatContextForPrompt,
}))
jest.mock('../smart-context-service', () => ({
  resolveSmartContextFiles: jest.fn().mockResolvedValue([]),
}))
jest.mock('../url-context', () => ({
  resolveUrlContext: jest.fn().mockResolvedValue([]),
}))
jest.mock('../history-logger', () => {
  const os = jest.requireActual('node:os') as typeof import('node:os')
  const path = jest.requireActual('node:path') as typeof import('node:path')
  return {
    appendToHistory: jest.fn().mockResolvedValue(undefined),
    resolveHistoryFilePath: jest.fn(() => {
      const envHome = process.env.HOME?.trim()
      const homeDir = envHome && envHome.length > 0 ? envHome : os.homedir()
      return path.join(homeDir, '.config', 'prompt-maker-cli', 'history.jsonl')
    }),
  }
})
jest.mock('../io', () => ({ readFromStdin: jest.fn().mockResolvedValue(null) }))
jest.mock('../image-loader', () => ({ resolveImageParts: jest.fn().mockResolvedValue([]) }))
jest.mock('../prompt-generator/video-parts', () => ({
  resolveVideoParts: jest.fn().mockResolvedValue([]),
}))
jest.mock('../prompt-generator/pdf-parts', () => ({
  resolvePdfParts: jest.fn().mockResolvedValue([]),
}))
jest.mock('../token-counter', () => ({
  countTokens: jest.fn().mockReturnValue(10),
  formatTokenCount: jest.fn((count: number) => `${count} tokens`),
}))
jest.mock('node:fs/promises', () => ({
  readFile: jest.fn(),
  stat: jest.fn(),
  writeFile: jest.fn(),
}))
const fs = jest.requireMock('node:fs/promises') as {
  readFile: jest.Mock
  stat: jest.Mock
  writeFile: jest.Mock
}
const promptService = { generatePrompt: jest.fn() }
const mockCreatePromptService = createPromptGeneratorService as jest.Mock
const mockResolveDefaultModel = resolveDefaultGenerateModel as jest.Mock
const mockResolveFileContext = resolveFileContext as jest.Mock
const mockResolveSmartContext = resolveSmartContextFiles as jest.Mock
const mockResolveUrlContext = resolveUrlContext as jest.Mock
const mockReadFromStdin = readFromStdin as jest.Mock
const mockCountTokens = countTokens as jest.Mock
const mockIsGemini = isGemini as jest.Mock
const mockCallLLM = callLLM as jest.Mock
mockCreatePromptService.mockResolvedValue(promptService)
mockResolveDefaultModel.mockResolvedValue('gpt-4o-mini')
const originalStdinIsTTY = process.stdin.isTTY
const originalStdoutIsTTY = process.stdout.isTTY
const setTtyState = (stdinTty: boolean, stdoutTty: boolean): void => {
  Object.defineProperty(process.stdin, 'isTTY', { value: stdinTty, configurable: true })
  Object.defineProperty(process.stdout, 'isTTY', { value: stdoutTty, configurable: true })
}
afterAll(() => {
  Object.defineProperty(process.stdin, 'isTTY', { value: originalStdinIsTTY })
  Object.defineProperty(process.stdout, 'isTTY', { value: originalStdoutIsTTY })
})
type TestInteractiveCommand = { type: 'refine'; instruction: string } | { type: 'finish' }
type LifecycleEmitter = Parameters<InteractiveTransport['setEventEmitter']>[0]
const setupTransportMock = (commands: TestInteractiveCommand[], events: string[]): (() => void) => {
  const commandQueue = [...commands]
  let lifecycleEmitter: LifecycleEmitter | null = null
  const startSpy = jest
    .spyOn(InteractiveTransport.prototype, 'start')
    .mockImplementation(async () => {
      lifecycleEmitter?.({ event: 'transport.listening', path: '/tmp/pmc.sock' })
      lifecycleEmitter?.({ event: 'transport.client.connected', status: 'connected' })
    })
  const stopSpy = jest
    .spyOn(InteractiveTransport.prototype, 'stop')
    .mockImplementation(async () => {
      lifecycleEmitter?.({ event: 'transport.client.disconnected', status: 'disconnected' })
    })
  const writerSpy = jest
    .spyOn(InteractiveTransport.prototype, 'getEventWriter')
    .mockReturnValue((chunk: string) => {
      events.push(chunk.trim())
    })
  const setEmitterSpy = jest
    .spyOn(InteractiveTransport.prototype, 'setEventEmitter')
    .mockImplementation(function (this: InteractiveTransport, emitter: LifecycleEmitter) {
      lifecycleEmitter = emitter
    })
  const nextCommandSpy = jest
    .spyOn(InteractiveTransport.prototype, 'nextCommand')
    .mockImplementation(async () => commandQueue.shift() ?? null)
  return () => {
    startSpy.mockRestore()
    stopSpy.mockRestore()
    writerSpy.mockRestore()
    setEmitterSpy.mockRestore()
    nextCommandSpy.mockRestore()
  }
}
describe('runGenerateCommand', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    mockCreatePromptService.mockResolvedValue(promptService)
    mockResolveDefaultModel.mockResolvedValue('gpt-4o-mini')
    mockLoadCliConfig.mockResolvedValue({
      promptGenerator: { defaultGeminiModel: 'gemini-1.5-pro' },
    })
    promptService.generatePrompt.mockResolvedValue('prompt v1')
    setTtyState(false, false)
    fs.readFile.mockReset()
    fs.stat.mockReset()
    fs.writeFile.mockReset()
    promptMock.mockReset()
    mockResolveFileContext.mockResolvedValue([{ path: 'ctx.md', content: '# ctx' }])
    mockResolveSmartContext.mockResolvedValue([])
    mockResolveUrlContext.mockResolvedValue([])
    mockReadFromStdin.mockResolvedValue(null)
    mockCountTokens.mockReturnValue(10)
  })
  it('generates a prompt with inline intent and logs output', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['Write something'])
    expect(promptService.generatePrompt).toHaveBeenCalledWith(
      expect.objectContaining({ intent: 'Write something' }),
    )
    expect(appendToHistory).toHaveBeenCalledWith(
      expect.objectContaining({ intent: 'Write something', prompt: 'prompt v1' }),
    )
    const sawPrompt = log.mock.calls.some(
      (args) => typeof args[0] === 'string' && args[0].includes('prompt v1'),
    )
    expect(sawPrompt).toBe(true)
    log.mockRestore()
  })
  it('reads intent from file when --intent-file is provided', async () => {
    fs.stat.mockResolvedValue({ size: 128 })
    fs.readFile.mockResolvedValue(Buffer.from(' file intent '))
    await runGenerateCommand(['--intent-file', 'intent.txt'])
    expect(fs.stat).toHaveBeenCalledWith('intent.txt')
    expect(promptService.generatePrompt).toHaveBeenCalledWith(
      expect.objectContaining({ intent: 'file intent' }),
    )
  })
  it('treats inline intent after -i as a file path when it exists', async () => {
    fs.stat.mockResolvedValue({ size: 256 })
    fs.readFile.mockResolvedValue(Buffer.from('interactive file intent'))
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    await runGenerateCommand(['-i', 'intent.md'])
    expect(promptService.generatePrompt).toHaveBeenCalledWith(
      expect.objectContaining({ intent: 'interactive file intent' }),
    )
    expect(warn).toHaveBeenCalledWith(expect.stringContaining('--intent-file'))
    warn.mockRestore()
  })
  it('falls back to stdin when no inline intent is provided', async () => {
    mockReadFromStdin.mockResolvedValue('stdin intent')
    await runGenerateCommand([])
    expect(promptService.generatePrompt).toHaveBeenCalledWith(
      expect.objectContaining({ intent: 'stdin intent' }),
    )
  })
  it('appends smart context files when enabled', async () => {
    mockResolveSmartContext.mockResolvedValue([{ path: 'smart.md', content: 'smart content' }])
    await runGenerateCommand(['intent', '--smart-context', '--context', 'ctx/**/*.md'])
    const call = promptService.generatePrompt.mock.calls[0][0]
    expect(mockResolveSmartContext).toHaveBeenCalled()
    expect(call.fileContext).toEqual([
      { path: 'ctx.md', content: '# ctx' },
      { path: 'smart.md', content: 'smart content' },
    ])
  })
  it('merges URL context before smart context resolution', async () => {
    mockResolveUrlContext.mockResolvedValue([
      { path: 'url:https://example.com', content: 'Example Domain' },
    ])
    mockResolveSmartContext.mockResolvedValue([{ path: 'smart.md', content: 'smart content' }])
    await runGenerateCommand(['intent text', '--url', 'https://example.com', '--smart-context'])
    const smartCallArgs = mockResolveSmartContext.mock.calls[0]
    expect(smartCallArgs[1]).toEqual([
      { path: 'ctx.md', content: '# ctx' },
      { path: 'url:https://example.com', content: 'Example Domain' },
    ])
    expect(smartCallArgs[3]).toBeUndefined()
    const call = promptService.generatePrompt.mock.calls[0][0]
    expect(call.fileContext).toEqual([
      { path: 'ctx.md', content: '# ctx' },
      { path: 'url:https://example.com', content: 'Example Domain' },
      { path: 'smart.md', content: 'smart content' },
    ])
  })
  it('switches to gemini model when video assets provided', async () => {
    mockIsGemini.mockImplementation((model: string) => model.startsWith('gemini'))
    mockLoadCliConfig.mockResolvedValueOnce(null)
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--video', 'clip.mp4'])
    const call = promptService.generatePrompt.mock.calls[0][0]
    expect(call.model).toBe('gemini-2.5-pro')
    expect(warn).toHaveBeenCalledWith('Switching to gemini-2.5-pro to support video input.')
    warn.mockRestore()
  })
  it('passes smart context root through when provided', async () => {
    mockResolveSmartContext.mockResolvedValue([{ path: 'smart.md', content: 'content' }])
    await runGenerateCommand(['intent text', '--smart-context', '--smart-context-root', 'apps'])
    const smartCallArgs = mockResolveSmartContext.mock.calls[0]
    expect(smartCallArgs[3]).toBe('apps')
  })
  it('prints context files when --show-context is provided', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--show-context'])
    const sawContextDump = log.mock.calls.some(
      (args) => typeof args[0] === 'string' && args[0]?.includes('<file path="ctx.md">'),
    )
    expect(sawContextDump).toBe(true)
    log.mockRestore()
  })
  it('prints json context when --show-context and --context-format json are provided', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--show-context', '--context-format', 'json'])
    const jsonCall = log.mock.calls.find(
      (args) => typeof args[0] === 'string' && args[0].trim().startsWith('['),
    )
    expect(jsonCall).toBeDefined()
    log.mockRestore()
  })
  it('writes resolved context to a file when --context-file is provided', async () => {
    fs.writeFile.mockResolvedValue(undefined)
    await runGenerateCommand(['intent text', '--context-file', 'ctx.out'])
    expect(fs.writeFile).toHaveBeenCalledWith(
      'ctx.out',
      expect.stringContaining('<file path="ctx.md">'),
      'utf8',
    )
  })
  it('writes json context when --context-file and --context-format json are used', async () => {
    fs.writeFile.mockResolvedValue(undefined)
    await runGenerateCommand([
      'intent text',
      '--context-file',
      'ctx.json',
      '--context-format',
      'json',
    ])
    const [, payload] = fs.writeFile.mock.calls[0]
    expect(typeof payload).toBe('string')
    expect(payload.trim().startsWith('[')).toBe(true)
  })
  it('runs interactive refinements when tty is present', async () => {
    setTtyState(true, true)
    promptService.generatePrompt
      .mockResolvedValueOnce('first prompt')
      .mockResolvedValueOnce('second prompt')
    promptMock
      .mockResolvedValueOnce({ refine: true })
      .mockResolvedValueOnce({ refinement: 'Refine tone' })
      .mockResolvedValueOnce({ refine: false })
    await runGenerateCommand(['intent text', '--interactive'])
    expect(promptService.generatePrompt).toHaveBeenCalledTimes(2)
    expect(promptService.generatePrompt).toHaveBeenLastCalledWith(
      expect.objectContaining({
        refinementInstruction: 'Refine tone',
        previousPrompt: 'first prompt',
      }),
    )
  })
  it('polishes prompt and copies/open as requested', async () => {
    mockCallLLM.mockResolvedValue('polished prompt')
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--polish', '--copy', '--open-chatgpt'])
    expect(callLLM).toHaveBeenCalledWith(expect.any(Array), 'gpt-4o-mini')
    expect(clipboard.write).toHaveBeenCalledWith('polished prompt')
    expect(open).toHaveBeenCalledWith(expect.stringContaining('https://chatgpt.com'))
    log.mockRestore()
  })
  it('sanitizes polished output to avoid leaking --target', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    mockCallLLM.mockResolvedValue(
      'Target runtime model: gpt-4o-mini\nPolished output for gpt-4o-mini.',
    )
    await runGenerateCommand(['intent text', '--polish', '--json', '--target', 'gpt-4o-mini'])
    const firstCall = log.mock.calls[0]
    if (!firstCall) {
      throw new Error('Expected JSON output')
    }
    const payload = JSON.parse(firstCall[0] as string) as {
      polishedPrompt?: string
      targetModel: string
    }
    expect(payload.targetModel).toBe('gpt-4o-mini')
    expect(payload.polishedPrompt).toBeDefined()
    expect(payload.polishedPrompt?.toLowerCase()).not.toContain('target runtime model')
    expect(payload.polishedPrompt?.toLowerCase()).not.toContain('gpt-4o-mini')
    log.mockRestore()
  })
  it('emits json payload when --json is provided', async () => {
    jest.useFakeTimers().setSystemTime(new Date('2024-01-01T00:00:00Z'))
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--json'])
    expect(log).toHaveBeenCalled()
    const firstCall = log.mock.calls[0]
    if (!firstCall) {
      throw new Error('Expected console.log to be called with JSON output')
    }
    const payload = JSON.parse(firstCall[0] as string) as {
      intent: string
      targetModel: string
      contextPaths: Array<{ path: string; source: string }>
    }
    expect(payload.intent).toBe('intent text')
    expect(payload.targetModel).toBe('gpt-4o-mini')
    expect(payload.contextPaths).toEqual(
      expect.arrayContaining([
        expect.objectContaining({ source: 'intent', path: 'inline-intent' }),
        expect.objectContaining({ source: 'file', path: 'ctx.md' }),
      ]),
    )
    expect(payload).not.toHaveProperty('outputPath')
    expect(appendToHistory).toHaveBeenCalledTimes(1)
    jest.useRealTimers()
    log.mockRestore()
  })
  it('uses explicit --target and defaults separately from --model', async () => {
    jest.useFakeTimers().setSystemTime(new Date('2024-01-01T00:00:00Z'))
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand([
      'intent text',
      '--model',
      'gpt-4o',
      '--target',
      'gpt-4o-mini',
      '--json',
    ])
    const firstCall = log.mock.calls[0]
    if (!firstCall) {
      throw new Error('Expected JSON output')
    }
    const payload = JSON.parse(firstCall[0] as string) as { model: string; targetModel: string }
    expect(payload.model).toBe('gpt-4o')
    expect(payload.targetModel).toBe('gpt-4o-mini')
    const generationCall = promptService.generatePrompt.mock.calls[0]?.[0]
    expect(generationCall).toEqual(
      expect.objectContaining({ model: 'gpt-4o', targetModel: 'gpt-4o-mini' }),
    )
    jest.useRealTimers()
    log.mockRestore()
  })
  it('streams jsonl events when enabled', async () => {
    const chunks: string[] = []
    const writeSpy = jest.spyOn(process.stdout, 'write').mockImplementation(((
      chunk: string | Uint8Array,
      encoding?: BufferEncoding,
      cb?: (err?: Error) => void,
    ) => {
      if (typeof chunk === 'string') {
        chunks.push(chunk)
      }
      if (typeof cb === 'function') {
        cb()
      }
      return true
    }) as unknown as typeof process.stdout.write)
    await runGenerateCommand(['intent text', '--stream', 'jsonl', '--progress=false'])
    writeSpy.mockRestore()
    const events = chunks
      .map((chunk) => chunk.trim())
      .filter((chunk) => chunk.startsWith('{') && chunk.endsWith('}'))
      .map((chunk) => JSON.parse(chunk) as { event: string } & Record<string, unknown>)
    const eventTypes = events.map((event) => event.event)
    expect(eventTypes).toContain('context.telemetry')
    expect(eventTypes).toContain('generation.iteration.start')
    expect(eventTypes).toContain('generation.iteration.complete')
    expect(eventTypes).toContain('generation.final')
    const generateStart = events.find((event) => {
      if (event.event !== 'progress.update') {
        return false
      }
      return (event as { label?: string }).label === 'Generating prompt'
    }) as { state?: string; scope?: string } | undefined
    expect(generateStart?.state).toBe('start')
    expect(generateStart?.scope).toBe('generate')
    const iterationStart = events.find((event) => event.event === 'generation.iteration.start') as
      | { inputTokens?: number }
      | undefined
    expect(iterationStart?.inputTokens).toBeGreaterThan(0)
    const finalEvent = events.find((event) => event.event === 'generation.final') as
      | { result?: { schemaVersion?: string; contextPaths?: Array<{ source: string }> } }
      | undefined
    expect(finalEvent?.result?.schemaVersion).toBe('1')
    expect(finalEvent?.result?.contextPaths).toEqual(
      expect.arrayContaining([expect.objectContaining({ source: 'intent' })]),
    )
  })
  it('emits context.overflow and prunes contextPaths when budget exceeded', async () => {
    mockResolveFileContext.mockResolvedValueOnce([
      { path: 'ctx1.md', content: 'c1' },
      { path: 'ctx2.md', content: 'c2' },
      { path: 'ctx3.md', content: 'c3' },
    ])
    mockCountTokens.mockImplementation((value: string) => {
      if (value === 'c1' || value === 'c2' || value === 'c3') {
        return 10
      }
      return 0
    })
    const chunks: string[] = []
    const writeSpy = jest.spyOn(process.stdout, 'write').mockImplementation(((
      chunk: string | Uint8Array,
      encoding?: BufferEncoding,
      cb?: (err?: Error) => void,
    ) => {
      if (typeof chunk === 'string') {
        chunks.push(chunk)
      }
      if (typeof cb === 'function') {
        cb()
      }
      return true
    }) as unknown as typeof process.stdout.write)
    await runGenerateCommand([
      'intent text',
      '--stream',
      'jsonl',
      '--progress=false',
      '--max-context-tokens',
      '10',
      '--context-overflow',
      'drop-oldest',
    ])
    writeSpy.mockRestore()
    const events = chunks
      .map((chunk) => chunk.trim())
      .filter((chunk) => chunk.startsWith('{') && chunk.endsWith('}'))
      .map((chunk) => JSON.parse(chunk) as { event: string } & Record<string, unknown>)
    const overflowEvent = events.find((event) => event.event === 'context.overflow') as
      | {
          strategy?: string
          before?: { fileTokens?: number }
          after?: { fileTokens?: number }
          droppedPaths?: Array<{ path: string; source: string }>
        }
      | undefined
    expect(overflowEvent?.strategy).toBe('drop-oldest')
    expect(overflowEvent?.before?.fileTokens).toBe(30)
    expect(overflowEvent?.after?.fileTokens).toBe(10)
    expect(overflowEvent?.droppedPaths).toEqual([
      { path: 'ctx1.md', source: 'file' },
      { path: 'ctx2.md', source: 'file' },
    ])
    const finalEvent = events.find((event) => event.event === 'generation.final') as
      | {
          result?: {
            schemaVersion?: string
            contextPaths?: Array<{ path: string; source: string }>
          }
        }
      | undefined
    expect(finalEvent?.result?.schemaVersion).toBe('1')
    expect(finalEvent?.result?.contextPaths).toEqual(
      expect.arrayContaining([
        { path: 'inline-intent', source: 'intent' },
        { path: 'ctx3.md', source: 'file' },
      ]),
    )
    const droppedInFinal = finalEvent?.result?.contextPaths?.filter((entry) =>
      ['ctx1.md', 'ctx2.md'].includes(entry.path),
    )
    expect(droppedInFinal).toEqual([])
  })
  it('emits only jsonl lines when quiet streaming is requested', async () => {
    const chunks: string[] = []
    const writeSpy = jest.spyOn(process.stdout, 'write').mockImplementation(((
      chunk: string | Uint8Array,
      encoding?: BufferEncoding,
      cb?: (err?: Error) => void,
    ) => {
      if (typeof chunk === 'string') {
        chunks.push(chunk)
      }
      if (typeof cb === 'function') {
        cb()
      }
      return true
    }) as unknown as typeof process.stdout.write)
    const logSpy = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--quiet', '--stream', 'jsonl', '--progress=false'])
    writeSpy.mockRestore()
    expect(logSpy).not.toHaveBeenCalled()
    logSpy.mockRestore()
    const jsonLines = chunks.map((chunk) => chunk.trim()).filter(Boolean)
    expect(jsonLines.length).toBeGreaterThan(0)
    jsonLines.forEach((line) => {
      expect(() => JSON.parse(line)).not.toThrow()
    })
  })
  it('suppresses UI banners when --quiet is provided', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--quiet'])
    expect(log).not.toHaveBeenCalled()
    log.mockRestore()
  })
  it('still prints JSON payload when --quiet and --json are combined', async () => {
    jest.useFakeTimers().setSystemTime(new Date('2024-01-01T00:00:00Z'))
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--quiet', '--json'])
    expect(log).toHaveBeenCalledTimes(1)
    expect(log).toHaveBeenCalledWith(expect.stringContaining('"schemaVersion": "1"'))
    expect(log).toHaveBeenCalledWith(expect.stringContaining('"intent": "intent text"'))
    jest.useRealTimers()
    log.mockRestore()
  })
  it('includes url and smart context metadata in json output', async () => {
    mockResolveUrlContext.mockResolvedValueOnce([
      { path: 'url:https://example.com', content: '# url' },
    ])
    mockResolveSmartContext.mockResolvedValueOnce([{ path: 'smart.md', content: '# smart' }])
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand([
      'intent text',
      '--json',
      '--url',
      'https://example.com',
      '--smart-context',
    ])
    const firstCall = log.mock.calls[0]
    if (!firstCall) {
      throw new Error('Expected JSON output to be logged')
    }
    const payload = JSON.parse(firstCall[0] as string) as {
      schemaVersion?: string
      contextPaths: Array<{ path: string; source: string }>
    }
    expect(payload.schemaVersion).toBe('1')
    expect(payload.contextPaths).toEqual(
      expect.arrayContaining([
        expect.objectContaining({ source: 'url', path: 'url:https://example.com' }),
        expect.objectContaining({ source: 'smart', path: 'smart.md' }),
      ]),
    )
    log.mockRestore()
  })
  it('records outputPath when writing context file in json mode', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--json', '--context-file', '/tmp/out.json'])
    const firstCall = log.mock.calls[0]
    if (!firstCall) {
      throw new Error('Expected JSON output to be logged')
    }
    const payload = JSON.parse(firstCall[0] as string) as {
      schemaVersion?: string
      outputPath?: string
    }
    expect(payload.schemaVersion).toBe('1')
    expect(payload.outputPath).toBe('/tmp/out.json')
    log.mockRestore()
  })
  it('copies to clipboard without emitting cosmetic logs when quiet', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--quiet', '--copy'])
    expect(clipboard.write).toHaveBeenCalledWith('prompt v1')
    const copiedMessages = log.mock.calls
      .map((args) => args[0])
      .filter((arg) => typeof arg === 'string' && arg.includes('Copied prompt'))
    expect(copiedMessages).toHaveLength(0)
    log.mockRestore()
  })
  it('opens ChatGPT silently when quiet mode suppresses success ticks', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--quiet', '--open-chatgpt'])
    expect(open).toHaveBeenCalled()
    const openMessages = log.mock.calls
      .map((args) => args[0])
      .filter((arg) => typeof arg === 'string' && arg.includes('Opened ChatGPT'))
    expect(openMessages).toHaveLength(0)
    log.mockRestore()
  })
  it('drives interactive refinements via transport commands without TTY', async () => {
    promptService.generatePrompt
      .mockResolvedValueOnce('first prompt')
      .mockResolvedValueOnce('second prompt')
    const transportEvents: string[] = []
    const restoreTransport = setupTransportMock(
      [{ type: 'refine', instruction: 'Tighten tone' }, { type: 'finish' }],
      transportEvents,
    )
    try {
      await runGenerateCommand(['intent text', '--interactive-transport', '/tmp/pmc.sock'])
    } finally {
      restoreTransport()
    }
    const parsedEvents = transportEvents
      .map((event) => event.trim())
      .filter((event) => event.startsWith('{') && event.endsWith('}'))
      .map((event) => JSON.parse(event) as { event: string })
    expect(promptMock).not.toHaveBeenCalled()
    expect(promptService.generatePrompt).toHaveBeenCalledTimes(2)
    expect(parsedEvents.some((event) => event.event === 'transport.listening')).toBe(true)
    expect(parsedEvents.some((event) => event.event === 'interactive.awaiting')).toBe(true)
    expect(parsedEvents.some((event) => event.event === 'interactive.state')).toBe(true)
  })
  it('ends interactive transport sessions when finish command arrives first', async () => {
    const transportEvents: string[] = []
    const restoreTransport = setupTransportMock([{ type: 'finish' }], transportEvents)
    try {
      await runGenerateCommand(['intent text', '--interactive-transport', '/tmp/pmc.sock'])
    } finally {
      restoreTransport()
    }
    const parsedEvents = transportEvents
      .map((event) => event.trim())
      .filter((event) => event.startsWith('{') && event.endsWith('}'))
      .map((event) => JSON.parse(event) as { event: string })
    expect(promptService.generatePrompt).toHaveBeenCalledTimes(1)
    expect(promptMock).not.toHaveBeenCalled()
    expect(parsedEvents.some((event) => event.event === 'transport.listening')).toBe(true)
  })
  it('throws when an unknown context template is provided', async () => {
    await expect(
      runGenerateCommand(['intent text', '--context-template', 'missing']),
    ).rejects.toThrow('Unknown context template')
  })
  it('applies the built-in nvim context template and surfaces metadata', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--json', '--context-template', 'nvim'])
    expect(log).toHaveBeenCalled()
    const firstCall = log.mock.calls[0]
    if (!firstCall) {
      throw new Error('console.log was not called')
    }
    const [jsonOutput] = firstCall as [string]
    const payload = JSON.parse(jsonOutput) as {
      schemaVersion?: string
      contextTemplate?: string
      renderedPrompt?: string
    }
    expect(payload.schemaVersion).toBe('1')
    expect(payload.contextTemplate).toBe('nvim')
    expect(payload.renderedPrompt).toContain('NeoVim Prompt Buffer')
    expect(payload.renderedPrompt).toContain('prompt v1')
    log.mockRestore()
  })
  it('uses user-defined templates from config and appends prompt when placeholder is missing', async () => {
    mockLoadCliConfig.mockResolvedValue({
      promptGenerator: { defaultGeminiModel: 'gemini-1.5-pro' },
      contextTemplates: { scratch: 'Paste into scratch buffer for review' },
    })
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--json', '--context-template', 'scratch'])
    expect(log).toHaveBeenCalled()
    const firstCall = log.mock.calls[0]
    if (!firstCall) {
      throw new Error('console.log was not called')
    }
    const [jsonOutput] = firstCall as [string]
    const payload = JSON.parse(jsonOutput) as { schemaVersion?: string; renderedPrompt?: string }
    expect(payload.schemaVersion).toBe('1')
    expect(payload.renderedPrompt).toContain('Paste into scratch buffer for review')
    expect(payload.renderedPrompt?.trim().endsWith('prompt v1')).toBe(true)
    log.mockRestore()
  })
  it('resumes from history in best-effort mode and emits resume.loaded', async () => {
    const originalHome = process.env.HOME
    process.env.HOME = '/tmp/pmc-resume-home'
    try {
      const historyPath = `${process.env.HOME}/.config/prompt-maker-cli/history.jsonl`
      const resumedPayload = {
        schemaVersion: '1',
        intent: 'resumed intent',
        model: 'gpt-4o-mini',
        targetModel: 'gpt-4o-mini',
        prompt: 'previous prompt',
        polishedPrompt: 'previous polished',
        polishModel: 'polish-model',
        metaInstructions: 'Be concise',
        refinements: ['prior refinement'],
        iterations: 2,
        interactive: true,
        timestamp: '2025-01-01T00:00:00.000Z',
        contextPaths: [
          { path: 'existing.md', source: 'file' },
          { path: 'missing.md', source: 'file' },
          { path: 'url:https://example.com', source: 'url' },
        ],
      }
      fs.readFile.mockImplementation(async (filePath: string) => {
        if (filePath === historyPath) {
          return `${JSON.stringify(resumedPayload)}\n`
        }
        if (filePath === 'existing.md') {
          return 'existing content'
        }
        const error = Object.assign(new Error('ENOENT'), { code: 'ENOENT' })
        throw error
      })
      mockReadFromStdin.mockResolvedValue(null)
      const chunks: string[] = []
      const writeSpy = jest.spyOn(process.stdout, 'write').mockImplementation(((
        chunk: string | Uint8Array,
        encoding?: BufferEncoding,
        cb?: (err?: Error) => void,
      ) => {
        if (typeof chunk === 'string') {
          chunks.push(chunk)
        }
        if (typeof cb === 'function') {
          cb()
        }
        return true
      }) as unknown as typeof process.stdout.write)
      const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
      await runGenerateCommand([
        '',
        '--resume-last',
        '--resume-mode',
        'best-effort',
        '--stream',
        'jsonl',
        '--quiet',
        '--progress=false',
      ])
      writeSpy.mockRestore()
      const events = chunks
        .join('')
        .split('\n')
        .map((line) => line.trim())
        .filter(Boolean)
        .map((line) => JSON.parse(line) as { event: string } & Record<string, unknown>)
      const resumeEvent = events.find((event) => event.event === 'resume.loaded') as
        | {
            reusedContextPaths?: Array<{ path: string; source: string }>
            missingContextPaths?: Array<{ path: string; source: string }>
          }
        | undefined
      expect(resumeEvent).toBeDefined()
      expect(resumeEvent?.reusedContextPaths).toEqual([{ path: 'existing.md', source: 'file' }])
      expect(resumeEvent?.missingContextPaths).toEqual(
        expect.arrayContaining([
          { path: 'missing.md', source: 'file' },
          { path: 'url:https://example.com', source: 'url' },
        ]),
      )
      expect(warn).toHaveBeenCalledWith(expect.stringContaining('Resume skipped missing context'))
      warn.mockRestore()
      expect(promptService.generatePrompt).toHaveBeenCalledTimes(0)
      expect(appendToHistory).toHaveBeenCalledWith(
        expect.objectContaining({ intent: 'resumed intent', prompt: 'previous polished' }),
      )
    } finally {
      if (originalHome === undefined) {
        delete process.env.HOME
      } else {
        process.env.HOME = originalHome
      }
    }
  })
  it('fails in strict resume mode when a context path is missing', async () => {
    const originalHome = process.env.HOME
    process.env.HOME = '/tmp/pmc-resume-home'
    try {
      const historyPath = `${process.env.HOME}/.config/prompt-maker-cli/history.jsonl`
      const resumedPayload = {
        schemaVersion: '1',
        intent: 'resumed intent',
        model: 'gpt-4o-mini',
        targetModel: 'gpt-4o-mini',
        prompt: 'previous prompt',
        refinements: [],
        iterations: 1,
        interactive: false,
        timestamp: '2025-01-01T00:00:00.000Z',
        contextPaths: [
          { path: 'existing.md', source: 'file' },
          { path: 'missing.md', source: 'file' },
        ],
      }
      fs.readFile.mockImplementation(async (filePath: string) => {
        if (filePath === historyPath) {
          return `${JSON.stringify(resumedPayload)}\n`
        }
        if (filePath === 'existing.md') {
          return 'existing content'
        }
        const error = Object.assign(new Error('ENOENT'), { code: 'ENOENT' })
        throw error
      })
      mockReadFromStdin.mockResolvedValue(null)
      await expect(
        runGenerateCommand(['', '--resume-last', '--resume-mode', 'strict', '--quiet']),
      ).rejects.toThrow(/Missing required resumed context file/i)
      expect(appendToHistory).not.toHaveBeenCalled()
    } finally {
      if (originalHome === undefined) {
        delete process.env.HOME
      } else {
        process.env.HOME = originalHome
      }
    }
  })
  it('throws when --json and --interactive are combined', async () => {
    await expect(runGenerateCommand(['intent text', '--json', '--interactive'])).rejects.toThrow(
      '--json cannot be combined with --interactive.',
    )
  })
})
</file>
<file path="src/tui/components/popups/ModelPopup.tsx">
import { useMemo } from 'react'
import { Box, Text, useStdout } from 'ink'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { PopupSheet } from './PopupSheet'
import { MODEL_PROVIDER_LABELS } from '../../../model-providers'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import type { InkColorValue } from '../../theme/theme-types'
import type { ModelOption, ProviderStatusMap } from '../../types'
import {
  buildModelPopupRows,
  resolveModelPopupListRows,
  resolveModelPopupVisibleRows,
} from './model-popup-model'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const POPUP_MIN_HEIGHT = 10
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
const joinColumns = (left: string, right: string, width: number): string => {
  const safeWidth = Math.max(0, width)
  if (safeWidth === 0) {
    return ''
  }
  const leftTrimmed = left.length > safeWidth ? left.slice(0, safeWidth) : left
  const remaining = Math.max(0, safeWidth - leftTrimmed.length)
  const rightTrimmed = right.length > remaining ? right.slice(0, remaining) : right
  const gap = Math.max(1, safeWidth - leftTrimmed.length - rightTrimmed.length)
  return `${leftTrimmed}${' '.repeat(gap)}${rightTrimmed}`
}
export type ModelPopupProps = {
  title?: string
  query: string
  options: readonly ModelOption[]
  selectedIndex: number
  recentCount: number
  maxHeight?: number
  providerStatuses: ProviderStatusMap
  onQueryChange: (value: string) => void
  onSubmit: (option: ModelOption | null | undefined) => void
}
const resolveListRows = (popupHeight: number): number =>
  resolveModelPopupListRows({ popupHeight, paddingY: POPUP_PADDING_Y, fixedRows: 6 })
export const ModelPopup = ({
  title,
  query,
  options,
  selectedIndex,
  recentCount,
  maxHeight,
  providerStatuses,
  onQueryChange,
  onSubmit,
}: ModelPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const resolveOptionColor = (option: ModelOption): InkColorValue => {
    const status = providerStatuses[option.provider]?.status
    if (status === 'missing') {
      return theme.warning
    }
    if (status === 'error') {
      return theme.error
    }
    return theme.text
  }
  const selectedOption = options[selectedIndex]
  const fallbackHeight = 16
  const popupHeight = Math.max(POPUP_MIN_HEIGHT, Math.floor(maxHeight ?? fallbackHeight))
  const listRows = useMemo(() => resolveListRows(popupHeight), [popupHeight])
  const rows = useMemo(() => buildModelPopupRows(options, recentCount), [options, recentCount])
  const { slice, visibleRows } = useMemo(
    () =>
      resolveModelPopupVisibleRows({
        rows,
        selectedOptionIndex: selectedIndex,
        maxVisibleRows: listRows,
      }),
    [listRows, rows, selectedIndex],
  )
  const selectedTextProps = {
    ...inkColorProps(theme.selectionText),
    ...inkBackgroundColorProps(theme.selectionBackground),
  }
  const headerLeft = title ?? 'Select model'
  const headerRight = 'esc'
  const headerGap = Math.max(0, contentWidth - headerLeft.length - headerRight.length)
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Box flexDirection="row">
        <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
          {headerLeft}
        </Text>
        <Text {...backgroundProps}>{' '.repeat(headerGap)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {headerRight}
        </Text>
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box>
        <SingleLineTextInput
          value={query}
          onChange={onQueryChange}
          onSubmit={() => onSubmit(selectedOption)}
          placeholder="Search"
          focus
          width={contentWidth}
          backgroundColor={theme.popupBackground}
        />
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column" height={listRows} overflow="hidden">
        {rows.length === 0 ? (
          <>
            <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
              {padRight('No models match.', contentWidth)}
            </Text>
            {Array.from({ length: Math.max(0, listRows - 1) }).map((_, index) => (
              <Text key={`empty-${index}`} {...backgroundProps}>
                {padRight('', contentWidth)}
              </Text>
            ))}
          </>
        ) : (
          visibleRows.map((row, rowIndex) => {
            if (row.type === 'spacer') {
              return (
                <Text key={`spacer-${slice.start + rowIndex}`} {...backgroundProps}>
                  {padRight('', contentWidth)}
                </Text>
              )
            }
            if (row.type === 'header') {
              return (
                <Text
                  key={`header-${row.title}-${slice.start + rowIndex}`}
                  {...backgroundProps}
                  {...inkColorProps(theme.accent)}
                >
                  {padRight(row.title, contentWidth)}
                </Text>
              )
            }
            const isSelected = row.optionIndex === selectedIndex
            const baseProviderLabel = MODEL_PROVIDER_LABELS[row.option.provider]
            const providerLabel =
              row.option.provider === 'gemini' && row.option.capabilities.includes('video')
                ? `${baseProviderLabel} video`
                : baseProviderLabel
            const rowTextProps = isSelected
              ? selectedTextProps
              : { ...backgroundProps, ...inkColorProps(resolveOptionColor(row.option)) }
            const line = joinColumns(row.option.label, providerLabel, contentWidth)
            return (
              <Text key={`option-${row.option.id}`} {...rowTextProps}>
                {padRight(line, contentWidth)}
              </Text>
            )
          })
        )}
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Enter to select', contentWidth)}
      </Text>
    </PopupSheet>
  )
}
</file>
<file path="src/tui/hooks/popup-manager/command-mapping.ts">
import type { NotifyOptions } from '../../notifier'
import type { CommandDescriptor, HistoryEntry, ToggleField } from '../../types'
import { parseUrlArgs, validateHttpUrlCandidate } from '../../screens/command/utils/url-args'
export const JSON_INTERACTIVE_ERROR =
  'JSON output is unavailable while interactive transport is enabled.'
type NotificationKind = Exclude<NotifyOptions['kind'], undefined>
type PushHistoryStep = {
  type: 'push-history'
  message: string
  kind?: HistoryEntry['kind']
}
type NotifyStep = {
  type: 'notify'
  message: string
  kind: NotificationKind
}
type SetInputStep = {
  type: 'set-input'
  value: string
}
type ClosePopupStep = {
  type: 'close-popup'
}
type OpenPopupStep = {
  type: 'open-popup'
  popup:
    | 'model'
    | 'target'
    | 'polish'
    | 'toggle'
    | 'file'
    | 'url'
    | 'image'
    | 'video'
    | 'pdf'
    | 'history'
    | 'resume'
    | 'export'
    | 'smart-root'
    | 'tokens'
    | 'budgets'
    | 'settings'
    | 'theme'
    | 'theme-mode'
    | 'reasoning'
    | 'test'
    | 'intent'
    | 'instructions'
  field?: ToggleField
}
type ApplyToggleStep = {
  type: 'apply-toggle'
  field: ToggleField
  value: boolean
}
type ClearPolishStep = {
  type: 'clear-polish'
}
type AddUrlStep = {
  type: 'add-url'
  value: string
}
type AddImageStep = {
  type: 'add-image'
  value: string
}
type AddVideoStep = {
  type: 'add-video'
  value: string
}
type AddPdfStep = {
  type: 'add-pdf'
  value: string
}
type SmartContextToggleStep = {
  type: 'toggle-smart-context'
}
type SetSmartRootStep = {
  type: 'set-smart-root'
  value: string
}
type SetIntentFileStep = {
  type: 'set-intent-file'
  value: string
}
type SetMetaInstructionsStep = {
  type: 'set-meta-instructions'
  value: string
}
type ClearScreenStep = {
  type: 'clear-screen'
}
type ExitAppStep = {
  type: 'exit-app'
}
type RunTestsStep = {
  type: 'run-tests'
  value: string
}
export type PopupManagerCommandStep =
  | PushHistoryStep
  | NotifyStep
  | SetInputStep
  | ClosePopupStep
  | OpenPopupStep
  | ApplyToggleStep
  | ClearPolishStep
  | AddUrlStep
  | AddImageStep
  | AddVideoStep
  | AddPdfStep
  | SmartContextToggleStep
  | SetSmartRootStep
  | SetIntentFileStep
  | SetMetaInstructionsStep
  | ClearScreenStep
  | ExitAppStep
  | RunTestsStep
export type CommandMappingResult =
  | { kind: 'steps'; steps: readonly PopupManagerCommandStep[] }
  | { kind: 'series'; trimmedArgs: string }
export type CommandMappingContext = {
  copyEnabled: boolean
  chatGptEnabled: boolean
  jsonOutputEnabled: boolean
  interactiveTransportPath?: string | undefined
  urls: readonly string[]
  images: readonly string[]
  videos: readonly string[]
  pdfs: readonly string[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
}
const OPEN_TOGGLE_ARGS = new Set(['on', 'off'])
const CLEAR_ARGS = new Set(['off', 'clear', '--clear'])
export const mapPopupCommandSelection = ({
  commandId,
  argsRaw,
  context,
}: {
  commandId: CommandDescriptor['id']
  argsRaw?: string | undefined
  context: CommandMappingContext
}): CommandMappingResult => {
  const trimmedArgs = argsRaw?.trim() ?? ''
  const normalizedToggleArgs = trimmedArgs.toLowerCase()
  switch (commandId) {
    case 'model':
      return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'model' }] }
    case 'target':
      return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'target' }] }
    case 'polish': {
      if (trimmedArgs && CLEAR_ARGS.has(normalizedToggleArgs)) {
        return { kind: 'steps', steps: [{ type: 'clear-polish' }] }
      }
      return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'polish' }] }
    }
    case 'copy': {
      if (!trimmedArgs) {
        return {
          kind: 'steps',
          steps: [{ type: 'apply-toggle', field: 'copy', value: !context.copyEnabled }],
        }
      }
      if (OPEN_TOGGLE_ARGS.has(normalizedToggleArgs)) {
        return {
          kind: 'steps',
          steps: [{ type: 'apply-toggle', field: 'copy', value: normalizedToggleArgs === 'on' }],
        }
      }
      return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'toggle', field: 'copy' }] }
    }
    case 'chatgpt': {
      if (!trimmedArgs) {
        return {
          kind: 'steps',
          steps: [{ type: 'apply-toggle', field: 'chatgpt', value: !context.chatGptEnabled }],
        }
      }
      if (OPEN_TOGGLE_ARGS.has(normalizedToggleArgs)) {
        return {
          kind: 'steps',
          steps: [{ type: 'apply-toggle', field: 'chatgpt', value: normalizedToggleArgs === 'on' }],
        }
      }
      return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'toggle', field: 'chatgpt' }] }
    }
    case 'json': {
      if (context.interactiveTransportPath) {
        return {
          kind: 'steps',
          steps: [
            { type: 'push-history', message: JSON_INTERACTIVE_ERROR, kind: 'system' },
            { type: 'set-input', value: '' },
          ],
        }
      }
      if (!trimmedArgs) {
        return {
          kind: 'steps',
          steps: [{ type: 'apply-toggle', field: 'json', value: !context.jsonOutputEnabled }],
        }
      }
      if (OPEN_TOGGLE_ARGS.has(normalizedToggleArgs)) {
        return {
          kind: 'steps',
          steps: [{ type: 'apply-toggle', field: 'json', value: normalizedToggleArgs === 'on' }],
        }
      }
      return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'toggle', field: 'json' }] }
    }
    case 'file':
      return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'file' }] }
    case 'url': {
      if (!trimmedArgs) {
        return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'url' }] }
      }
      const candidates = parseUrlArgs(trimmedArgs)
      if (candidates.length === 0) {
        return {
          kind: 'steps',
          steps: [{ type: 'set-input', value: '' }, { type: 'close-popup' }],
        }
      }
      const steps: PopupManagerCommandStep[] = []
      const seen = new Set<string>()
      for (const candidate of candidates) {
        if (seen.has(candidate)) {
          continue
        }
        seen.add(candidate)
        const validation = validateHttpUrlCandidate(candidate)
        if (!validation.ok) {
          steps.push({
            type: 'push-history',
            message: `Warning: ${validation.message}`,
            kind: 'system',
          })
          continue
        }
        if (context.urls.includes(candidate)) {
          steps.push({
            type: 'push-history',
            message: `Context URL already added: ${candidate}`,
            kind: 'system',
          })
          continue
        }
        steps.push({ type: 'add-url', value: candidate })
        steps.push({
          type: 'push-history',
          message: `Context URL added: ${candidate}`,
          kind: 'system',
        })
      }
      steps.push({ type: 'set-input', value: '' })
      steps.push({ type: 'close-popup' })
      return { kind: 'steps', steps }
    }
    case 'image': {
      if (!trimmedArgs) {
        return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'image' }] }
      }
      if (context.images.includes(trimmedArgs)) {
        return {
          kind: 'steps',
          steps: [
            {
              type: 'push-history',
              message: `[image] Already attached: ${trimmedArgs}`,
              kind: 'system',
            },
            { type: 'set-input', value: '' },
            { type: 'close-popup' },
          ],
        }
      }
      return {
        kind: 'steps',
        steps: [
          { type: 'add-image', value: trimmedArgs },
          { type: 'push-history', message: `[image] Attached: ${trimmedArgs}`, kind: 'system' },
          { type: 'set-input', value: '' },
          { type: 'close-popup' },
        ],
      }
    }
    case 'video': {
      if (!trimmedArgs) {
        return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'video' }] }
      }
      if (context.videos.includes(trimmedArgs)) {
        return {
          kind: 'steps',
          steps: [
            {
              type: 'push-history',
              message: `[video] Already attached: ${trimmedArgs}`,
              kind: 'system',
            },
            { type: 'set-input', value: '' },
            { type: 'close-popup' },
          ],
        }
      }
      return {
        kind: 'steps',
        steps: [
          { type: 'add-video', value: trimmedArgs },
          { type: 'push-history', message: `[video] Attached: ${trimmedArgs}`, kind: 'system' },
          { type: 'set-input', value: '' },
          { type: 'close-popup' },
        ],
      }
    }
    case 'pdf': {
      if (!trimmedArgs) {
        return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'pdf' }] }
      }
      if (context.pdfs.includes(trimmedArgs)) {
        return {
          kind: 'steps',
          steps: [
            {
              type: 'push-history',
              message: `[pdf] Already attached: ${trimmedArgs}`,
              kind: 'system',
            },
            { type: 'set-input', value: '' },
            { type: 'close-popup' },
          ],
        }
      }
      return {
        kind: 'steps',
        steps: [
          { type: 'add-pdf', value: trimmedArgs },
          { type: 'push-history', message: `[pdf] Attached: ${trimmedArgs}`, kind: 'system' },
          { type: 'set-input', value: '' },
          { type: 'close-popup' },
        ],
      }
    }
    case 'smart': {
      const nextEnabled = !trimmedArgs
        ? !context.smartContextEnabled
        : normalizedToggleArgs === 'on'
          ? true
          : normalizedToggleArgs === 'off'
            ? false
            : null
      if (nextEnabled === null) {
        return {
          kind: 'steps',
          steps: [
            { type: 'notify', message: 'Smart context expects /smart on|off', kind: 'warning' },
            { type: 'set-input', value: '' },
            { type: 'close-popup' },
          ],
        }
      }
      const steps: PopupManagerCommandStep[] = []
      const isDisabling = nextEnabled === false
      const shouldClearRoot = isDisabling && Boolean(context.smartContextRoot)
      if (shouldClearRoot) {
        steps.push({ type: 'set-smart-root', value: '' })
      }
      if (context.smartContextEnabled !== nextEnabled) {
        steps.push({ type: 'toggle-smart-context' })
      }
      steps.push({
        type: 'notify',
        message: nextEnabled
          ? 'Smart context enabled'
          : shouldClearRoot
            ? 'Smart context disabled; root cleared'
            : 'Smart context disabled',
        kind: nextEnabled ? 'info' : 'warning',
      })
      steps.push({ type: 'set-input', value: '' })
      steps.push({ type: 'close-popup' })
      return { kind: 'steps', steps }
    }
    case 'smart-root': {
      if (!trimmedArgs) {
        return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'smart-root' }] }
      }
      const normalizedRootArgs = trimmedArgs.toLowerCase()
      const rootValue = CLEAR_ARGS.has(normalizedRootArgs) ? '' : trimmedArgs
      const shouldEnable = Boolean(rootValue) && !context.smartContextEnabled
      const steps: PopupManagerCommandStep[] = [{ type: 'set-smart-root', value: rootValue }]
      if (shouldEnable) {
        steps.push({ type: 'toggle-smart-context' })
      }
      steps.push({
        type: 'notify',
        message: rootValue
          ? shouldEnable
            ? `Smart context enabled; root set to ${rootValue}`
            : `Smart context root set to ${rootValue}`
          : 'Smart context root cleared',
        kind: rootValue ? 'info' : 'warning',
      })
      steps.push({ type: 'set-input', value: '' })
      steps.push({ type: 'close-popup' })
      return { kind: 'steps', steps }
    }
    case 'tokens':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'tokens' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'budgets':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'budgets' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'settings':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'settings' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'theme':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'theme' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'theme-mode':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'theme-mode' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'reasoning':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'reasoning' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'history':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'history' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'resume':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'resume' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'export':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'export' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'intent': {
      if (!trimmedArgs) {
        return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'intent' }] }
      }
      const normalizedValue = trimmedArgs.trim()
      return {
        kind: 'steps',
        steps: [
          { type: 'set-intent-file', value: normalizedValue },
          {
            type: 'push-history',
            message: normalizedValue
              ? `Intent file set to ${normalizedValue}`
              : 'Intent file cleared; using typed intent.',
          },
          { type: 'set-input', value: '' },
          { type: 'close-popup' },
        ],
      }
    }
    case 'instructions': {
      if (!trimmedArgs) {
        return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'instructions' }] }
      }
      const normalizedValue = trimmedArgs.trim()
      return {
        kind: 'steps',
        steps: [
          { type: 'set-meta-instructions', value: normalizedValue },
          {
            type: 'push-history',
            message: normalizedValue ? `[instr] ${normalizedValue}` : '[instr] cleared',
          },
          { type: 'set-input', value: '' },
          { type: 'close-popup' },
        ],
      }
    }
    case 'exit':
      return {
        kind: 'steps',
        steps: [
          { type: 'push-history', message: 'ExitingâŚ', kind: 'system' },
          { type: 'set-input', value: '' },
          { type: 'clear-screen' },
          { type: 'exit-app' },
        ],
      }
    case 'series':
      return { kind: 'series', trimmedArgs }
    case 'test': {
      if (trimmedArgs) {
        return {
          kind: 'steps',
          steps: [
            {
              type: 'push-history',
              message: `[tests] Running /test ${trimmedArgs}`,
              kind: 'system',
            },
            { type: 'run-tests', value: trimmedArgs },
          ],
        }
      }
      return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'test' }] }
    }
    default:
      return { kind: 'steps', steps: [{ type: 'push-history', message: `Selected ${commandId}` }] }
  }
}
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenPopupAndView.ts">
import { useCommandScreenBindings } from './useCommandScreenBindings'
import { useCommandScreenViewModel } from './useCommandScreenViewModel'
import type {
  UseCommandScreenPopupAndViewOptions,
  UseCommandScreenPopupAndViewResult,
} from './useCommandScreenPopupAndView.types'
export const useCommandScreenPopupAndView = (
  options: UseCommandScreenPopupAndViewOptions,
): UseCommandScreenPopupAndViewResult => {
  const { context, input, popup, history, generation } = options
  const { popupManager, shell, bindings, enhancedStatusChips } = useCommandScreenBindings(options)
  const viewModel = useCommandScreenViewModel({
    transport: { isAwaitingTransportInput: shell.isAwaitingTransportInput },
    panes: {
      history: {
        lines: shell.history,
        visibleRows: shell.historyRows,
        scrollOffset: shell.scrollOffset,
      },
      menu: {
        isActive: shell.isCommandMenuActive,
        height: shell.menuHeight,
        commands: shell.visibleCommands,
        selectedIndex: input.commandSelectionIndex,
      },
    },
    popup: {
      base: {
        popupState: popupManager.popupState,
        helpOpen: popup.helpOpen,
        overlayHeight: shell.overlayHeight,
      },
      model: {
        modelPopupOptions: bindings.popup.model.options,
        modelPopupSelection: bindings.popup.model.selection,
        modelPopupRecentCount: bindings.popup.model.recentCount,
        providerStatuses: generation.providerStatuses,
        onModelPopupQueryChange: bindings.popup.model.onQueryChange,
        onModelPopupSubmit: popupManager.actions.handleModelPopupSubmit,
      },
      context: {
        files: context.files,
        filePopupSuggestions: bindings.popup.context.file.suggestions,
        filePopupSuggestionSelectionIndex: bindings.popup.context.file.suggestionSelectionIndex,
        filePopupSuggestionsFocused: bindings.popup.context.file.suggestionsFocused,
        onFilePopupDraftChange: bindings.popup.context.file.onDraftChange,
        onAddFile: bindings.popup.context.file.onAdd,
        urls: context.urls,
        onUrlPopupDraftChange: bindings.popup.context.url.onDraftChange,
        onAddUrl: bindings.popup.context.url.onAdd,
        images: context.images,
        imagePopupSuggestions: bindings.popup.context.image.suggestions,
        imagePopupSuggestionSelectionIndex: bindings.popup.context.image.suggestionSelectionIndex,
        imagePopupSuggestionsFocused: bindings.popup.context.image.suggestionsFocused,
        onImagePopupDraftChange: bindings.popup.context.image.onDraftChange,
        onAddImage: bindings.popup.context.image.onAdd,
        videos: context.videos,
        videoPopupSuggestions: bindings.popup.context.video.suggestions,
        videoPopupSuggestionSelectionIndex: bindings.popup.context.video.suggestionSelectionIndex,
        videoPopupSuggestionsFocused: bindings.popup.context.video.suggestionsFocused,
        onVideoPopupDraftChange: bindings.popup.context.video.onDraftChange,
        onAddVideo: bindings.popup.context.video.onAdd,
        pdfs: context.pdfs,
        pdfPopupSuggestions: bindings.popup.context.pdf.suggestions,
        pdfPopupSuggestionSelectionIndex: bindings.popup.context.pdf.suggestionSelectionIndex,
        pdfPopupSuggestionsFocused: bindings.popup.context.pdf.suggestionsFocused,
        onPdfPopupDraftChange: bindings.popup.context.pdf.onDraftChange,
        onAddPdf: bindings.popup.context.pdf.onAdd,
        smartContextEnabled: context.smartContextEnabled,
        smartContextRoot: context.smartContextRoot,
        smartPopupSuggestions: bindings.popup.context.smart.suggestions,
        smartPopupSuggestionSelectionIndex: bindings.popup.context.smart.suggestionSelectionIndex,
        smartPopupSuggestionsFocused: bindings.popup.context.smart.suggestionsFocused,
        onSmartPopupDraftChange: bindings.popup.context.smart.onDraftChange,
        onSmartRootSubmit: bindings.popup.context.smart.onRootSubmit,
      },
      history: {
        historyPopupItems: bindings.popup.history.items,
        onHistoryPopupDraftChange: bindings.popup.history.onDraftChange,
        onHistoryPopupSubmit: bindings.popup.history.onSubmit,
      },
      resume: {
        onResumePayloadPathDraftChange: bindings.popup.misc.onResumePayloadPathDraftChange,
        onResumeSubmit: popupManager.actions.handleResumeSubmit,
      },
      export: {
        onExportOutPathDraftChange: bindings.popup.misc.onExportOutPathDraftChange,
        onExportSubmit: popupManager.actions.handleExportSubmit,
      },
      intent: {
        intentPopupSuggestions: bindings.popup.intent.suggestions,
        intentPopupSuggestionSelectionIndex: bindings.popup.intent.suggestionSelectionIndex,
        intentPopupSuggestionsFocused: bindings.popup.intent.suggestionsFocused,
        onIntentPopupDraftChange: bindings.popup.intent.onDraftChange,
        onIntentFileSubmit: popupManager.actions.handleIntentFileSubmit,
      },
      instructions: {
        onInstructionsDraftChange: bindings.popup.misc.onInstructionsDraftChange,
        onInstructionsSubmit: popupManager.actions.handleInstructionsSubmit,
      },
      series: {
        isGenerating: generation.isGenerating,
        onSeriesDraftChange: bindings.popup.misc.onSeriesDraftChange,
        onSeriesSubmit: bindings.submit.onSeriesSubmit,
      },
      test: {
        isTestCommandRunning: history.isTestCommandRunning,
        onTestDraftChange: bindings.popup.misc.onTestDraftChange,
        onTestSubmit: history.onTestPopupSubmit,
      },
      tokens: {
        tokenUsageRun: generation.tokenUsageRun,
        tokenUsageBreakdown: generation.tokenUsageBreakdown,
        maxContextTokens: context.maxContextTokens,
        maxInputTokens: context.maxInputTokens,
        contextOverflowStrategy: context.contextOverflowStrategy,
        latestContextOverflow: generation.latestContextOverflow,
      },
      budgets: {
        onBudgetsMaxContextTokensDraftChange:
          bindings.popup.misc.onBudgetsMaxContextTokensDraftChange,
        onBudgetsMaxInputTokensDraftChange: bindings.popup.misc.onBudgetsMaxInputTokensDraftChange,
        onBudgetsSubmit: popupManager.actions.handleBudgetsSubmit,
      },
      settings: { statusChips: enhancedStatusChips },
      reasoning: {
        reasoningPopupLines: bindings.popup.reasoning.lines,
        reasoningPopupVisibleRows: bindings.popup.reasoning.visibleRows,
      },
    },
    input: {
      base: {
        value: input.inputValue,
        onChange: bindings.input.onChange,
        onSubmit: bindings.submit.onSubmit,
        isPasteActive: input.isPasteActive,
        hint: shell.inputBarHint,
        debugLine: shell.inputBarDebugLine,
        tokenLabel: bindings.input.tokenLabel,
        debugKeysEnabled: input.debugKeysEnabled,
        onDebugKeyEvent: input.onDebugKeyEvent,
      },
      state: {
        isPopupOpen: popupManager.isPopupOpen,
        helpOpen: popup.helpOpen,
        isAwaitingRefinement: generation.isAwaitingRefinement,
        isBusy: generation.isGenerating || history.isTestCommandRunning,
      },
      statusChips: enhancedStatusChips,
    },
  })
  return {
    transportMessage: viewModel.transportMessage,
    historyPaneProps: viewModel.historyPaneProps,
    popupAreaProps: viewModel.popupAreaProps,
    commandMenuPaneProps: viewModel.commandMenuPaneProps,
    commandInputProps: viewModel.commandInputProps,
  }
}
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenViewModel.ts">
import { useMemo } from 'react'
import type { DebugKeyEvent } from '../../../components/core/MultilineTextInput'
import type { HistoryEntry, ModelOption, PopupState, ProviderStatusMap } from '../../../types'
import type { TokenUsageBreakdown, TokenUsageRun } from '../../../token-usage-store'
import type { CommandInputProps } from '../components/CommandInput'
import type { CommandMenuPaneProps } from '../components/CommandMenuPane'
import type { HistoryPaneProps } from '../components/HistoryPane'
import type { PopupAreaProps } from '../components/PopupArea'
export type UseCommandScreenViewModelOptions = {
  transport: {
    isAwaitingTransportInput: boolean
  }
  panes: {
    history: HistoryPaneProps
    menu: CommandMenuPaneProps
  }
  popup: {
    base: {
      popupState: PopupState
      helpOpen: boolean
      overlayHeight: number
    }
    model: {
      modelPopupOptions: ModelOption[]
      modelPopupSelection: number
      modelPopupRecentCount: number
      providerStatuses: ProviderStatusMap
      onModelPopupQueryChange: (next: string) => void
      onModelPopupSubmit: (option: ModelOption | null | undefined) => void
    }
    context: {
      files: string[]
      filePopupSuggestions: string[]
      filePopupSuggestionSelectionIndex: number
      filePopupSuggestionsFocused: boolean
      onFilePopupDraftChange: (next: string) => void
      onAddFile: (value: string) => void
      urls: string[]
      onUrlPopupDraftChange: (next: string) => void
      onAddUrl: (value: string) => void
      images: string[]
      imagePopupSuggestions: string[]
      imagePopupSuggestionSelectionIndex: number
      imagePopupSuggestionsFocused: boolean
      onImagePopupDraftChange: (next: string) => void
      onAddImage: (value: string) => void
      videos: string[]
      videoPopupSuggestions: string[]
      videoPopupSuggestionSelectionIndex: number
      videoPopupSuggestionsFocused: boolean
      onVideoPopupDraftChange: (next: string) => void
      onAddVideo: (value: string) => void
      pdfs: string[]
      pdfPopupSuggestions: string[]
      pdfPopupSuggestionSelectionIndex: number
      pdfPopupSuggestionsFocused: boolean
      onPdfPopupDraftChange: (next: string) => void
      onAddPdf: (value: string) => void
      smartContextEnabled: boolean
      smartContextRoot: string | null
      smartPopupSuggestions: string[]
      smartPopupSuggestionSelectionIndex: number
      smartPopupSuggestionsFocused: boolean
      onSmartPopupDraftChange: (next: string) => void
      onSmartRootSubmit: (value: string) => void
    }
    history: {
      historyPopupItems: string[]
      onHistoryPopupDraftChange: (next: string) => void
      onHistoryPopupSubmit: (value: string) => void
    }
    resume: {
      onResumePayloadPathDraftChange: (next: string) => void
      onResumeSubmit: () => void
    }
    export: {
      onExportOutPathDraftChange: (next: string) => void
      onExportSubmit: () => void
    }
    intent: {
      intentPopupSuggestions: string[]
      intentPopupSuggestionSelectionIndex: number
      intentPopupSuggestionsFocused: boolean
      onIntentPopupDraftChange: (next: string) => void
      onIntentFileSubmit: (value: string) => void
    }
    instructions: {
      onInstructionsDraftChange: (next: string) => void
      onInstructionsSubmit: (value: string) => void
    }
    series: {
      isGenerating: boolean
      onSeriesDraftChange: (next: string) => void
      onSeriesSubmit: (value: string) => void
    }
    test: {
      isTestCommandRunning: boolean
      onTestDraftChange: (next: string) => void
      onTestSubmit: (value: string) => void
    }
    tokens: {
      tokenUsageRun: TokenUsageRun | null
      tokenUsageBreakdown: TokenUsageBreakdown | null
      maxContextTokens: number | null
      maxInputTokens: number | null
      contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
      latestContextOverflow:
        | import('../../../generation-pipeline-reducer').ContextOverflowDetails
        | null
    }
    budgets: {
      onBudgetsMaxContextTokensDraftChange: (next: string) => void
      onBudgetsMaxInputTokensDraftChange: (next: string) => void
      onBudgetsSubmit: () => void
    }
    settings: {
      statusChips: string[]
    }
    reasoning: {
      reasoningPopupLines: HistoryEntry[]
      reasoningPopupVisibleRows: number
    }
  }
  input: {
    base: {
      value: string
      onChange: (next: string) => void
      onSubmit: (value: string) => void
      isPasteActive: boolean
      hint: string | undefined
      debugLine: string | undefined
      tokenLabel: (token: string) => string | null
      debugKeysEnabled: boolean
      onDebugKeyEvent: (event: DebugKeyEvent) => void
    }
    state: {
      isPopupOpen: boolean
      helpOpen: boolean
      isAwaitingRefinement: boolean
      isBusy: boolean
    }
    statusChips: string[]
  }
}
export type UseCommandScreenViewModelResult = {
  transportMessage: string | null
  historyPaneProps: HistoryPaneProps
  popupAreaProps: PopupAreaProps
  commandMenuPaneProps: CommandMenuPaneProps
  commandInputProps: CommandInputProps
}
export const useCommandScreenViewModel = ({
  transport,
  panes,
  popup,
  input,
}: UseCommandScreenViewModelOptions): UseCommandScreenViewModelResult => {
  const transportMessage = transport.isAwaitingTransportInput
    ? 'Waiting for interactive transport input (send refine/finish).'
    : null
  const historyPaneProps = panes.history
  const commandMenuPaneProps = panes.menu
  const popupAreaProps = useMemo<PopupAreaProps>(
    () => ({
      ...popup.base,
      ...popup.model,
      ...popup.context,
      ...popup.history,
      ...popup.resume,
      ...popup.export,
      ...popup.intent,
      ...popup.instructions,
      ...popup.series,
      ...popup.test,
      ...popup.tokens,
      ...popup.budgets,
      ...popup.settings,
      ...popup.reasoning,
    }),
    [
      popup.base,
      popup.context,
      popup.history,
      popup.resume,
      popup.export,
      popup.instructions,
      popup.intent,
      popup.model,
      popup.reasoning,
      popup.series,
      popup.settings,
      popup.test,
      popup.tokens,
      popup.budgets,
    ],
  )
  const commandInputProps = useMemo<CommandInputProps>(
    () => ({
      value: input.base.value,
      onChange: input.base.onChange,
      onSubmit: input.base.onSubmit,
      mode: input.state.isAwaitingRefinement ? 'refinement' : 'intent',
      isDisabled: input.state.isPopupOpen || input.state.helpOpen,
      isPasteActive: input.base.isPasteActive,
      isBusy: input.state.isBusy,
      statusChips: input.statusChips,
      hint: input.base.hint,
      debugLine: input.base.debugLine,
      tokenLabel: input.base.tokenLabel,
      onDebugKeyEvent: input.base.debugKeysEnabled ? input.base.onDebugKeyEvent : undefined,
      placeholder: input.state.isAwaitingRefinement
        ? 'Describe refinement (or empty to finish)...'
        : 'Describe your goal or type /command',
    }),
    [input],
  )
  return {
    transportMessage,
    historyPaneProps,
    popupAreaProps,
    commandMenuPaneProps,
    commandInputProps,
  }
}
</file>
<file path="src/tui/file-suggestions.ts">
/*
 * File/directory suggestion helpers used by the Ink TUI.
 *
 * This module contains two layers:
 * 1) Discovery (IO): scan the workspace using `fast-glob`.
 * 2) Filtering (pure): rank/limit suggestions based on a user query.
 *
 * Keeping filtering logic pure makes it easy to unit test and helps ensure
 * later UI refactors donât accidentally change suggestion behavior.
 */
import path from 'node:path'
import fg from 'fast-glob'
import { byLengthAsc, byStartAsc, Fzf, extendedMatch } from 'fzf'
const FILE_SUGGESTION_PATTERNS = ['**/*']
export const FILE_SUGGESTION_IGNORE_PATTERNS = [
  '**/node_modules/**',
  '**/dist/**',
  '**/coverage/**',
  '**/.git/**',
  '**/.nx/**',
  '**/.next/**',
  '**/.turbo/**',
  '**/.cache/**',
  '**/build/**',
  '**/out/**',
  '**/package-lock.json',
  '**/pnpm-lock.yaml',
  '**/yarn.lock',
]
const DEFAULT_FILE_SUGGESTION_LIMIT = 200
const normalizeToPosix = (value: string): string => value.split(path.sep).join('/')
const toDisplayPath = (cwd: string, candidatePath: string): string | null => {
  const absolutePath = path.isAbsolute(candidatePath)
    ? candidatePath
    : path.resolve(cwd, candidatePath)
  const relative = path.relative(cwd, absolutePath)
  if (!relative || relative.startsWith('..')) {
    return null
  }
  return normalizeToPosix(relative)
}
export type DiscoverFileSuggestionsOptions = {
  cwd?: string
  limit?: number
}
export const discoverFileSuggestions = async (
  options: DiscoverFileSuggestionsOptions = {},
): Promise<string[]> => {
  const cwd = options.cwd ?? process.cwd()
  const limit = options.limit ?? DEFAULT_FILE_SUGGESTION_LIMIT
  const matches = await fg(FILE_SUGGESTION_PATTERNS, {
    cwd,
    dot: true,
    absolute: true,
    onlyFiles: true,
    unique: true,
    suppressErrors: true,
    followSymbolicLinks: false,
    ignore: FILE_SUGGESTION_IGNORE_PATTERNS,
  })
  const unique = new Set<string>()
  for (const match of matches) {
    const displayPath = toDisplayPath(cwd, match)
    if (!displayPath) {
      continue
    }
    unique.add(displayPath)
  }
  return [...unique].sort().slice(0, limit)
}
export type DiscoverDirectorySuggestionsOptions = {
  cwd?: string
  limit?: number
}
export const discoverDirectorySuggestions = async (
  options: DiscoverDirectorySuggestionsOptions = {},
): Promise<string[]> => {
  const cwd = options.cwd ?? process.cwd()
  const limit = options.limit ?? DEFAULT_FILE_SUGGESTION_LIMIT
  const matches = await fg(FILE_SUGGESTION_PATTERNS, {
    cwd,
    dot: true,
    absolute: true,
    onlyDirectories: true,
    unique: true,
    suppressErrors: true,
    followSymbolicLinks: false,
    ignore: FILE_SUGGESTION_IGNORE_PATTERNS,
  })
  const unique = new Set<string>()
  for (const match of matches) {
    const displayPath = toDisplayPath(cwd, match)
    if (!displayPath) {
      continue
    }
    unique.add(displayPath)
  }
  return [...unique].sort().slice(0, limit)
}
const INTENT_FILE_SUGGESTION_PATTERNS = ['**/*.{md,markdown,txt}']
export type DiscoverIntentFileSuggestionsOptions = {
  cwd?: string
  limit?: number
}
export const discoverIntentFileSuggestions = async (
  options: DiscoverIntentFileSuggestionsOptions = {},
): Promise<string[]> => {
  const cwd = options.cwd ?? process.cwd()
  const limit = options.limit ?? DEFAULT_FILE_SUGGESTION_LIMIT
  const matches = await fg(INTENT_FILE_SUGGESTION_PATTERNS, {
    cwd,
    dot: true,
    absolute: true,
    onlyFiles: true,
    unique: true,
    suppressErrors: true,
    followSymbolicLinks: false,
    ignore: FILE_SUGGESTION_IGNORE_PATTERNS,
  })
  const unique = new Set<string>()
  for (const match of matches) {
    const displayPath = toDisplayPath(cwd, match)
    if (!displayPath) {
      continue
    }
    unique.add(displayPath)
  }
  return [...unique].sort().slice(0, limit)
}
export type FilterIntentFileSuggestionsOptions = {
  suggestions: readonly string[]
  query: string
  exclude?: readonly string[]
  limit?: number
}
export const filterIntentFileSuggestions = ({
  suggestions,
  query,
  exclude = [],
  limit = DEFAULT_FILE_SUGGESTION_LIMIT,
}: FilterIntentFileSuggestionsOptions): string[] => {
  const excluded = new Set(exclude)
  const eligible = suggestions.filter((suggestion) => !excluded.has(suggestion))
  return fuzzyFilterStrings(eligible, query, limit)
}
export type FilterFileSuggestionsOptions = {
  suggestions: readonly string[]
  query: string
  exclude?: readonly string[]
  limit?: number
}
const normalizeFzfToken = (token: string): string => {
  if (!token) {
    return token
  }
  // Preserve fzf query operators while normalizing slashes / absolute paths.
  let rest = token
  let prefix = ''
  let suffix = ''
  while (
    rest.startsWith('!') ||
    rest.startsWith('^') ||
    rest.startsWith("'") ||
    rest.startsWith('"')
  ) {
    prefix += rest.slice(0, 1)
    rest = rest.slice(1)
  }
  if (rest.endsWith('$')) {
    suffix = '$'
    rest = rest.slice(0, -1)
  }
  if (!rest) {
    return `${prefix}${suffix}`
  }
  // Normalize absolute paths to a workspace-relative form so matching can work
  // against `discoverFileSuggestions()` results.
  if (!path.isAbsolute(rest)) {
    return `${prefix}${normalizeToPosix(rest)}${suffix}`
  }
  const relative = path.relative(process.cwd(), rest)
  if (relative && !relative.startsWith('..')) {
    return `${prefix}${normalizeToPosix(relative)}${suffix}`
  }
  return `${prefix}${normalizeToPosix(path.basename(rest))}${suffix}`
}
const normalizeQueryForFzf = (query: string): string => {
  const trimmed = query.trim()
  if (!trimmed) {
    return trimmed
  }
  return trimmed
    .split(/\s+/)
    .map((token) => normalizeFzfToken(token))
    .join(' ')
}
const fuzzyFilterStrings = (items: readonly string[], query: string, limit: number): string[] => {
  const trimmed = normalizeQueryForFzf(query)
  if (!trimmed) {
    return items.slice(0, limit)
  }
  // Heuristic: if the query contains path separators, treat it as a path search
  // and use forward matching (fzf's default). Otherwise prefer matching from the
  // end to bias toward filenames.
  const isPathQuery = trimmed.includes('/')
  const fzf = new Fzf(items, {
    casing: 'smart-case',
    normalize: true,
    fuzzy: 'v2',
    forward: isPathQuery,
    match: extendedMatch,
    tiebreakers: [byStartAsc, byLengthAsc],
    limit,
  })
  return fzf
    .find(trimmed)
    .map((result) => result.item)
    .slice(0, limit)
}
export const filterFileSuggestions = ({
  suggestions,
  query,
  exclude = [],
  limit = DEFAULT_FILE_SUGGESTION_LIMIT,
}: FilterFileSuggestionsOptions): string[] => {
  const excluded = new Set(exclude)
  const eligible = suggestions.filter((suggestion) => !excluded.has(suggestion))
  return fuzzyFilterStrings(eligible, query, limit)
}
export type FilterDirectorySuggestionsOptions = FilterFileSuggestionsOptions
export const filterDirectorySuggestions = (options: FilterDirectorySuggestionsOptions): string[] =>
  filterFileSuggestions(options)
</file>
<file path="docs/cookbook.md">
# The prompt-maker-cli Cookbook
**prompt-maker-cli** is a terminal-first prompt generator with two faces:
- A flag-driven CLI (`generate`, `test`, plus utility subcommands like `export` and `compose`)
- An Ink-based TUI (default when you run with no args)
Under the hood, the CLI routes commands via `src/index.ts` and the generate workflow runs through `src/generate/pipeline.ts` (context resolution â optional smart context â prompt generation â optional polishing/template â history logging). This cookbook focuses on practical ârecipesâ and flags you can combine to build repeatable prompt contracts.
A note on versions: this doc preserves all existing recipes. When something is likely outdated, itâs kept and labeled as **Legacy / may be outdated** with a pointer to the current replacement.
## Table of Contents
- [Quickstart (CLI + TUI)](#quickstart-cli--tui)
- [Prompting Masterclass](#prompting-masterclass)
- [Flag Strategy & Mechanics](#flag-strategy--mechanics)
- [Debugging Prompt Runs](#debugging-prompt-runs)
- [Template Playbook](#template-playbook)
- [AI Systems Recipes](#ai-systems-recipes)
- [Developer Recipes](#developer-recipes)
- [Self-Directed Learning Recipes](#self-directed-learning-recipes)
- [Git Commit Workflows](#git-commit-workflows)
- [Editor Workflow Recipes](#editor-workflow-recipes)
- [Jira Ticket Recipes](#jira-ticket-recipes)
- [CI Integration Recipes](#ci-integration-recipes)
- [NeoVim Plugin Integration Recipes](#neovim-plugin-integration-recipes)
- [Recipes](#recipes)
## Quickstart (CLI + TUI)
Authoritative references for the current TUI behavior and structure:
- `docs/tui-design.md` (user-facing behavior, keybinds, input routing invariants)
- `src/tui/DEVELOPER_NOTE.md` (architecture + reducer/hook structure)
- `AGENTS.md` (dev commands + project conventions)
### Recipe: Install + run from source
**Prereqs**
- Node.js `>=18` (repo commonly uses Node `22.x`; see `.nvmrc`)
**Solution**
```bash
npm ci
npm run build
# TUI (default when no args)
npm start
# Generate (one-shot) from source
npm start -- "Draft a confident onboarding-bot spec" --model gpt-4o-mini
# Prompt tests (from source)
npm start -- test prompt-tests.yaml
```
**Expected result**
- `npm start` launches the Ink TUI.
- `npm start -- <intent> ...` runs the generate pipeline and prints the final prompt (or JSON if requested).
- `npm start -- test ...` runs the YAML test suite and sets a non-zero exit code on failures.
**Troubleshooting**
- If you see âMissing OpenAI credentialsâ or âMissing Gemini credentialsâ, configure env vars or a config file (see recipe below).
### Recipe: Run the published CLI (after install)
**Prereqs**
- `prompt-maker-cli` installed globally (or available on your PATH)
**Solution**
```bash
# TUI
prompt-maker-cli
# Explicit TUI entry
prompt-maker-cli ui
# Generate
prompt-maker-cli "Summarize src/tui/ in 5 bullets" -c "src/tui/**/*.ts*" --polish
# Prompt tests
prompt-maker-cli test
```
**Expected result**
- `prompt-maker-cli` with no args starts the TUI.
- `prompt-maker-cli test` runs `prompt-tests.yaml` by default.
### Recipe: Configure credentials + defaults
**Problem**
You want `prompt-maker-cli` to run without re-exporting env vars every time, and you want the TUIâs theme settings to persist.
**Solution**
1. Create (or set) a config file:
   - `PROMPT_MAKER_CLI_CONFIG=/path/to/config.json` (highest precedence)
   - `~/.config/prompt-maker-cli/config.json`
   - `~/.prompt-maker-cli.json`
2. Add credentials and optional defaults:
```json
{
  "openaiApiKey": "sk-...",
  "geminiApiKey": "gk-...",
  "promptGenerator": {
    "defaultModel": "gpt-4o-mini",
    "defaultGeminiModel": "gemini-3-pro-preview",
    "maxInputTokens": 12000,
    "maxContextTokens": 8000,
    "contextOverflowStrategy": "drop-smart"
  },
  "contextTemplates": {
    "scratch": "# Scratch\n\n{{prompt}}"
  },
  "theme": "ocean",
  "themeMode": "system"
}
```
**Discussion**
- Env vars override config:
  - `OPENAI_API_KEY` (optional `OPENAI_BASE_URL`)
  - `GEMINI_API_KEY` (optional `GEMINI_BASE_URL`)
- Some TUI choices persist by writing back into the same config file (see `src/config.ts`):
  - Theme: `theme`, `themeMode`
  - Budgets (`/budgets`): `promptGenerator.maxInputTokens`, `promptGenerator.maxContextTokens`, `promptGenerator.contextOverflowStrategy`
  - Resume defaults (`/resume`): `resumeMode`, `resumeSourceKind`
  - Export defaults (`/export`): `exportFormat`, `exportOutDir`
### Recipe: Learn the TUI quickly
**Solution**
```bash
prompt-maker-cli
```
Then:
- Type `/help` to open the help overlay (it is the definitive keybind list).
- Press `Ctrl+G` to open the command palette.
- Press `Ctrl+T` to switch to the Test Runner view.
**Discussion**
The TUI is intentionally âkeyboard-firstâ and follows strict input-routing invariants (help overlay > popup > screen > global keys). See `docs/tui-design.md`.
### Recipe: Use the command palette (/commands)
**Problem**
You want to discover what the TUI can do (models, context, tests, theming) without memorizing flags.
**Solution**
1. Press `Ctrl+G` (or type `/`) to open the command palette.
2. Type to filter commands.
3. Use arrow keys to select.
4. Press Enter to run the selected command (some open a popup).
**Expected result**
- The palette shows the current command list from `src/tui/config.ts` (`COMMAND_DESCRIPTORS`).
- If a command opens a popup, the popup owns input until you close it (usually with `Esc`).
**Troubleshooting**
- If keys âdonât work,â check whether the help overlay is open (`/help`) or a popup is active; those layers intentionally suppress screen/global keys.
### Recipe: Add context in the TUI (files, URLs, images, video, smart)
**Problem**
You want to attach context interactively without rebuilding a long CLI command.
**Solution**
- Open the palette, then use:
  - `/file` to add file globs (repeatable)
  - `/url` to add URL context
  - `/image` to attach image paths
  - `/video` to attach video paths (will force Gemini at generation time)
  - `/smart` to toggle smart context
  - `/smart-root` to set/clear the smart-context scan root
**Fast path: drag + drop**
- Drag an absolute file path into the terminal.
- When you see the hint âPress Tab to add âŚ to contextâ, press `Tab`.
**Expected result**
- Context items are tracked as chips in the input bar and used for subsequent generations.
- Popups support list management (add/remove) and some provide suggestions (toggle with `Tab`).
### Recipe: Generate an atomic prompt series (TUI)
**Problem**
You want a set of standalone âatomic promptsâ you can hand off step-by-step (no cross-references).
**Solution**
- Type an intent, then press `Tab`.
  - If an absolute file path is currently âdroppedâ in the input, `Tab` adds that file to context instead.
- Or run `/series` from the command palette.
**Expected result**
- The TUI generates a series and writes markdown files under `generated/series/<timestamped-folder>/`.
- The folder includes `00-overview.md` plus one file per atomic prompt.
**Troubleshooting**
- If writing fails (permissions/readonly FS), the TUI still generates but will report file-write errors in history.
### Recipe: Run prompt tests in the TUI
**Problem**
You want to run `prompt-tests.yaml` without leaving the UI.
**Solution**
1. Press `Ctrl+T` to switch to the Test Runner view.
2. Keep `prompt-tests.yaml` (default) or enter another path.
3. Press `Tab` to focus the Run button, then `Enter` to start.
**Expected result**
- The test runner loads the YAML suite, runs each test, and shows PASS/FAIL with reasons.
### Recipe: Theme + theme mode (TUI)
**Problem**
You want the TUI colors to match your terminal and persist across sessions.
**Solution**
- Run `/theme` to pick a theme.
- Run `/theme-mode dark|light|system` to switch appearance mode.
**Expected result**
- The selection persists in your CLI config (`theme` and `themeMode`; see `src/config.ts`).
### Recipe: Run the TUI with interactive transport
**Problem**
You want an external tool (editor plugin, automation) to push refine/finish commands during interactive runs.
**Solution**
```bash
prompt-maker-cli ui --interactive-transport /tmp/prompt-maker.sock
```
**Expected result**
- The TUI starts and the underlying generate pipeline listens on the provided socket/pipe.
- When the pipeline reaches an interactive step, the UI shows it is waiting for transport input.
**Troubleshooting**
- The transport protocol is implemented in `src/generate/interactive-transport.ts` and emits `transport.*` + `interactive.*` stream events.
### Recipe: Use prompt-maker-cli without a TTY (headless)
**Problem**
Youâre running in CI or piping output, so the Ink TUI and interactive prompts wonât work.
**Solution**
```bash
echo "Draft a release risk summary" | prompt-maker-cli --quiet --progress=false --stream jsonl
```
**Expected result**
- The CLI runs non-interactively and emits JSONL progress events to stdout.
**Troubleshooting**
- Interactive mode (`--interactive`) is ignored without a TTY; use `--interactive-transport` for headless interactive workflows.
### Recipe: Machine-readable output (JSON + JSONL)
**Problem**
You want prompt-maker to integrate with other tooling (scripts, bots, editor plugins).
**Solution (JSON payload)**
```bash
prompt-maker-cli "Summarize the changes" -c /tmp/staged.patch --json > run.json
```
Notes:
- `--json` prints a pretty JSON payload to stdout and still appends a JSONL history entry.
- If you also use `--show-context` with `--json`, the context is printed to stderr to avoid corrupting the JSON payload.
**Solution (JSONL stream events)**
```bash
prompt-maker-cli "Summarize the changes" -c /tmp/staged.patch \
  --stream jsonl --quiet --progress=false > events.jsonl
```
Notes:
- `--stream jsonl` writes newline-delimited JSON events to stdout (see `src/generate/stream.ts`).
- Use `--quiet` if you want _only_ JSONL lines on stdout.
### Recipe: Token budgets + deterministic overflow (and `context.overflow`)
**When to use this**
Youâre automating prompt generation and need deterministic behavior when your text context is too large (instead of âsometimes it fitsâ).
**Steps**
```bash
# Intentionally set a small context budget to force trimming.
# Use --quiet + --stream jsonl so stdout stays machine-readable.
prompt-maker-cli "Summarize the selected modules" \
  -c "src/**/*.ts" \
  --max-context-tokens 800 \
  --context-overflow drop-largest \
  --stream jsonl \
  --quiet \
  --progress=false > events.jsonl
```
**Expected output (snippet)**
`events.jsonl` includes a `context.overflow` line before generation begins:
```json
{
  "event": "context.overflow",
  "strategy": "drop-largest",
  "droppedPaths": [{ "path": "...", "source": "file" }]
}
```
**Notes**
- Budgets apply only to **text** context entries (`--context`, `--url`, `--smart-context`), not images/videos.
- With `--stream jsonl`, the CLI treats stdout as a machine channel; combine with `--quiet` to avoid mixing human output.
**Troubleshooting / pitfalls**
- If no `context.overflow` appears, your context likely fit within the budget.
- If you see a hard failure, you may have left `--context-overflow` unset (default is `fail` when budgets are enabled).
---
### Recipe: Export a generate payload (portable JSON/YAML)
**When to use this**
You want a portable artifact you can check into a repo, attach to a ticket, or diff in CI â without scraping `history.jsonl` manually.
**Steps**
```bash
# Export the most recent generate payload from history.
prompt-maker-cli export --format json --out runs/last-run.json
# Export an older entry ("third from last") as YAML.
prompt-maker-cli export --from-history last:3 --format yaml --out runs/third-from-last.yaml
```
**Expected output (snippet)**
- Files are written to `runs/*.json` or `runs/*.yaml`.
- The payload includes a `schemaVersion` field:
```json
{
  "schemaVersion": "1",
  "intent": "...",
  "prompt": "..."
}
```
**Notes (determinism + stdout/stderr)**
- The export command writes **only** to the file path you provide.
- Success messages are printed to **stderr** by default; pass `--quiet` to suppress them.
**Troubleshooting / pitfalls**
- âHistory file not found âŚâ: run any generate command first; exports read from `~/.config/prompt-maker-cli/history.jsonl`.
- âNo generate payload entries found âŚâ: your history file may contain non-payload JSONL; run a normal generate and try again.
- âUnsupported history payload schemaVersion=âŚâ: upgrade/downgrade prompt-maker-cli, or re-run generation to create a compatible history entry.
**TUI equivalent**
- In the TUI, run `/export` to pick a history entry and write JSON/YAML.
- The picker marks incompatible history entries as unsupported, and export will fail if you select one (`src/history/generate-history.ts`).
---
### Recipe: Resume from history or an exported payload (`resume.loaded`)
**When to use this**
You want to iterate on a previous run (including refinements) without retyping intent/model/meta settings â and you want explicit, testable handling of missing context files.
**Steps (resume from history)**
```bash
# Resume from the last history entry.
# Note the explicit empty intent string: resume supplies the intent.
prompt-maker-cli "" --resume-last --resume-mode best-effort \
  --stream jsonl --quiet --progress=false > resume-events.jsonl
# Resume from the 5th entry from the end.
prompt-maker-cli "" --resume 5 --resume-mode best-effort --quiet
```
**Steps (resume from an exported payload file)**
```bash
# Export, then resume from the exported file.
prompt-maker-cli export --format json --out runs/last-run.json
# Strict mode fails if any resumed file context paths are missing.
prompt-maker-cli "" --resume-from runs/last-run.json --resume-mode strict --quiet
```
**Expected output (snippet)**
With `--stream jsonl`, the run emits `resume.loaded` early:
```json
{
  "event": "resume.loaded",
  "source": "history",
  "reusedContextPaths": [{ "path": "notes/existing.md", "source": "file" }],
  "missingContextPaths": [{ "path": "notes/missing.md", "source": "file" }]
}
```
**Notes (precedence + machine-readable output)**
- Explicit CLI flags override resumed payload values:
  - `--model` overrides `payload.model`
  - `--target` overrides `payload.targetModel`
  - `metaInstructions` is reused from the payload when present (there is no generate-mode flag for it today)
- Explicit `--context ...` overrides resumed `contextPaths`.
- `--resume-mode best-effort` warns on missing files and continues.
- `--resume-mode strict` fails fast if any resumed `source:"file"` paths are missing.
**TUI equivalent**
- In the TUI, run `/resume` and select:
  - `Source: history` (pick `last`, `last:2`, âŚ), or
  - `Source: file` (enter a payload path)
- The TUI persists the default source/mode (`resumeSourceKind`, `resumeMode`) back into config (`src/config.ts`).
**Troubleshooting / pitfalls**
- If you pass both `--resume` and `--resume-last`, argument parsing fails (`--resume and --resume-last cannot be combined.`).
- If you see interactive/JSON incompatibility errors, remember: `--json` cannot be combined with `--interactive` or `--interactive-transport`.
---
### Recipe: Compare two runs (export + diff)
**When to use this**
You want a simple, deterministic way to compare outputs across runs (prompt text, model choice, context paths) without adding a bespoke âcompareâ mode.
**Steps**
```bash
mkdir -p runs
# Export the last two runs.
prompt-maker-cli export --from-history last --format json --out runs/run-a.json
prompt-maker-cli export --from-history last:2 --format json --out runs/run-b.json
# Extract just the prompt text for diffing (portable; no jq required).
node --input-type=module -e "
  import fs from 'node:fs/promises'
  const a = JSON.parse(await fs.readFile('runs/run-a.json', 'utf8'))
  const b = JSON.parse(await fs.readFile('runs/run-b.json', 'utf8'))
  await fs.writeFile('runs/run-a.prompt.txt', a.renderedPrompt ?? a.polishedPrompt ?? a.prompt)
  await fs.writeFile('runs/run-b.prompt.txt', b.renderedPrompt ?? b.polishedPrompt ?? b.prompt)
"
diff -u runs/run-b.prompt.txt runs/run-a.prompt.txt || true
```
**Expected output (snippet)**
A unified diff showing prompt-level changes:
```diff
- Old wording...
+ New wording...
```
**Notes**
- Prefer `renderedPrompt` over `polishedPrompt` over `prompt` (matches how integrations choose the final artifact).
- This workflow is âcomparativeâ by construction: the exported payloads are stable artifacts you can diff, review, or attach to tickets.
**Troubleshooting / pitfalls**
- If `diff` shows nothing, the final artifact text is identical.
- If your last two runs used different templates/polish settings, compare both `prompt` and `renderedPrompt` fields to understand why.
---
### Recipe: Deterministic compose scaffold (no LLM)
**When to use this**
You want a deterministic, offline way to produce a prompt-like artifact from a ârecipeâ and some input (useful for scaffolding integrations).
**Steps**
```bash
prompt-maker-cli compose --recipe docs/tui-design.md --input "Draft a summary" > composed.txt
```
**Expected output (snippet)**
`composed.txt` is:
```text
<recipe file contents>
---
Draft a summary
```
**Notes (stdout/stderr)**
- Compose writes successful output to **stdout** only; errors are on **stderr**.
- Current behavior is deliberately simple: it normalizes newlines and joins recipe + input with a `---` delimiter.
**Troubleshooting / pitfalls**
- âFailed to read recipe file âŚâ: check the path and permissions.
- This command is scaffolding; it does not parse YAML/JSON recipes yet.
### Recipe: Where outputs are stored (history)
**Problem**
You want an audit trail of what was sent, with what context, and when.
**Solution**
- Every generate run appends a JSONL line to:
  - `~/.config/prompt-maker-cli/history.jsonl`
**Discussion**
- The TUI also exposes history browsing via `/history`.
- The saved JSON includes `contextPaths` and any `refinements`, which makes it possible to replay or diff runs.
### Recipe: Common troubleshooting checklist
**Problem**
A run fails or behaves unexpectedly.
**Checklist**
- **Credentials**: set `OPENAI_API_KEY` / `GEMINI_API_KEY` or configure `~/.config/prompt-maker-cli/config.json`.
- **No files matched**: your `-c/--context` glob matched nothing (see warning from `src/file-context.ts`).
- **Interactive seems ignored**: youâre in a non-TTY environment; use `--interactive-transport` or run in a real terminal.
- **Video forces Gemini**: `--video` switches to `resolveGeminiVideoModel()`; ensure Gemini is configured.
- **Stream output looks âmixedâ**: combine `--stream jsonl` with `--quiet`.
## Prompting Masterclass
### Mental Models that Travel Across Models
- **Chain of Thought (CoT)**: Ask the model to reason step-by-step when tackling logic-heavy work (tracing bugs, drafting proofs). Combine CoT language in your intent with `--interactive` so you can append refinement instructions as fresh insights appear.
- **Few-Shot Priming**: Embed curated exemplars via `-c examples/*.md` or `--context-template` to bias style and structure. Works best when the examples are short and sharply relevantâwatch token bloat via the CLIâs token telemetry.
- **Persona Adoption**: State the persona plus decision criteria directly in the intent (`"Adopt the voice of a staff engineer..."`) or maintain persona snippets in markdown files referenced through `-c personas/staff-engineer.md`. Personas pair well with the polish pass because the `POLISH_SYSTEM_PROMPT` preserves headings while tightening tone.
- **Constraint Stacking**: The CLIâs default format enforces Context â Intent â Output Format; use bullet lists, acceptance criteria, and schema-like checklists to corral powerful models. When you need absolute structure, emit `--json` so downstream tools can parse the run artifact.
### Model-Specific Tactics
- **OpenAI**: Favor detailed work orders with enumerated deliverables; use `--polish` to squeeze extra clarity after interactive refinement.
- **Gemini**: Lean into multimodal runsâ`--image` and `--video` feed attachments through `prompt-generator-service`. When a video is present and the requested model is not Gemini, the CLI automatically switches to the configured Gemini video model (default: `gemini-3-pro-preview`) so you stay within supported modalities.
- **Smaller / Local Models**: Prune context aggressively. Combine targeted globs (`-c "src/core/**/*.ts"`) with `--smart-context` to fetch only the top-N embedding matches, keeping token counts within local limits.
### Automating Prompt Structure
1. **Context**: Mix and match `-c/--context` globs, `--url`, and `--smart-context` (which indexes files under `--smart-context-root` via `smart-context-service.ts`). Use `--show-context` or `--context-file` with `--context-format json` to inspect what will be sent upstream.
2. **Intent**: Pass a positional string, pipe stdin, or rely on `--intent-file/-f`. The parser prevents ambiguous mixes (inline + file) so you always know which source won.
3. **Output Format & Delivery**: Apply a template (`--context-template nvim`), request a polish pass (`--polish`/`--polish-model`), copy results to the clipboard (`--copy`), or jump straight into ChatGPT (`--open-chatgpt`). Interactive refinement and JSON streaming (`--stream jsonl`) round out the automation loop.
## Flag Strategy & Mechanics
### Core Run Modes
- `--interactive/-i`: Launches a refinement loop (TTY or `--interactive-transport`). You can add instructions between iterations; transport mode enables external tooling to push JSON commands.
- `--json`: Emits the final payload as structured JSON; **cannot** be combined with `--interactive` or `--interactive-transport` (enforced in `src/generate/pipeline.ts`), so pick one output path.
- `--stream jsonl`: Mirrors telemetry and lifecycle events to stdoutâideal for logging or UI bridges. Combine with `--quiet` to suppress boxed UI while still receiving machine-friendly events.
### Token Budgets & Context Overflow
When you attach lots of context, you may need deterministic trimming to stay within model limits.
Flags (generate mode):
- `--max-input-tokens <n>`: caps total input tokens (`intentTokens + systemTokens + fileTokens`).
- `--max-context-tokens <n>`: caps tokens reserved for **text context entries** (`fileTokens`).
- `--context-overflow <strategy>`: how to respond if budgets are exceeded.
Strategies:
- `fail`: throw and abort.
- `drop-smart`: drop smart-context entries first, then oldest-first.
- `drop-url`: drop URL context first, then oldest-first.
- `drop-largest`: drop largest entries first.
- `drop-oldest`: drop in original attachment order.
Notes:
- Budgets apply only to text context entries (`--context`, `--url`, `--smart-context`). Images/videos are not trimmed by these strategies.
- If budgets are enabled and trimming happens, the CLI emits a `context.overflow` stream event and prunes `contextPaths` in the final JSON payload.
Example (budgets + streaming):
```bash
prompt-maker-cli "Summarize the selected modules" \
  -c "src/**/*.ts" \
  --stream jsonl \
  --quiet \
  --progress=false \
  --max-context-tokens 8000 \
  --context-overflow drop-smart
```
### Context Assembly Flags
- `-c/--context <glob>`: Backed by `fast-glob`, supports includes/excludes (prefix with `!`), repeatable. Great for language- or folder-specific pulls.
- `--url <link>`: Fetches remote docs with progress callbacks.
- `--smart-context`: Runs the RAG pipeline (`smart-context-service.ts`) to index code/text under the current working tree or a custom `--smart-context-root`, automatically attaching the top 5 files under 25 KB.
- `--show-context`, `--context-file`, `--context-format text|json`: Inspect or persist the resolved context envelope for auditing.
### Media Inputs
- `--image <path>`: Attaches one or more images; they flow through to the prompt generator for multimodal models such as GPT-4o or Gemini.
- `--video <path>`: Triggers the Gemini pipeline. `src/generate/pipeline.ts` switches the model to `resolveGeminiVideoModel()` (default comes from config; commonly `gemini-3-pro-preview`) when the selected model is not Gemini, and prints a warning like `Switching to âŚ to support video input.` Uploads run through `media-loader.ts`, which requires `GEMINI_API_KEY` and polls `GoogleAIFileManager` until the file becomes `ACTIVE`.
### Output Tailoring
- `--polish`, `--polish-model`: Runs a final LLM pass with the baked-in system prompt to tighten formatting while preserving structure. You can also set `PROMPT_MAKER_POLISH_MODEL` to choose a default polish model when `--polish-model` is omitted.
- `--context-template <name>`: Wraps the final prompt inside a named template (`nvim` is built-in; custom templates live in CLI config). The parser enforces non-empty template names.
- `--copy`, `--open-chatgpt`: Quality-of-life delivery flags.
### High-Value Combinations
- `--smart-context` + `--interactive`: Start with an embedding-ranked snapshot, then iteratively refine based on what you learn during the sessionâideal for sprawling repos.
- `-c "<glob>"` + `--context-file prompt-context.md`: Capture exactly which files were read so teammates can replay the run.
- `--stream jsonl` + `--json`: Emit JSONL events during the run and print the final JSON payload at the end. (Note: this mixes JSONL + pretty JSON on stdout; for strictly machine-parseable stdout, prefer `--stream jsonl --quiet` and read the final payload from `~/.config/prompt-maker-cli/history.jsonl`.)
- `--context-template nvim` + `--copy`: Spits out an editor-ready buffer and places it on your clipboard for immediate paste.
- `--video` + `--polish`: Lean on Gemini for multimodal understanding, then run a polish pass (which reuses the Gemini credentials) for clean instructions.
### Conflicts and Guardrails
- `--json` vs `--interactive`: Mutually exclusive; the CLI throws early to prevent orphaned interactive sessions.
- Inline intent vs `--intent-file`: You must pick one; `resolveIntent()` enforces this and warns if you accidentally pass a file path immediately after `-i`.
- `--video` vs non-Gemini models: The CLI switches to the configured Gemini video model (see `resolveGeminiVideoModel()` in `src/generate/models.ts`) and prints a warning so you donât accidentally run an unsupported combination. Ensure `GEMINI_API_KEY` is set.
- `--interactive` without a TTY: The CLI warns and downgrades to non-interactive mode; use `--interactive-transport` for headless setups.
- Empty `--context-template` or `--interactive-transport`: The parser trims values and rejects blank strings so you donât end up with silent no-ops.
## Debugging Prompt Runs
- **Trace Context Inputs**: Pair `--show-context` with `--context-format json` during dry runs to print the exact `<file>` payloads gathered by `resolveFileContext` and `resolveSmartContextFiles`. When you need an audit trail, add `--context-file tmp/context-dump.md` to persist the snapshot that fed the LLM.
- **Watch Token Telemetry**: Every generation prints a Context Telemetry box sourced from `countTokens()`. Large spikes in `fileTokens` signal sloppy globs; tighten them or let `--smart-context` re-rank files automatically.
- **Budget overflows**: If you enable `--max-input-tokens`/`--max-context-tokens` and choose a non-`fail` overflow strategy, the CLI may emit `context.overflow` (streaming) and drop some context entries. Treat this as a meaningful warning and consider re-running with a higher budget or tighter globs.
- **Stream Everything**: `--stream jsonl` mirrors `progress.update`, `context.telemetry`, `context.overflow`, `generation.iteration.*`, and `upload.state` events to stdout. If you want to pipe directly into `jq`, combine with `--quiet` so non-JSON output doesnât interleave with the stream.
- **Replay with History Artifacts**: `--json` writes the final payload (intent, context paths, iterations, polish metadata) and `appendToHistory()` stores it locally. Diff these blobs to understand how refinements changed the contract over time.
- **Interactive Diagnostics**: In TTY mode, each refinement is boxed via `displayPrompt()`. When headless, use `--interactive-transport /tmp/prompt.sock` and send JSON commands from another process; hook into the emitted `transport.*` events to orchestrate automated QA.
- **Clipboard & Flag Tracing**: Set `PROMPT_MAKER_DEBUG_FLAGS=1` to log a JSON snapshot of parsed flags (copy, polish, quiet, json, etc.) right after argument parsing. Add `PROMPT_MAKER_COPY_TRACE=1` (or rely on the debug flag) to emit `[pmc:copy âŚ]` diagnostics showing when clipboard writes are attempted, skipped, or fail. Example: `PROMPT_MAKER_DEBUG_FLAGS=1 PROMPT_MAKER_COPY_TRACE=1 prompt-maker-cli --polish --copy âŚ` instantly proves whether the CLI saw `--copy` and what happened inside `clipboardy`.
- **Media Upload Issues**: Stuck video uploads surface as repeated `upload.state` events. If they never flip from `start` to `stop`, confirm `GEMINI_API_KEY` and MIME support in `media-loader.ts` (e.g., `.mp4`, `.webm`).
- **Spinner Hygiene**: Disable spinners with `--progress=false` when your logs run in CI/CD; combine with `--quiet` to keep transcripts clean while still consuming JSONL telemetry.
## Template Playbook
- **Built-in templates**: Pass `--context-template nvim` to wrap the prompt inside the bundled buffer-friendly layout (resolved via `src/generate/context-templates.ts`). The template drops your artifact where `{{prompt}}` lives, so headings and shortcuts remain intact.
- **Custom templates**: Add entries to your CLI config (`contextTemplates` map). Reference them with `--context-template my-handoff`. The parser enforces non-empty names and throws if the template is missing, saving you from silent fallbacks.
- **Composable Delivery**: Templates stack with `--copy`, `--open-chatgpt`, and `--context-file`. Render a Neovim scratch buffer, copy it to the clipboard, and archive the text file in one run.
- **Previewing Output**: Pair `--context-template` with `--json` to capture both the raw prompt (in the JSON payload) and the rendered template (saved as `renderedPrompt`). This is handy when diffing changes across runs.
**Example â Sprint Handoff Template**
```bash
prompt-maker-cli "Summarize sprint 42 backend work" \
  -c "src/services/**/*.ts" \
  -c docs/notes/sprint-42.md \
  --context-template nvim \
  --copy
```
This command collects key files, wraps the result in the Neovim template, and drops it on your clipboard so you can open a scratch buffer and paste immediately.
## AI Systems Recipes
### Recipe: Agent Persona Contract
**Problem**
Youâre designing a new autonomous agent and need a consistent system prompt that references existing SOPs, guardrails, and escalation rules.
**Solution**
```bash
prompt-maker-cli "Draft an agent persona spec for the Atlas migration agent, including goals, redlines, and escalation protocol." \
  -c agents/atlas/mission.md \
  -c agents/atlas/playbooks/*.md \
  -c security/guardrails.md \
  --context-template nvim \
  --polish
```
**Discussion**
Pulling SOPs and guardrails grounds the agent spec in real policy. The Neovim template yields a ready-to-paste contract, while the polish pass keeps tone tightâperfect for feeding into downstream orchestration frameworks.
### Recipe: MCP Server Capability Brief
**Problem**
Youâre exposing a new Model Context Protocol (MCP) server and want a prompt that enumerates capabilities, auth model, and sample invocations for agent developers.
**Solution**
```bash
prompt-maker-cli "Summarize MCP server capabilities (tools, auth, rate limits) for documentation consumers." \
  -c mcp/servers/inventory/**/*.ts \
  -c mcp/docs/authentication.md \
  --smart-context-root mcp \
  --json \
  --context-file docs/mcp/inventory-brief.md
```
**Discussion**
Using both implementation files and docs ensures the LLM sees handler signatures plus narrative context. JSON output can be fed into build steps that publish docs or update MCP registries, while the context file anchors what inputs were used.
### Recipe: Multi-Agent Handoff Matrix
**Problem**
You have multiple specialized agents (research, implementation, QA) and need a structured prompt that defines handoff triggers and shared artifacts.
**Solution**
```bash
prompt-maker-cli "Create a multi-agent collaboration matrix (Research â Build â QA) with handoff triggers and shared artifacts." \
  -c agents/research/*.md \
  -c agents/build/*.md \
  -c agents/qa/*.md \
  -c docs/process/handoff-checklist.md \
  --interactive \
  --context-template nvim
```
**Discussion**
Stacking context from each agentâs playbook plus the handoff checklist produces a unified contract. Interactive mode lets stakeholders refine responsibilities live (e.g., add âQA can bounce back to Research if acceptance criteria missingâ). The resulting template is ready to drop into orchestration configs or Confluence.
### Recipe: Auto Tool Discovery Guide
**Problem**
Youâre wiring an agent runtime that needs to decide which internal tools to load per request; you want a prompt that inspects tool metadata and emits a selection strategy.
**Solution**
```bash
prompt-maker-cli "Given these tool manifests, decide when to load each tool and define fallback heuristics." \
  -c tools/manifests/**/*.json \
  -c docs/agents/tool-governance.md \
  --smart-context-root tools \
  --polish \
  --context-file agents/tool-discovery-plan.md
```
**Discussion**
Tool manifests plus governance rules allow the model to derive eligibility matrices (âUse `vector-search` when intents mention embeddings, fall back to `doc-search` otherwiseâ). Persisting the plan to `agents/tool-discovery-plan.md` lets your orchestrator ingest it or flag deviations during audits.
### Recipe: Embedding Pipeline Playbook
**Problem**
Youâre rolling out a new embedding pipeline (chunking, filtering, indexing) and want a prompt that stitches together engineering docs, ETL scripts, and schema definitions into a deployment plan.
**Solution**
```bash
prompt-maker-cli "Produce an embedding ingestion playbook (chunking, filters, index rollout) for the Docs corpus." \
  -c pipelines/embeddings/docs/**/*.ts \
  -c pipelines/embeddings/docs/config/*.yaml \
  -c docs/data-quality/*.md \
  --stream jsonl \
  --context-template nvim
```
**Discussion**
Mixing code, config, and policy text yields actionable steps (pre-flight validation, TF-IDF filters, reindex cadence). JSONL streaming captures intermediate telemetry if you need to trace iterations. Pair with `--json` if you want to store the final prompt alongside the rollout ticket.
### Recipe: Evaluation Harness Generator
**Problem**
You need a consistent process for evaluating agent prompts against regression suites (fixtures, rubrics, scoring scripts).
**Solution**
```bash
prompt-maker-cli "Design an evaluation harness (fixtures, metrics, scoring scripts) for the Atlas agent." \
  -c eval/fixtures/atlas/**/*.json \
  -c eval/rubrics/**/*.md \
  -c scripts/metrics/atlas_eval.ts \
  --polish \
  --context-template nvim
```
**Discussion**
By combining fixtures, qualitative rubrics, and scoring code, the CLI outputs a contract describing how to run and interpret evaluations. Feed the result into CI (e.g., nightly evaluation jobs) or share with red teams.
### Recipe: Retriever Tuning Blueprint
**Problem**
Your RAG system needs per-domain retriever settings (chunk sizes, rerankers, max matches). You want a prompt that reviews current telemetry and proposes tuned parameters.
**Solution**
```bash
prompt-maker-cli "Recommend retriever tuning parameters for the Support knowledge base." \
  -c rag/support/telemetry/*.json \
  -c rag/support/config/*.yaml \
  -c docs/rag/retriever-guidelines.md \
  --smart-context-root rag \
  --json
```
**Discussion**
Telemetry plus configs plus guidelines help the model suggest evidence-backed changes. JSON output is perfect for feeding into dashboards or PR bots that annotate config diffs with rationale.
### Recipe: Safety Review Packet
**Problem**
Before launching a new agent, compliance requires a safety packet covering data handling, prompt safeguards, and escalation paths.
**Solution**
```bash
prompt-maker-cli "Assemble a safety review packet for the Commerce agent (data flow, prompt safeguards, escalation)." \
  -c agents/commerce/persona.md \
  -c agents/commerce/prompts/*.md \
  -c security/safety-checklist.md \
  -c docs/legal/data-retention.md \
  --polish \
  --context-file reviews/commerce-safety.md
```
**Discussion**
Mixing persona, prompt contracts, safety checklists, and legal guidance lets the model produce a thorough review doc. Writing to `reviews/commerce-safety.md` ensures auditors and approvers have a consistent artifact to sign off.
### Recipe: Launch Readiness Checklist
**Problem**
Product and Ops need a single document confirming an agent is launch-ready (docs, telemetry, on-call coverage, rollback plan).
**Solution**
```bash
prompt-maker-cli "Create a launch readiness checklist for the Commerce agent (docs, telemetry SLAs, on-call, rollback)." \
  -c agents/commerce/persona.md \
  -c docs/launch/commerce/*.md \
  -c ops/runbooks/commerce-oncall.md \
  -c metrics/commerce/uptime.json \
  --context-template nvim \
  --json
```
**Discussion**
Combining product docs, runbooks, and live metrics results in a comprehensive go/no-go sheet. JSON output can feed dashboards or Slack bots that notify stakeholders when all boxes are checked.
### Recipe: Hallucination Test Suite Plan
**Problem**
Before shipping an assistant, you want targeted hallucination tests covering sensitive topics, long contexts, and out-of-domain queries.
**Solution**
```bash
prompt-maker-cli "Draft a hallucination testing plan (datasets, prompts, scoring) for the Support assistant." \
  -c eval/hallucination/support-fixtures/**/*.json \
  -c docs/safety/hallucination-tests.md \
  -c scripts/eval/hallucination_runner.ts \
  --interactive \
  --context-file eval/support-hallucination-plan.md
```
**Discussion**
The CLI merges fixtures, policy docs, and runner scripts to build a plan across scenarios. Interactive refinement lets researchers iteratively add edge cases discovered mid-review. The saved plan becomes a living document tied to release gates.
### Recipe: Automated Post-Mortem Primer
**Problem**
After an incident, you want a prompt that synthesizes logs, user reports, and recovery steps into a blameless post-mortem draft.
**Solution**
```bash
prompt-maker-cli "Produce a blameless post-mortem draft for incident INC-423 (timeline, impact, action items)." \
  -c incidents/INC-423/logs/*.log \
  -c incidents/INC-423/notes.md \
  -c docs/postmortem/template.md \
  --polish \
  --context-file incidents/INC-423/postmortem-draft.md
```
**Discussion**
Feeding raw logs, investigator notes, and the official template ensures the output respects your post-mortem format. The polish pass tidies tone, while the saved draft accelerates follow-up reviews and action-item tracking.
## Developer Recipes
### Recipe: Crash Reproduction Capsule
**Problem**
QA reported an intermittent null-pointer crashâyou need a prompt that guides the LLM through logs, stack traces, and reproduction steps.
**Solution**
```bash
prompt-maker-cli "Diagnose and propose fixes for the null-pointer crash when saving drafts." \
  -c "logs/crash/*.log" \
  -c "src/app/**/DraftService.ts" \
  --smart-context \
  --context-file crash-context.md \
  --json
```
**Discussion**
Combining explicit globs with `--smart-context` pulls in the most relevant nearby files. Writing `crash-context.md` preserves the exact evidence bundle, while `--json` records iterations/refinements for ticket attachments.
### Recipe: API Contract Snapshot
**Problem**
You must brief another team on the current REST/GraphQL surface, including payload shapes and validation rules.
**Solution**
```bash
prompt-maker-cli "Summarize public API endpoints with request/response schemas and validation rules." \
  -c "src/api/**/*.ts" \
  -c "docs/api/*.md" \
  --polish \
  --show-context
```
**Discussion**
The TypeScript + markdown mix gives the model both typed contracts and human notes. `--show-context` lets you validate that only the intended files were loaded, and `--polish` ensures the final document reads like a publishable API brief.
### Recipe: Framework Migration Coach
**Problem**
Youâre migrating from Redux Toolkit to Zustand and need a structured plan referencing existing state slices.
**Solution**
```bash
prompt-maker-cli "Create a step-by-step plan to migrate Redux Toolkit slices to Zustand with shared selectors." \
  -c "src/state/**/*.ts" \
  -c "docs/architecture/state.md" \
  --smart-context-root src \
  --interactive
```
**Discussion**
Initial context sketches the architecture, while `--interactive` lets you add refinements after the first draft (e.g., âaddress SSR data hydrationâ). Restrict smart-context scanning to `src` to keep embeddings fast.
### Recipe: Dependency Upgrade Risk Brief
**Problem**
Before upgrading `nx` and `vite`, you want a prompt that enumerates risks, test plans, and rollback steps using release notes and local config.
**Solution**
```bash
prompt-maker-cli "Assess upgrading Nx and Vite to the next minor release, listing risky plugins and verification steps." \
  -c package.json \
  -c nx.json \
  -c "docs/releases/nx/*.md" \
  --polish-model gpt-4o-mini \
  --copy
```
**Discussion**
Pointing to config files plus curated release notes equips the model with both current state and vendor guidance. Overriding the polish model keeps consistency with other platform reviews, and `--copy` macros the result straight into your change request doc.
## Self-Directed Learning Recipes
### Recipe: Technical Textbook Navigator
**Problem**
Youâre working through a dense systems textbook and want a prompt to summarize each chapter, surface prerequisites, and suggest practice problems.
**Solution**
```bash
prompt-maker-cli "Break down Chapter 6 of 'Distributed Systems' into prerequisites, key ideas, and practice drills." \
  -c notes/distributed-systems/ch06/*.md \
  -c "books/distributed-systems/ch06/**/*.pdf" \
  --context-template nvim \
  --polish
```
**Discussion**
Attach your reading notes plus exported chapter snippets (converted to markdown/pdf text). The template outputs a structured study card, while polishing keeps terminology precise. Repeat per chapter to build a learning map.
### Recipe: Deep Research Dossier
**Problem**
You need a self-study prompt that orchestrates multi-source research (papers, blogs, RFCs) and proposes a reading/experiment plan.
**Solution**
```bash
prompt-maker-cli "Assemble a deep-research plan for vector databases (questions, sources, experiments)." \
  -c research/vector-db/papers/*.md \
  -c research/vector-db/notes/**/*.md \
  -c bookmarks/vector-db/**/*.md \
  --smart-context \
  --json \
  --context-file research/vector-db/dossier.md
```
**Discussion**
Combining curated sources with smart-context ensures the prompt generator sees both high-signal references and ambient notes. JSON output lets you sync the research plan to Notion or Obsidian while the markdown dossier becomes your running log.
### Recipe: Language Learning Companion
**Problem**
Youâre teaching yourself Japanese and want prompts that adapt grammar drills to your mistakes and native material.
**Solution**
```bash
prompt-maker-cli "Design a Japanese study session focusing on keigo and business email corrections." \
  -c language/japanese/mistake-log.md \
  -c language/japanese/reading/sales-emails/*.txt \
  --interactive \
  --context-template nvim
```
**Discussion**
Feed the CLI your mistake log plus authentic examples; interactive mode lets you refine instructions after each session (âadd listening exercisesâ, âincrease kanji coverageâ). The template keeps sections organized (Warm-up, Drills, Reflection) for spaced repetition apps.
### Recipe: Software Engineering Mastery Sprint
**Problem**
You want a weekly mastery plan that targets architectural topics, code katas, and review prompts tailored to your repo.
**Solution**
```bash
prompt-maker-cli "Plan a weekly mastery sprint covering event-driven architecture patterns in this repo." \
  -c "src/**/*.ts" \
  -c docs/architecture/*.md \
  -c notes/learning-goals.md \
  --smart-context \
  --polish \
  --context-file learning/eda-week.md
```
**Discussion**
Mixing real code with architecture docs ensures the prompt references live examples. Smart context narrows the scope to relevant files, while the polish pass produces a schedule you can commit to version control and revisit in retros.
### Recipe: Spaced-Repetition Exporter
**Problem**
You want to convert textbook notes into spaced-repetition cards (Front/Back/Extra) compatible with tools like Anki or Mochi.
**Solution**
```bash
prompt-maker-cli "Turn these chapter notes into spaced repetition cards with cloze deletions." \
  -c notes/distributed-systems/ch06/*.md \
  -c notes/distributed-systems/glossary.md \
  --context-template nvim \
  --polish \
  --context-file learning/ch06-cards.md
```
**Discussion**
Feed the CLI your notes and glossary; the template organizes each card, while polishing enforces concise wording. The saved markdown can be imported or transformed into CSV for your spaced-repetition app.
### Recipe: Socratic Tutor Flow
**Problem**
You want prompts that guide you via questions instead of answers, forcing active recall for each concept.
**Solution**
```bash
prompt-maker-cli "Act as a Socratic tutor for Chapter 3 of 'Programming Languages', asking layered questions and hints." \
  -c notes/pl/ch03/*.md \
  -c exercises/pl/ch03/*.md \
  --interactive \
  --stream jsonl
```
**Discussion**
Context from notes plus exercises lets the model pose targeted questions. Interactive mode means you can answer, refine, or request hints between iterations; JSONL streaming logs each tutor exchange for later review.
### Recipe: Feynman Method Worksheet
**Problem**
You want to explain a dense topic in simple language, identify gaps, then loop back with targeted readings.
**Solution**
```bash
prompt-maker-cli "Apply the Feynman technique to Raft consensus (explain like I'm new, spot gaps, suggest drills)." \
  -c notes/distributed-systems/raft.md \
  -c "src/consensus/raft/**/*.ts" \
  -c research/raft/open-questions.md \
  --polish \
  --context-template nvim
```
**Discussion**
Combining conceptual notes, actual code, and open questions yields a worksheet with four sections: Teach, Identify Gaps, Review, Simplify. Re-run after each study session to track how explanations improve over time.
## Git Commit Workflows
### Recipe: Conventional Commit Forges
**Problem**
You want consistent conventional commit messages derived from the staged diff plus nearby docs.
**Solution**
```bash
git diff --cached > /tmp/staged.patch && \
prompt-maker-cli "Write a conventional commit message with summary + body + testing notes." \
  -c /tmp/staged.patch \
  -c docs/CONTRIBUTING.md \
  --polish \
  --copy
```
**Discussion**
Export the staged diff to a temp file so `-c` can ingest it alongside your contributing guide. The polish pass enforces tone guidelines, while `--copy` lets you paste the final result directly into `git commit`.
### Recipe: Multi-Commit Release Notes
**Problem**
Youâre preparing a release branch and want a prompt that condenses the last N commits into user-facing notes plus internal TODOs.
**Solution**
```bash
git log -n 20 --pretty=medium > /tmp/release-log.txt && \
prompt-maker-cli "Summarize these commits into release highlights, breaking changes, and QA focus." \
  -c /tmp/release-log.txt \
  -c CHANGELOG.md \
  --context-template nvim \
  --json
```
**Discussion**
Feeding `git log` output plus the existing changelog ensures the model sees both history and format expectations. Capturing JSON output gives you a structured artifact you can commit or attach to release tickets.
## Editor Workflow Recipes
### Recipe: VS Code Task Runner
**Problem**
You want a one-click VS Code task that summarizes the currently open file plus related tests for rubber-ducking.
**Solution**
```bash
prompt-maker-cli "Explain the active module, its dependencies, and edge cases." \
  -c "${file}" \
  -c "${workspaceFolder}/src/**/*.spec.ts" \
  --smart-context-root ${workspaceFolder} \
  --context-template nvim
```
**Discussion**
Define this as a VS Code task with `type: shell` so `${file}` and `${workspaceFolder}` expand automatically. The smart-context scan pulls in nearby helpers while the template keeps the response readable inside VS Codeâs terminal panel.
### Recipe: JetBrains External Tool for Code Reviews
**Problem**
You need an IDE command (WebStorm, IntelliJ, etc.) that packages currently selected files and generates code-review talking points.
**Solution**
```bash
prompt-maker-cli "Prepare code review notes for the selected files, focusing on risks and tests." \
  -c "$FilePath$" \
  -c "$ContentRoot$/tests/**/*.ts" \
  --show-context \
  --polish
```
**Discussion**
Configure an External Tool that sends `$FilePath$` and `$ContentRoot$` placeholders. JetBrains pipes output to the Run tool window, so `--show-context` doubles as a sanity check before you paste the generated review notes into your PR.
### Recipe: Zed Editor Tasks for Pairing Sessions
**Problem**
Youâre hosting a remote pairing session in Zed and want quick, repeatable prompts capturing the current pane and design doc.
**Solution**
```bash
prompt-maker-cli "Act as a pairing partner; summarize this buffer and list open design questions." \
  -c "$ZED_FOCUSED_FILE" \
  -c docs/design/active/*.md \
  --progress=false \
  --stream jsonl
```
**Discussion**
Register a Zed Task that exports `ZED_FOCUSED_FILE`. Disabling the spinner keeps Zedâs task output tidy, while JSONL streaming lets you capture telemetry in a side panel or send it to collaborators via `websocat`.
## Jira Ticket Recipes
### Recipe: Convert Requirements to Gherkin
**Problem**
Product dropped a prose requirements blob into a Jira ticket; you need executable acceptance criteria in Cucumber/Gherkin format.
**Solution**
```bash
prompt-maker-cli "Rewrite these requirements as Gherkin acceptance criteria with Scenario/Scenario Outline blocks." \
  -c jira/REQ-582-description.md \
  -c jira/REQ-582-comments/*.md \
  --context-template nvim \
  --polish
```
**Discussion**
Export the Jira description/comments (many teams sync them via API into `jira/`). Feeding both files gives the model complete context. The template delivers a clean block ready for Jira Markdown, while the polish pass enforces consistent Given/When/Then phrasing.
### Recipe: Ticket Grooming Checklist
**Problem**
You want to confirm that high-priority tickets include personas, acceptance tests, and data considerations before sprint planning.
**Solution**
```bash
prompt-maker-cli "Audit this Jira ticket for grooming completeness (personas, data, test cases, open questions)." \
  -c jira/REQ-610-description.md \
  -c jira/REQ-610-attachments/*.md \
  --smart-context-root jira \
  --json
```
**Discussion**
Smart context pulls in related tickets or design notes under `jira/`. Emitting JSON lets you store the audit result back in Jira via automation (each run lists contextPaths so you can trace the evidence used for the checklist).
### Recipe: Regression Matrix for Linked Issues
**Problem**
Before closing a ticket with multiple linked bugs, you need a regression plan referencing the linked IDs and their components.
**Solution**
```bash
prompt-maker-cli "Create a regression checklist covering all linked Jira issues and their components." \
  -c jira/REQ-599-description.md \
  -c jira/links/REQ-599-linked-issues.md \
  -c docs/testing/regression-template.md \
  --copy \
  --context-file jira/REQ-599-regression.md
```
**Discussion**
Combining the ticket body, linked-issue export, and a regression template ensures the LLM maps each linked bug to concrete verification steps. Writing the output to `jira/REQ-599-regression.md` gives QA a canonical doc, while `--copy` makes it easy to paste into the Jira comment thread.
### Recipe: Automating via Jira Webhooks
**Problem**
You want Jira to trigger prompt-maker-cli automatically whenever a ticket changes state (e.g., when QA moves an issue to âReady for Testâ).
**Solution**
1. Configure a Jira webhook targeting an internal endpoint (e.g., a lightweight Node/Express or serverless function).
2. Inside the webhook handler, fetch the ticket body/attachments via Jira REST and write them to disk:
   ```bash
   curl -u "$JIRA_USER:$JIRA_TOKEN" \
     "$JIRA_BASE/rest/api/3/issue/$ISSUE_KEY?expand=renderedFields" \
     -o "/tmp/${ISSUE_KEY}.json"
   ```
3. Invoke prompt-maker-cli with the downloaded payload and any playbook files:
   ```bash
   prompt-maker-cli "Generate QA test matrix for ${ISSUE_KEY}" \
     -c "/tmp/${ISSUE_KEY}.json" \
     -c docs/testing/regression-template.md \
     --json \
     --context-file "jira/${ISSUE_KEY}-artifact.md"
   ```
4. Post the resulting artifact back to Jira (comment or attachment) using the JSON output.
**Discussion**
The webhook handler acts as glue: it hydrates the ticket data, runs the CLI headlessly, and records the artifact paths provided in the JSON payload. Because prompt-maker-cli streams progress, you can enable `--stream jsonl` for observability or push logs to your monitoring stack. Remember to guard concurrent runs with a queue so multiple Jira events donât clobber `/tmp` assets.
### Sample Node.js Webhook Handler
```ts
import express from 'express'
import { execFile } from 'node:child_process'
import { writeFile } from 'node:fs/promises'
import fetch from 'node-fetch'
const app = express()
app.use(express.json({ limit: '1mb' }))
app.post('/jira-webhook', async (req, res) => {
  const issueKey = req.body.issue?.key
  if (!issueKey) {
    return res.status(400).send('Missing issue key')
  }
  const jiraResponse = await fetch(
    `${process.env.JIRA_BASE}/rest/api/3/issue/${issueKey}?expand=renderedFields`,
    {
      headers: {
        Authorization: `Basic ${Buffer.from(`${process.env.JIRA_USER}:${process.env.JIRA_TOKEN}`).toString('base64')}`,
        Accept: 'application/json',
      },
    },
  )
  const payload = await jiraResponse.text()
  const inputPath = `/tmp/${issueKey}.json`
  await writeFile(inputPath, payload, 'utf8')
  execFile(
    'prompt-maker-cli',
    [
      `Generate QA test matrix for ${issueKey}`,
      '-c',
      inputPath,
      '-c',
      'docs/testing/regression-template.md',
      '--json',
      '--context-file',
      `jira/${issueKey}-artifact.md`,
    ],
    (error, stdout, stderr) => {
      if (error) {
        console.error(stderr)
      } else {
        console.log(stdout)
      }
    },
  )
  res.status(202).send('Processing')
})
app.listen(process.env.PORT ?? 3000, () => {
  console.log('Webhook listener ready')
})
```
**Highlights**
- Uses the Jira REST API to pull fresh ticket data.
- Writes the payload to `/tmp` and shells out to prompt-maker-cli.
- Streams CLI output to server logs; connect this to your observability stack or ship the JSON artifact back to Jira asynchronously.
## CI Integration Recipes
### Recipe: Pull Request Prompt Validator (Generic CI)
**Problem**
You want CI to fail fast when prompt-maker-cli canât assemble prompts for changed files (missing context, model issues, etc.).
**Solution**
Add a CI job that:
```bash
CHANGED=$(git diff --name-only origin/main...HEAD | tr '\n' ' ')
prompt-maker-cli "Summarize risk areas for this PR" \
  -c $CHANGED \
  --smart-context \
  --progress=false \
  --json
```
**Discussion**
Treat failures (non-zero exit codes) as CI blockers, surfacing actionable errors (missing files, credential issues) before reviewers even open the PR. Capturing JSON output allows downstream steps to upload artifacts or comment on the PR.
### Recipe: Nightly Requirements Drift Report
**Problem**
You run nightly jobs that compare Jira specs to actual code; when they drift, generate prompts instructing teams to reconcile differences.
**Solution**
```bash
prompt-maker-cli "Detect drift between Jira specs and implementation for ${SERVICE_NAME}" \
  -c "exports/jira/${SERVICE_NAME}-requirements.md" \
  -c "src/services/${SERVICE_NAME}/**/*.ts" \
  --context-template nvim \
  --stream jsonl \
  --context-file "reports/${SERVICE_NAME}-drift.md"
```
**Discussion**
Schedule this in your CI orchestrator (CircleCI, Buildkite, etc.). JSONL streaming feeds live telemetry into logs, while the context file serves as a paper trail for compliance/audit teams.
### Recipe: GitHub Actions Artifact Builder
**Problem**
You want a GitHub Actions workflow that generates prompts for every PR and attaches the result as an artifact/comment.
**Solution**
```yaml
name: prompt-maker
on:
  pull_request:
    types: [opened, synchronize]
jobs:
  build-prompt:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - uses: actions/setup-node@v4
        with:
          node-version: 20
      - run: npm ci
      - name: Generate PR prompt
        run: |
          CHANGED=$(git diff --name-only origin/${{ github.base_ref }}...HEAD)
          prompt-maker-cli "Review context for PR #${{ github.event.number }}" \
            -c "$CHANGED" \
            --smart-context \
            --json \
            --context-file pr-prompts/${{ github.event.number }}.md > prompt.json
      - uses: actions/upload-artifact@v4
        with:
          name: pr-${{ github.event.number }}-prompt
          path: |
            prompt.json
            pr-prompts/${{ github.event.number }}.md
```
**Discussion**
Actions runners install dependencies once (`npm ci`). The workflow diffs against the PR base branch, generates prompts, and uploads both the JSON payload and rendered markdown. Pair this with a follow-up step that comments on the PR using `gh api` if you want reviewers to see the artifact inline.
### Recipe: CircleCI Prompt Gate
**Problem**
You need CircleCI to enforce that prompt-maker-cli succeeds whenever a PR touches specific directories (e.g., `src/` or `packages/core/`).
**Solution**
```yaml
version: 2.1
orbs:
  node: circleci/node@6.2.0
jobs:
  prompt_gate:
    executor: node/default
    steps:
      - checkout
      - node/install-packages:
          pkg-manager: npm
      - run:
          name: Detect changed files
          command: |
            git fetch origin ${CIRCLE_BRANCH} --depth=1
            git diff --name-only origin/main...HEAD > changed.txt
      - run:
          name: Generate prompt artifact
          command: |
            CHANGED=$(tr '\n' ' ' < changed.txt)
            prompt-maker-cli "CircleCI prompt gate for ${CIRCLE_BRANCH}" \
              -c "$CHANGED" \
              --smart-context \
              --json \
              --context-file ci-artifacts/${CIRCLE_BUILD_NUM}.md > prompt.json
      - store_artifacts:
          path: ci-artifacts
      - store_artifacts:
          path: prompt.json
workflows:
  prompt-workflow:
    jobs:
      - prompt_gate
```
**Discussion**
This job runs on every pipeline. The stored artifacts (JSON + markdown) let reviewers inspect prompt contracts directly from the CircleCI UI. You can gate merges by requiring this workflow to pass.
### Recipe: CircleCI Nightly Prompt Publisher
**Problem**
You want a scheduled CircleCI workflow that enumerates services, runs prompt-maker-cli per service, and uploads the outputs for compliance.
**Solution**
```yaml
workflows:
  nightly-prompts:
    triggers:
      - schedule:
          cron: '0 3 * * *'
          filters:
            branches:
              only: main
    jobs:
      - prompt_gate
      - run_service_prompts
jobs:
  run_service_prompts:
    docker:
      - image: cimg/node:20.11
    steps:
      - checkout
      - run: npm ci
      - run:
          name: Generate prompts per service
          command: |
            for SERVICE in billing auth analytics; do
              prompt-maker-cli "Nightly drift check for $SERVICE" \
                -c "exports/jira/${SERVICE}-requirements.md" \
                -c "services/${SERVICE}/src/**/*.ts" \
                --context-file reports/${SERVICE}-drift.md \
                --json > reports/${SERVICE}-drift.json
            done
      - store_artifacts:
          path: reports
```
**Discussion**
Scheduled workflows keep documentation in sync. Each service run produces both markdown and JSON artifacts, giving compliance and platform engineers a searchable trail. Expand the `SERVICE` list or drive it from a manifest file as your platform grows.
## NeoVim Plugin Integration Recipes
### Recipe: Buffer Snapshot from a Plugin Command
**Problem**
You maintain a NeoVim plugin that exports the active buffer to a temp file and wants prompt-maker-cli to ingest it with surrounding context.
**Solution**
```bash
prompt-maker-cli "Review the attached buffer for race conditions and propose fixes." \
  -c "/tmp/nvim-buffer-*.md" \
  --context-template nvim \
  --copy
```
**Discussion**
Have your plugin write the current buffer to `/tmp/nvim-buffer-<id>.md`, then call the CLI via `vim.fn.jobstart`. Using the `nvim` template means the returned prompt is already formatted for a scratch buffer.
### Recipe: Interactive Refinement via Remote Transport
**Problem**
You want the plugin to send refinement commands without leaving NeoVim.
**Solution**
```bash
prompt-maker-cli "Draft a refactor plan for the active file." \
  -c "/tmp/nvim-buffer-current.ts" \
  --interactive-transport /tmp/prompt-maker.sock \
  --stream jsonl
```
**Discussion**
The plugin listens for `interactive.awaiting` events from the JSONL stream and surfaces prompts inside NeoVim. Users type refinements, and the plugin pushes `{"type":"refine","instruction":"..."}` messages through the Unix socket.
### Recipe: Project-Wide Summaries from Telescope Picks
**Problem**
You use Telescope to select files and want to pass all selections as context without manual globs.
**Solution**
```bash
prompt-maker-cli "Summarize the selected files for code review notes." \
  -c "/tmp/nvim-selected-files/*.md" \
  --smart-context-root $(pwd) \
  --context-template nvim \
  --progress=false
```
**Discussion**
The plugin writes each Telescope selection to `/tmp/nvim-selected-files/`. Adding `--smart-context-root` brings in nearby matches, while `--progress=false` keeps Neovimâs command output clean during background runs.
## Recipes
### Recipe: Advanced Context Selection
**Problem**
You need every TypeScript file under `src/`, but none of the tests or stories should pollute the prompt.
**Solution**
```bash
prompt-maker-cli "Document the shared data loader contract" \
  -c "src/**/*.ts" \
  -c "!src/**/*.test.ts" \
  -c "!src/**/*.spec.ts" \
  -c "!src/**/__tests__/**" \
  --show-context --context-format json
```
**Discussion**
`fast-glob` honors negated patterns, so you can stack `!` excludes to prune tests. `--show-context --context-format json` prints the resolved files (path + content) to stderr/stdout so you can verify exactly what the LLM sees before generating.
---
### Recipe: Image Enhancement with âNano Bananaâ
**Problem**
You want to attach a marketing mockup and have your AI partnerâcode-named **Nano Banana**âdescribe improvements for accessibility and polish.
**Solution**
```bash
prompt-maker-cli "Nano Banana, critique and enhance the attached hero mockup for accessibility and contrast." \
  --image assets/hero-v2.png \
  --polish \
  --model gpt-4o-mini
```
**Discussion**
`--image` accepts repeatable paths, so drop in multiple angles if needed. Mention Nano Banana directly in the intent to anchor the persona. Adding `--polish` runs the meta-refinement pass, giving you a crisp, well-structured instruction set tailored to GPT-4oâs multimodal strengths.
---
### Recipe: Shopping Assistant Prompt
**Problem**
You need a prompt that tells an LLM to comb Amazon for a specific brand/price window and report recommended products.
**Solution**
```bash
prompt-maker-cli "Research Amazon listings for Breville espresso machines under $900 and surface top 3 matches with pros/cons, freshness check, and price volatility notes." \
  --context-template nvim \
  --copy
```
**Discussion**
Here the intent fully encodes the search criteria, and `--context-template nvim` wraps the response in a scratch-buffer-friendly format so you can paste it into Neovim and keep iterating. `--copy` places the final prompt on your clipboard for immediate use in your preferred chat client.
---
### Recipe: Engineering / CAD Generation
**Problem**
You must solicit OpenSCAD or Python (CadQuery) code that produces a printable enclosure, combining local design guidelines as context.
**Solution**
```bash
prompt-maker-cli "Produce OpenSCAD or CadQuery code for a snap-fit Raspberry Pi 5 enclosure with filleted edges and removable lid." \
  -c "docs/cad/clearance-table.md" \
  -c "docs/cad/materials/*.md" \
  --smart-context --smart-context-root ./hardware \
  --interactive
```
**Discussion**
Static globs inject canonical clearance/material tables, while `--smart-context` surfaces the five most relevant hardware notes under `./hardware`. Kick on `--interactive` to iterate: after each draft, feed refinements like âincrease wall thickness to 2.2âŻmmâ without rebuilding the command.
---
### Recipe: Genealogical Research Plan
**Problem**
You need a structured research strategy for a specific ancestor, weaving in source notes stored locally.
**Solution**
```bash
prompt-maker-cli "Draft a genealogical research plan for Mary O'Hara (b. 1884, County Mayo â Boston 1906)." \
  -c "research/mayo-family/*.md" \
  --smart-context-root research \
  --context-file mary-ohara-plan.md \
  --context-format text \
  --json
```
**Discussion**
The CLI resolves explicit notes plus smart-context matches from the broader `research` directory, then writes the merged context to `mary-ohara-plan.md` for archival. `--json` emits the final prompt payload (intent, context paths, iterations) so you can log runs programmaticallyâremember this disables interactive mode.
---
### Recipe: Martial Arts Video Analysis
**Problem**
You captured a sparring session and need a prompt that asks the model to analyze timing, guard discipline, and footwork.
**Solution**
```bash
prompt-maker-cli "Break down this kali sparring clipâfocus on timing windows, guard recovery, and footwork corrections." \
  --video media/kali-round3.mp4 \
  --model gemini-3-pro-preview \
  --polish \
  --progress=false
```
**Discussion**
Passing `--video` causes `src/generate/pipeline.ts` to call `resolveGeminiVideoModel()`, overriding non-Gemini choices with your configured Gemini video model (commonly `gemini-3-pro-preview`) so the Files API can ingest your clip. The upload path (`media-loader.ts`) demands a readable file and `GEMINI_API_KEY`; the CLI shows upload progress via `upload.state` events. Geminiâs multimodal context pairs well with a polish pass to distill the final coaching checklist.
---
</file>
<file path="src/__tests__/useGenerationPipeline.test.ts">
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import { useGenerationPipeline } from '../tui/hooks/useGenerationPipeline'
jest.mock('wrap-ansi', () => jest.fn((text: string) => text))
jest.mock('../tui/provider-status', () => ({
  checkModelProviderStatus: jest.fn(),
}))
jest.mock('../generate-command', () => ({
  runGeneratePipeline: jest.fn().mockResolvedValue({
    finalPrompt: 'Prompt',
    model: 'gpt-4o-mini',
    iterations: 1,
    telemetry: null,
    payload: {},
  }),
  maybeCopyToClipboard: jest.fn(),
  maybeOpenChatGpt: jest.fn(),
}))
jest.mock('../prompt-generator-service', () => ({
  generatePromptSeries: jest.fn().mockResolvedValue({
    reasoning: 'r',
    overviewPrompt: '# Overview',
    atomicPrompts: [{ title: 'Step', content: 'Do a thing\n\nValidation: check' }],
  }),
  isGemini: jest.fn((model: string) => model.startsWith('gemini')),
}))
jest.mock('../generate/models', () => {
  const actual = jest.requireActual('../generate/models') as typeof import('../generate/models')
  return {
    ...actual,
    resolveGeminiVideoModel: jest.fn().mockResolvedValue('gemini-2.5-pro'),
  }
})
jest.mock('../file-context', () => ({ resolveFileContext: jest.fn().mockResolvedValue([]) }))
jest.mock('../url-context', () => ({ resolveUrlContext: jest.fn().mockResolvedValue([]) }))
jest.mock('../smart-context-service', () => ({
  resolveSmartContextFiles: jest.fn().mockResolvedValue([]),
}))
jest.mock('node:fs/promises', () => ({ mkdir: jest.fn(), writeFile: jest.fn() }))
const providerStatusModule = jest.requireMock('../tui/provider-status') as {
  checkModelProviderStatus: jest.Mock
}
const generateCommandModule = jest.requireMock('../generate-command') as {
  runGeneratePipeline: jest.Mock
}
const promptGeneratorModule = jest.requireMock('../prompt-generator-service') as {
  generatePromptSeries: jest.Mock
}
const fileContextModule = jest.requireMock('../file-context') as {
  resolveFileContext: jest.Mock
}
const urlContextModule = jest.requireMock('../url-context') as {
  resolveUrlContext: jest.Mock
}
const smartContextModule = jest.requireMock('../smart-context-service') as {
  resolveSmartContextFiles: jest.Mock
}
const fsPromisesModule = jest.requireMock('node:fs/promises') as {
  mkdir: jest.Mock
  writeFile: jest.Mock
}
const dom = new JSDOM('<!doctype html><html><body></body></html>')
const globalEnv = globalThis as typeof globalThis & {
  window: Window & typeof globalThis
  document: Document
  navigator: Navigator
}
globalEnv.window = dom.window as typeof globalEnv.window
globalEnv.document = dom.window.document as Document
globalEnv.navigator = dom.window.navigator
describe('useGenerationPipeline', () => {
  const baseOptions = {
    files: [] as string[],
    urls: [] as string[],
    images: [] as string[],
    videos: [] as string[],
    pdfs: [] as string[],
    smartContextEnabled: false,
    smartContextRoot: null,
    metaInstructions: '',
    budgets: {
      maxContextTokens: null,
      maxInputTokens: null,
      contextOverflowStrategy: null,
    },
    interactiveTransportPath: undefined as string | undefined,
    terminalColumns: 80,
    polishModelId: null,
    jsonOutputEnabled: false,
    copyEnabled: false,
    chatGptEnabled: false,
    isTestCommandRunning: false,
    notify: jest.fn(),
  }
  beforeEach(() => {
    jest.clearAllMocks()
    generateCommandModule.runGeneratePipeline.mockResolvedValue({
      finalPrompt: 'Prompt',
      model: 'gpt-4o-mini',
      iterations: 1,
      telemetry: null,
      payload: {},
    })
    promptGeneratorModule.generatePromptSeries.mockResolvedValue({
      reasoning: 'r',
      overviewPrompt: '# Overview',
      atomicPrompts: [{ title: 'Step', content: 'Do a thing\n\nValidation: check' }],
    })
    fileContextModule.resolveFileContext.mockResolvedValue([])
    urlContextModule.resolveUrlContext.mockResolvedValue([])
    smartContextModule.resolveSmartContextFiles.mockResolvedValue([])
    fsPromisesModule.mkdir.mockResolvedValue(undefined)
    fsPromisesModule.writeFile.mockResolvedValue(undefined)
  })
  it('aborts runGeneration when provider credentials are missing', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'missing',
      message: 'OPENAI_API_KEY missing',
    })
    const pushHistory = jest.fn()
    const onProviderStatusUpdate = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
        onProviderStatusUpdate,
      }),
    )
    await act(async () => {
      await result.current.runGeneration({ intent: 'Do a thing' })
    })
    expect(pushHistory).toHaveBeenCalledWith(
      'Generation aborted: OpenAI unavailable (OPENAI_API_KEY missing).',
      'system',
    )
    expect(generateCommandModule.runGeneratePipeline).not.toHaveBeenCalled()
    expect(onProviderStatusUpdate).toHaveBeenCalledWith(
      expect.objectContaining({ provider: 'openai', status: 'missing' }),
    )
  })
  it('blocks runGeneration when intent is empty and resume is missing', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
      }),
    )
    await act(async () => {
      await result.current.runGeneration({})
    })
    expect(generateCommandModule.runGeneratePipeline).not.toHaveBeenCalled()
    expect(pushHistory).toHaveBeenCalledWith(
      'No intent provided. Enter text or set an intent file.',
      'system',
    )
  })
  it('runs generation when provider check passes', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    const onLastGeneratedPromptUpdate = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
        onLastGeneratedPromptUpdate,
      }),
    )
    await act(async () => {
      await result.current.runGeneration({ intent: 'Ship it' })
    })
    expect(generateCommandModule.runGeneratePipeline).toHaveBeenCalled()
    expect(pushHistory).toHaveBeenCalledWith('Prompt', 'system', 'markdown')
    expect(onLastGeneratedPromptUpdate).toHaveBeenCalledWith('Prompt')
  })
  it('passes resume selector through and allows empty intent', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
        files: ['src/index.ts'],
        urls: ['https://example.com'],
        smartContextEnabled: true,
      }),
    )
    await act(async () => {
      await result.current.runGeneration({
        resume: { kind: 'history', selector: 'last', mode: 'best-effort' },
      })
    })
    const args = generateCommandModule.runGeneratePipeline.mock.calls[0]?.[0] as unknown
    expect(args).toEqual(
      expect.objectContaining({
        resume: 'last',
        resumeMode: 'best-effort',
        context: [],
        urls: [],
        smartContext: false,
      }),
    )
    if (typeof args === 'object' && args !== null && 'intent' in args) {
      expect((args as { intent?: unknown }).intent).toBeUndefined()
    }
  })
  it('passes selected polish model into generation args', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
        polishModelId: 'gpt-4o',
      }),
    )
    await act(async () => {
      await result.current.runGeneration({ intent: 'Ship it' })
    })
    const args = generateCommandModule.runGeneratePipeline.mock.calls[0]?.[0] as unknown
    expect(args).toEqual(expect.objectContaining({ polish: true, polishModel: 'gpt-4o' }))
    expect(result.current.statusChips).toEqual(expect.arrayContaining(['[polish:gpt-4o]']))
  })
  it('updates status chips with token telemetry', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
      }),
    )
    await act(async () => {
      await result.current.runGeneration({ intent: 'Ship it' })
    })
    const optionsArg = generateCommandModule.runGeneratePipeline.mock.calls[0]?.[1] as unknown
    const onStreamEvent =
      typeof optionsArg === 'object' &&
      optionsArg !== null &&
      'onStreamEvent' in optionsArg &&
      typeof (optionsArg as { onStreamEvent?: unknown }).onStreamEvent === 'function'
        ? ((optionsArg as { onStreamEvent?: unknown }).onStreamEvent as (event: unknown) => void)
        : null
    expect(onStreamEvent).not.toBeNull()
    await act(async () => {
      onStreamEvent?.({
        event: 'context.telemetry',
        telemetry: {
          files: [],
          intentTokens: 200,
          fileTokens: 300,
          systemTokens: 700,
          totalTokens: 1200,
        },
      })
    })
    expect(result.current.statusChips).toEqual(expect.arrayContaining(['[tokens:1.2k]']))
  })
  it('surfaces upload state via toasts instead of history', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    const notify = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        notify,
        currentModel: 'gpt-4o-mini',
      }),
    )
    await act(async () => {
      await result.current.runGeneration({ intent: 'Ship it' })
    })
    const optionsArg = generateCommandModule.runGeneratePipeline.mock.calls[0]?.[1] as unknown
    const onStreamEvent =
      typeof optionsArg === 'object' &&
      optionsArg !== null &&
      'onStreamEvent' in optionsArg &&
      typeof (optionsArg as { onStreamEvent?: unknown }).onStreamEvent === 'function'
        ? ((optionsArg as { onStreamEvent?: unknown }).onStreamEvent as (event: unknown) => void)
        : null
    expect(onStreamEvent).not.toBeNull()
    await act(async () => {
      onStreamEvent?.({
        event: 'upload.state',
        state: 'start',
        detail: { kind: 'image', filePath: '/tmp/image.png' },
      })
    })
    expect(notify).toHaveBeenCalledWith('Uploading image: /tmp/image.png', expect.any(Object))
    const uploadHistoryCalls = pushHistory.mock.calls.filter(
      (call) => typeof call[0] === 'string' && call[0].includes('Uploading image:'),
    )
    expect(uploadHistoryCalls).toHaveLength(0)
  })
  it('surfaces transport waiting state in status', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    let resolvePipeline: ((value: unknown) => void) | null = null
    generateCommandModule.runGeneratePipeline.mockImplementation(
      (_args: unknown, _options: unknown) => {
        return new Promise((resolve) => {
          resolvePipeline = resolve
        })
      },
    )
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
        interactiveTransportPath: '/tmp/socket',
      }),
    )
    let runPromise: Promise<void> | null = null
    act(() => {
      runPromise = result.current.runGeneration({ intent: 'Ship it' })
    })
    await act(async () => {
      await Promise.resolve()
    })
    const optionsArg = generateCommandModule.runGeneratePipeline.mock.calls[0]?.[1] as unknown
    const onStreamEvent =
      typeof optionsArg === 'object' &&
      optionsArg !== null &&
      'onStreamEvent' in optionsArg &&
      typeof (optionsArg as { onStreamEvent?: unknown }).onStreamEvent === 'function'
        ? ((optionsArg as { onStreamEvent?: unknown }).onStreamEvent as (event: unknown) => void)
        : null
    expect(onStreamEvent).not.toBeNull()
    await act(async () => {
      onStreamEvent?.({ event: 'interactive.awaiting', mode: 'transport' })
    })
    expect(result.current.awaitingInteractiveMode).toBe('transport')
    expect(result.current.statusMessage).toBe('Waiting for interactive transport inputâŚ')
    expect(pushHistory).toHaveBeenCalledWith(
      'Waiting for interactive transport inputâŚ',
      'progress',
      undefined,
    )
    expect(pushHistory).toHaveBeenCalledWith(
      'Tip: connect a client and send refine/finish to continue.',
      'system',
      undefined,
    )
    await act(async () => {
      resolvePipeline?.({
        finalPrompt: 'Prompt',
        model: 'gpt-4o-mini',
        iterations: 1,
        telemetry: null,
        payload: {},
      })
      await runPromise
    })
  })
  it('sets the status chip while tests run', () => {
    jest.useFakeTimers()
    const pushHistory = jest.fn()
    const { result, rerender } = renderHook(
      ({ isTestCommandRunning }: { isTestCommandRunning: boolean }) =>
        useGenerationPipeline({
          ...baseOptions,
          pushHistory,
          currentModel: 'gpt-4o-mini',
          isTestCommandRunning,
        }),
      { initialProps: { isTestCommandRunning: false } },
    )
    expect(result.current.statusChips[0]).toBe('[status:Idle]')
    rerender({ isTestCommandRunning: true })
    const firstFrame = result.current.statusChips[0]
    expect(firstFrame).toBe('[status:Running tests]')
    act(() => {
      jest.advanceTimersByTime(240)
    })
    const secondFrame = result.current.statusChips[0]
    expect(secondFrame).toBe(firstFrame)
    rerender({ isTestCommandRunning: false })
    expect(result.current.statusChips[0]).toBe('[status:Idle]')
    jest.useRealTimers()
  })
  it('passes meta instructions to the generation pipeline', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
        metaInstructions: 'Be concise',
      }),
    )
    await act(async () => {
      await result.current.runGeneration({ intent: 'Add feature' })
    })
    expect(generateCommandModule.runGeneratePipeline).toHaveBeenCalledWith(
      expect.objectContaining({ metaInstructions: 'Be concise' }),
      expect.any(Object),
    )
  })
  it('passes image/video paths to the generation pipeline', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
        images: ['diagram.png'],
        videos: ['clip.mp4'],
      }),
    )
    await act(async () => {
      await result.current.runGeneration({ intent: 'Ship it' })
    })
    expect(generateCommandModule.runGeneratePipeline).toHaveBeenCalledWith(
      expect.objectContaining({ images: ['diagram.png'], video: ['clip.mp4'] }),
      expect.any(Object),
    )
  })
  it('aborts series generation when provider is unavailable', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'gemini',
      status: 'missing',
      message: 'GEMINI_API_KEY missing',
    })
    const pushHistory = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
        videos: ['clip.mp4'],
      }),
    )
    await act(async () => {
      await result.current.runSeriesGeneration('Plan work')
    })
    expect(promptGeneratorModule.generatePromptSeries).not.toHaveBeenCalled()
    expect(pushHistory).toHaveBeenCalledWith(
      'Generation aborted: Gemini unavailable (GEMINI_API_KEY missing).',
      'system',
    )
  })
  it('surfaces series validation failures in history output', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    promptGeneratorModule.generatePromptSeries.mockRejectedValueOnce(
      new Error(
        'Atomic prompt 1 contains forbidden cross-reference phrase "from step N". Atomic prompts must be standalone.',
      ),
    )
    const pushHistory = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
      }),
    )
    await act(async () => {
      await result.current.runSeriesGeneration('Plan work')
    })
    expect(pushHistory).toHaveBeenCalledWith('[series] Starting series generationâŚ', 'progress')
    expect(pushHistory).toHaveBeenCalledWith(
      expect.stringContaining(
        '[series] Failed: Atomic prompt 1 contains forbidden cross-reference phrase',
      ),
      'progress',
    )
  })
})
</file>
<file path="src/generate/pipeline.ts">
import fs from 'node:fs/promises'
import { stdin as input, stdout as output } from 'node:process'
import chalk from 'chalk'
import { loadCliConfig } from '../config'
import { resolveFileContext, type FileContext } from '../file-context'
import { appendToHistory, resolveHistoryFilePath } from '../history-logger'
import { resolveSmartContextFiles } from '../smart-context-service'
import type { ResolveUrlContextOptions } from '../url-context'
import { resolveUrlContext } from '../url-context'
import {
  createPromptGeneratorService,
  isGemini,
  resolveDefaultGenerateModel,
} from '../prompt-generator-service'
import { maybeCopyToClipboard, maybeOpenChatGpt } from './actions'
import { resolveContextTemplate, renderContextTemplate } from './context-templates'
import { evaluateContextBudget, type ContextEntry, type ContextEntrySource } from './context-budget'
import { displayContextFiles, writeContextFile } from './context-output'
import { displayContextTemplatePrompt, displayPolishedPrompt } from './display'
import { shouldTraceFlags } from './debug'
import { runGenerationWorkflow } from './interactive'
import { InteractiveTransport } from './interactive-transport'
import { resolveIntent } from './intent'
import { resolveGeminiVideoModel, resolveTargetModel } from './models'
import { loadGeneratePayloadFromFile } from './payload-io'
import { polishPrompt } from './polish'
import { createUploadStateTracker, startProgress, type ProgressHandle } from './progress'
import { createStreamDispatcher, type StreamDispatcher } from './stream'
import { buildTokenTelemetry } from './token-telemetry'
import {
  GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
  type ContextPathMetadata,
  type GenerateArgs,
  type GenerateJsonPayload,
  type GeneratePipelineOptions,
  type GeneratePipelineResult,
  type InteractiveMode,
  type ProgressScope,
  type ResumeMode,
  type StreamEventInput,
} from './types'
type ResumePayload = GenerateJsonPayload & {
  metaInstructions?: string
}
type ResumeLoadResult = {
  payload: ResumePayload
  source: 'history' | 'file'
}
type HistorySelector = {
  fromEnd: number
  label: string
}
type ResumeContextResult = {
  fileContext: FileContext[]
  reusedContextPaths: GenerateJsonPayload['contextPaths']
  missingContextPaths: GenerateJsonPayload['contextPaths']
}
const parseHistorySelector = (selector: string): HistorySelector => {
  const trimmed = selector.trim()
  if (trimmed === 'last') {
    return { fromEnd: 1, label: 'last' }
  }
  const lastMatch = trimmed.match(/^last:(\d+)$/)
  if (lastMatch) {
    return { fromEnd: Number(lastMatch[1]), label: trimmed }
  }
  const numericMatch = trimmed.match(/^(\d+)$/)
  if (numericMatch) {
    return { fromEnd: Number(numericMatch[1]), label: trimmed }
  }
  throw new Error(
    `Invalid resume selector "${selector}". Use "last", "last:N", or "N" (N-th from end).`,
  )
}
const isRecord = (value: unknown): value is Record<string, unknown> =>
  typeof value === 'object' && value !== null && !Array.isArray(value)
const isStringArray = (value: unknown): value is string[] =>
  Array.isArray(value) && value.every((entry) => typeof entry === 'string')
const isContextPaths = (value: unknown): value is GenerateJsonPayload['contextPaths'] =>
  Array.isArray(value) &&
  value.every(
    (entry) =>
      isRecord(entry) && typeof entry.path === 'string' && typeof entry.source === 'string',
  )
const isGenerateJsonPayload = (value: unknown): value is ResumePayload => {
  if (!isRecord(value)) {
    return false
  }
  if (value.schemaVersion !== GENERATE_JSON_PAYLOAD_SCHEMA_VERSION) {
    return false
  }
  return (
    typeof value.intent === 'string' &&
    typeof value.model === 'string' &&
    typeof value.targetModel === 'string' &&
    typeof value.prompt === 'string' &&
    isStringArray(value.refinements) &&
    typeof value.iterations === 'number' &&
    Number.isFinite(value.iterations) &&
    typeof value.interactive === 'boolean' &&
    typeof value.timestamp === 'string' &&
    isContextPaths(value.contextPaths)
  )
}
const selectFromEnd = <T>(entries: T[], fromEnd: number): T => {
  const index = entries.length - fromEnd
  if (index < 0 || index >= entries.length) {
    const noun = entries.length === 1 ? 'entry' : 'entries'
    throw new Error(
      `History selector is out of range. Requested ${fromEnd} from end but only ${entries.length} ${noun} available.`,
    )
  }
  const selected = entries[index]
  if (!selected) {
    throw new Error('Invariant violation: selected history entry is missing.')
  }
  return selected
}
const readJsonlPayloads = async (filePath: string): Promise<ResumePayload[]> => {
  let raw: string
  try {
    raw = await fs.readFile(filePath, 'utf8')
  } catch (error) {
    const code =
      typeof error === 'object' &&
      error !== null &&
      'code' in error &&
      typeof (error as { code?: unknown }).code === 'string'
        ? (error as { code: string }).code
        : null
    if (code === 'ENOENT') {
      throw new Error(
        `History file not found at ${filePath}. Run a generate command first to create it.`,
      )
    }
    const message = error instanceof Error ? error.message : 'Unknown file error.'
    throw new Error(`Failed to read history file ${filePath}: ${message}`)
  }
  if (raw.trim().length === 0) {
    throw new Error(`History file ${filePath} is empty.`)
  }
  const entries: ResumePayload[] = []
  raw
    .split('\n')
    .map((line) => line.trim())
    .filter((line) => line.length > 0)
    .forEach((line) => {
      try {
        const parsed = JSON.parse(line) as unknown
        if (isGenerateJsonPayload(parsed)) {
          entries.push(parsed)
        }
      } catch {
        // ignore invalid json
      }
    })
  if (entries.length === 0) {
    throw new Error(`No valid generate payloads found in history file ${filePath}.`)
  }
  return entries
}
const loadResumePayload = async (args: GenerateArgs): Promise<ResumeLoadResult | null> => {
  if (!args.resume && !args.resumeLast && !args.resumeFrom) {
    return null
  }
  if (args.resumeFrom) {
    const filePath = args.resumeFrom
    const payload = await loadGeneratePayloadFromFile(filePath)
    return { payload, source: 'file' }
  }
  const selector = args.resumeLast ? 'last' : (args.resume ?? 'last')
  const parsedSelector = parseHistorySelector(selector)
  const historyPath = resolveHistoryFilePath()
  const entries = await readJsonlPayloads(historyPath)
  const payload = selectFromEnd(entries, parsedSelector.fromEnd)
  return { payload, source: 'history' }
}
const resolveResumeContext = async (
  payload: ResumePayload,
  mode: ResumeMode,
): Promise<ResumeContextResult> => {
  const reused: GenerateJsonPayload['contextPaths'] = []
  const missing: GenerateJsonPayload['contextPaths'] = []
  const fileContext: FileContext[] = []
  const contextCandidates = payload.contextPaths.filter((entry) => entry.source !== 'intent')
  for (const entry of contextCandidates) {
    if (entry.source !== 'file') {
      missing.push(entry)
      continue
    }
    try {
      const content = await fs.readFile(entry.path, 'utf8')
      fileContext.push({ path: entry.path, content })
      reused.push(entry)
    } catch {
      missing.push(entry)
    }
  }
  if (missing.length > 0 && mode === 'best-effort') {
    const fileMissing = missing.filter((entry) => entry.source === 'file')
    if (fileMissing.length > 0) {
      const paths = fileMissing.map((entry) => entry.path).join(', ')
      console.warn(chalk.yellow(`Resume skipped missing context file(s): ${paths}`))
    }
  }
  return { fileContext, reusedContextPaths: reused, missingContextPaths: missing }
}
const isMissingIntentError = (error: unknown): boolean =>
  error instanceof Error && error.message.startsWith('Intent text is required.')
const logFlagSnapshot = (args: GenerateArgs): void => {
  if (!shouldTraceFlags()) {
    return
  }
  const snapshot = {
    copy: args.copy,
    polish: args.polish,
    openChatGpt: args.openChatGpt,
    json: args.json,
    quiet: args.quiet,
    stream: args.stream,
    interactive: args.interactive,
    showContext: args.showContext,
    contextTemplate: args.contextTemplate ?? null,
    contextFile: args.contextFile ?? null,
    smartContext: args.smartContext,
  }
  console.error(chalk.dim('[pmc:flags]'), JSON.stringify(snapshot, null, 2))
}
type ContextPathSource = ContextPathMetadata['source']
type TransportCleanupHandler = {
  event: NodeJS.Signals | 'exit'
  handler: () => void
}
export const runGeneratePipeline = async (
  args: GenerateArgs,
  options: GeneratePipelineOptions = {},
): Promise<GeneratePipelineResult> => {
  logFlagSnapshot(args)
  const interactiveTransportPath = args.interactiveTransport?.trim()
  if (args.interactiveTransport && !interactiveTransportPath) {
    throw new Error('--interactive-transport requires a non-empty path.')
  }
  const wantsInteractiveSession = args.interactive || Boolean(interactiveTransportPath)
  if (args.json && wantsInteractiveSession) {
    throw new Error('--json cannot be combined with --interactive.')
  }
  const contextTemplateName = args.contextTemplate?.trim()
  if (args.contextTemplate && !contextTemplateName) {
    throw new Error('--context-template requires a non-empty template name.')
  }
  const transportCleanupHandlers: TransportCleanupHandler[] = []
  const interactiveTransport = interactiveTransportPath
    ? new InteractiveTransport(interactiveTransportPath)
    : null
  try {
    const resume = await loadResumePayload(args)
    let intent: string
    let intentMetadataPath: string
    if (resume) {
      // Precedence: if the user provides a new intent (inline, --intent-file, or stdin), it wins.
      // Otherwise, fall back to the resumed payload intent.
      try {
        intent = await resolveIntent(args)
        intentMetadataPath = args.intentFile
          ? args.intentFile
          : args.intent?.trim()
            ? 'inline-intent'
            : 'stdin-intent'
      } catch (error) {
        if (!isMissingIntentError(error)) {
          throw error
        }
        intent = resume.payload.intent
        intentMetadataPath = resume.source === 'history' ? 'history-intent' : 'resume-file-intent'
      }
    } else {
      intent = await resolveIntent(args)
      intentMetadataPath = args.intentFile
        ? args.intentFile
        : args.intent?.trim()
          ? 'inline-intent'
          : 'stdin-intent'
    }
    const cliConfig = await loadCliConfig()
    let contextPaths: ContextPathMetadata[] = []
    const recordContextPaths = (entries: FileContext[], source: ContextPathSource): void => {
      entries.forEach((entry) => {
        contextPaths.push({ path: entry.path, source })
      })
    }
    const pruneContextPaths = (
      paths: ContextPathMetadata[],
      dropped: ContextPathMetadata[],
    ): ContextPathMetadata[] => {
      if (dropped.length === 0) {
        return paths
      }
      const remainingDrops = new Map<string, number>()
      dropped.forEach((entry) => {
        const key = `${entry.source}:${entry.path}`
        remainingDrops.set(key, (remainingDrops.get(key) ?? 0) + 1)
      })
      const next: ContextPathMetadata[] = []
      paths.forEach((entry) => {
        const key = `${entry.source}:${entry.path}`
        const remaining = remainingDrops.get(key)
        if (remaining !== undefined) {
          if (remaining <= 1) {
            remainingDrops.delete(key)
          } else {
            remainingDrops.set(key, remaining - 1)
          }
          return
        }
        next.push(entry)
      })
      return next
    }
    contextPaths.push({ path: intentMetadataPath, source: 'intent' })
    const resumeMode = args.resumeMode ?? 'best-effort'
    let resumeContext: ResumeContextResult | null = null
    let fileContext: FileContext[]
    // Precedence: explicit `--context` file paths override resumed context paths.
    // When no explicit context is provided, we attempt to reuse contextPaths from the resume payload
    // (and enforce `--resume-mode` for missing files).
    if (args.context.length > 0 || !resume) {
      fileContext = await resolveFileContext(args.context)
      recordContextPaths(fileContext, 'file')
    } else {
      resumeContext = await resolveResumeContext(resume.payload, resumeMode)
      fileContext = resumeContext.fileContext
      recordContextPaths(fileContext, 'file')
    }
    let contextEntrySources: ContextEntrySource[] = fileContext.map(() => 'file')
    const service = await createPromptGeneratorService()
    const defaultGenerateModel = await resolveDefaultGenerateModel()
    // Precedence: explicit CLI flags override resumed payload values, which override defaults.
    let model = args.model ?? resume?.payload.model ?? defaultGenerateModel
    const targetModel = await resolveTargetModel({
      defaultTargetModel: resume?.payload.targetModel ?? defaultGenerateModel,
      ...(args.target !== undefined ? { explicitTarget: args.target } : {}),
    })
    if (args.video.length > 0 && !isGemini(model)) {
      const previousModel = model
      model = await resolveGeminiVideoModel()
      if (model !== previousModel) {
        console.warn(`Switching to ${model} to support video input.`)
      }
    }
    const contextTemplateDefinition = contextTemplateName
      ? await resolveContextTemplate(contextTemplateName)
      : null
    const refinements: string[] = resume ? [...resume.payload.refinements] : []
    // Precedence: explicit CLI meta instructions override resumed meta instructions.
    const trimmedMetaInstructions = (
      args.metaInstructions ??
      resume?.payload.metaInstructions ??
      ''
    ).trim()
    const streamDispatcher = createStreamDispatcher(args.stream, {
      ...(interactiveTransport ? { taps: [interactiveTransport.getEventWriter()] } : {}),
    })
    const emitEvent = (event: StreamEventInput): void => {
      if (options.onStreamEvent) {
        try {
          options.onStreamEvent(event)
        } catch {
          // ignore listener errors to avoid breaking pipeline
        }
      }
      streamDispatcher.emit(event)
    }
    const streamProxy: StreamDispatcher = {
      mode: streamDispatcher.mode,
      emit: emitEvent,
    }
    if (
      resumeContext &&
      (resumeContext.reusedContextPaths.length > 0 || resumeContext.missingContextPaths.length > 0)
    ) {
      emitEvent({
        event: 'resume.loaded',
        source: resume?.source ?? 'history',
        reusedContextPaths: resumeContext.reusedContextPaths,
        missingContextPaths: resumeContext.missingContextPaths,
      })
      if (
        resumeMode === 'strict' &&
        resumeContext.missingContextPaths.some((entry) => entry.source === 'file')
      ) {
        const missingFiles = resumeContext.missingContextPaths
          .filter((entry) => entry.source === 'file')
          .map((entry) => entry.path)
          .join(', ')
        throw new Error(`Missing required resumed context file(s): ${missingFiles}`)
      }
    }
    interactiveTransport?.setEventEmitter((event) => {
      emitEvent(event)
    })
    if (interactiveTransport) {
      await interactiveTransport.start()
      const signals: Array<NodeJS.Signals | 'exit'> = ['SIGINT', 'SIGTERM', 'exit']
      signals.forEach((signal) => {
        const handler = (): void => {
          void interactiveTransport.stop()
        }
        process.once(signal, handler)
        transportCleanupHandlers.push({ event: signal, handler })
      })
    }
    const interactiveMode: InteractiveMode = interactiveTransport
      ? 'transport'
      : args.interactive && input.isTTY && output.isTTY
        ? 'tty'
        : 'none'
    if (args.interactive && interactiveMode === 'none') {
      console.warn(
        'Interactive mode requested but no TTY detected; continuing non-interactive run.',
      )
    }
    const uiSuppressed = args.quiet || streamDispatcher.mode !== 'none'
    const shouldDisplay = !args.json && !args.quiet
    const progressSpinnersEnabled = args.progress && interactiveMode === 'none'
    const startSpinner = (label: string): ProgressHandle | null =>
      progressSpinnersEnabled ? startProgress(label, { showSpinner: !uiSuppressed }) : null
    const emitProgress = (
      label: string,
      state: 'start' | 'update' | 'stop',
      scope: ProgressScope = 'generic',
    ): void => {
      emitEvent({ event: 'progress.update', label, state, scope })
    }
    emitProgress('Resolving context', 'start', 'generic')
    if (args.urls.length > 0) {
      const label = 'Fetching URL context'
      emitProgress(label, 'start', 'url')
      const urlSpinner = startSpinner(label)
      const urlOptions: ResolveUrlContextOptions = {
        onProgress: (message: string) => {
          urlSpinner?.setLabel(message)
          emitProgress(message, 'update', 'url')
        },
      }
      try {
        const urlFiles = await resolveUrlContext(args.urls, urlOptions)
        if (urlFiles.length > 0) {
          fileContext = [...fileContext, ...urlFiles]
          contextEntrySources = [
            ...contextEntrySources,
            ...urlFiles.map((): ContextEntrySource => 'url'),
          ]
          recordContextPaths(urlFiles, 'url')
        }
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown URL fetch error.'
        console.warn(chalk.yellow(`Failed to fetch URL context: ${message}`))
        emitEvent({ event: 'progress.update', label: `URL error: ${message}`, state: 'update' })
      } finally {
        urlSpinner?.stop('URL context ready')
        emitProgress(label, 'stop', 'url')
      }
    }
    if (args.smartContext) {
      const label = 'Preparing smart context'
      emitProgress(label, 'start', 'smart')
      const smartSpinner = startSpinner(label)
      try {
        const smartFiles = await resolveSmartContextFiles(
          intent,
          fileContext,
          (message) => {
            smartSpinner?.setLabel(message)
            emitProgress(message, 'update', 'smart')
          },
          args.smartContextRoot,
        )
        if (smartFiles.length > 0) {
          fileContext = [...fileContext, ...smartFiles]
          contextEntrySources = [
            ...contextEntrySources,
            ...smartFiles.map((): ContextEntrySource => 'smart'),
          ]
          recordContextPaths(smartFiles, 'smart')
        }
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown smart context error.'
        console.warn(chalk.yellow(`Smart context failed: ${message}`))
        emitEvent({
          event: 'progress.update',
          label: `Smart context error: ${message}`,
          state: 'update',
        })
      } finally {
        smartSpinner?.stop('Smart context ready')
        emitProgress(label, 'stop', 'smart')
      }
    }
    emitProgress('Resolving context', 'stop', 'generic')
    const promptGeneratorConfig = cliConfig?.promptGenerator
    const maxInputTokens = args.maxInputTokens ?? promptGeneratorConfig?.maxInputTokens
    const maxContextTokens = args.maxContextTokens ?? promptGeneratorConfig?.maxContextTokens
    const overflowStrategy = args.contextOverflow ?? promptGeneratorConfig?.contextOverflowStrategy
    const contextEntries: ContextEntry[] = fileContext.map((entry, index) => {
      const source = contextEntrySources[index]
      if (!source) {
        throw new Error(`Invariant violation: missing source for context entry ${entry.path}.`)
      }
      return { ...entry, source }
    })
    const budgetEvaluation = evaluateContextBudget({
      intentText: intent,
      metaInstructions: trimmedMetaInstructions,
      contextEntries,
      ...(maxInputTokens !== undefined ? { maxInputTokens } : {}),
      ...(maxContextTokens !== undefined ? { maxContextTokens } : {}),
      ...(overflowStrategy ? { strategy: overflowStrategy } : {}),
      buildTelemetry: (intentText, files, metaInstructions) =>
        buildTokenTelemetry(intentText, files, metaInstructions),
    })
    if (budgetEvaluation.droppedEntries.length > 0) {
      fileContext = budgetEvaluation.keptEntries.map(({ path, content }) => ({ path, content }))
      contextEntrySources = budgetEvaluation.keptEntries.map((entry) => entry.source)
      contextPaths = pruneContextPaths(contextPaths, budgetEvaluation.droppedPaths)
      emitEvent({
        event: 'context.overflow',
        strategy: budgetEvaluation.strategy ?? 'fail',
        before: budgetEvaluation.before,
        after: budgetEvaluation.after,
        droppedPaths: budgetEvaluation.droppedPaths,
      })
    }
    const telemetry = budgetEvaluation.after
    emitEvent({ event: 'context.telemetry', telemetry })
    if (args.showContext) {
      const writeLine = args.json
        ? (value: string): void => {
            console.error(value)
          }
        : (value: string): void => {
            console.log(value)
          }
      displayContextFiles(fileContext, args.contextFormat, writeLine)
    }
    let outputPath: string | undefined
    if (args.contextFile) {
      await writeContextFile(args.contextFile, args.contextFormat, fileContext)
      outputPath = args.contextFile
    }
    emitProgress('Generating prompt', 'start', 'generate')
    const generationSpinner = startSpinner('Generating prompt')
    const handleUploadStateChange = createUploadStateTracker(
      generationSpinner,
      'Generating prompt',
      streamProxy,
    )
    const resumeState = resume
      ? {
          prompt: resume.payload.polishedPrompt ?? resume.payload.prompt,
          iterations: resume.payload.iterations,
        }
      : undefined
    const {
      prompt: generatedPrompt,
      reasoning: generationReasoning,
      iterations,
    } = await runGenerationWorkflow({
      service,
      context: {
        intent,
        refinements,
        model,
        targetModel,
        fileContext,
        images: args.images,
        videos: args.video,
        pdfs: args.pdf,
        metaInstructions: trimmedMetaInstructions,
      },
      telemetry,
      interactiveMode,
      interactiveTransport,
      interactiveDelegate: options.interactiveDelegate,
      display: shouldDisplay,
      stream: streamProxy,
      onUploadStateChange: handleUploadStateChange,
      ...(resumeState ? { resume: resumeState } : {}),
    })
    generationSpinner?.stop('Generated prompt â')
    emitProgress('Generating prompt', 'stop', 'generate')
    const polishModel =
      args.polishModel ??
      resume?.payload.polishModel ??
      process.env.PROMPT_MAKER_POLISH_MODEL ??
      model
    let polishedPrompt: string | undefined
    if (args.polish) {
      const label = 'Polishing prompt'
      emitProgress(label, 'start', 'polish')
      const polishSpinner = startSpinner(label)
      try {
        polishedPrompt = await polishPrompt(intent, generatedPrompt, polishModel, targetModel)
      } finally {
        polishSpinner?.stop('Polished prompt â')
        emitProgress(label, 'stop', 'polish')
      }
    }
    const artifact = polishedPrompt ?? generatedPrompt
    const renderedPrompt = contextTemplateDefinition
      ? renderContextTemplate(contextTemplateDefinition, artifact)
      : undefined
    const finalArtifact = renderedPrompt ?? artifact
    await maybeCopyToClipboard(args.copy, finalArtifact, shouldDisplay)
    await maybeOpenChatGpt(args.openChatGpt, finalArtifact, shouldDisplay)
    const payload: GenerateJsonPayload = {
      schemaVersion: GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
      intent,
      model,
      targetModel,
      prompt: generatedPrompt,
      ...(typeof generationReasoning === 'string' ? { reasoning: generationReasoning } : {}),
      ...(trimmedMetaInstructions ? { metaInstructions: trimmedMetaInstructions } : {}),
      refinements: [...refinements],
      iterations,
      interactive: interactiveMode !== 'none',
      timestamp: new Date().toISOString(),
      contextPaths,
      images: [...args.images],
      videos: [...args.video],
      pdfs: [...args.pdf],
      ...(outputPath ? { outputPath } : {}),
    }
    if (polishedPrompt) {
      payload.polishedPrompt = polishedPrompt
      payload.polishModel = polishModel
    } else if (resume?.payload.polishedPrompt && resume?.payload.polishModel && !args.polish) {
      payload.polishedPrompt = resume.payload.polishedPrompt
      payload.polishModel = resume.payload.polishModel
    }
    if (contextTemplateName && renderedPrompt) {
      payload.contextTemplate = contextTemplateName
      payload.renderedPrompt = renderedPrompt
    }
    emitEvent({ event: 'generation.final', result: payload })
    const pipelineResult: GeneratePipelineResult = {
      payload,
      telemetry,
      generatedPrompt,
      ...(typeof generationReasoning === 'string' ? { reasoning: generationReasoning } : {}),
      finalPrompt: finalArtifact,
      iterations,
      model,
      contextPaths,
      ...(polishedPrompt ? { polishedPrompt } : {}),
    }
    if (args.json) {
      console.log(JSON.stringify(payload, null, 2))
      await appendToHistory(payload)
      return pipelineResult
    }
    if (renderedPrompt && shouldDisplay && contextTemplateName) {
      displayContextTemplatePrompt(renderedPrompt, contextTemplateName)
    } else if (polishedPrompt && shouldDisplay) {
      displayPolishedPrompt(polishedPrompt, polishModel)
    }
    await appendToHistory(payload)
    return pipelineResult
  } finally {
    transportCleanupHandlers.forEach(({ event, handler }) => {
      process.off(event, handler)
    })
    await interactiveTransport?.stop()
  }
}
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenController.ts">
import { useMemo } from 'react'
import { useStdout } from 'ink'
import type { NotifyOptions } from '../../../notifier'
import { useContextDispatch, useContextState } from '../../../context-store'
import { useCommandScreenHistoryAndTests } from './useCommandScreenHistoryAndTests'
import { useCommandScreenInputState } from './useCommandScreenInputState'
import {
  useCommandScreenModelGeneration,
  type UseCommandScreenModelGenerationResult,
} from './useCommandScreenModelGeneration'
import { useCommandScreenPopupAndView } from './useCommandScreenPopupAndView'
import type {
  UseCommandScreenPopupAndViewOptions,
  UseCommandScreenPopupAndViewResult,
} from './useCommandScreenPopupAndView.types'
export type UseCommandScreenControllerOptions = {
  transport?: {
    interactiveTransportPath?: string | undefined
  }
  popup: {
    onPopupVisibilityChange?: (isOpen: boolean) => void
    commandMenuSignal?: number
    helpOpen: boolean
    reservedRows: number
    onOpenHelp?: () => void
  }
  notify: (message: string, options?: NotifyOptions) => void
}
export type UseCommandScreenControllerResult = {
  view: UseCommandScreenPopupAndViewResult
  actions: {
    suppressNextInput: () => void
  }
}
export const useCommandScreenController = ({
  transport,
  popup,
  notify,
}: UseCommandScreenControllerOptions): UseCommandScreenControllerResult => {
  const { stdout } = useStdout()
  const interactiveTransportPath = transport?.interactiveTransportPath
  const { onPopupVisibilityChange, commandMenuSignal, helpOpen, reservedRows, onOpenHelp } = popup
  const {
    files,
    urls,
    images,
    videos,
    pdfs,
    smartContextEnabled,
    smartContextRoot,
    metaInstructions,
    maxContextTokens,
    maxInputTokens,
    contextOverflowStrategy,
    lastReasoning,
    lastGeneratedPrompt,
  } = useContextState()
  const {
    addFile,
    removeFile,
    addUrl,
    removeUrl,
    updateUrl,
    addImage,
    removeImage,
    addVideo,
    removeVideo,
    addPdf,
    removePdf,
    toggleSmartContext,
    setSmartRoot,
    setMetaInstructions,
    setBudgets,
    setLastReasoning,
    setLastGeneratedPrompt,
    resetContext,
  } = useContextDispatch()
  const historyAndTests = useCommandScreenHistoryAndTests()
  const inputState = useCommandScreenInputState({
    pushHistoryProxy: historyAndTests.pushHistoryProxy,
  })
  const modelAndGeneration: UseCommandScreenModelGenerationResult = useCommandScreenModelGeneration(
    {
      pushHistoryProxy: historyAndTests.pushHistoryProxy,
      notify,
      files,
      urls,
      images,
      videos,
      pdfs,
      smartContextEnabled,
      smartContextRoot,
      metaInstructions,
      budgets: { maxContextTokens, maxInputTokens, contextOverflowStrategy },
      ...(interactiveTransportPath ? { interactiveTransportPath } : {}),
      terminalColumns: inputState.terminalColumns,
      copyEnabled: inputState.copyEnabled,
      chatGptEnabled: inputState.chatGptEnabled,
      jsonOutputEnabled: inputState.jsonOutputEnabled,
      isTestCommandRunning: historyAndTests.isTestCommandRunning,
      setLastReasoning,
      setLastGeneratedPrompt,
    },
  )
  const contextOptions: UseCommandScreenPopupAndViewOptions['context'] = useMemo(
    () => ({
      interactiveTransportPath,
      notify,
      stdout,
      files,
      urls,
      images,
      videos,
      pdfs,
      smartContextEnabled,
      smartContextRoot,
      metaInstructions,
      maxContextTokens,
      maxInputTokens,
      contextOverflowStrategy,
      lastReasoning,
      lastGeneratedPrompt,
      addFile,
      removeFile,
      addUrl,
      removeUrl,
      updateUrl,
      addImage,
      removeImage,
      addVideo,
      removeVideo,
      addPdf,
      removePdf,
      toggleSmartContext,
      setSmartRoot,
      setMetaInstructions,
      setBudgets,
      resetContext,
    }),
    [
      interactiveTransportPath,
      notify,
      stdout,
      files,
      urls,
      images,
      videos,
      smartContextEnabled,
      smartContextRoot,
      metaInstructions,
      maxContextTokens,
      maxInputTokens,
      contextOverflowStrategy,
      lastReasoning,
      lastGeneratedPrompt,
      addFile,
      removeFile,
      addUrl,
      removeUrl,
      updateUrl,
      addImage,
      removeImage,
      addVideo,
      removeVideo,
      addPdf,
      removePdf,
      toggleSmartContext,
      setSmartRoot,
      setMetaInstructions,
      setBudgets,
      resetContext,
    ],
  )
  const inputOptions: UseCommandScreenPopupAndViewOptions['input'] = useMemo(
    () => ({
      terminalRows: inputState.terminalRows,
      terminalColumns: inputState.terminalColumns,
      inputValue: inputState.inputValue,
      isPasteActive: inputState.isPasteActive,
      commandSelectionIndex: inputState.commandSelectionIndex,
      debugKeyLine: inputState.debugKeyLine,
      debugKeysEnabled: inputState.debugKeysEnabled,
      setTerminalSize: inputState.setTerminalSize,
      setInputValue: inputState.setInputValue,
      setPasteActive: inputState.setPasteActive,
      setCommandSelectionIndex: inputState.setCommandSelectionIndex,
      intentFilePath: inputState.intentFilePath,
      setIntentFilePath: inputState.setIntentFilePath,
      copyEnabled: inputState.copyEnabled,
      setCopyEnabled: inputState.setCopyEnabled,
      chatGptEnabled: inputState.chatGptEnabled,
      setChatGptEnabled: inputState.setChatGptEnabled,
      jsonOutputEnabled: inputState.jsonOutputEnabled,
      setJsonOutputEnabled: inputState.setJsonOutputEnabled,
      lastUserIntentRef: inputState.lastUserIntentRef,
      lastTypedIntentRef: inputState.lastTypedIntentRef,
      consumeSuppressedTextInputChange: inputState.consumeSuppressedTextInputChange,
      suppressNextInput: inputState.suppressNextInput,
      updateLastTypedIntent: inputState.updateLastTypedIntent,
      onDebugKeyEvent: inputState.onDebugKeyEvent,
    }),
    [
      inputState.terminalRows,
      inputState.terminalColumns,
      inputState.inputValue,
      inputState.isPasteActive,
      inputState.commandSelectionIndex,
      inputState.debugKeyLine,
      inputState.debugKeysEnabled,
      inputState.setTerminalSize,
      inputState.setInputValue,
      inputState.setPasteActive,
      inputState.setCommandSelectionIndex,
      inputState.intentFilePath,
      inputState.setIntentFilePath,
      inputState.copyEnabled,
      inputState.setCopyEnabled,
      inputState.chatGptEnabled,
      inputState.setChatGptEnabled,
      inputState.jsonOutputEnabled,
      inputState.setJsonOutputEnabled,
      inputState.lastUserIntentRef,
      inputState.lastTypedIntentRef,
      inputState.consumeSuppressedTextInputChange,
      inputState.suppressNextInput,
      inputState.updateLastTypedIntent,
      inputState.onDebugKeyEvent,
    ],
  )
  const popupOptions: UseCommandScreenPopupAndViewOptions['popup'] = useMemo(
    () => ({
      onPopupVisibilityChange,
      commandMenuSignal,
      helpOpen,
      reservedRows,
      ...(onOpenHelp ? { onOpenHelp } : {}),
    }),
    [onPopupVisibilityChange, commandMenuSignal, helpOpen, reservedRows, onOpenHelp],
  )
  const historyOptions: UseCommandScreenPopupAndViewOptions['history'] = useMemo(
    () => ({
      pushHistoryRef: historyAndTests.pushHistoryRef,
      pushHistoryProxy: historyAndTests.pushHistoryProxy,
      clearHistoryRef: historyAndTests.clearHistoryRef,
      scrollToRef: historyAndTests.scrollToRef,
      scrollToProxy: historyAndTests.scrollToProxy,
      closeTestPopupRef: historyAndTests.closeTestPopupRef,
      commandHistoryValues: historyAndTests.commandHistoryValues,
      addCommandHistoryEntry: historyAndTests.addCommandHistoryEntry,
      isTestCommandRunning: historyAndTests.isTestCommandRunning,
      lastTestFile: historyAndTests.lastTestFile,
      runTestsFromCommandProxy: historyAndTests.runTestsFromCommandProxy,
      onTestPopupSubmit: historyAndTests.onTestPopupSubmit,
    }),
    [
      historyAndTests.pushHistoryRef,
      historyAndTests.pushHistoryProxy,
      historyAndTests.clearHistoryRef,
      historyAndTests.scrollToRef,
      historyAndTests.scrollToProxy,
      historyAndTests.closeTestPopupRef,
      historyAndTests.commandHistoryValues,
      historyAndTests.addCommandHistoryEntry,
      historyAndTests.isTestCommandRunning,
      historyAndTests.lastTestFile,
      historyAndTests.runTestsFromCommandProxy,
      historyAndTests.onTestPopupSubmit,
    ],
  )
  const generationOptions: UseCommandScreenPopupAndViewOptions['generation'] = useMemo(
    () => ({
      currentModel: modelAndGeneration.currentModel,
      polishModelId: modelAndGeneration.polishModelId,
      currentTargetModel: modelAndGeneration.currentTargetModel,
      modelOptions: modelAndGeneration.modelOptions,
      providerStatuses: modelAndGeneration.providerStatuses,
      selectModel: modelAndGeneration.selectModel,
      selectPolishModel: modelAndGeneration.selectPolishModel,
      selectTargetModel: modelAndGeneration.selectTargetModel,
      isGenerating: modelAndGeneration.pipeline.isGenerating,
      runGeneration: modelAndGeneration.pipeline.runGeneration,
      runSeriesGeneration: modelAndGeneration.pipeline.runSeriesGeneration,
      statusChips: modelAndGeneration.pipeline.statusChips,
      isAwaitingRefinement: modelAndGeneration.pipeline.isAwaitingRefinement,
      submitRefinement: modelAndGeneration.pipeline.submitRefinement,
      awaitingInteractiveMode: modelAndGeneration.pipeline.awaitingInteractiveMode,
      tokenUsageRun: modelAndGeneration.pipeline.tokenUsageRun,
      tokenUsageBreakdown: modelAndGeneration.pipeline.tokenUsageBreakdown,
      latestContextOverflow: modelAndGeneration.pipeline.latestContextOverflow,
    }),
    [
      modelAndGeneration.currentModel,
      modelAndGeneration.polishModelId,
      modelAndGeneration.currentTargetModel,
      modelAndGeneration.modelOptions,
      modelAndGeneration.providerStatuses,
      modelAndGeneration.selectModel,
      modelAndGeneration.selectPolishModel,
      modelAndGeneration.selectTargetModel,
      modelAndGeneration.pipeline.isGenerating,
      modelAndGeneration.pipeline.runGeneration,
      modelAndGeneration.pipeline.runSeriesGeneration,
      modelAndGeneration.pipeline.statusChips,
      modelAndGeneration.pipeline.isAwaitingRefinement,
      modelAndGeneration.pipeline.submitRefinement,
      modelAndGeneration.pipeline.awaitingInteractiveMode,
      modelAndGeneration.pipeline.tokenUsageRun,
      modelAndGeneration.pipeline.tokenUsageBreakdown,
      modelAndGeneration.pipeline.latestContextOverflow,
    ],
  )
  const view = useCommandScreenPopupAndView({
    context: contextOptions,
    input: inputOptions,
    popup: popupOptions,
    history: historyOptions,
    generation: generationOptions,
  })
  const actions = useMemo(
    () => ({
      suppressNextInput: inputState.suppressNextInput,
    }),
    [inputState.suppressNextInput],
  )
  return useMemo(
    () => ({
      view,
      actions,
    }),
    [view, actions],
  )
}
</file>
<file path="src/tui/screens/command/hooks/useContextPopupGlue.ts">
import { useInput, type Key } from 'ink'
import { useStableCallback } from '../../../hooks/useStableCallback'
import type { CommandDescriptor, PopupState } from '../../../types'
import { useFilePopupGlue } from './context-popup-glue/useFilePopupGlue'
import { useImagePopupGlue } from './context-popup-glue/useImagePopupGlue'
import { usePdfPopupGlue } from './context-popup-glue/usePdfPopupGlue'
import { useSmartPopupGlue } from './context-popup-glue/useSmartPopupGlue'
import { useUrlPopupGlue } from './context-popup-glue/useUrlPopupGlue'
import { useVideoPopupGlue } from './context-popup-glue/useVideoPopupGlue'
export type UseContextPopupGlueOptions = {
  inputValue: string
  popupState: PopupState
  helpOpen: boolean
  isPopupOpen: boolean
  isCommandMode: boolean
  isCommandMenuActive: boolean
  isGenerating: boolean
  droppedFilePath: string | null
  files: string[]
  urls: string[]
  images: string[]
  videos: string[]
  pdfs: string[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  addFile: (value: string) => void
  removeFile: (index: number) => void
  addUrl: (value: string) => void
  removeUrl: (index: number) => void
  updateUrl: (index: number, value: string) => void
  addImage: (value: string) => void
  removeImage: (index: number) => void
  addVideo: (value: string) => void
  removeVideo: (index: number) => void
  addPdf: (value: string) => void
  removePdf: (index: number) => void
  toggleSmartContext: () => void
  setSmartRoot: (value: string) => void
  setInputValue: (value: string) => void
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  suppressNextInput: () => void
  notify: (message: string) => void
  pushHistory: (content: string, kind?: 'system' | 'user' | 'progress') => void
  addCommandHistoryEntry: (value: string) => void
  handleCommandSelection: (commandId: CommandDescriptor['id'], argsRaw?: string) => void
  consumeSuppressedTextInputChange: () => boolean
  isFilePath: (candidate: string) => boolean
}
export type UseContextPopupGlueResult = {
  // File
  filePopupSuggestions: string[]
  filePopupSuggestionSelectionIndex: number
  filePopupSuggestionsFocused: boolean
  onFilePopupDraftChange: (next: string) => void
  onAddFile: (value: string) => void
  onRemoveFile: (index: number) => void
  // URL
  onUrlPopupDraftChange: (next: string) => void
  onAddUrl: (value: string) => void
  onRemoveUrl: (index: number) => void
  // Image
  imagePopupSuggestions: string[]
  imagePopupSuggestionSelectionIndex: number
  imagePopupSuggestionsFocused: boolean
  onImagePopupDraftChange: (next: string) => void
  onAddImage: (value: string) => void
  onRemoveImage: (index: number) => void
  // Video
  videoPopupSuggestions: string[]
  videoPopupSuggestionSelectionIndex: number
  videoPopupSuggestionsFocused: boolean
  onVideoPopupDraftChange: (next: string) => void
  onAddVideo: (value: string) => void
  onRemoveVideo: (index: number) => void
  // PDF
  pdfPopupSuggestions: string[]
  pdfPopupSuggestionSelectionIndex: number
  pdfPopupSuggestionsFocused: boolean
  onPdfPopupDraftChange: (next: string) => void
  onAddPdf: (value: string) => void
  onRemovePdf: (index: number) => void
  // Smart
  smartPopupSuggestions: string[]
  smartPopupSuggestionSelectionIndex: number
  smartPopupSuggestionsFocused: boolean
  onSmartPopupDraftChange: (next: string) => void
  onSmartToggle: (nextEnabled: boolean) => void
  onSmartRootSubmit: (value: string) => void
}
export const useContextPopupGlue = ({
  inputValue,
  popupState,
  helpOpen,
  isPopupOpen,
  isCommandMode,
  isCommandMenuActive,
  isGenerating,
  droppedFilePath,
  files,
  urls,
  images,
  videos,
  pdfs,
  smartContextEnabled,
  smartContextRoot,
  addFile,
  removeFile,
  addUrl,
  removeUrl,
  updateUrl,
  addImage,
  removeImage,
  addVideo,
  removeVideo,
  addPdf,
  removePdf,
  toggleSmartContext,
  setSmartRoot,
  setInputValue,
  setPopupState,
  suppressNextInput,
  notify,
  pushHistory,
  addCommandHistoryEntry,
  handleCommandSelection,
  consumeSuppressedTextInputChange,
  isFilePath,
}: UseContextPopupGlueOptions): UseContextPopupGlueResult => {
  const fileGlue = useFilePopupGlue({
    popupState,
    files,
    setPopupState,
    pushHistory,
    addFile,
    removeFile,
    consumeSuppressedTextInputChange,
    isFilePath,
  })
  const urlGlue = useUrlPopupGlue({
    popupState,
    urls,
    setPopupState,
    pushHistory,
    addUrl,
    removeUrl,
    updateUrl,
    consumeSuppressedTextInputChange,
  })
  const imageGlue = useImagePopupGlue({
    popupState,
    images,
    setPopupState,
    pushHistory,
    addImage,
    removeImage,
    consumeSuppressedTextInputChange,
    isFilePath,
  })
  const videoGlue = useVideoPopupGlue({
    popupState,
    videos,
    setPopupState,
    pushHistory,
    addVideo,
    removeVideo,
    consumeSuppressedTextInputChange,
    isFilePath,
  })
  const pdfGlue = usePdfPopupGlue({
    popupState,
    pdfs,
    setPopupState,
    pushHistory,
    addPdf,
    removePdf,
    consumeSuppressedTextInputChange,
    isFilePath,
  })
  const smartGlue = useSmartPopupGlue({
    popupState,
    smartContextEnabled,
    smartContextRoot,
    setPopupState,
    notify,
    toggleSmartContext,
    setSmartRoot,
    consumeSuppressedTextInputChange,
  })
  const handleSeriesShortcut = useStableCallback((_input: string, key: Key) => {
    if (popupState || isCommandMenuActive || isCommandMode) {
      return
    }
    if (!key.tab || key.shift) {
      return
    }
    if (droppedFilePath) {
      fileGlue.onAddFile(droppedFilePath)
      suppressNextInput()
      setInputValue('')
      return
    }
    if (isGenerating) {
      pushHistory('Generation already running. Please wait.', 'system')
      return
    }
    const trimmedArgs = inputValue.trim()
    addCommandHistoryEntry(`/series${trimmedArgs ? ` ${trimmedArgs}` : ''}`)
    handleCommandSelection('series', inputValue)
  })
  useInput(handleSeriesShortcut, { isActive: !isPopupOpen && !helpOpen })
  return {
    ...fileGlue,
    ...urlGlue,
    ...imageGlue,
    ...videoGlue,
    ...pdfGlue,
    ...smartGlue,
  }
}
</file>
<file path="docs/prompt-maker-cli-tui-encyclopedia.md">
# Prompt Maker CLI + TUI Encyclopedia (Current Codebase)
This document is the **source-of-truth user + maintainer reference** for `@perceptron/prompt-maker-cli`âs Ink TUI and the underlying generate/test CLI workflows.
- Package: `@perceptron/prompt-maker-cli` (ESM)
- Node: `>=18` (`package.json`)
- Binary: `prompt-maker-cli` â `dist/index.js` (`package.json#bin`)
Where possible, each behavior links back to an implementation file under `src/`.
> Note: The previous version of this encyclopedia referenced an older repo layout (`apps/prompt-maker-cli/...`) and embedded line-ranges. This repoâs current layout is **single-package** at the root (`src/`, `dist/`).
## Table of Contents
- [1. CLI Entry Points & Modes](#1-cli-entry-points--modes)
- [2. Export Mode (CLI)](#2-export-mode-cli)
- [3. Compose Mode (CLI)](#3-compose-mode-cli)
- [4. Generate Mode (CLI)](#4-generate-mode-cli)
- [5. Context Ingestion](#5-context-ingestion)
- [6. Interactive Refinement (TTY and Transport)](#6-interactive-refinement-tty-and-transport)
- [7. Outputs, Streaming, Telemetry, and Persistence](#7-outputs-streaming-telemetry-and-persistence)
- [8. Test Runner (CLI)](#8-test-runner-cli)
- [9. TUI Overview](#9-tui-overview)
- [10. TUI Keybindings](#10-tui-keybindings)
- [11. TUI Commands (`/command` palette)](#11-tui-commands-command-palette)
- [12. TUI Themes](#12-tui-themes)
- [13. TUI Architecture Map (for maintainers)](#13-tui-architecture-map-for-maintainers)
- [14. TODOs / Known Quirks](#14-todos--known-quirks)
---
## 1. CLI Entry Points & Modes
Routing is implemented in `src/index.ts`.
### Commands and routing rules
`prompt-maker-cli` has five top-level modes:
- `ui`: Ink TUI
- `generate`: prompt generation pipeline (default)
- `test`: prompt test runner
- `export`: export a past generate payload from history
- `compose`: deterministic (non-LLM) prompt composition scaffold
Routing rules (`src/index.ts`):
- No args â `ui`
- First arg `ui` â `ui`
- First arg `test` â `test`
- First arg `export` â `export`
- First arg `compose` â `compose`
- First arg `generate` or `expand` â `generate` (alias)
- Anything else (including flags like `--json`) â `generate`
Help behavior:
- `prompt-maker-cli --help` shows **generate** help only.
- Use `prompt-maker-cli export --help` / `prompt-maker-cli compose --help` for subcommand help.
### Practical examples
```bash
# TUI (default)
prompt-maker-cli
# TUI explicitly
prompt-maker-cli ui
# Generate
prompt-maker-cli "Draft an onboarding prompt" --model gpt-4o-mini
# Generate (explicit subcommand)
prompt-maker-cli generate "Draft an onboarding prompt"
# Tests
prompt-maker-cli test
prompt-maker-cli test prompt-tests.yaml
# Export the last run from history
prompt-maker-cli export --format json --out runs/last-run.json
# Deterministic (non-LLM) composition scaffold
prompt-maker-cli compose --recipe recipes/triage.txt --input "Hello"
```
---
## 2. Export Mode (CLI)
Export pulls a previously-generated `GenerateJsonPayload` from the JSONL history log and writes it to disk as JSON or YAML.
Implementation: `src/export-command.ts` (history selection: `src/history/generate-history.ts`, file writer: `src/export/export-generate-payload.ts`).
### Usage
```bash
prompt-maker-cli export --format json --out runs/last-run.json
prompt-maker-cli export --format yaml --out runs/last-run.yaml
# Select an older entry from history
prompt-maker-cli export --from-history last:3 --format json --out runs/third-from-last.json
prompt-maker-cli export --from-history 10 --format yaml --out runs/tenth-from-last.yaml
```
### Flags
- `--from-history <selector>`: which entry to export from the history file (`last`, `last:N`, or `N`-th from end). Default: `last`.
- `--format json|yaml`: required output format.
- `--out <path>`: required output file path (directories are created).
- `--quiet`: suppresses human-readable **stderr** logs.
### Schema compatibility
- Export validates the selected history entryâs `schemaVersion`.
- If the selected entry is from a newer/older `schemaVersion` than the current binary supports, export fails with an actionable error message.
### Stdout/stderr
- Export does not print payloads to stdout. It writes the payload to `--out`.
- Success messages are printed to **stderr** unless `--quiet` is set.
---
## 3. Compose Mode (CLI)
Compose is a deterministic, non-LLM scaffold for building a prompt-like artifact from a "recipe" file plus an input string.
Implementation: `src/compose-command.ts`.
### Usage
```bash
prompt-maker-cli compose --recipe recipes/triage.txt --input "Summarize this diff"
```
### Output behavior (current scaffold)
- Reads the recipe file as plain text.
- Prints a deterministic composition to **stdout**:
```text
<recipe file contents>
---
<input>
```
Notes:
- This is scaffolding (no recipe semantics yet): it does not parse YAML, expand templates, or call any LLM.
- All successful output is written to **stdout** (safe for piping); errors are written to **stderr**.
---
## 4. Generate Mode (CLI)
Generate-mode argument parsing lives in `src/generate/args.ts` and the main pipeline is `src/generate/pipeline.ts`.
### Intent sources
Generate consumes intent from one of:
- Positional string: `prompt-maker-cli "..."`
- File: `--intent-file path/to/intent.md`
- `stdin`: pipe into the command
Implementation: `src/generate/intent.ts` (called from `src/generate/pipeline.ts`).
### Flags (authoritative)
The `--help` output for generate is produced by yargs in `src/generate/args.ts` and matches `node dist/index.js --help`.
| Flag                      |                  Type |       Default | Notes                                                                              |
| ------------------------- | --------------------: | ------------: | ---------------------------------------------------------------------------------- |
| `-f, --intent-file`       |                string |             - | Read intent from file                                                              |
| `--model`                 |                string |             - | Generation model override                                                          |
| `--target`                |                string |             - | Target/runtime model for optimization guidance (not included in prompt text)       |
| `--polish-model`          |                string |             - | Model used for polish pass                                                         |
| `-i, --interactive`       |               boolean |       `false` | Enables interactive refinement if a TTY is available                               |
| `--interactive-transport` |                string |             - | Local socket/pipe to drive refinements remotely                                    |
| `--polish`                |               boolean |       `false` | Run polish pass after generation                                                   |
| `--json`                  |               boolean |       `false` | Emit JSON payload to stdout (non-interactive only)                                 |
| `--stream`                |         `none\|jsonl` |        `none` | Emit JSONL event stream to stdout                                                  |
| `--quiet`                 |               boolean |       `false` | Suppress human-oriented output (banners/telemetry)                                 |
| `--progress`              |               boolean |        `true` | Show progress spinner (non-interactive only)                                       |
| `--copy`                  |               boolean |       `false` | Copy final prompt to clipboard                                                     |
| `--open-chatgpt`          |               boolean |       `false` | Open a ChatGPT URL prefilled with the prompt                                       |
| `--resume-last`           |               boolean |       `false` | Resume from the last history entry                                                 |
| `--resume`                |                string |             - | Resume from history selector (`last`, `last:N`, or `N`-th from end)                |
| `--resume-from`           |                string |             - | Resume from an exported payload file (`.json` / `.yaml` / `.yml`)                  |
| `--resume-mode`           | `strict\|best-effort` | `best-effort` | How to handle missing resumed context file paths                                   |
| `--context-template`      |                string |             - | Wrap final prompt using a named template                                           |
| `--show-context`          |               boolean |       `false` | Print resolved context files before generation                                     |
| `-c, --context`           |              string[] |          `[]` | File glob patterns (repeatable)                                                    |
| `--url`                   |              string[] |          `[]` | URL context entries (repeatable)                                                   |
| `--image`                 |              string[] |          `[]` | Image file paths (repeatable)                                                      |
| `--video`                 |              string[] |          `[]` | Video file paths (repeatable)                                                      |
| `--context-file`          |                string |             - | Write resolved context to a file                                                   |
| `--context-format`        |          `text\|json` |        `text` | Format used by `--show-context` and `--context-file`                               |
| `--smart-context`         |               boolean |       `false` | Auto-attach relevant local files via embeddings                                    |
| `--smart-context-root`    |                string |             - | Base directory for smart-context scan                                              |
| `--max-input-tokens`      |                number |             - | Enforce a maximum input budget (intent + system + text context)                    |
| `--max-context-tokens`    |                number |             - | Enforce a maximum budget for text context entries (file/url/smart)                 |
| `--context-overflow`      |                string |             - | Overflow handling: `fail`, `drop-smart`, `drop-url`, `drop-largest`, `drop-oldest` |
### Flag incompatibilities
Enforced in `src/generate/pipeline.ts`:
- `--json` **cannot** be combined with interactive modes (`--interactive` or `--interactive-transport`).
  - Error text: `--json cannot be combined with --interactive.`
### Common generate examples
```bash
# Minimal
prompt-maker-cli "Draft a confident onboarding-bot prompt" --model gpt-4o-mini
# Intent file + context + JSON output
prompt-maker-cli --intent-file drafts/intent.md \
  --context src/**/*.ts \
  --context "!src/**/__tests__/**" \
  --json > runs/intent.json
# Show exactly what gets embedded (text)
prompt-maker-cli "Explain this module" --context src/tui/**/*.ts --show-context
# Capture resolved context to a file (JSON array of {path, content})
prompt-maker-cli "Summarize" --url https://example.com \
  --context-file resolved-context.json --context-format json
```
---
## 5. Context Ingestion
### 3.1 File context (`--context`)
Implementation: `src/file-context.ts`.
- Globs are expanded with `fast-glob` using `{ dot: true }`.
- If no files match, the CLI prints a warning and continues with no file context.
- Each matched file is read as UTF-8; unreadable files are skipped with a warning.
Embedding format used in prompts (`src/file-context.ts`):
```xml
<file path="relative/or/absolute/path">
...file contents...
</file>
```
### 3.2 URL context (`--url`)
Implementation: `src/url-context.ts`.
- Only `http:` / `https:` URLs are accepted.
- HTML is downloaded with a timeout and a maximum size (`MAX_HTML_BYTES = 1MB`).
- HTML is converted to text via `html-to-text` (scripts/styles skipped).
- Each URL context entry is stored as a virtual file:
  - `path: url:https://example.com/...`
### 3.3 GitHub URL context
GitHub URLs are handled specially inside `src/url-context.ts` and resolved by `src/github-context.ts`.
Supported GitHub URL shapes (parsed in `src/github-context.ts`):
- Repository root: `https://github.com/<owner>/<repo>`
- Tree view: `https://github.com/<owner>/<repo>/tree/<ref>/<path?>`
- Blob view: `https://github.com/<owner>/<repo>/blob/<ref>/<path>`
Safety limits (`src/github-context.ts`):
- Max files: `MAX_GITHUB_FILES = 60`
- Max file size: `MAX_GITHUB_FILE_BYTES = 64KB`
- Skips common large/binary paths (node_modules, dist, lockfiles, archives, etc.)
GitHub files become virtual context entries:
- `path: github:<owner>/<repo>/<path>`
### 3.4 Smart context (`--smart-context`)
Implementation: `src/smart-context-service.ts` and `src/rag/vector-store.ts`.
Workflow:
1. Scan workspace for candidate files (default patterns):
   - `**/*.{ts,tsx,js,jsx,py,md,json}`
   - Ignored: `node_modules`, `dist`, `.git`, etc.
2. Only files smaller than 25KB are indexed (`MAX_EMBEDDING_FILE_SIZE`).
3. Index embeddings (cached on disk).
4. Search top K results (`k = 5`) relative to the userâs intent.
5. Read matching files and add them to context (skipping files already present).
Embeddings cache location (`src/rag/vector-store.ts`):
- `~/.config/prompt-maker-cli/embeddings_cache.json`
### 3.5 Images (`--image`)
Implementation: `src/image-loader.ts`.
- Supported extensions: `.png`, `.jpg`, `.jpeg`, `.webp`, `.gif`
- Max size: 20MB per image
- Images are embedded as base64 âimage partsâ (see `@prompt-maker/core` `ImagePart`).
### 3.6 Video (`--video`)
Implementation:
- Upload: `src/media-loader.ts` (Gemini Files API)
- Message parts: `src/prompt-generator/video-parts.ts`
- Model switching: `src/generate/pipeline.ts` + `src/generate/models.ts`
Behavior:
- Video files are uploaded to Gemini Files API and referenced by URI.
- Supported extensions (mime-inferred): `.mp4`, `.mov`, `.m4v`, `.webm`, `.avi`, `.mpeg`, `.mpg`, `.gif`.
- Requires `GEMINI_API_KEY` (or an explicit API key passed to the uploader).
- If videos are provided and the selected generation model is not Gemini, the CLI switches to a Gemini model that supports video input:
  - It prefers `gemini-2.5-pro` when available, otherwise falls back (see `src/generate/models.ts`).
### 3.7 Token budgets and context overflow
Token budgets are applied after all text context is resolved (files, URLs, smart context) and before generation begins.
Flags (or config equivalents) are consumed in `src/generate/pipeline.ts` and evaluated by `src/generate/context-budget.ts`:
- `--max-input-tokens`: caps total input tokens (`intentTokens + systemTokens + fileTokens`).
- `--max-context-tokens`: caps tokens reserved for text context entries (`fileTokens`).
- `--context-overflow`: chooses how to respond when `fileTokens` exceeds the allowed budget.
Notes:
- Budgets apply only to **text context entries** (`--context`, `--url`, `--smart-context`).
  - Images/videos are not included in the token budget model and are never trimmed by these strategies.
- If both `--max-input-tokens` and `--max-context-tokens` are set, the effective allowance for text context is:
  - `allowedFileTokens = min(maxContextTokens, maxInputTokens - intentTokens - systemTokens)`
- If budgets are set but `--context-overflow` is omitted, the default strategy is `fail`.
- If no budgets are set, overflow strategy is ignored and no trimming occurs.
Overflow strategies (`ContextOverflowStrategy` in `src/generate/types.ts`):
- `fail`: throw an error and abort generation.
- `drop-smart`: drop smart-context entries first, then drop remaining entries oldest-first.
- `drop-url`: drop URL entries first, then drop remaining entries oldest-first.
- `drop-largest`: drop the largest token entries first (ties break by age).
- `drop-oldest`: drop entries in original attachment order (oldest-first).
When trimming drops any text context entries:
- `contextPaths` in the final JSON payload is pruned to match the kept entries.
- A `context.overflow` stream event is emitted (see Streaming below).
---
## 6. Interactive Refinement (TTY and Transport)
Interactive logic is in `src/generate/interactive.ts`.
### 4.1 TTY interactive (`--interactive`)
- If `--interactive` is enabled and stdin/stdout are TTYs, the CLI uses `enquirer` prompts to ask whether to refine, and for the refinement instruction.
- If `--interactive` is enabled but no TTY is detected, the CLI prints a warning and proceeds as a normal non-interactive run.
### 4.2 Interactive transport (`--interactive-transport <path>`)
Implementation: `src/generate/interactive-transport.ts`.
- Creates a local `net.Server` on either:
  - Unix socket path (e.g. `/tmp/pmc.sock`), or
  - Windows named pipe path (must start with `\\.\pipe\...`).
The client sends **newline-delimited JSON** commands:
- Refine:
  - `{"type":"refine","instruction":"Make it shorter"}`
- Finish:
  - `{"type":"finish"}`
The transport stream also receives JSONL events emitted by the pipeline (even if `--stream` is `none`), because the transport is always attached as an event âtapâ.
Constraint:
- `--json` cannot be used with `--interactive-transport` (enforced in `src/generate/pipeline.ts`).
---
## 7. Outputs, Streaming, Telemetry, and Persistence
### 5.1 JSON output (`--json`)
When `--json` is enabled, the CLI prints a JSON payload to **stdout**:
- Shape: `GenerateJsonPayload` (`src/generate/types.ts`)
- Includes: `schemaVersion`, `intent`, `model`, `targetModel`, `prompt`, optional `polishedPrompt`, `iterations`, `refinements`, `contextPaths`, timestamps, etc.
`--show-context` behavior when combined with `--json`:
- Context is printed to **stderr** (so stdout can remain machine-readable). Implementation: `src/generate/pipeline.ts`.
### 5.2 Streaming (`--stream jsonl`)
Implementation: `src/generate/stream.ts` and event types in `src/generate/types.ts`.
When enabled:
- The CLI writes newline-delimited JSON events to **stdout**.
Example (streaming + budgets):
```bash
prompt-maker-cli "Summarize these files" \
  --context src/**/*.ts \
  --stream jsonl \
  --progress=false \
  --max-context-tokens 8000 \
  --context-overflow drop-smart
```
- Event names include (non-exhaustive):
  - `progress.update`
  - `context.telemetry`
  - `context.overflow`
  - `upload.state`
  - `generation.iteration.start`
  - `generation.iteration.complete`
  - `interactive.state`
  - `interactive.awaiting`
  - `transport.*`
  - `generation.final`
#### `resume.loaded` event
Emitted early in a run when `--resume-last`, `--resume`, or `--resume-from` successfully loads a previous payload.
Shape (`src/generate/types.ts`):
- `event`: `"resume.loaded"`
- `timestamp`: ISO-8601 string
- `source`: `"history" | "file"`
- `reusedContextPaths`: subset of context paths that were successfully reloaded (today this is limited to `source: "file"` entries)
- `missingContextPaths`: context paths that could not be reused (missing files, plus any non-file sources like `url`/`smart`)
Notes:
- **Explicit `--context` overrides** resumed `contextPaths`.
- **Explicit flags override** resumed payload values (e.g. `--model` overrides `payload.model`).
- `--resume-mode best-effort` warns and continues if some file paths are missing.
- `--resume-mode strict` throws if any resumed `source: "file"` paths are missing.
#### `context.overflow` event
Emitted when token budgets are enabled and the CLI drops one or more text context entries to satisfy the budget.
Shape (`src/generate/types.ts`):
- `event`: `"context.overflow"`
- `timestamp`: ISO-8601 string
- `strategy`: `fail | drop-smart | drop-url | drop-largest | drop-oldest`
- `before`: token telemetry before trimming
- `after`: token telemetry after trimming
- `droppedPaths`: array of `{ path, source }` entries removed from context
Example JSONL line (formatted for readability):
```json
{
  "event": "context.overflow",
  "timestamp": "2026-01-04T16:33:17.000Z",
  "strategy": "drop-smart",
  "before": {
    "files": [
      { "path": "src/a.ts", "tokens": 1200 },
      { "path": "src/b.ts", "tokens": 900 }
    ],
    "intentTokens": 200,
    "fileTokens": 2100,
    "systemTokens": 700,
    "totalTokens": 3000
  },
  "after": {
    "files": [{ "path": "src/a.ts", "tokens": 1200 }],
    "intentTokens": 200,
    "fileTokens": 1200,
    "systemTokens": 700,
    "totalTokens": 2100
  },
  "droppedPaths": [{ "path": "src/b.ts", "source": "smart" }]
}
```
### 5.3 Token telemetry
Implementation: `src/generate/token-telemetry.ts`.
- Token telemetry is computed for:
  - the intent
  - each context file
  - the system prompt (+ optional meta instructions)
- A summary is printed in non-JSON, non-quiet mode.
- Telemetry is always emitted as a `context.telemetry` stream event and returned as `GeneratePipelineResult.telemetry`.
### 5.4 History logging (generate runs)
Implementation: `src/history-logger.ts`.
Every generate run appends JSONL to:
- `~/.config/prompt-maker-cli/history.jsonl`
### 5.5 Config resolution
Implementation: `src/config.ts`.
Config resolution order:
1. `PROMPT_MAKER_CLI_CONFIG=/path/to/config.json`
2. `~/.config/prompt-maker-cli/config.json`
3. `~/.prompt-maker-cli.json`
Config keys validated today (`src/config.ts`):
- Provider credentials:
  - `openaiApiKey`, `openaiBaseUrl`
  - `geminiApiKey`, `geminiBaseUrl`
- Prompt generator:
  - `promptGenerator.defaultModel`
  - `promptGenerator.defaultGeminiModel`
  - `promptGenerator.models[]` (custom model registry)
  - `promptGenerator.maxInputTokens` (budget default)
  - `promptGenerator.maxContextTokens` (budget default)
  - `promptGenerator.contextOverflowStrategy` (budget default)
- Templates:
  - `contextTemplates.<name>`
- TUI theming (persisted):
  - `theme`
  - `themeMode` (`light` / `dark` / `system`; `auto` is accepted as an alias for `system`)
- TUI resume defaults (persisted):
  - `resumeMode` (`best-effort` / `strict`)
  - `resumeSourceKind` (`history` / `file`)
- TUI export defaults (persisted):
  - `exportFormat` (`json` / `yaml`)
  - `exportOutDir` (directory path)
### 5.6 TUI-local persistence
TUI command history is stored separately from generate history:
- `~/.config/prompt-maker-cli/tui-history.json`
Implementation: `src/tui/command-history.ts`.
---
## 8. Test Runner (CLI)
Implementation: `src/test-command.ts`.
Usage:
```bash
prompt-maker-cli test
prompt-maker-cli test prompt-tests.yaml
```
- Default file: `prompt-tests.yaml` (repo root)
- Test suite format is validated by zod schema in `src/testing/test-schema.ts`.
---
## 9. TUI Overview
The Ink TUI is implemented under `src/tui/` and launched via `src/tui/index.tsx`.
### Launch
```bash
prompt-maker-cli
# or
prompt-maker-cli ui
# With interactive transport
prompt-maker-cli ui --interactive-transport /tmp/pmc.sock
```
TUI argument parsing is intentionally minimal (`src/tui/index.tsx`):
- Only `--interactive-transport` is parsed.
- `ui --help` is **not** implemented; passing `--help` will still attempt to run Ink.
TTY requirement:
- Ink requires raw-mode input; running the TUI without a real TTY will throw an Ink âRaw mode is not supportedâ error.
### Views
`AppContainer` (`src/tui/AppContainer.tsx`) has two views:
- Generate view (default)
- Test Runner view
---
## 10. TUI Keybindings
Authoritative sources:
- Global key router: `src/tui/app-container-keymap.ts`
- Help overlay text: `src/tui/help-config.ts`
### Global
- `Ctrl+G`: open command palette (and switch to Generate view if needed)
- `Ctrl+T`: switch to Test Runner view
- `/help`: open help overlay
- `Ctrl+C`: exit
- `Esc`: dismiss UI elements (never exits)
### Generate view
The Generate view is a âchat-likeâ screen:
- Type freeform text and press Enter to generate.
- Type `/` to enter command mode (command palette filtering).
- `â/â`: scroll history (when no popup/help is active)
Tab behavior (verified in `src/tui/screens/command/hooks/useContextPopupGlue.ts`):
- If the current input looks like a dropped absolute file path, `Tab` adds that file to context.
- Otherwise, `Tab` opens the `/series` flow.
### Test Runner view
Verified in `src/tui/screens/test-runner/TestRunnerScreen.tsx`:
- `Tab` / `Shift+Tab`: change focus
- `Enter` (file input): moves focus to actions
- `Enter` (actions): runs tests
---
## 11. TUI Commands (`/command` palette)
Commands are listed in the palette via `src/tui/config.ts`.
Execution is split:
- Popup-backed commands are handled by `src/tui/hooks/usePopupManager.ts`.
- Session commands like `/new` and `/reuse` are handled in the command screen layer (see `src/tui/screens/command/hooks/useSessionCommands.ts`).
### Command list and behavior
| Command                   | Args                     | Behavior                                                                                |
| ------------------------- | ------------------------ | --------------------------------------------------------------------------------------- |
| `/help`                   | -                        | Show keyboard shortcuts (help overlay)                                                  |
| `/model`                  | -                        | Open model picker popup (generation model)                                              |
| `/target`                 | -                        | Open target model picker popup                                                          |
| `/polish`                 | `off\|clear\|--clear`    | Clear polish model; otherwise open polish model picker                                  |
| `/intent`                 | `[path]`                 | Set intent file path; without args opens picker popup                                   |
| `/meta` / `/instructions` | `[text]`                 | Set meta instructions; without args opens editor popup                                  |
| `/new`                    | -                        | Reset session state (see `src/tui/screens/command/hooks/useSessionCommands.ts`)         |
| `/reuse`                  | -                        | Reset and reuse last prompt (see `src/tui/screens/command/hooks/useSessionCommands.ts`) |
| `/file`                   | -                        | Open file-context popup                                                                 |
| `/url`                    | `[url ...]`              | Add URLs directly (HTTP(S) only) or open popup                                          |
| `/smart`                  | `on\|off`                | Toggle smart context                                                                    |
| `/smart-root`             | `<path>\|--clear\|clear` | Set/clear smart context root (may auto-enable smart context)                            |
| `/image`                  | `[path]`                 | Attach an image path directly or open popup                                             |
| `/video`                  | `[path]`                 | Attach a video path directly or open popup                                              |
| `/copy`                   | `[on\|off]`              | Toggle auto-copy or open toggle popup                                                   |
| `/chatgpt`                | `[on\|off]`              | Toggle auto-open-chatgpt or open toggle popup                                           |
| `/json`                   | `[on\|off]`              | Toggle showing JSON payload in TUI history (blocked when transport active)              |
| `/tokens`                 | -                        | Open token breakdown popup                                                              |
| `/budgets`                | -                        | Configure token budgets and overflow strategy (popup)                                   |
| `/resume`                 | -                        | Resume generation from history or an exported payload (popup)                           |
| `/export`                 | -                        | Export a selected history payload to JSON/YAML (popup)                                  |
| `/settings`               | -                        | Open settings popup                                                                     |
| `/theme`                  | -                        | Open theme picker popup                                                                 |
| `/theme-mode`             | -                        | Open theme mode picker popup                                                            |
| `/reasoning` / `/why`     | -                        | Open last reasoning popup                                                               |
| `/history`                | -                        | Open command history popup                                                              |
| `/series`                 | `[draft text]`           | Start atomic prompt series flow (prefills from typed/last intent or intent file)        |
| `/test`                   | `[file]`                 | Run tests (with arg) or open test popup                                                 |
| `/exit`                   | -                        | Exit the app                                                                            |
Notes:
- `/theme-mode` currently **opens a popup**; it does not parse inline args (despite some older docs claiming `/theme-mode dark|light|system`). Implementation: `src/tui/hooks/usePopupManager.ts`.
- `/json` in the TUI controls whether the final JSON payload is shown in the history pane; it does not change the underlying generate-mode `--json` behavior.
---
## 12. TUI Themes
Theme implementation lives under `src/tui/theme/`.
### Theme discovery
Implementation: `src/tui/theme/theme-loader.ts`.
Themes are loaded from:
- Built-ins: `src/tui/theme/builtins/*`
- Global custom themes:
  - `~/.config/prompt-maker-cli/themes/*.json`
- Project themes (walk up from `process.cwd()`):
  - `<cwd-or-parent>/.prompt-maker-cli/themes/*.json`
Precedence (highest wins on name collision):
1. Project theme closest to the current working directory
2. Project themes in ancestor directories
3. Global themes
4. Built-in themes
If a custom theme overrides a built-in by name, the built-in label is kept.
### Theme JSON schema (enforced)
Validation is performed in `src/tui/theme/theme-loader.ts`.
- Required top-level keys:
  - `theme` (object)
  - `defs` (optional object)
- Required theme slots are enumerated in `src/tui/theme/theme-types.ts` (`REQUIRED_THEME_SLOTS`).
- Supported color value types:
  - string (hex, keyword, or reference)
  - number (ANSI 0â255)
  - variant object: `{ "dark": <value>, "light": <value> }`
Opencode compatibility:
- Some Opencode theme shapes are adapted on load (see `adaptOpencodeThemeJson` in `src/tui/theme/theme-loader.ts`).
### Persistence
Theme selection is persisted to CLI config (`src/config.ts`) via `updateCliThemeSettings`:
- `theme`: selected theme name
- `themeMode`: `dark` / `light` / `system`
Other TUI workflow defaults are also persisted to CLI config:
- Budgets (`promptGenerator.maxInputTokens`, `promptGenerator.maxContextTokens`, `promptGenerator.contextOverflowStrategy`)
- Resume defaults (`resumeMode`, `resumeSourceKind`)
- Export defaults (`exportFormat`, `exportOutDir`)
The theme provider loads and saves theme settings via `src/tui/theme/theme-settings-service.ts`.
---
## 13. TUI Architecture Map (for maintainers)
High-level module boundaries:
- Entrypoint + routing: `src/index.ts`
- Generate pipeline: `src/generate/pipeline.ts`
- Test runner: `src/test-command.ts`
- TUI entry: `src/tui/index.tsx`
- TUI app shell: `src/tui/AppContainer.tsx`
- Generate view (Command screen): `src/tui/screens/command/*`
- Test Runner view: `src/tui/screens/test-runner/*`
Key design invariants (reinforced by `AGENTS.md`):
- Input routing priority: Help overlay > Popups > Screen input > AppContainer global keys
- Reducers are kept pure and unit-testable (many tests live under `src/__tests__/tui/`)
---
## 14. TODOs / Known Quirks
These are verified behaviors that may surprise users:
- `prompt-maker-cli test --help` prints help but **still runs** the test suite afterwards.
  - Reason: `src/test-command.ts` parses args but does not special-case `--help`.
- `prompt-maker-cli ui --help` is not implemented; it will try to start Ink.
  - `src/tui/index.tsx` only parses `--interactive-transport`.
- `prompt-maker-cli --version` prints a version but does not exit the process.
  - If you run it without an intent, generate mode will then error with âIntent text is requiredâŚâ.
</file>
<file path="src/generate/types.ts">
import type { FileContext } from '../file-context'
import type { UploadDetail, UploadState } from '../prompt-generator-service'
export type StreamMode = 'none' | 'jsonl'
export type ResumeMode = 'strict' | 'best-effort'
export type ContextOverflowStrategy =
  | 'fail'
  | 'drop-smart'
  | 'drop-url'
  | 'drop-largest'
  | 'drop-oldest'
export type GenerateArgs = {
  intent?: string
  intentFile?: string
  model?: string
  target?: string
  interactive: boolean
  copy: boolean
  openChatGpt: boolean
  polish: boolean
  polishModel?: string
  json: boolean
  quiet: boolean
  progress: boolean
  stream: StreamMode
  resumeLast?: boolean
  resume?: string
  resumeFrom?: string
  resumeMode?: ResumeMode
  maxInputTokens?: number
  maxContextTokens?: number
  contextOverflow?: ContextOverflowStrategy
  showContext: boolean
  contextTemplate?: string
  contextFile?: string
  interactiveTransport?: string
  contextFormat: 'text' | 'json'
  help: boolean
  context: string[]
  urls: string[]
  images: string[]
  video: string[]
  pdf: string[]
  metaInstructions?: string
  smartContext: boolean
  smartContextRoot?: string
  inlineIntentAfterInteractive?: boolean
}
export type ContextPathMetadata = {
  path: string
  source: ContextPathSource
}
type ContextPathSource = 'intent' | 'file' | 'url' | 'smart'
export const GENERATE_JSON_PAYLOAD_SCHEMA_VERSION = '1' as const
export type GenerateJsonPayload = {
  schemaVersion: typeof GENERATE_JSON_PAYLOAD_SCHEMA_VERSION
  intent: string
  model: string
  targetModel: string
  prompt: string
  reasoning?: string
  metaInstructions?: string
  refinements: string[]
  iterations: number
  interactive: boolean
  timestamp: string
  contextPaths: ContextPathMetadata[]
  images?: string[]
  videos?: string[]
  pdfs?: string[]
  outputPath?: string
  polishedPrompt?: string
  polishModel?: string
  contextTemplate?: string
  renderedPrompt?: string
}
export type GeneratePipelineResult = {
  payload: GenerateJsonPayload
  telemetry: TokenTelemetry
  generatedPrompt: string
  reasoning?: string
  polishedPrompt?: string
  finalPrompt: string
  iterations: number
  model: string
  contextPaths: ContextPathMetadata[]
}
export type GeneratePipelineOptions = {
  onStreamEvent?: (event: StreamEventInput) => void
  interactiveDelegate?: InteractiveDelegate
}
export type InteractiveDelegate = {
  getNextAction: (context: {
    iteration: number
    currentPrompt: string
  }) => Promise<{ type: 'refine'; instruction: string } | { type: 'finish' }>
}
export type TokenTelemetry = {
  files: FileTokenSummary[]
  intentTokens: number
  fileTokens: number
  systemTokens: number
  totalTokens: number
}
type FileTokenSummary = {
  path: string
  tokens: number
}
export type LoopContext = {
  intent: string
  refinements: string[]
  model: string
  targetModel: string
  fileContext: FileContext[]
  images: string[]
  videos: string[]
  pdfs: string[]
  metaInstructions: string
}
export type InteractiveMode = 'transport' | 'tty' | 'none'
export type ProgressScope = 'url' | 'smart' | 'generate' | 'polish' | 'generic'
type StreamEventBase<EventName extends string, Payload extends object> = {
  event: EventName
  timestamp: string
} & Payload
type ResumeLoadedStreamEvent = StreamEventBase<
  'resume.loaded',
  {
    source: 'history' | 'file'
    reusedContextPaths: GenerateJsonPayload['contextPaths']
    missingContextPaths: GenerateJsonPayload['contextPaths']
  }
>
type ContextTelemetryStreamEvent = StreamEventBase<
  'context.telemetry',
  { telemetry: TokenTelemetry }
>
type ContextOverflowStreamEvent = StreamEventBase<
  'context.overflow',
  {
    strategy: ContextOverflowStrategy
    before: TokenTelemetry
    after: TokenTelemetry
    droppedPaths: ContextPathMetadata[]
  }
>
type ProgressStreamEvent = StreamEventBase<
  'progress.update',
  {
    label: string
    state: 'start' | 'update' | 'stop'
    scope?: ProgressScope
  }
>
type UploadStreamEvent = StreamEventBase<
  'upload.state',
  { state: UploadState; detail: UploadDetail }
>
type GenerationIterationStartEvent = StreamEventBase<
  'generation.iteration.start',
  {
    iteration: number
    intent: string
    model: string
    interactive: boolean
    inputTokens: number
    refinements: string[]
    latestRefinement?: string
  }
>
type GenerationIterationCompleteEvent = StreamEventBase<
  'generation.iteration.complete',
  {
    iteration: number
    prompt: string
    tokens: number
    reasoningTokens?: number
  }
>
type InteractiveMilestoneStreamEvent = StreamEventBase<
  'interactive.state',
  {
    phase: 'start' | 'prompt' | 'refine' | 'complete'
    iteration: number
  }
>
type InteractiveAwaitingStreamEvent = StreamEventBase<
  'interactive.awaiting',
  { mode: InteractiveMode }
>
export type TransportListeningEvent = StreamEventBase<'transport.listening', { path: string }>
export type TransportClientConnectedEvent = StreamEventBase<
  'transport.client.connected',
  { status: 'connected' }
>
export type TransportClientDisconnectedEvent = StreamEventBase<
  'transport.client.disconnected',
  { status: 'disconnected' }
>
export type TransportEvent =
  | TransportListeningEvent
  | TransportClientConnectedEvent
  | TransportClientDisconnectedEvent
type GenerationFinalStreamEvent = StreamEventBase<
  'generation.final',
  { result: GenerateJsonPayload }
>
type StreamEvent =
  | ResumeLoadedStreamEvent
  | ContextTelemetryStreamEvent
  | ContextOverflowStreamEvent
  | ProgressStreamEvent
  | UploadStreamEvent
  | GenerationIterationStartEvent
  | GenerationIterationCompleteEvent
  | InteractiveMilestoneStreamEvent
  | InteractiveAwaitingStreamEvent
  | TransportEvent
  | GenerationFinalStreamEvent
export type StreamEventInput = {
  [EventName in StreamEvent['event']]: Omit<Extract<StreamEvent, { event: EventName }>, 'timestamp'>
}[StreamEvent['event']]
export type TransportLifecycleEventInput = Extract<
  StreamEventInput,
  {
    event: 'transport.listening' | 'transport.client.connected' | 'transport.client.disconnected'
  }
>
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenPopupAndView.types.ts">
import type { WriteStream } from 'node:tty'
import type { MutableRefObject } from 'react'
import type { NotifyOptions } from '../../../notifier'
import type { HistoryEntry, ModelOption, ProviderStatusMap } from '../../../types'
export type PushHistory = (
  content: string,
  kind?: HistoryEntry['kind'],
  format?: HistoryEntry['format'],
) => void
export type CommandContextOptions = {
  interactiveTransportPath?: string | undefined
  notify: (message: string, options?: NotifyOptions) => void
  stdout: WriteStream | undefined
  // context state
  files: string[]
  urls: string[]
  images: string[]
  videos: string[]
  pdfs: string[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  metaInstructions: string
  maxContextTokens: number | null
  maxInputTokens: number | null
  contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
  lastReasoning: string | null
  lastGeneratedPrompt: string | null
  // context dispatch
  addFile: (value: string) => void
  removeFile: (index: number) => void
  addUrl: (value: string) => void
  removeUrl: (index: number) => void
  updateUrl: (index: number, value: string) => void
  addImage: (value: string) => void
  removeImage: (index: number) => void
  addVideo: (value: string) => void
  removeVideo: (index: number) => void
  addPdf: (value: string) => void
  removePdf: (index: number) => void
  toggleSmartContext: () => void
  setSmartRoot: (value: string) => void
  setMetaInstructions: (value: string) => void
  setBudgets: (value: {
    maxContextTokens: number | null
    maxInputTokens: number | null
    contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
  }) => void
  resetContext: () => void
}
export type CommandInputOptions = {
  // screen state
  terminalRows: number
  terminalColumns: number
  inputValue: string
  isPasteActive: boolean
  commandSelectionIndex: number
  debugKeyLine: string | null
  debugKeysEnabled: boolean
  setTerminalSize: (rows: number, columns: number) => void
  setInputValue: (value: string | ((prev: string) => string)) => void
  setPasteActive: (active: boolean) => void
  setCommandSelectionIndex: (next: number | ((prev: number) => number)) => void
  // input local
  intentFilePath: string
  setIntentFilePath: (value: string) => void
  copyEnabled: boolean
  setCopyEnabled: (value: boolean) => void
  chatGptEnabled: boolean
  setChatGptEnabled: (value: boolean) => void
  jsonOutputEnabled: boolean
  setJsonOutputEnabled: (value: boolean) => void
  // refs
  lastUserIntentRef: MutableRefObject<string | null>
  lastTypedIntentRef: MutableRefObject<string>
  // suppression
  consumeSuppressedTextInputChange: () => boolean
  suppressNextInput: () => void
  updateLastTypedIntent: (next: string) => void
  onDebugKeyEvent: (
    event: import('../../../components/core/MultilineTextInput').DebugKeyEvent,
  ) => void
}
export type CommandPopupOptions = {
  onPopupVisibilityChange?: ((isOpen: boolean) => void) | undefined
  commandMenuSignal?: number | undefined
  helpOpen: boolean
  reservedRows: number
  onOpenHelp?: (() => void) | undefined
}
export type CommandHistoryOptions = {
  // history/test plumbing
  pushHistoryRef: MutableRefObject<PushHistory>
  pushHistoryProxy: PushHistory
  clearHistoryRef: MutableRefObject<() => void>
  scrollToRef: MutableRefObject<(row: number) => void>
  scrollToProxy: (row: number) => void
  closeTestPopupRef: MutableRefObject<() => void>
  commandHistoryValues: string[]
  addCommandHistoryEntry: (value: string) => void
  isTestCommandRunning: boolean
  lastTestFile: string | null
  runTestsFromCommandProxy: (value: string) => void
  onTestPopupSubmit: (value: string) => void
}
export type CommandGenerationOptions = {
  // model/generation
  currentModel: ModelOption['id']
  polishModelId: ModelOption['id'] | null
  currentTargetModel: ModelOption['id']
  modelOptions: ModelOption[]
  providerStatuses: ProviderStatusMap
  selectModel: (nextId: ModelOption['id']) => void
  selectPolishModel: (nextId: ModelOption['id'] | null) => void
  selectTargetModel: (nextId: ModelOption['id']) => void
  isGenerating: boolean
  runGeneration: (payload: {
    intent?: string
    intentFile?: string
    resume?:
      | { kind: 'history'; selector: string; mode: import('../../../types').ResumeMode }
      | { kind: 'file'; payloadPath: string; mode: import('../../../types').ResumeMode }
  }) => Promise<void>
  runSeriesGeneration: (intent: string) => void
  statusChips: string[]
  isAwaitingRefinement: boolean
  submitRefinement: (value: string) => void
  awaitingInteractiveMode:
    | import('../../../generation-pipeline-reducer').InteractiveAwaitingMode
    | null
  tokenUsageRun: import('../../../token-usage-store').TokenUsageRun | null
  tokenUsageBreakdown: import('../../../token-usage-store').TokenUsageBreakdown | null
  latestContextOverflow:
    | import('../../../generation-pipeline-reducer').ContextOverflowDetails
    | null
}
export type UseCommandScreenPopupAndViewOptions = {
  context: CommandContextOptions
  input: CommandInputOptions
  popup: CommandPopupOptions
  history: CommandHistoryOptions
  generation: CommandGenerationOptions
}
export type UseCommandScreenPopupAndViewResult = {
  transportMessage: string | null
  historyPaneProps: Parameters<
    typeof import('./useCommandScreenViewModel').useCommandScreenViewModel
  >[0]['panes']['history']
  popupAreaProps: ReturnType<
    typeof import('./useCommandScreenViewModel').useCommandScreenViewModel
  >['popupAreaProps']
  commandMenuPaneProps: Parameters<
    typeof import('./useCommandScreenViewModel').useCommandScreenViewModel
  >[0]['panes']['menu']
  commandInputProps: ReturnType<
    typeof import('./useCommandScreenViewModel').useCommandScreenViewModel
  >['commandInputProps']
}
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenPopupManager.ts">
import { useApp, useStdout } from 'ink'
import { useCallback, useEffect, useState } from 'react'
import { loadCliConfig } from '../../../../config'
import { usePopupManager } from '../../../hooks/usePopupManager'
import type { NotifyOptions } from '../../../notifier'
import { useTheme } from '../../../theme/theme-provider'
import type {
  HistoryEntry,
  ModelOption,
  PopupState,
  ResumeMode,
  ResumeSourceKind,
} from '../../../types'
const DEFAULT_TEST_FILE = 'prompt-tests.yaml'
type PushHistory = (
  content: string,
  kind?: HistoryEntry['kind'],
  format?: HistoryEntry['format'],
) => void
type UseCommandScreenPopupManagerOptions = {
  currentModel: ModelOption['id']
  currentTargetModel: ModelOption['id']
  modelOptions: readonly ModelOption[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  toggleSmartContext: () => void
  setSmartRoot: (value: string) => void
  urls: string[]
  addUrl: (value: string) => void
  images: string[]
  videos: string[]
  pdfs: string[]
  addImage: (value: string) => void
  addVideo: (value: string) => void
  addPdf: (value: string) => void
  lastTestFile: string | null
  interactiveTransportPath?: string | undefined
  isGenerating: boolean
  lastUserIntentRef: import('react').MutableRefObject<string | null>
  lastTypedIntentRef: import('react').MutableRefObject<string>
  pushHistoryProxy: PushHistory
  notify: (message: string, options?: NotifyOptions) => void
  setInputValue: (value: string | ((prev: string) => string)) => void
  runGeneration: (payload: {
    intent?: string
    intentFile?: string
    resume?:
      | { kind: 'history'; selector: string; mode: ResumeMode }
      | { kind: 'file'; payloadPath: string; mode: ResumeMode }
  }) => Promise<void>
  runSeriesGeneration: (intent: string) => void
  runTestsFromCommandProxy: (value: string) => void
  setCurrentModel: (value: ModelOption['id']) => void
  setCurrentTargetModel: (value: ModelOption['id']) => void
  setPolishModelId: (value: ModelOption['id'] | null) => void
  setCopyEnabled: (value: boolean) => void
  setChatGptEnabled: (value: boolean) => void
  setJsonOutputEnabled: (value: boolean) => void
  intentFilePath: string
  setIntentFilePath: (value: string) => void
  metaInstructions: string
  setMetaInstructions: (value: string) => void
  budgets: {
    maxContextTokens: number | null
    maxInputTokens: number | null
    contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
  }
  setBudgets: (value: {
    maxContextTokens: number | null
    maxInputTokens: number | null
    contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
  }) => void
  polishModelId: ModelOption['id'] | null
  copyEnabled: boolean
  chatGptEnabled: boolean
  jsonOutputEnabled: boolean
}
export type UseCommandScreenPopupManagerResult = {
  popupState: PopupState
  setPopupState: import('react').Dispatch<import('react').SetStateAction<PopupState>>
  actions: ReturnType<typeof usePopupManager>['actions']
  isPopupOpen: boolean
}
export const useCommandScreenPopupManager = ({
  currentModel,
  currentTargetModel,
  modelOptions,
  smartContextEnabled,
  smartContextRoot,
  toggleSmartContext,
  setSmartRoot,
  urls,
  addUrl,
  images,
  videos,
  pdfs,
  addImage,
  addVideo,
  addPdf,
  lastTestFile,
  interactiveTransportPath,
  isGenerating,
  lastUserIntentRef,
  lastTypedIntentRef,
  pushHistoryProxy,
  notify,
  setInputValue,
  runGeneration,
  runSeriesGeneration,
  runTestsFromCommandProxy,
  setCurrentModel,
  setCurrentTargetModel,
  setPolishModelId,
  setCopyEnabled,
  setChatGptEnabled,
  setJsonOutputEnabled,
  intentFilePath,
  setIntentFilePath,
  metaInstructions,
  setMetaInstructions,
  budgets,
  setBudgets,
  polishModelId,
  copyEnabled,
  chatGptEnabled,
  jsonOutputEnabled,
}: UseCommandScreenPopupManagerOptions): UseCommandScreenPopupManagerResult => {
  const { exit } = useApp()
  const { stdout } = useStdout()
  const clearScreen = useCallback(() => {
    if (stdout && stdout.isTTY) {
      stdout.write('\u001b[2J\u001b[H')
    }
  }, [stdout])
  const getLatestTypedIntent = useCallback(() => {
    const trimmed = lastTypedIntentRef.current.trim()
    return trimmed.length > 0 ? trimmed : null
  }, [lastTypedIntentRef])
  const syncTypedIntentRef = useCallback(
    (intent: string) => {
      lastTypedIntentRef.current = intent
    },
    [lastTypedIntentRef],
  )
  const { activeThemeName, mode, themes } = useTheme()
  const [resumeDefaults, setResumeDefaults] = useState<{
    sourceKind: ResumeSourceKind
    mode: ResumeMode
  }>({
    sourceKind: 'history',
    mode: 'best-effort',
  })
  const [exportDefaults, setExportDefaults] = useState<{
    format: 'json' | 'yaml'
    outDir: string | null
  }>({
    format: 'json',
    outDir: null,
  })
  useEffect(() => {
    let cancelled = false
    const hydrate = async (): Promise<void> => {
      const config = await loadCliConfig().catch(() => null)
      if (cancelled) {
        return
      }
      const sourceKind = config?.resumeSourceKind === 'file' ? 'file' : 'history'
      const resumeMode = config?.resumeMode === 'strict' ? 'strict' : 'best-effort'
      setResumeDefaults({ sourceKind, mode: resumeMode })
      const exportFormat = config?.exportFormat === 'yaml' ? 'yaml' : 'json'
      const exportOutDir = config?.exportOutDir?.trim() || null
      setExportDefaults({ format: exportFormat, outDir: exportOutDir })
    }
    void hydrate()
    return () => {
      cancelled = true
    }
  }, [])
  const popupManager = usePopupManager({
    currentModel,
    currentTargetModel,
    modelOptions,
    activeThemeName,
    themeMode: mode,
    themes: themes.map((theme) => ({ name: theme.name, label: theme.label })),
    smartContextEnabled,
    smartContextRoot,
    toggleSmartContext,
    setSmartRoot,
    urls,
    addUrl,
    images,
    videos,
    pdfs,
    addImage,
    addVideo,
    addPdf,
    lastTestFile,
    defaultTestFile: DEFAULT_TEST_FILE,
    ...(interactiveTransportPath ? { interactiveTransportPath } : {}),
    isGenerating,
    lastUserIntentRef,
    pushHistory: pushHistoryProxy,
    notify,
    setInputValue,
    runGeneration,
    runSeriesGeneration,
    runTestsFromCommand: runTestsFromCommandProxy,
    clearScreen,
    exitApp: exit,
    setCurrentModel,
    setCurrentTargetModel,
    setPolishModelId,
    setCopyEnabled,
    setChatGptEnabled,
    setJsonOutputEnabled,
    setIntentFilePath,
    intentFilePath,
    metaInstructions,
    setMetaInstructions,
    budgets,
    setBudgets,
    polishModelId,
    copyEnabled,
    chatGptEnabled,
    jsonOutputEnabled,
    getLatestTypedIntent,
    syncTypedIntentRef,
    resumeDefaults,
    setResumeDefaults,
    exportDefaults,
    setExportDefaults,
  })
  return {
    popupState: popupManager.popupState,
    setPopupState: popupManager.setPopupState,
    actions: popupManager.actions,
    isPopupOpen: popupManager.popupState !== null,
  }
}
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenBindings.ts">
import { POPUP_HEIGHTS } from '../../../config'
import { useStableCallback } from '../../../hooks/useStableCallback'
import { useCommandScreenChips } from './useCommandScreenChips'
import { useCommandScreenPopupBindings } from './useCommandScreenPopupBindings'
import { useCommandScreenPopupManager } from './useCommandScreenPopupManager'
import { useCommandScreenPopupVisibility } from './useCommandScreenPopupVisibility'
import { useCommandScreenShell } from './useCommandScreenShell'
import { useDroppedFilePath } from './useDroppedFilePath'
import type {
  PushHistory,
  UseCommandScreenPopupAndViewOptions,
} from './useCommandScreenPopupAndView.types'
export type UseCommandScreenBindingsResult = {
  popupManager: ReturnType<typeof useCommandScreenPopupManager>
  shell: ReturnType<typeof useCommandScreenShell>
  bindings: ReturnType<typeof useCommandScreenPopupBindings>
  enhancedStatusChips: string[]
}
export const useCommandScreenBindings = (
  options: UseCommandScreenPopupAndViewOptions,
): UseCommandScreenBindingsResult => {
  const { context, input, popup, history, generation } = options
  const popupManager = useCommandScreenPopupManager({
    currentModel: generation.currentModel,
    polishModelId: generation.polishModelId,
    currentTargetModel: generation.currentTargetModel,
    modelOptions: generation.modelOptions,
    smartContextEnabled: context.smartContextEnabled,
    smartContextRoot: context.smartContextRoot,
    toggleSmartContext: context.toggleSmartContext,
    setSmartRoot: context.setSmartRoot,
    urls: context.urls,
    addUrl: context.addUrl,
    images: context.images,
    videos: context.videos,
    pdfs: context.pdfs,
    addImage: context.addImage,
    addVideo: context.addVideo,
    addPdf: context.addPdf,
    lastTestFile: history.lastTestFile,
    ...(context.interactiveTransportPath
      ? { interactiveTransportPath: context.interactiveTransportPath }
      : {}),
    isGenerating: generation.isGenerating,
    lastUserIntentRef: input.lastUserIntentRef,
    lastTypedIntentRef: input.lastTypedIntentRef,
    pushHistoryProxy: history.pushHistoryProxy,
    notify: context.notify,
    setInputValue: input.setInputValue,
    runGeneration: generation.runGeneration,
    runSeriesGeneration: generation.runSeriesGeneration,
    runTestsFromCommandProxy: history.runTestsFromCommandProxy,
    setCurrentModel: generation.selectModel,
    setCurrentTargetModel: generation.selectTargetModel,
    setPolishModelId: generation.selectPolishModel,
    setCopyEnabled: input.setCopyEnabled,
    setChatGptEnabled: input.setChatGptEnabled,
    setJsonOutputEnabled: input.setJsonOutputEnabled,
    intentFilePath: input.intentFilePath,
    setIntentFilePath: input.setIntentFilePath,
    metaInstructions: context.metaInstructions,
    setMetaInstructions: context.setMetaInstructions,
    budgets: {
      maxContextTokens: context.maxContextTokens,
      maxInputTokens: context.maxInputTokens,
      contextOverflowStrategy: context.contextOverflowStrategy,
    },
    setBudgets: context.setBudgets,
    copyEnabled: input.copyEnabled,
    chatGptEnabled: input.chatGptEnabled,
    jsonOutputEnabled: input.jsonOutputEnabled,
  })
  history.closeTestPopupRef.current = () => {
    popupManager.setPopupState((prev) => (prev?.type === 'test' ? null : prev))
  }
  useCommandScreenPopupVisibility({
    isPopupOpen: popupManager.isPopupOpen,
    onPopupVisibilityChange: popup.onPopupVisibilityChange,
  })
  const pushHistory: PushHistory = useStableCallback((content, kind) => {
    history.pushHistoryRef.current(content, kind)
  })
  const droppedFilePath = useDroppedFilePath(input.inputValue)
  const shell = useCommandScreenShell({
    stdout: context.stdout,
    setTerminalSize: input.setTerminalSize,
    ...(context.interactiveTransportPath
      ? { interactiveTransportPath: context.interactiveTransportPath }
      : {}),
    terminalRows: input.terminalRows,
    inputValue: input.inputValue,
    debugKeyLine: input.debugKeyLine,
    debugKeysEnabled: input.debugKeysEnabled,
    helpOpen: popup.helpOpen,
    reservedRows: popup.reservedRows,
    popupState: popupManager.popupState,
    isPopupOpen: popupManager.isPopupOpen,
    setPopupState: popupManager.setPopupState,
    ...(popup.commandMenuSignal !== undefined
      ? { commandMenuSignal: popup.commandMenuSignal }
      : {}),
    commandSelectionIndex: input.commandSelectionIndex,
    setCommandSelectionIndex: input.setCommandSelectionIndex,
    isGenerating: generation.isGenerating,
    awaitingInteractiveMode: generation.awaitingInteractiveMode,
    files: context.files,
    urls: context.urls,
    lastGeneratedPrompt: context.lastGeneratedPrompt,
    resetContext: context.resetContext,
    lastUserIntentRef: input.lastUserIntentRef,
    lastTypedIntentRef: input.lastTypedIntentRef,
    setInputValue: input.setInputValue,
    setIntentFilePath: input.setIntentFilePath,
    setMetaInstructions: context.setMetaInstructions,
    scrollToRef: history.scrollToRef,
    clearHistoryRef: history.clearHistoryRef,
    pushHistoryRef: history.pushHistoryRef,
    scrollToProxy: history.scrollToProxy,
  })
  const { enhancedStatusChips } = useCommandScreenChips({
    currentModel: generation.currentModel,
    providerStatuses: generation.providerStatuses,
    statusChips: generation.statusChips,
    intentFilePath: input.intentFilePath,
    metaInstructions: context.metaInstructions,
  })
  const notify = useStableCallback((message: string) => {
    context.notify(message)
  })
  const bindings = useCommandScreenPopupBindings({
    input: {
      value: input.inputValue,
      setValue: input.setInputValue,
      setPasteActive: input.setPasteActive,
      consumeSuppressedTextInputChange: input.consumeSuppressedTextInputChange,
      suppressNextInput: input.suppressNextInput,
      updateLastTypedIntent: input.updateLastTypedIntent,
      intentFilePath: input.intentFilePath,
      lastUserIntentRef: input.lastUserIntentRef,
    },
    popup: {
      state: popupManager.popupState,
      setState: popupManager.setPopupState,
      isOpen: popupManager.isPopupOpen,
      helpOpen: popup.helpOpen,
      ...(popup.onOpenHelp ? { openHelp: popup.onOpenHelp } : {}),
      close: popupManager.actions.closePopup,
      actions: {
        handleCommandSelection: popupManager.actions.handleCommandSelection,
        handleModelPopupSubmit: popupManager.actions.handleModelPopupSubmit,
        applyToggleSelection: popupManager.actions.applyToggleSelection,
        handleIntentFileSubmit: popupManager.actions.handleIntentFileSubmit,
        handleResumeSubmit: popupManager.actions.handleResumeSubmit,
        handleExportSubmit: popupManager.actions.handleExportSubmit,
        handleBudgetsSubmit: popupManager.actions.handleBudgetsSubmit,
        handleSeriesIntentSubmit: popupManager.actions.handleSeriesIntentSubmit,
      },
    },
    menu: {
      isActive: shell.isCommandMenuActive,
      selectedCommandId: shell.selectedCommand?.id ?? null,
      argsRaw: shell.commandMenuArgsRaw,
      isCommandMode: shell.isCommandMode,
      actions: {
        handleNewCommand: shell.handleNewCommand,
        handleReuseCommand: shell.handleReuseCommand,
      },
    },
    generation: {
      isGenerating: generation.isGenerating,
      isAwaitingRefinement: generation.isAwaitingRefinement,
      submitRefinement: generation.submitRefinement,
      runGeneration: generation.runGeneration,
    },
    history: {
      pushHistory,
      addCommandHistoryEntry: history.addCommandHistoryEntry,
      commandHistoryValues: history.commandHistoryValues,
    },
    context: {
      droppedFilePath,
      files: context.files,
      urls: context.urls,
      images: context.images,
      videos: context.videos,
      pdfs: context.pdfs,
      smartContextEnabled: context.smartContextEnabled,
      smartContextRoot: context.smartContextRoot,
      addFile: context.addFile,
      removeFile: context.removeFile,
      addUrl: context.addUrl,
      removeUrl: context.removeUrl,
      updateUrl: context.updateUrl,
      addImage: context.addImage,
      removeImage: context.removeImage,
      addVideo: context.addVideo,
      removeVideo: context.removeVideo,
      addPdf: context.addPdf,
      removePdf: context.removePdf,
      toggleSmartContext: context.toggleSmartContext,
      setSmartRoot: context.setSmartRoot,
      notify,
      modelOptions: generation.modelOptions,
      lastReasoning: context.lastReasoning,
      terminalColumns: input.terminalColumns,
      reasoningPopupHeight: POPUP_HEIGHTS.reasoning,
    },
  })
  return {
    popupManager,
    shell,
    bindings,
    enhancedStatusChips,
  }
}
</file>
<file path="src/tui/popup-reducer.ts">
/*
 * Popup reducer (pure state transitions).
 *
 * This file intentionally contains *no Ink/React runtime imports*.
 *
 * Why a reducer?
 * - Popups behave like a small state machine: open, close, update fields, and
 *   sometimes receive async data (workspace scan suggestions).
 * - Reducers make transitions explicit and testable: (state, action) => nextState.
 *
 * Important behavior we preserve:
 * - `CommandScreen` and popup components still call `setPopupState(prev => ...)`.
 *   We model that as a `set` action that can optionally keep active scan state
 *   when the popup type does not change.
 * - Async scan results must not overwrite newer popups.
 */
import type {
  PopupState,
  ResumeMode,
  ResumeSourceKind,
  ResumeHistoryItem,
  ExportHistoryItem,
  ToggleField,
} from './types'
import type { ThemeMode } from './theme/theme-types'
// Lightweight replacement for React's SetStateAction type.
// Keeping it local avoids importing React into a pure helper.
export type SetStateAction<State> = State | ((prev: State) => State)
export type PopupScanKind = 'file' | 'image' | 'video' | 'pdf' | 'smart' | 'intent' | 'resume'
export type PopupManagerState = {
  popupState: PopupState
  activeScan: { kind: PopupScanKind; id: number } | null
}
export type PopupAction =
  | { type: 'set'; next: SetStateAction<PopupState> }
  | { type: 'close' }
  | {
      type: 'open-model'
      kind: 'generation' | 'polish' | 'target'
      query: string
      selectionIndex: number
    }
  | { type: 'open-toggle'; field: ToggleField; selectionIndex: number }
  | { type: 'open-file'; scanId: number }
  | { type: 'open-url' }
  | { type: 'open-image'; scanId: number }
  | { type: 'open-video'; scanId: number }
  | { type: 'open-pdf'; scanId: number }
  | { type: 'open-history' }
  | {
      type: 'open-resume'
      scanId: number | null
      sourceKind: ResumeSourceKind
      mode: ResumeMode
      payloadPathDraft: string
      historyItems: ResumeHistoryItem[]
      historySelectionIndex: number
      historyErrorMessage: string | null
    }
  | {
      type: 'open-export'
      format: 'json' | 'yaml'
      outPathDraft: string
      historyItems: ExportHistoryItem[]
      historySelectionIndex: number
      historyErrorMessage: string | null
    }
  | { type: 'open-smart'; scanId: number; draft: string }
  | { type: 'open-tokens' }
  | {
      type: 'open-budgets'
      maxContextTokens: number | null
      maxInputTokens: number | null
      contextOverflowStrategy: import('../config').ContextOverflowStrategy | null
    }
  | { type: 'open-settings' }
  | { type: 'open-theme'; selectionIndex: number; initialThemeName: string }
  | {
      type: 'open-theme-mode'
      selectionIndex: number
      initialMode: ThemeMode
    }
  | { type: 'open-reasoning'; scrollOffset: number }
  | { type: 'open-test'; draft: string }
  | { type: 'open-intent'; scanId: number; draft: string }
  | { type: 'open-instructions'; draft: string }
  | { type: 'open-series'; draft: string; hint: string }
  | { type: 'scan-suggestions-success'; kind: PopupScanKind; scanId: number; suggestions: string[] }
export const INITIAL_POPUP_MANAGER_STATE: PopupManagerState = { popupState: null, activeScan: null }
const buildFilePopupState = (): PopupState => ({
  type: 'file',
  draft: '',
  selectionIndex: 0,
  selectedFocused: false,
  suggestedItems: [],
  suggestedSelectionIndex: 0,
  suggestedFocused: false,
})
const buildImagePopupState = (): PopupState => ({
  type: 'image',
  draft: '',
  selectionIndex: 0,
  selectedFocused: false,
  suggestedItems: [],
  suggestedSelectionIndex: 0,
  suggestedFocused: false,
})
const buildVideoPopupState = (): PopupState => ({
  type: 'video',
  draft: '',
  selectionIndex: 0,
  selectedFocused: false,
  suggestedItems: [],
  suggestedSelectionIndex: 0,
  suggestedFocused: false,
})
const buildPdfPopupState = (): PopupState => ({
  type: 'pdf',
  draft: '',
  selectionIndex: 0,
  selectedFocused: false,
  suggestedItems: [],
  suggestedSelectionIndex: 0,
  suggestedFocused: false,
})
const buildSmartPopupState = (draft: string): PopupState => ({
  type: 'smart',
  draft,
  suggestedItems: [],
  suggestedSelectionIndex: 0,
  suggestedFocused: false,
})
const buildIntentPopupState = (draft: string): PopupState => ({
  type: 'intent',
  draft,
  suggestedItems: [],
  suggestedSelectionIndex: 0,
  suggestedFocused: false,
})
const applySuggestions = (
  popupState: PopupState,
  kind: PopupScanKind,
  suggestions: string[],
): PopupState => {
  if (kind === 'file' && popupState?.type === 'file') {
    return {
      ...popupState,
      suggestedItems: suggestions,
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    }
  }
  if (kind === 'image' && popupState?.type === 'image') {
    return {
      ...popupState,
      suggestedItems: suggestions,
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    }
  }
  if (kind === 'video' && popupState?.type === 'video') {
    return {
      ...popupState,
      suggestedItems: suggestions,
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    }
  }
  if (kind === 'pdf' && popupState?.type === 'pdf') {
    return {
      ...popupState,
      suggestedItems: suggestions,
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    }
  }
  if (kind === 'smart' && popupState?.type === 'smart') {
    return {
      ...popupState,
      suggestedItems: suggestions,
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    }
  }
  if (kind === 'intent' && popupState?.type === 'intent') {
    return {
      ...popupState,
      suggestedItems: suggestions,
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    }
  }
  if (kind === 'resume' && popupState?.type === 'resume') {
    return {
      ...popupState,
      suggestedItems: suggestions,
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    }
  }
  return popupState
}
export const popupReducer = (state: PopupManagerState, action: PopupAction): PopupManagerState => {
  switch (action.type) {
    case 'set': {
      const next = action.next
      const nextPopupState = typeof next === 'function' ? next(state.popupState) : next
      // Preserve ongoing scan state only if the popup type stays the same.
      // If we switch popup types (file -> smart), any in-flight scan becomes stale.
      const previousType = state.popupState?.type ?? null
      const nextType = nextPopupState?.type ?? null
      return {
        popupState: nextPopupState,
        activeScan: previousType === nextType ? state.activeScan : null,
      }
    }
    case 'close':
      return INITIAL_POPUP_MANAGER_STATE
    case 'open-model':
      return {
        popupState: {
          type: 'model',
          kind: action.kind,
          query: action.query,
          selectionIndex: action.selectionIndex,
        },
        activeScan: null,
      }
    case 'open-toggle':
      return {
        popupState: { type: 'toggle', field: action.field, selectionIndex: action.selectionIndex },
        activeScan: null,
      }
    case 'open-file':
      return { popupState: buildFilePopupState(), activeScan: { kind: 'file', id: action.scanId } }
    case 'open-url':
      return {
        popupState: {
          type: 'url',
          draft: '',
          selectionIndex: 0,
          selectedFocused: false,
          editingIndex: null,
        },
        activeScan: null,
      }
    case 'open-image':
      return {
        popupState: buildImagePopupState(),
        activeScan: { kind: 'image', id: action.scanId },
      }
    case 'open-video':
      return {
        popupState: buildVideoPopupState(),
        activeScan: { kind: 'video', id: action.scanId },
      }
    case 'open-pdf':
      return {
        popupState: buildPdfPopupState(),
        activeScan: { kind: 'pdf', id: action.scanId },
      }
    case 'open-history':
      return { popupState: { type: 'history', draft: '', selectionIndex: 0 }, activeScan: null }
    case 'open-resume':
      return {
        popupState: {
          type: 'resume',
          selectionIndex: 0,
          sourceKind: action.sourceKind,
          mode: action.mode,
          historyItems: action.historyItems,
          historySelectionIndex: action.historySelectionIndex,
          historyErrorMessage: action.historyErrorMessage,
          payloadPathDraft: action.payloadPathDraft,
          suggestedItems: [],
          suggestedSelectionIndex: 0,
          suggestedFocused: false,
        },
        activeScan: action.scanId === null ? null : { kind: 'resume', id: action.scanId },
      }
    case 'open-export':
      return {
        popupState: {
          type: 'export',
          selectionIndex: 0,
          historyItems: action.historyItems,
          historySelectionIndex: action.historySelectionIndex,
          historyErrorMessage: action.historyErrorMessage,
          format: action.format,
          outPathDraft: action.outPathDraft,
        },
        activeScan: null,
      }
    case 'open-smart':
      return {
        popupState: buildSmartPopupState(action.draft),
        activeScan: { kind: 'smart', id: action.scanId },
      }
    case 'open-tokens':
      return { popupState: { type: 'tokens' }, activeScan: null }
    case 'open-budgets':
      return {
        popupState: {
          type: 'budgets',
          selectionIndex: 0,
          maxContextTokensDraft:
            action.maxContextTokens === null ? '' : String(action.maxContextTokens),
          maxInputTokensDraft: action.maxInputTokens === null ? '' : String(action.maxInputTokens),
          contextOverflowStrategyDraft: action.contextOverflowStrategy ?? '',
          errorMessage: null,
        },
        activeScan: null,
      }
    case 'open-settings':
      return { popupState: { type: 'settings' }, activeScan: null }
    case 'open-theme':
      return {
        popupState: {
          type: 'theme',
          selectionIndex: action.selectionIndex,
          initialThemeName: action.initialThemeName,
        },
        activeScan: null,
      }
    case 'open-theme-mode':
      return {
        popupState: {
          type: 'themeMode',
          selectionIndex: action.selectionIndex,
          initialMode: action.initialMode,
        },
        activeScan: null,
      }
    case 'open-reasoning':
      return {
        popupState: { type: 'reasoning', scrollOffset: action.scrollOffset },
        activeScan: null,
      }
    case 'open-test':
      return { popupState: { type: 'test', draft: action.draft }, activeScan: null }
    case 'open-intent':
      return {
        popupState: buildIntentPopupState(action.draft),
        activeScan: { kind: 'intent', id: action.scanId },
      }
    case 'open-instructions':
      return { popupState: { type: 'instructions', draft: action.draft }, activeScan: null }
    case 'open-series':
      return {
        popupState: { type: 'series', draft: action.draft, hint: action.hint },
        activeScan: null,
      }
    case 'scan-suggestions-success': {
      if (state.activeScan?.kind !== action.kind || state.activeScan.id !== action.scanId) {
        return state
      }
      return {
        popupState: applySuggestions(state.popupState, action.kind, action.suggestions),
        activeScan: null,
      }
    }
    default:
      return state
  }
}
</file>
<file path="src/__tests__/usePopupManager.test.ts">
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import type { MutableRefObject } from 'react'
import { resetRecentSessionModelsForTests } from '../tui/model-session'
import { usePopupManager } from '../tui/hooks/usePopupManager'
import type { UsePopupManagerOptions } from '../tui/hooks/usePopupManager'
import type { ModelOption } from '../tui/types'
jest.mock('../tui/file-suggestions', () => ({
  discoverDirectorySuggestions: jest.fn(),
  discoverFileSuggestions: jest.fn(),
}))
jest.mock('node:fs/promises', () => ({
  readFile: jest.fn(),
}))
const dom = new JSDOM('<!doctype html><html><body></body></html>')
const globalScope = globalThis as typeof globalThis & {
  window: Window & typeof globalThis
  document: Document
  navigator: Navigator
}
globalScope.window = dom.window
globalScope.document = dom.window.document
globalScope.navigator = dom.window.navigator
beforeEach(() => {
  resetRecentSessionModelsForTests()
})
const defaultModelOptions: ModelOption[] = [
  {
    id: 'gpt-4o-mini',
    label: 'gpt-4o-mini',
    provider: 'openai',
    description: 'test',
    capabilities: [],
    source: 'builtin',
  },
]
const createOptions = (overrides: Partial<UsePopupManagerOptions> = {}): UsePopupManagerOptions => {
  const baseRef: MutableRefObject<string | null> = { current: null }
  const defaults: UsePopupManagerOptions = {
    currentModel: 'gpt-4o-mini',
    currentTargetModel: 'gpt-4o-mini',
    modelOptions: defaultModelOptions,
    activeThemeName: 'pm-dark',
    themeMode: 'dark',
    themes: [
      { name: 'pm-dark', label: 'Prompt Maker Dark' },
      { name: 'pm-light', label: 'Prompt Maker Light' },
    ],
    smartContextEnabled: false,
    smartContextRoot: null,
    toggleSmartContext: jest.fn(),
    setSmartRoot: jest.fn(),
    urls: [],
    addUrl: jest.fn(),
    images: [],
    videos: [],
    pdfs: [],
    addImage: jest.fn(),
    addVideo: jest.fn(),
    addPdf: jest.fn(),
    lastTestFile: null,
    defaultTestFile: 'prompt.test.ts',
    interactiveTransportPath: undefined,
    isGenerating: false,
    lastUserIntentRef: baseRef,
    pushHistory: jest.fn(),
    notify: jest.fn(),
    setInputValue: jest.fn(),
    runGeneration: jest.fn(async () => undefined),
    runSeriesGeneration: jest.fn(),
    runTestsFromCommand: jest.fn(),
    exitApp: jest.fn(),
    setCurrentModel: jest.fn(),
    setCurrentTargetModel: jest.fn(),
    setPolishModelId: jest.fn(),
    setCopyEnabled: jest.fn(),
    setChatGptEnabled: jest.fn(),
    setJsonOutputEnabled: jest.fn(),
    setIntentFilePath: jest.fn(),
    intentFilePath: '',
    metaInstructions: '',
    setMetaInstructions: jest.fn(),
    budgets: {
      maxContextTokens: null,
      maxInputTokens: null,
      contextOverflowStrategy: null,
    },
    setBudgets: jest.fn(),
    polishModelId: null,
    copyEnabled: false,
    chatGptEnabled: false,
    jsonOutputEnabled: false,
    getLatestTypedIntent: jest.fn(() => null),
    syncTypedIntentRef: jest.fn(),
    resumeDefaults: { sourceKind: 'history', mode: 'best-effort' },
    setResumeDefaults: jest.fn(),
    exportDefaults: { format: 'json', outDir: null },
    setExportDefaults: jest.fn(),
  }
  return { ...defaults, ...overrides }
}
type Deferred<T> = {
  promise: Promise<T>
  resolve: (value: T) => void
  reject: (reason?: unknown) => void
}
const createDeferred = <T>(): Deferred<T> => {
  let resolve: (value: T) => void = (_value) => undefined
  let reject: (reason?: unknown) => void = (_reason) => undefined
  const promise = new Promise<T>((promiseResolve, promiseReject) => {
    resolve = promiseResolve
    reject = promiseReject
  })
  return { promise, resolve, reject }
}
const fileSuggestions = jest.requireMock('../tui/file-suggestions') as {
  discoverDirectorySuggestions: jest.Mock
  discoverFileSuggestions: jest.Mock
}
const getFsMock = () =>
  jest.requireMock('node:fs/promises') as {
    readFile: jest.MockedFunction<(file: string, encoding: string) => Promise<string>>
  }
describe('usePopupManager theme popup', () => {
  it('opens theme popup with current selection', () => {
    const options = createOptions({
      activeThemeName: 'pm-light',
      themes: [
        { name: 'pm-dark', label: 'Prompt Maker Dark' },
        { name: 'pm-light', label: 'Prompt Maker Light' },
      ],
    })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openThemePopup()
    })
    expect(result.current.popupState).toEqual({
      type: 'theme',
      selectionIndex: 1,
      initialThemeName: 'pm-light',
    })
  })
})
describe('usePopupManager theme mode popup', () => {
  it('opens theme mode popup with current mode', () => {
    const options = createOptions({
      themeMode: 'system',
    })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openThemeModePopup()
    })
    expect(result.current.popupState).toEqual({
      type: 'themeMode',
      selectionIndex: 0,
      initialMode: 'system',
    })
  })
})
describe('usePopupManager file popup', () => {
  beforeEach(() => {
    resetRecentSessionModelsForTests()
    fileSuggestions.discoverFileSuggestions.mockReset()
  })
  it('initializes file popup with suggestion defaults', () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverFileSuggestions.mockReturnValue(deferred.promise)
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openFilePopup()
    })
    expect(fileSuggestions.discoverFileSuggestions).toHaveBeenCalledWith(
      expect.objectContaining({ limit: 5000 }),
    )
    expect(result.current.popupState).toEqual({
      type: 'file',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
  })
  it('populates file popup suggestions after scanning', async () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverFileSuggestions.mockReturnValue(deferred.promise)
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openFilePopup()
    })
    expect(fileSuggestions.discoverFileSuggestions).toHaveBeenCalledWith(
      expect.objectContaining({ limit: 5000 }),
    )
    await act(async () => {
      deferred.resolve(['src/index.ts', 'README.md'])
      await deferred.promise
    })
    expect(result.current.popupState).toEqual({
      type: 'file',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: ['src/index.ts', 'README.md'],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
  })
  it('logs a history entry when scanning fails', async () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverFileSuggestions.mockReturnValue(deferred.promise)
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openFilePopup()
    })
    await act(async () => {
      deferred.reject(new Error('boom'))
      try {
        await deferred.promise
      } catch {
        // ignored
      }
    })
    expect(options.pushHistory).toHaveBeenCalledWith(
      '[file] Failed to scan workspace: boom',
      'system',
    )
  })
})
describe('usePopupManager image popup', () => {
  beforeEach(() => {
    fileSuggestions.discoverFileSuggestions.mockReset()
  })
  it('initializes image popup with suggestion defaults', () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverFileSuggestions.mockReturnValue(deferred.promise)
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openImagePopup()
    })
    expect(result.current.popupState).toEqual({
      type: 'image',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
  })
  it('filters image popup suggestions after scanning', async () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverFileSuggestions.mockReturnValue(deferred.promise)
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openImagePopup()
    })
    await act(async () => {
      deferred.resolve(['src/index.ts', 'diagram.png', 'clip.mp4', 'photo.JPG'])
      await deferred.promise
    })
    expect(result.current.popupState).toEqual({
      type: 'image',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: ['diagram.png', 'photo.JPG'],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
  })
  it('attaches an image via command args', () => {
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('image', 'diagram.png')
    })
    expect(options.addImage).toHaveBeenCalledWith('diagram.png')
    expect(options.pushHistory).toHaveBeenCalledWith('[image] Attached: diagram.png', 'system')
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toBeNull()
  })
})
describe('usePopupManager video popup', () => {
  beforeEach(() => {
    fileSuggestions.discoverFileSuggestions.mockReset()
  })
  it('filters video popup suggestions after scanning', async () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverFileSuggestions.mockReturnValue(deferred.promise)
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openVideoPopup()
    })
    await act(async () => {
      deferred.resolve(['clip.mp4', 'diagram.png', 'movie.MOV', 'README.md'])
      await deferred.promise
    })
    expect(result.current.popupState).toEqual({
      type: 'video',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: ['clip.mp4', 'movie.MOV'],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
  })
  it('attaches a video via command args', () => {
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('video', 'clip.mp4')
    })
    expect(options.addVideo).toHaveBeenCalledWith('clip.mp4')
    expect(options.pushHistory).toHaveBeenCalledWith('[video] Attached: clip.mp4', 'system')
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toBeNull()
  })
})
describe('usePopupManager smart-root popup', () => {
  beforeEach(() => {
    fileSuggestions.discoverDirectorySuggestions.mockReset()
  })
  it('initializes smart-root popup with suggestion defaults', () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverDirectorySuggestions.mockReturnValue(deferred.promise)
    const options = createOptions({ smartContextRoot: 'src' })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openSmartRootPopup()
    })
    expect(result.current.popupState).toEqual({
      type: 'smart',
      draft: 'src',
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
  })
  it('opens smart-root popup from command selection', () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverDirectorySuggestions.mockReturnValue(deferred.promise)
    const options = createOptions({ smartContextRoot: 'src' })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('smart-root')
    })
    expect(result.current.popupState).toEqual({
      type: 'smart',
      draft: 'src',
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
  })
  it('auto-enables smart context when setting a root via args', () => {
    const toggleSmartContext = jest.fn()
    const setSmartRoot = jest.fn()
    const options = createOptions({
      smartContextEnabled: false,
      toggleSmartContext,
      setSmartRoot,
    })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('smart-root', 'src')
    })
    expect(setSmartRoot).toHaveBeenCalledWith('src')
    expect(toggleSmartContext).toHaveBeenCalledTimes(1)
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toBeNull()
  })
  it('clears the root when running /smart off', () => {
    const toggleSmartContext = jest.fn()
    const setSmartRoot = jest.fn()
    const options = createOptions({
      smartContextEnabled: true,
      smartContextRoot: 'src',
      toggleSmartContext,
      setSmartRoot,
    })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('smart', 'off')
    })
    expect(setSmartRoot).toHaveBeenCalledWith('')
    expect(toggleSmartContext).toHaveBeenCalledTimes(1)
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toBeNull()
  })
  it('populates smart-root popup suggestions after scanning', async () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverDirectorySuggestions.mockReturnValue(deferred.promise)
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openSmartRootPopup()
    })
    await act(async () => {
      deferred.resolve(['src', 'apps/prompt-maker-cli'])
      await deferred.promise
    })
    expect(result.current.popupState).toEqual({
      type: 'smart',
      draft: '',
      suggestedItems: ['src', 'apps/prompt-maker-cli'],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
  })
  it('logs a history entry when scanning fails for smart-root', async () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverDirectorySuggestions.mockReturnValue(deferred.promise)
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openSmartRootPopup()
    })
    await act(async () => {
      deferred.reject(new Error('boom'))
      try {
        await deferred.promise
      } catch {
        // ignored
      }
    })
    expect(options.pushHistory).toHaveBeenCalledWith(
      '[smart] Failed to scan workspace: boom',
      'system',
    )
  })
})
describe('usePopupManager instructions command', () => {
  it('opens and saves meta instructions', () => {
    const options = createOptions({ metaInstructions: 'Be friendly' })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('instructions')
    })
    expect(result.current.popupState).toEqual({ type: 'instructions', draft: 'Be friendly' })
    act(() => {
      result.current.actions.handleInstructionsSubmit('Focus on security')
    })
    expect(options.setMetaInstructions).toHaveBeenCalledWith('Focus on security')
    expect(options.pushHistory).toHaveBeenCalledWith('[instr] Focus on security')
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toBeNull()
  })
  it('applies meta instructions from command args', () => {
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('instructions', 'Focus on security')
    })
    expect(options.setMetaInstructions).toHaveBeenCalledWith('Focus on security')
    expect(options.pushHistory).toHaveBeenCalledWith('[instr] Focus on security')
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toBeNull()
  })
})
describe('usePopupManager intent command', () => {
  it('applies intent file from command args', () => {
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('intent', '/tmp/intent.md')
    })
    expect(options.setIntentFilePath).toHaveBeenCalledWith('/tmp/intent.md')
    expect(options.pushHistory).toHaveBeenCalledWith('Intent file set to /tmp/intent.md')
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toBeNull()
  })
})
describe('usePopupManager exit command', () => {
  it('clears the screen before exiting', () => {
    const clearScreen = jest.fn()
    const exitApp = jest.fn()
    const options = createOptions({ clearScreen, exitApp })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('exit')
    })
    expect(options.pushHistory).toHaveBeenCalledWith('ExitingâŚ', 'system')
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(clearScreen).toHaveBeenCalledTimes(1)
    expect(exitApp).toHaveBeenCalledTimes(1)
    const clearOrder = clearScreen.mock.invocationCallOrder[0]
    const exitOrder = exitApp.mock.invocationCallOrder[0]
    if (clearOrder === undefined || exitOrder === undefined) {
      throw new Error('Expected clearScreen and exitApp to be called')
    }
    expect(clearOrder).toBeLessThan(exitOrder)
  })
})
describe('usePopupManager tokens command', () => {
  it('opens the token usage popup', () => {
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('tokens')
    })
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toEqual({ type: 'tokens' })
  })
})
describe('usePopupManager reasoning command', () => {
  it('opens the reasoning popup', () => {
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('reasoning')
    })
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toEqual({ type: 'reasoning', scrollOffset: 0 })
  })
})
describe('usePopupManager series command', () => {
  beforeEach(() => {
    const fs = getFsMock()
    fs.readFile.mockReset()
  })
  it('prefills the series popup from command args', async () => {
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    await act(async () => {
      result.current.actions.handleCommandSelection('series', 'plan a feature')
      await Promise.resolve()
    })
    expect(options.pushHistory).toHaveBeenCalledWith(
      '[series] Using provided text as intent draft.',
      'system',
    )
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toEqual({
      type: 'series',
      draft: 'plan a feature',
      hint: 'Draft prefills from typed/last intent (or pass /series <intent>).',
    })
  })
  it('prefills the series popup from typed intent', async () => {
    const options = createOptions({ getLatestTypedIntent: jest.fn(() => 'typed intent') })
    const { result } = renderHook(() => usePopupManager(options))
    await act(async () => {
      result.current.actions.handleCommandSelection('series')
      await Promise.resolve()
    })
    expect(options.pushHistory).toHaveBeenCalledWith(
      '[series] Using typed intent as draft.',
      'system',
    )
    expect(result.current.popupState).toEqual({
      type: 'series',
      draft: 'typed intent',
      hint: 'Draft prefills from typed/last intent (or pass /series <intent>).',
    })
  })
  it('prefills the series popup from the last run intent', async () => {
    const lastUserIntentRef: MutableRefObject<string | null> = { current: 'last intent' }
    const options = createOptions({ lastUserIntentRef })
    const { result } = renderHook(() => usePopupManager(options))
    await act(async () => {
      result.current.actions.handleCommandSelection('series')
      await Promise.resolve()
    })
    expect(options.pushHistory).toHaveBeenCalledWith(
      '[series] Reusing last intent as draft.',
      'system',
    )
    expect(result.current.popupState).toEqual({
      type: 'series',
      draft: 'last intent',
      hint: 'Draft prefills from typed/last intent (or pass /series <intent>).',
    })
  })
  it('loads the series popup draft from an intent file when empty', async () => {
    const fs = getFsMock()
    fs.readFile.mockResolvedValueOnce('intent from file')
    const options = createOptions({ intentFilePath: '/tmp/intent.md' })
    const { result } = renderHook(() => usePopupManager(options))
    await act(async () => {
      result.current.actions.handleCommandSelection('series')
      await Promise.resolve()
    })
    expect(fs.readFile).toHaveBeenCalledWith('/tmp/intent.md', 'utf8')
    expect(options.pushHistory).toHaveBeenCalledWith(
      '[series] Loaded draft from intent file intent.md.',
      'system',
    )
    expect(options.syncTypedIntentRef).toHaveBeenCalledWith('intent from file')
    expect(result.current.popupState).toEqual({
      type: 'series',
      draft: 'intent from file',
      hint: 'Loaded from intent file intent.md',
    })
  })
})
describe('usePopupManager test command', () => {
  it('logs a hint when running /test with args', () => {
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('test', 'prompt-tests.yaml')
    })
    expect(options.pushHistory).toHaveBeenCalledWith(
      '[tests] Running /test prompt-tests.yaml',
      'system',
    )
    expect(options.runTestsFromCommand).toHaveBeenCalledWith('prompt-tests.yaml')
  })
})
describe('usePopupManager quick toggles', () => {
  it('opens polish model picker without arguments', () => {
    const options = createOptions({ polishModelId: null })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('polish')
    })
    expect(result.current.popupState).toEqual({
      type: 'model',
      kind: 'polish',
      query: '',
      selectionIndex: 0,
    })
  })
  it('sets polish model when submitting selection', () => {
    const options = createOptions({ polishModelId: null })
    const { result } = renderHook(() => usePopupManager(options))
    const option = defaultModelOptions[0]
    if (!option) {
      throw new Error('Expected a default model option')
    }
    act(() => {
      result.current.actions.handleCommandSelection('polish')
    })
    act(() => {
      result.current.actions.handleModelPopupSubmit(option)
    })
    expect(options.setPolishModelId).toHaveBeenCalledWith('gpt-4o-mini')
    expect(options.notify).toHaveBeenCalledWith(
      'Selected polish model: gpt-4o-mini (gpt-4o-mini)',
      { kind: 'info' },
    )
    expect(options.setInputValue).toHaveBeenCalledWith('')
  })
  it('clears polish model when submitting null', () => {
    const options = createOptions({ polishModelId: 'gpt-4o-mini' })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('polish')
    })
    act(() => {
      result.current.actions.handleModelPopupSubmit(null)
    })
    expect(options.setPolishModelId).toHaveBeenCalledWith(null)
  })
  it('accepts explicit on/off arguments for copy', () => {
    const options = createOptions({ copyEnabled: true })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('copy', 'off')
    })
    expect(options.setCopyEnabled).toHaveBeenCalledWith(false)
    expect(options.pushHistory).toHaveBeenCalledWith('Copy disabled')
  })
  it('opens the toggle popup when chatgpt args are invalid', () => {
    const options = createOptions({ chatGptEnabled: false })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('chatgpt', 'maybe')
    })
    expect(result.current.popupState).toEqual({
      type: 'toggle',
      field: 'chatgpt',
      selectionIndex: 1,
    })
  })
  it('toggles json output with no args when allowed', () => {
    const options = createOptions({ jsonOutputEnabled: false })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('json')
    })
    expect(options.setJsonOutputEnabled).toHaveBeenCalledWith(true)
    expect(options.notify).toHaveBeenCalledWith('JSON output is ON (payload shown in history)', {
      kind: 'info',
    })
    expect(options.pushHistory).not.toHaveBeenCalled()
  })
  it('blocks json toggling when interactive transport is active', () => {
    const options = createOptions({ interactiveTransportPath: '/tmp/socket' })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('json')
    })
    expect(options.setJsonOutputEnabled).not.toHaveBeenCalled()
    expect(options.pushHistory).toHaveBeenCalledWith(
      'JSON output is unavailable while interactive transport is enabled.',
      'system',
    )
    expect(options.setInputValue).toHaveBeenCalledWith('')
  })
  it('accepts explicit json arguments', () => {
    const options = createOptions({ jsonOutputEnabled: true })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('json', 'off')
    })
    expect(options.setJsonOutputEnabled).toHaveBeenCalledWith(false)
    expect(options.notify).toHaveBeenCalledWith('JSON output is OFF (payload hidden)', {
      kind: 'warning',
    })
    expect(options.pushHistory).not.toHaveBeenCalled()
  })
})
describe('usePopupManager model popup', () => {
  it('notifies on model selection without history', () => {
    const modelOptions: ModelOption[] = [
      {
        id: 'gpt-4o-mini',
        label: 'GPT-4o Mini',
        provider: 'openai',
        description: 'test',
        capabilities: [],
        source: 'builtin',
      },
      {
        id: 'gemini-1.5-pro',
        label: 'Gemini 1.5 Pro',
        provider: 'gemini',
        description: 'test',
        capabilities: [],
        source: 'builtin',
      },
    ]
    const options = createOptions({ currentModel: 'gpt-4o-mini', modelOptions })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openModelPopup()
    })
    act(() => {
      result.current.actions.handleModelPopupSubmit(modelOptions[1])
    })
    expect(options.setCurrentModel).toHaveBeenCalledWith('gemini-1.5-pro')
    expect(options.notify).toHaveBeenCalledWith('Selected model: Gemini 1.5 Pro (gemini-1.5-pro)', {
      kind: 'info',
    })
    expect(options.pushHistory).not.toHaveBeenCalled()
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toBeNull()
  })
})
</file>
<file path="src/tui/hooks/useGenerationPipeline.ts">
import path from 'node:path'
import { useCallback, useEffect, useMemo, useReducer, useRef } from 'react'
import { createBufferedHistoryWriter } from './buffered-history-writer'
import {
  buildIterationCompleteHistoryMessages,
  buildJsonPayloadHistoryMessages,
  extractValidationSection,
  formatCompactTokens,
  getHistoryWrapWidth,
  wrapTextForHistory,
} from './generation-history-formatters'
import {
  prepareSeriesOutputDir,
  type WriteSeriesArtifactsResult,
  writeSeriesArtifacts,
} from './series-artifacts-io'
import { useLatestRef } from './useLatestRef'
import {
  INITIAL_GENERATION_PIPELINE_STATE,
  generationPipelineReducer,
  type InteractiveAwaitingMode,
} from '../generation-pipeline-reducer'
import {
  maybeCopyToClipboard,
  maybeOpenChatGpt,
  runGeneratePipeline,
  type GenerateArgs,
  type GeneratePipelineOptions,
  type GeneratePipelineResult,
  type InteractiveDelegate,
  type StreamEventInput,
} from '../../generate-command'
import { evaluateContextBudget, type ContextEntry } from '../../generate/context-budget'
import { resolveGeminiVideoModel } from '../../generate/models'
import { buildTokenTelemetry } from '../../generate/token-telemetry'
import { generatePromptSeries, isGemini } from '../../prompt-generator-service'
import type { PromptGenerationRequest, SeriesResponse } from '../../prompt-generator-service'
import { resolveFileContext } from '../../file-context'
import { resolveSmartContextFiles } from '../../smart-context-service'
import { resolveUrlContext } from '../../url-context'
import type { UploadStateChange } from '../../prompt-generator-service'
import { MODEL_PROVIDER_LABELS } from '../../model-providers'
import { checkModelProviderStatus } from '../provider-status'
import type { TokenUsageStore } from '../token-usage-store'
import type { BudgetSettings } from '../budget-settings'
import type { NotifyOptions } from '../notifier'
import type { HistoryEntry, ProviderStatus, ResumeMode } from '../types'
export type UseGenerationPipelineOptions = {
  pushHistory: (
    content: string,
    kind?: HistoryEntry['kind'],
    format?: HistoryEntry['format'],
  ) => void
  notify?: (message: string, options?: NotifyOptions) => void
  files: string[]
  urls: string[]
  images: string[]
  videos: string[]
  pdfs: string[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  metaInstructions: string
  budgets: BudgetSettings
  currentModel: string
  targetModel?: string
  interactiveTransportPath?: string | undefined
  terminalColumns: number
  polishModelId: string | null
  jsonOutputEnabled: boolean
  copyEnabled: boolean
  chatGptEnabled: boolean
  isTestCommandRunning: boolean
  onProviderStatusUpdate?: (status: ProviderStatus) => void
  tokenUsageStore?: TokenUsageStore
  onReasoningUpdate?: (reasoning: string | null) => void
  onLastGeneratedPromptUpdate?: (prompt: string) => void
}
export const useGenerationPipeline = ({
  pushHistory,
  notify,
  files,
  urls,
  images,
  videos,
  pdfs,
  smartContextEnabled,
  smartContextRoot,
  metaInstructions,
  budgets,
  currentModel,
  targetModel,
  interactiveTransportPath,
  terminalColumns,
  polishModelId,
  jsonOutputEnabled,
  copyEnabled,
  chatGptEnabled,
  isTestCommandRunning,
  onProviderStatusUpdate,
  tokenUsageStore,
  onReasoningUpdate,
  onLastGeneratedPromptUpdate,
}: UseGenerationPipelineOptions) => {
  const [pipelineState, dispatch] = useReducer(
    generationPipelineReducer,
    INITIAL_GENERATION_PIPELINE_STATE,
  )
  const {
    isGenerating,
    statusMessage,
    isAwaitingRefinement,
    awaitingInteractiveMode,
    latestTelemetry,
  } = pipelineState
  const normalizedMetaInstructions = metaInstructions.trim()
  // âStale closureâ explanation (plain-English):
  // React callbacks capture the variables that were in scope when they were created.
  // If we keep a callback stable (so we donât recreate it every render), it would
  // otherwise keep using old values.
  //
  // Example: if `handleStreamEvent` closed over an old `terminalColumns`, it would
  // keep wrapping text to the wrong width after the terminal is resized.
  //
  // Solution used here: keep the callback stable, but read changing values from refs
  // (kept fresh via useLatestRef).
  const pushHistoryRef = useLatestRef(pushHistory)
  const tokenUsageStoreRef = useLatestRef(tokenUsageStore)
  const terminalColumnsRef = useLatestRef(terminalColumns)
  const interactiveTransportPathRef = useLatestRef(interactiveTransportPath)
  const notifyRef = useLatestRef(notify)
  const activeRunIdRef = useRef<string | null>(null)
  const lastGeneratedPromptUpdateRef = useLatestRef<((prompt: string) => void) | null>(
    onLastGeneratedPromptUpdate ?? null,
  )
  type PendingRefinement = {
    requestId: number
    resolveText: (text: string) => void
  }
  const pendingRefinementRef = useRef<PendingRefinement | null>(null)
  const refinementRequestIdRef = useRef(0)
  const isGeneratingRef = useLatestRef(isGenerating)
  const transportAwaitingHintShownRef = useRef(false)
  const setAwaitingInteractiveMode = useCallback(
    (nextMode: InteractiveAwaitingMode | null, nextStatusMessage?: string): void => {
      dispatch({
        type: 'set-awaiting-interactive',
        awaitingInteractiveMode: nextMode,
        ...(nextStatusMessage ? { statusMessage: nextStatusMessage } : {}),
      })
    },
    [],
  )
  const setAwaitingRefinement = useCallback((next: boolean): void => {
    dispatch({ type: 'set-awaiting-refinement', isAwaitingRefinement: next })
  }, [])
  const setLatestTelemetry = useCallback(
    (telemetry: GeneratePipelineResult['telemetry'] | null) => {
      dispatch({ type: 'set-telemetry', telemetry })
    },
    [],
  )
  const setStatusMessage = useCallback((message: string): void => {
    dispatch({ type: 'set-status', statusMessage: message })
  }, [])
  const submitRefinement = useCallback(
    (text: string): void => {
      const pending = pendingRefinementRef.current
      if (!pending) {
        return
      }
      pendingRefinementRef.current = null
      setAwaitingRefinement(false)
      pending.resolveText(text)
    },
    [setAwaitingRefinement],
  )
  useEffect(() => {
    if (isGenerating) {
      return
    }
    // Cleanup is important:
    // - Without it, a pending refinement promise could keep the UI in a âwaitingâ state.
    // - It also prevents ârunaway updatesâ after generation stops.
    submitRefinement('')
    setAwaitingRefinement(false)
    setAwaitingInteractiveMode(null)
    transportAwaitingHintShownRef.current = false
  }, [isGenerating, setAwaitingInteractiveMode, setAwaitingRefinement, submitRefinement])
  useEffect(() => {
    return () => {
      submitRefinement('')
    }
  }, [submitRefinement])
  const bufferedHistory = useMemo(
    () =>
      createBufferedHistoryWriter({
        push: (content, kind, format) => {
          pushHistoryRef.current(content, kind, format)
        },
      }),
    [pushHistoryRef],
  )
  useEffect(() => {
    if (!isGenerating) {
      bufferedHistory.flush()
    }
  }, [bufferedHistory, isGenerating])
  useEffect(() => {
    return () => {
      bufferedHistory.flush()
    }
  }, [bufferedHistory])
  const handleStreamEvent = useCallback(
    (event: StreamEventInput) => {
      const tokenUsageStoreLatest = tokenUsageStoreRef.current
      const wrapWidth = getHistoryWrapWidth(terminalColumnsRef.current)
      switch (event.event) {
        case 'progress.update': {
          const scope = event.scope ? `[${event.scope}] ` : ''
          const message = `${scope}${event.label} (${event.state})`
          bufferedHistory.pushBuffered(message, 'progress')
          setStatusMessage(message)
          return
        }
        case 'upload.state': {
          const action = event.state === 'start' ? 'Uploading' : 'Uploaded'
          const message = `${action} ${event.detail.kind}: ${event.detail.filePath}`
          const notifyLatest = notifyRef.current
          if (notifyLatest) {
            notifyLatest(message, {
              kind: 'progress',
              autoDismissMs: event.state === 'start' ? 6000 : 2600,
            })
            return
          }
          bufferedHistory.pushBuffered(message, 'progress')
          return
        }
        case 'generation.iteration.start':
          bufferedHistory.pushBuffered(`Iteration ${event.iteration} started`, 'progress')
          return
        case 'generation.iteration.complete': {
          const reasoningTokens = event.reasoningTokens ?? 0
          const activeRunId = activeRunIdRef.current
          if (activeRunId && tokenUsageStoreLatest) {
            tokenUsageStoreLatest.recordIteration(activeRunId, {
              iteration: event.iteration,
              promptTokens: event.tokens,
              reasoningTokens,
            })
          }
          bufferedHistory.pushManyBuffered(
            buildIterationCompleteHistoryMessages({
              iteration: event.iteration,
              tokens: event.tokens,
              ...(event.reasoningTokens !== undefined
                ? { reasoningTokens: event.reasoningTokens }
                : {}),
              prompt: event.prompt,
              wrapWidth,
            }),
          )
          return
        }
        case 'resume.loaded': {
          dispatch({ type: 'set-resume-loaded', details: event })
          const reusedCount = event.reusedContextPaths.length
          const missingCount = event.missingContextPaths.length
          const message = `Resume loaded (${event.source}) Âˇ reused ${reusedCount} Âˇ missing ${missingCount}`
          bufferedHistory.pushBuffered(message, 'progress')
          const notifyLatest = notifyRef.current
          if (notifyLatest) {
            notifyLatest(message, { kind: missingCount > 0 ? 'warning' : 'info' })
          }
          return
        }
        case 'context.overflow': {
          dispatch({ type: 'set-context-overflow', details: event })
          const droppedCount = event.droppedPaths.length
          const message = `Context overflow (${event.strategy}) Âˇ dropped ${droppedCount}`
          bufferedHistory.pushBuffered(message, 'system')
          const notifyLatest = notifyRef.current
          if (notifyLatest) {
            notifyLatest(message, { kind: 'warning' })
          }
          return
        }
        case 'context.telemetry': {
          const telemetry = event.telemetry
          setLatestTelemetry(telemetry)
          const activeRunId = activeRunIdRef.current
          if (activeRunId && tokenUsageStoreLatest) {
            tokenUsageStoreLatest.recordTelemetry(activeRunId, telemetry)
          }
          bufferedHistory.pushBuffered(
            `Telemetry Âˇ total ${telemetry.totalTokens} Âˇ intent ${telemetry.intentTokens} Âˇ files ${telemetry.fileTokens} Âˇ system ${telemetry.systemTokens}`,
            'progress',
          )
          return
        }
        case 'generation.final':
          setAwaitingInteractiveMode(null)
          bufferedHistory.pushBuffered('Generation stream finalized.', 'progress')
          return
        case 'transport.listening':
          bufferedHistory.pushBuffered(`Transport listening on ${event.path}`, 'progress')
          return
        case 'transport.client.connected':
          bufferedHistory.pushBuffered('Transport client connected.', 'progress')
          return
        case 'transport.client.disconnected':
          bufferedHistory.pushBuffered('Transport client disconnected.', 'progress')
          return
        case 'interactive.awaiting': {
          const normalizedMode =
            event.mode === 'transport' || event.mode === 'tty' ? event.mode : null
          const waitingMessage =
            normalizedMode === 'transport'
              ? 'Waiting for interactive transport inputâŚ'
              : 'Waiting for interactive inputâŚ'
          // One dispatch updates both mode + message.
          setAwaitingInteractiveMode(normalizedMode, waitingMessage)
          bufferedHistory.pushBuffered(waitingMessage, 'progress')
          const transportPath = interactiveTransportPathRef.current
          if (
            normalizedMode === 'transport' &&
            transportPath &&
            !transportAwaitingHintShownRef.current
          ) {
            bufferedHistory.pushBuffered(
              'Tip: connect a client and send refine/finish to continue.',
              'system',
            )
            transportAwaitingHintShownRef.current = true
          }
          return
        }
        case 'interactive.state': {
          const message = `Interactive ${event.phase}`
          setAwaitingInteractiveMode(null, message)
          bufferedHistory.pushBuffered(
            `Interactive ${event.phase} (iteration ${event.iteration})`,
            'progress',
          )
          return
        }
        default:
          return
      }
    },
    [
      bufferedHistory,
      interactiveTransportPathRef,
      notifyRef,
      terminalColumnsRef,
      tokenUsageStoreRef,
      setAwaitingInteractiveMode,
      setLatestTelemetry,
      setStatusMessage,
    ],
  )
  const interactiveDelegate: InteractiveDelegate = useMemo(
    () => ({
      getNextAction: async ({ iteration }) => {
        if (!isGeneratingRef.current) {
          return { type: 'finish' }
        }
        refinementRequestIdRef.current += 1
        const requestId = refinementRequestIdRef.current
        if (pendingRefinementRef.current) {
          submitRefinement('')
        }
        setAwaitingRefinement(true)
        pushHistoryRef.current(
          `Refine the prompt above (iteration ${iteration}): describe changes or press Enter on empty line to finish.`,
          'system',
        )
        try {
          return await new Promise<{ type: 'refine'; instruction: string } | { type: 'finish' }>(
            (resolve) => {
              pendingRefinementRef.current = {
                requestId,
                resolveText: (submittedText: string) => {
                  const trimmed = submittedText.trim()
                  if (!isGeneratingRef.current) {
                    resolve({ type: 'finish' })
                    return
                  }
                  if (!trimmed) {
                    pushHistoryRef.current('Interactive refinement complete.', 'system')
                    resolve({ type: 'finish' })
                    return
                  }
                  pushHistoryRef.current(`> [refine] ${trimmed}`, 'user')
                  resolve({ type: 'refine', instruction: trimmed })
                },
              }
            },
          )
        } finally {
          if (pendingRefinementRef.current?.requestId === requestId) {
            pendingRefinementRef.current = null
          }
          if (refinementRequestIdRef.current === requestId) {
            setAwaitingRefinement(false)
          }
        }
      },
    }),
    [isGeneratingRef, pushHistoryRef, submitRefinement, setAwaitingRefinement],
  )
  const onProviderStatusUpdateRef = useLatestRef(onProviderStatusUpdate)
  const ensureProviderReady = useCallback(
    async (modelId: string): Promise<boolean> => {
      try {
        const status = await checkModelProviderStatus(modelId)
        onProviderStatusUpdateRef.current?.(status)
        if (status.status === 'ok') {
          return true
        }
        const providerLabel = MODEL_PROVIDER_LABELS[status.provider]
        pushHistoryRef.current(
          `Generation aborted: ${providerLabel} unavailable (${status.message}).`,
          'system',
        )
        return false
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown provider check error.'
        pushHistoryRef.current(`Generation aborted: provider check failed (${message}).`, 'system')
        return false
      }
    },
    [onProviderStatusUpdateRef, pushHistoryRef],
  )
  const runGeneration = useCallback(
    async (intentInput: {
      intent?: string
      intentFile?: string
      resume?:
        | { kind: 'history'; selector: string; mode: ResumeMode }
        | { kind: 'file'; payloadPath: string; mode: ResumeMode }
    }) => {
      const trimmedIntent = intentInput.intent?.trim() ?? ''
      const trimmedIntentFile = intentInput.intentFile?.trim() ?? ''
      const resume = intentInput.resume
      if (!trimmedIntent && !trimmedIntentFile && !resume) {
        pushHistoryRef.current('No intent provided. Enter text or set an intent file.', 'system')
        return
      }
      const normalizedModel = currentModel.trim() || 'gpt-4o-mini'
      const generationModel =
        videos.length > 0 && !isGemini(normalizedModel)
          ? await resolveGeminiVideoModel()
          : normalizedModel
      if (generationModel !== normalizedModel) {
        pushHistoryRef.current(
          `Switching to ${generationModel} to support video input.`,
          'progress',
        )
      }
      const normalizedTargetModel = (targetModel ?? '').trim() || generationModel
      const normalizedPolishModel = (polishModelId ?? '').trim()
      const polishEnabled = normalizedPolishModel.length > 0
      const providerReady = await ensureProviderReady(generationModel)
      if (!providerReady) {
        return
      }
      if (polishEnabled && normalizedPolishModel !== generationModel) {
        const polishProviderReady = await ensureProviderReady(normalizedPolishModel)
        if (!polishProviderReady) {
          return
        }
      }
      activeRunIdRef.current = tokenUsageStoreRef.current
        ? tokenUsageStoreRef.current.startRun(generationModel)
        : null
      setLatestTelemetry(null)
      onReasoningUpdate?.(null)
      dispatch({ type: 'generation-start', statusMessage: 'Preparing generationâŚ' })
      transportAwaitingHintShownRef.current = false
      pushHistoryRef.current('Starting generationâŚ')
      let stopStatusMessage: string | undefined
      try {
        const transportPath = interactiveTransportPathRef.current
        const usesTransportInteractive = Boolean(transportPath)
        const usesTuiInteractiveDelegate = !usesTransportInteractive && !jsonOutputEnabled
        const shouldIgnoreContextForResume = Boolean(resume)
        const args: GenerateArgs = {
          interactive: usesTransportInteractive || usesTuiInteractiveDelegate,
          copy: false,
          openChatGpt: false,
          polish: polishEnabled,
          json: jsonOutputEnabled,
          quiet: true,
          progress: false,
          stream: 'none',
          showContext: false,
          contextFormat: 'text',
          help: false,
          context: shouldIgnoreContextForResume ? [] : [...files],
          urls: shouldIgnoreContextForResume ? [] : [...urls],
          images: [...images],
          video: [...videos],
          pdf: [...pdfs],
          smartContext: shouldIgnoreContextForResume ? false : smartContextEnabled,
          model: generationModel,
          target: normalizedTargetModel,
          ...(budgets.maxInputTokens !== null ? { maxInputTokens: budgets.maxInputTokens } : {}),
          ...(budgets.maxContextTokens !== null
            ? { maxContextTokens: budgets.maxContextTokens }
            : {}),
          ...(budgets.contextOverflowStrategy !== null
            ? { contextOverflow: budgets.contextOverflowStrategy }
            : {}),
        }
        if (normalizedMetaInstructions) {
          args.metaInstructions = normalizedMetaInstructions
        }
        if (trimmedIntentFile) {
          args.intentFile = trimmedIntentFile
        } else if (trimmedIntent) {
          args.intent = trimmedIntent
        }
        if (resume) {
          args.resumeMode = resume.mode
          if (resume.kind === 'history') {
            args.resume = resume.selector
          } else {
            args.resumeFrom = resume.payloadPath
          }
        }
        if (polishEnabled) {
          args.polishModel = normalizedPolishModel
        }
        if (!shouldIgnoreContextForResume && smartContextEnabled && smartContextRoot) {
          args.smartContextRoot = smartContextRoot
        }
        if (transportPath) {
          args.interactiveTransport = transportPath
        }
        const options: GeneratePipelineOptions = {
          onStreamEvent: handleStreamEvent,
          ...(usesTuiInteractiveDelegate ? { interactiveDelegate } : {}),
        }
        const result: GeneratePipelineResult = await runGeneratePipeline(args, options)
        onReasoningUpdate?.(result.reasoning ?? null)
        setStatusMessage('Finalizing promptâŚ')
        const wrapWidth = getHistoryWrapWidth(terminalColumnsRef.current)
        const iterationLabel = result.iterations ? ` Âˇ ${result.iterations} iterations` : ''
        pushHistoryRef.current(`Final prompt (${result.model}${iterationLabel}):`, 'system')
        wrapTextForHistory(result.finalPrompt, wrapWidth).forEach((line) => {
          pushHistoryRef.current(line, 'system', 'markdown')
        })
        lastGeneratedPromptUpdateRef.current?.(result.finalPrompt)
        if (result.telemetry) {
          setLatestTelemetry(result.telemetry)
          const activeRunId = activeRunIdRef.current
          if (activeRunId && tokenUsageStoreRef.current) {
            tokenUsageStoreRef.current.recordTelemetry(activeRunId, result.telemetry)
          }
          pushHistoryRef.current(
            `Telemetry Âˇ total ${result.telemetry.totalTokens} Âˇ intent ${result.telemetry.intentTokens} Âˇ files ${result.telemetry.fileTokens} Âˇ system ${result.telemetry.systemTokens}`,
            'system',
          )
        }
        if (jsonOutputEnabled) {
          const wrapWidth = getHistoryWrapWidth(terminalColumnsRef.current)
          bufferedHistory.pushManyBuffered(
            buildJsonPayloadHistoryMessages(result.payload, wrapWidth),
          )
        }
        if (copyEnabled) {
          await maybeCopyToClipboard(true, result.finalPrompt, false)
          pushHistoryRef.current('Copied prompt to clipboard.', 'system')
        }
        if (chatGptEnabled) {
          await maybeOpenChatGpt(true, result.finalPrompt, false)
          pushHistoryRef.current('Opened ChatGPT with generated prompt.', 'system')
        }
        stopStatusMessage = 'Complete'
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown generation error.'
        pushHistoryRef.current(`Generation failed: ${message}`)
        onReasoningUpdate?.(null)
        stopStatusMessage = 'Failed'
      } finally {
        submitRefinement('')
        dispatch({
          type: 'generation-stop',
          ...(stopStatusMessage ? { statusMessage: stopStatusMessage } : {}),
        })
      }
    },
    [
      chatGptEnabled,
      copyEnabled,
      currentModel,
      targetModel,
      files,
      urls,
      images,
      videos,
      polishModelId,
      jsonOutputEnabled,
      smartContextEnabled,
      budgets,
      smartContextRoot,
      normalizedMetaInstructions,
      interactiveTransportPathRef,
      pushHistoryRef,
      bufferedHistory,
      terminalColumnsRef,
      tokenUsageStoreRef,
      handleStreamEvent,
      interactiveDelegate,
      submitRefinement,
      ensureProviderReady,
      lastGeneratedPromptUpdateRef,
      onReasoningUpdate,
      setLatestTelemetry,
      setStatusMessage,
    ],
  )
  const runSeriesGeneration = useCallback(
    async (intent: string) => {
      let generationModel = currentModel.trim() || 'gpt-4o-mini'
      if (videos.length > 0 && !isGemini(generationModel)) {
        generationModel = await resolveGeminiVideoModel()
        pushHistoryRef.current(
          `[series] Switching to ${generationModel} to support video input.`,
          'progress',
        )
      }
      const runtimeTargetModel = (targetModel ?? '').trim() || generationModel
      const providerReady = await ensureProviderReady(generationModel)
      if (!providerReady) {
        return
      }
      dispatch({ type: 'generation-start', statusMessage: 'Series: resolving contextâŚ' })
      pushHistoryRef.current('[series] Starting series generationâŚ', 'progress')
      const prepareDirResult = await prepareSeriesOutputDir(intent)
      const seriesDir = prepareDirResult.seriesDir
      let canWriteFiles = prepareDirResult.canWriteFiles
      if (!canWriteFiles) {
        const message = prepareDirResult.errorMessage ?? 'Unknown filesystem error.'
        pushHistoryRef.current(
          `[series] Failed to prepare output directory: ${message}`,
          'progress',
        )
      }
      try {
        let contextEntries: ContextEntry[] = (
          await resolveFileContext(Array.from(files) as string[])
        ).map((entry) => ({ ...entry, source: 'file' }))
        if (contextEntries.length > 0) {
          pushHistoryRef.current(
            `[series] Added ${contextEntries.length} file context entr${contextEntries.length === 1 ? 'y' : 'ies'}.`,
            'progress',
          )
        }
        if (urls.length > 0) {
          pushHistoryRef.current(`[series] Fetching ${urls.length} URL source(s)âŚ`, 'progress')
          try {
            const urlFiles = await resolveUrlContext(urls, {
              onProgress: (message: string) => {
                pushHistoryRef.current(`[series] ${message}`, 'progress')
                setStatusMessage(`Series: ${message}`)
              },
            })
            if (urlFiles.length > 0) {
              contextEntries = [
                ...contextEntries,
                ...urlFiles.map((entry) => ({ ...entry, source: 'url' as const })),
              ]
            }
          } catch (error) {
            const message = error instanceof Error ? error.message : 'Unknown URL context error.'
            pushHistoryRef.current(`[series] URL context failed: ${message}`, 'progress')
          }
        }
        if (smartContextEnabled) {
          if (notify) {
            notify('Smart context: resolvingâŚ', { kind: 'progress' })
          } else {
            pushHistoryRef.current('[series] Resolving smart contextâŚ', 'progress')
          }
          try {
            const smartFiles = await resolveSmartContextFiles(
              intent,
              contextEntries.map(({ path, content }) => ({ path, content })),
              (message: string) => {
                if (notify) {
                  notify(`Smart context: ${message}`, { kind: 'progress' })
                } else {
                  pushHistoryRef.current(`[series] ${message}`, 'progress')
                }
                setStatusMessage(`Series: ${message}`)
              },
              smartContextRoot ?? undefined,
            )
            if (smartFiles.length > 0) {
              contextEntries = [
                ...contextEntries,
                ...smartFiles.map((entry) => ({ ...entry, source: 'smart' as const })),
              ]
            }
          } catch (error) {
            const message = error instanceof Error ? error.message : 'Unknown smart context error.'
            pushHistoryRef.current(`[series] Smart context failed: ${message}`, 'progress')
          }
        }
        const budgetEvaluation = evaluateContextBudget({
          intentText: intent,
          metaInstructions: normalizedMetaInstructions,
          contextEntries,
          ...(budgets.maxInputTokens !== null ? { maxInputTokens: budgets.maxInputTokens } : {}),
          ...(budgets.maxContextTokens !== null
            ? { maxContextTokens: budgets.maxContextTokens }
            : {}),
          ...(budgets.contextOverflowStrategy !== null
            ? { strategy: budgets.contextOverflowStrategy }
            : {}),
          buildTelemetry: (intentText, files, metaInstructions) =>
            buildTokenTelemetry(intentText, files, metaInstructions),
        })
        setLatestTelemetry(budgetEvaluation.after)
        if (budgetEvaluation.droppedEntries.length > 0) {
          contextEntries = budgetEvaluation.keptEntries
          const droppedCount = budgetEvaluation.droppedEntries.length
          dispatch({
            type: 'set-context-overflow',
            details: {
              event: 'context.overflow',
              strategy: budgetEvaluation.strategy ?? 'fail',
              before: budgetEvaluation.before,
              after: budgetEvaluation.after,
              droppedPaths: budgetEvaluation.droppedPaths,
            },
          })
          pushHistoryRef.current(
            `[series] Context overflow (${budgetEvaluation.strategy ?? 'fail'}) Âˇ dropped ${droppedCount}`,
            'system',
          )
        }
        pushHistoryRef.current(
          `[series] Context ready (${contextEntries.length} file(s)).`,
          'progress',
        )
        const handleUploadState: UploadStateChange = (state, detail) => {
          const action = state === 'start' ? 'Uploading' : 'Uploaded'
          const message = `${action} ${detail.kind}: ${detail.filePath}`
          if (notify) {
            notify(message, { kind: 'progress', autoDismissMs: state === 'start' ? 6000 : 2600 })
            return
          }
          pushHistoryRef.current(`[series] ${message}`, 'progress')
        }
        const request: PromptGenerationRequest = {
          intent,
          model: generationModel,
          targetModel: runtimeTargetModel,
          fileContext: contextEntries.map(({ path, content }) => ({ path, content })),
          images: [...images],
          videos: [...videos],
          pdfs: [...pdfs],
          onUploadStateChange: handleUploadState,
          onSeriesRepairAttempt: ({ attempt, maxAttempts, validationError }) => {
            const normalizedError = validationError.replace(/\s+/g, ' ').trim()
            const shortError =
              normalizedError.length > 140 ? `${normalizedError.slice(0, 137)}âŚ` : normalizedError
            pushHistoryRef.current(
              `[series] Validation failed; attempting automatic repair (${attempt}/${maxAttempts})âŚ Reason: ${shortError}`,
              'progress',
            )
            if (process.env.DEBUG || process.env.VERBOSE) {
              pushHistoryRef.current(
                `[series][debug] Full validation error: ${normalizedError}`,
                'progress',
              )
            }
          },
          ...(normalizedMetaInstructions ? { metaInstructions: normalizedMetaInstructions } : {}),
        }
        setStatusMessage('Series: generatingâŚ')
        const series: SeriesResponse = await generatePromptSeries(request)
        const totalPrompts = 1 + series.atomicPrompts.length
        let writeResult: WriteSeriesArtifactsResult | null = null
        if (canWriteFiles) {
          try {
            writeResult = await writeSeriesArtifacts(seriesDir, series)
            writeResult.errors.forEach((entry) => {
              pushHistoryRef.current(
                `[series] Failed to write ${entry.fileName}: ${entry.message}`,
                'progress',
              )
            })
          } catch (error) {
            canWriteFiles = false
            const message = error instanceof Error ? error.message : 'Unknown filesystem error.'
            pushHistoryRef.current(
              `[series] Failed to write series artifacts: ${message}`,
              'progress',
            )
          }
        }
        pushHistoryRef.current('[series] Overview ready.', 'progress')
        series.atomicPrompts.forEach((step, index) => {
          const stepNumber = index + 1
          const validationSection = extractValidationSection(step.content)
          if (validationSection) {
            pushHistoryRef.current(
              `[Step ${stepNumber}: ${step.title}] Validation section:\n${validationSection}`,
              'system',
            )
            return
          }
          pushHistoryRef.current(
            `[Step ${stepNumber}: ${step.title}] (no Validation section found)`,
            'system',
          )
        })
        if (canWriteFiles) {
          const relativeDir = path.relative(process.cwd(), seriesDir) || seriesDir
          const writtenCount = writeResult?.writtenCount ?? 0
          pushHistoryRef.current(
            `[Series] Saved ${writtenCount}/${totalPrompts} prompts to ${relativeDir}`,
            'system',
          )
        } else {
          pushHistoryRef.current(`[Series] Generated ${totalPrompts} prompts (not saved)`, 'system')
        }
        dispatch({ type: 'generation-stop', statusMessage: 'Series complete' })
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown series generation error.'
        pushHistoryRef.current(`[series] Failed: ${message}`, 'progress')
        dispatch({ type: 'generation-stop', statusMessage: 'Series failed' })
      } finally {
        dispatch({ type: 'generation-stop' })
      }
    },
    [
      currentModel,
      targetModel,
      files,
      urls,
      images,
      videos,
      smartContextEnabled,
      smartContextRoot,
      normalizedMetaInstructions,
      pushHistoryRef,
      notify,
      ensureProviderReady,
      setStatusMessage,
    ],
  )
  const statusChips = useMemo(() => {
    const effectiveStatusMessage = isGenerating
      ? statusMessage
      : isTestCommandRunning
        ? 'Running tests'
        : statusMessage
    const statusChip = `[status:${effectiveStatusMessage}]`
    const normalizedTarget = (targetModel ?? '').trim() || currentModel
    const chips = [statusChip, `[${currentModel}]`, `[target:${normalizedTarget}]`]
    if (latestTelemetry) {
      chips.push(`[tokens:${formatCompactTokens(latestTelemetry.totalTokens)}]`)
    }
    const normalizedPolishModel = (polishModelId ?? '').trim()
    if (normalizedPolishModel) {
      chips.push(`[polish:${normalizedPolishModel}]`)
    }
    chips.push(`[copy:${copyEnabled ? 'on' : 'off'}]`)
    chips.push(`[chatgpt:${chatGptEnabled ? 'on' : 'off'}]`)
    chips.push(`[json:${jsonOutputEnabled ? 'on' : 'off'}]`)
    chips.push(`[files:${files.length}]`)
    chips.push(`[urls:${urls.length}]`)
    chips.push(`[smart:${smartContextEnabled ? 'on' : 'off'}]`)
    chips.push(`[tests:${isTestCommandRunning ? 'running' : 'idle'}]`)
    if (smartContextRoot) {
      chips.push(`[root:${smartContextRoot}]`)
    }
    return chips
  }, [
    isGenerating,
    statusMessage,
    currentModel,
    targetModel,
    latestTelemetry,
    polishModelId,
    copyEnabled,
    chatGptEnabled,
    jsonOutputEnabled,
    files.length,
    urls.length,
    smartContextEnabled,
    smartContextRoot,
    isTestCommandRunning,
  ])
  return {
    isGenerating,
    statusMessage,
    runGeneration,
    runSeriesGeneration,
    statusChips,
    latestContextOverflow: pipelineState.latestContextOverflow,
    isAwaitingRefinement,
    submitRefinement,
    awaitingInteractiveMode,
  }
}
</file>
<file path="src/tui/hooks/usePopupManager.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import { useCallback, useMemo, useReducer, useRef } from 'react'
import {
  JSON_INTERACTIVE_ERROR,
  mapPopupCommandSelection,
  type PopupManagerCommandStep,
} from './popup-manager/command-mapping'
import { createPopupScanOrchestrator } from './popup-manager/scan-orchestrator'
import {
  INITIAL_POPUP_MANAGER_STATE,
  popupReducer,
  type PopupAction,
  type SetStateAction,
} from '../popup-reducer'
import {
  updateCliExportSettings,
  updateCliPromptGeneratorSettings,
  updateCliResumeSettings,
} from '../../config'
import { TOGGLE_LABELS } from '../config'
import { parseBudgetSettingsDraft } from '../budget-settings'
import {
  scanFileSuggestions,
  scanImageSuggestions,
  scanIntentSuggestions,
  scanPdfSuggestions,
  scanSmartSuggestions,
  scanVideoSuggestions,
} from './popup-scans'
import type { NotifyOptions } from '../notifier'
import type { ThemeMode } from '../theme/theme-types'
import { loadGeneratePayloadFromFile } from '../../generate/payload-io'
import { GENERATE_JSON_PAYLOAD_SCHEMA_VERSION } from '../../generate/types'
import { writeGeneratePayloadExport } from '../../export/export-generate-payload'
import {
  loadGenerateHistoryPickerItems,
  loadGeneratePayloadFromHistory,
} from '../../history/generate-history'
import { buildModelPopupOptions } from '../model-popup-options'
import { loadResumeHistoryItems } from '../resume-history'
import { getRecentSessionModels, recordRecentSessionModel } from '../model-session'
import type {
  CommandDescriptor,
  HistoryEntry,
  ModelOption,
  PopupState,
  ResumeMode,
  ResumeSourceKind,
  ToggleField,
} from '../types'
export type PopupManagerActions = {
  openModelPopup: () => void
  openPolishModelPopup: () => void
  openTargetModelPopup: () => void
  openTogglePopup: (field: ToggleField) => void
  openFilePopup: () => void
  openUrlPopup: () => void
  openImagePopup: () => void
  openVideoPopup: () => void
  openPdfPopup: () => void
  openHistoryPopup: () => void
  openResumePopup: () => void
  openExportPopup: () => void
  openSmartRootPopup: () => void
  openTokensPopup: () => void
  openBudgetsPopup: () => void
  openSettingsPopup: () => void
  openThemePopup: () => void
  openThemeModePopup: () => void
  openReasoningPopup: () => void
  openTestPopup: () => void
  openIntentPopup: () => void
  openInstructionsPopup: () => void
  openSeriesPopup: (initialDraft?: string, hintOverride?: string) => void
  closePopup: () => void
  handleCommandSelection: (commandId: CommandDescriptor['id'], argsRaw?: string) => void
  handleModelPopupSubmit: (option: ModelOption | null | undefined) => void
  applyToggleSelection: (field: ToggleField, value: boolean) => void
  handleIntentFileSubmit: (value: string) => void
  handleInstructionsSubmit: (value: string) => void
  handleBudgetsSubmit: () => void
  handleResumeSubmit: () => void
  handleExportSubmit: () => void
  handleSeriesIntentSubmit: (value: string) => void
}
export type ThemeOption = {
  name: string
  label: string
}
export type UsePopupManagerOptions = {
  currentModel: ModelOption['id']
  currentTargetModel: ModelOption['id']
  modelOptions: readonly ModelOption[]
  activeThemeName: string
  themeMode: ThemeMode
  themes: readonly ThemeOption[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  toggleSmartContext: () => void
  setSmartRoot: (value: string) => void
  urls: string[]
  addUrl: (value: string) => void
  images: string[]
  videos: string[]
  pdfs: string[]
  addImage: (value: string) => void
  addVideo: (value: string) => void
  addPdf: (value: string) => void
  lastTestFile: string | null
  defaultTestFile: string
  interactiveTransportPath?: string | undefined
  isGenerating: boolean
  lastUserIntentRef: React.MutableRefObject<string | null>
  pushHistory: (content: string, kind?: HistoryEntry['kind']) => void
  notify: (message: string, options?: NotifyOptions) => void
  setInputValue: (value: string) => void
  runGeneration: (payload: {
    intent?: string
    intentFile?: string
    resume?:
      | { kind: 'history'; selector: string; mode: ResumeMode }
      | { kind: 'file'; payloadPath: string; mode: ResumeMode }
  }) => Promise<void>
  runSeriesGeneration: (intent: string) => void
  runTestsFromCommand: (value: string) => void
  clearScreen?: () => void
  exitApp: () => void
  setCurrentModel: (value: ModelOption['id']) => void
  setCurrentTargetModel: (value: ModelOption['id']) => void
  setPolishModelId: (value: ModelOption['id'] | null) => void
  setCopyEnabled: (value: boolean) => void
  setChatGptEnabled: (value: boolean) => void
  setJsonOutputEnabled: (value: boolean) => void
  setIntentFilePath: (value: string) => void
  intentFilePath: string
  metaInstructions: string
  setMetaInstructions: (value: string) => void
  budgets: {
    maxContextTokens: number | null
    maxInputTokens: number | null
    contextOverflowStrategy: import('../../config').ContextOverflowStrategy | null
  }
  setBudgets: (value: {
    maxContextTokens: number | null
    maxInputTokens: number | null
    contextOverflowStrategy: import('../../config').ContextOverflowStrategy | null
  }) => void
  polishModelId: ModelOption['id'] | null
  copyEnabled: boolean
  chatGptEnabled: boolean
  jsonOutputEnabled: boolean
  getLatestTypedIntent: () => string | null
  syncTypedIntentRef: (intent: string) => void
  resumeDefaults: {
    sourceKind: ResumeSourceKind
    mode: ResumeMode
  }
  setResumeDefaults: (value: { sourceKind: ResumeSourceKind; mode: ResumeMode }) => void
  exportDefaults: {
    format: 'json' | 'yaml'
    outDir: string | null
  }
  setExportDefaults: (value: { format: 'json' | 'yaml'; outDir: string | null }) => void
}
/*
 * Popup state management for the Ink TUI.
 *
 * This hook wires UI actions (open/close/submit) to a pure reducer:
 * `apps/prompt-maker-cli/src/tui/popup-reducer.ts`.
 *
 * Keeping the reducer in a separate module lets us unit test popup transitions
 * without a TTY and keeps this hook focused on effects (async scans, commands).
 */
const POPUP_SUGGESTION_SCAN_LIMIT = 5000
export const usePopupManager = ({
  currentModel,
  currentTargetModel,
  modelOptions,
  activeThemeName,
  themeMode,
  themes,
  smartContextEnabled,
  smartContextRoot,
  toggleSmartContext,
  setSmartRoot,
  urls,
  addUrl,
  images,
  videos,
  pdfs,
  addImage,
  addVideo,
  addPdf,
  lastTestFile,
  defaultTestFile,
  interactiveTransportPath,
  isGenerating,
  lastUserIntentRef,
  pushHistory,
  notify,
  setInputValue,
  runGeneration,
  runSeriesGeneration,
  runTestsFromCommand,
  clearScreen,
  exitApp,
  setCurrentModel,
  setCurrentTargetModel,
  setPolishModelId,
  setCopyEnabled,
  setChatGptEnabled,
  setJsonOutputEnabled,
  setIntentFilePath,
  intentFilePath,
  metaInstructions,
  setMetaInstructions,
  budgets,
  setBudgets,
  polishModelId,
  copyEnabled,
  chatGptEnabled,
  jsonOutputEnabled,
  getLatestTypedIntent,
  syncTypedIntentRef,
  resumeDefaults,
  setResumeDefaults,
  exportDefaults,
  setExportDefaults,
}: UsePopupManagerOptions): {
  popupState: PopupState
  setPopupState: React.Dispatch<React.SetStateAction<PopupState>>
  actions: PopupManagerActions
} => {
  const scanIdRef = useRef(0)
  const [popupManagerState, dispatch] = useReducer(popupReducer, INITIAL_POPUP_MANAGER_STATE)
  const popupState = popupManagerState.popupState
  const { runSuggestionScan } = useMemo(
    () => createPopupScanOrchestrator({ scanIdRef, dispatch, pushHistory }),
    [dispatch, pushHistory],
  )
  // Compatibility shim: keeps the existing `setPopupState(prev => ...)` call sites working.
  // Internally we treat it as a reducer action.
  const setPopupState = useCallback<React.Dispatch<SetStateAction<PopupState>>>((next) => {
    dispatch({ type: 'set', next } satisfies PopupAction)
  }, [])
  const closePopup = useCallback(() => {
    dispatch({ type: 'close' })
  }, [])
  const openModelPopup = useCallback(() => {
    const recentModelIds = getRecentSessionModels()
    const { options } = buildModelPopupOptions({ query: '', modelOptions, recentModelIds })
    const defaultIndex = Math.max(
      0,
      options.findIndex((option) => option.id === currentModel),
    )
    dispatch({ type: 'open-model', kind: 'generation', query: '', selectionIndex: defaultIndex })
  }, [currentModel, modelOptions])
  const openPolishModelPopup = useCallback(() => {
    const recentModelIds = getRecentSessionModels()
    const { options } = buildModelPopupOptions({ query: '', modelOptions, recentModelIds })
    const selectedId = polishModelId ?? currentModel
    const defaultIndex = Math.max(
      0,
      options.findIndex((option) => option.id === selectedId),
    )
    dispatch({ type: 'open-model', kind: 'polish', query: '', selectionIndex: defaultIndex })
  }, [currentModel, modelOptions, polishModelId])
  const openTargetModelPopup = useCallback(() => {
    const recentModelIds = getRecentSessionModels()
    const { options } = buildModelPopupOptions({ query: '', modelOptions, recentModelIds })
    const defaultIndex = Math.max(
      0,
      options.findIndex((option) => option.id === currentTargetModel),
    )
    dispatch({ type: 'open-model', kind: 'target', query: '', selectionIndex: defaultIndex })
  }, [currentTargetModel, modelOptions])
  const openTogglePopup = useCallback(
    (field: ToggleField) => {
      const currentValue =
        field === 'copy' ? copyEnabled : field === 'chatgpt' ? chatGptEnabled : jsonOutputEnabled
      dispatch({
        type: 'open-toggle',
        field,
        selectionIndex: currentValue ? 0 : 1,
      })
    },
    [copyEnabled, chatGptEnabled, jsonOutputEnabled],
  )
  const openFilePopup = useCallback(() => {
    runSuggestionScan({
      kind: 'file',
      open: (scanId) => ({ type: 'open-file', scanId }),
      scan: () => scanFileSuggestions({ cwd: process.cwd(), limit: POPUP_SUGGESTION_SCAN_LIMIT }),
    })
  }, [runSuggestionScan])
  const openUrlPopup = useCallback(() => {
    dispatch({ type: 'open-url' })
  }, [])
  const openImagePopup = useCallback(() => {
    runSuggestionScan({
      kind: 'image',
      open: (scanId) => ({ type: 'open-image', scanId }),
      scan: () => scanImageSuggestions({ cwd: process.cwd(), limit: POPUP_SUGGESTION_SCAN_LIMIT }),
    })
  }, [runSuggestionScan])
  const openVideoPopup = useCallback(() => {
    runSuggestionScan({
      kind: 'video',
      open: (scanId) => ({ type: 'open-video', scanId }),
      scan: () => scanVideoSuggestions({ cwd: process.cwd(), limit: POPUP_SUGGESTION_SCAN_LIMIT }),
    })
  }, [runSuggestionScan])
  const openPdfPopup = useCallback(() => {
    runSuggestionScan({
      kind: 'pdf',
      open: (scanId) => ({ type: 'open-pdf', scanId }),
      scan: () => scanPdfSuggestions({ cwd: process.cwd(), limit: POPUP_SUGGESTION_SCAN_LIMIT }),
    })
  }, [runSuggestionScan])
  const openHistoryPopup = useCallback(() => {
    dispatch({ type: 'open-history' })
  }, [])
  const openResumePopup = useCallback(() => {
    const resumeDefaultsSnapshot = resumeDefaults
    runSuggestionScan({
      kind: 'resume',
      open: (scanId) => ({
        type: 'open-resume',
        scanId,
        sourceKind: resumeDefaultsSnapshot.sourceKind,
        mode: resumeDefaultsSnapshot.mode,
        payloadPathDraft: '',
        historyItems: [],
        historySelectionIndex: 0,
        historyErrorMessage: null,
      }),
      scan: () => scanFileSuggestions({ cwd: process.cwd(), limit: POPUP_SUGGESTION_SCAN_LIMIT }),
    })
    const hydrate = async (): Promise<void> => {
      const historyResult = await loadResumeHistoryItems({ limit: 30 })
      setPopupState((prev) => {
        if (prev?.type !== 'resume') {
          return prev
        }
        return {
          ...prev,
          historyItems: historyResult.ok ? historyResult.items : [],
          historyErrorMessage: historyResult.ok ? null : historyResult.errorMessage,
        }
      })
    }
    void hydrate()
  }, [resumeDefaults, runSuggestionScan, setPopupState])
  const openExportPopup = useCallback(() => {
    const exportDefaultsSnapshot = exportDefaults
    const fileName = `prompt-export.${exportDefaultsSnapshot.format}`
    const outPathDraft = exportDefaultsSnapshot.outDir
      ? path.join(exportDefaultsSnapshot.outDir, fileName)
      : fileName
    dispatch({
      type: 'open-export',
      format: exportDefaultsSnapshot.format,
      outPathDraft,
      historyItems: [],
      historySelectionIndex: 0,
      historyErrorMessage: null,
    })
    const hydrate = async (): Promise<void> => {
      const historyResult = await loadGenerateHistoryPickerItems({ limit: 30 })
      setPopupState((prev) => {
        if (prev?.type !== 'export') {
          return prev
        }
        return {
          ...prev,
          historyItems: historyResult.ok ? historyResult.items : [],
          historyErrorMessage: historyResult.ok ? null : historyResult.errorMessage,
        }
      })
    }
    void hydrate()
  }, [exportDefaults, setPopupState])
  const openSmartRootPopup = useCallback(() => {
    const draft = smartContextRoot ?? ''
    runSuggestionScan({
      kind: 'smart',
      open: (scanId) => ({ type: 'open-smart', scanId, draft }),
      scan: () => scanSmartSuggestions({ cwd: process.cwd(), limit: POPUP_SUGGESTION_SCAN_LIMIT }),
    })
  }, [runSuggestionScan, smartContextRoot])
  const openTokensPopup = useCallback(() => {
    dispatch({ type: 'open-tokens' })
  }, [])
  const openBudgetsPopup = useCallback(() => {
    dispatch({
      type: 'open-budgets',
      maxContextTokens: budgets.maxContextTokens,
      maxInputTokens: budgets.maxInputTokens,
      contextOverflowStrategy: budgets.contextOverflowStrategy,
    })
  }, [budgets.contextOverflowStrategy, budgets.maxContextTokens, budgets.maxInputTokens])
  const openSettingsPopup = useCallback(() => {
    dispatch({ type: 'open-settings' })
  }, [])
  const openThemePopup = useCallback(() => {
    const selectionIndex = Math.max(
      0,
      themes.findIndex((theme) => theme.name === activeThemeName),
    )
    dispatch({ type: 'open-theme', selectionIndex, initialThemeName: activeThemeName })
  }, [activeThemeName, themes])
  const openThemeModePopup = useCallback(() => {
    const selectionIndex = themeMode === 'system' ? 0 : themeMode === 'dark' ? 1 : 2
    dispatch({ type: 'open-theme-mode', selectionIndex, initialMode: themeMode })
  }, [themeMode])
  const openReasoningPopup = useCallback(() => {
    dispatch({ type: 'open-reasoning', scrollOffset: 0 })
  }, [])
  const openTestPopup = useCallback(() => {
    dispatch({ type: 'open-test', draft: lastTestFile ?? defaultTestFile })
  }, [defaultTestFile, lastTestFile])
  const openIntentPopup = useCallback(() => {
    runSuggestionScan({
      kind: 'intent',
      open: (scanId) => ({ type: 'open-intent', scanId, draft: intentFilePath }),
      scan: () => scanIntentSuggestions({ cwd: process.cwd(), limit: POPUP_SUGGESTION_SCAN_LIMIT }),
    })
  }, [intentFilePath, runSuggestionScan])
  const openInstructionsPopup = useCallback(() => {
    dispatch({ type: 'open-instructions', draft: metaInstructions })
  }, [metaInstructions])
  const openSeriesPopup = useCallback(
    (initialDraft?: string, hintOverride?: string) => {
      const trimmedIntentFile = intentFilePath.trim()
      const defaultHint = trimmedIntentFile
        ? 'Draft prefills from typed/last intent; if empty, loads the intent file.'
        : 'Draft prefills from typed/last intent (or pass /series <intent>).'
      dispatch({
        type: 'open-series',
        draft: initialDraft ?? '',
        hint: hintOverride ?? defaultHint,
      })
    },
    [intentFilePath],
  )
  const applyModelSelection = useCallback(
    (option?: ModelOption) => {
      if (!option) {
        return
      }
      recordRecentSessionModel(option.id)
      setCurrentModel(option.id)
      notify(`Selected model: ${option.label} (${option.id})`, { kind: 'info' })
      setInputValue('')
      closePopup()
    },
    [closePopup, notify, setCurrentModel, setInputValue],
  )
  const applyTargetModelSelection = useCallback(
    (option?: ModelOption) => {
      if (!option) {
        return
      }
      recordRecentSessionModel(option.id)
      setCurrentTargetModel(option.id)
      notify(`Selected target model: ${option.label} (${option.id})`, { kind: 'info' })
      setInputValue('')
      closePopup()
    },
    [closePopup, notify, setCurrentTargetModel, setInputValue],
  )
  const applyPolishModelSelection = useCallback(
    (option: ModelOption | null | undefined) => {
      if (option === null) {
        setPolishModelId(null)
        notify('Polish disabled', { kind: 'warning' })
        setInputValue('')
        closePopup()
        return
      }
      if (!option) {
        return
      }
      recordRecentSessionModel(option.id)
      setPolishModelId(option.id)
      notify(`Selected polish model: ${option.label} (${option.id})`, { kind: 'info' })
      setInputValue('')
      closePopup()
    },
    [closePopup, notify, setInputValue, setPolishModelId],
  )
  const handleModelPopupSubmit = useCallback(
    (option: ModelOption | null | undefined) => {
      if (popupState?.type === 'model') {
        if (popupState.kind === 'target') {
          applyTargetModelSelection(option ?? undefined)
          return
        }
        if (popupState.kind === 'polish') {
          applyPolishModelSelection(option)
          return
        }
      }
      applyModelSelection(option ?? undefined)
    },
    [applyModelSelection, applyPolishModelSelection, applyTargetModelSelection, popupState],
  )
  const applyToggleSelection = useCallback(
    (field: ToggleField, value: boolean) => {
      // Guardrail: JSON output and interactive transport both want to âownâ stdout.
      if (field === 'json' && value && interactiveTransportPath) {
        pushHistory(JSON_INTERACTIVE_ERROR, 'system')
        setInputValue('')
        closePopup()
        return
      }
      if (field === 'json') {
        setJsonOutputEnabled(value)
        notify(
          value
            ? 'JSON output is ON (payload shown in history)'
            : 'JSON output is OFF (payload hidden)',
          { kind: value ? 'info' : 'warning' },
        )
        setInputValue('')
        closePopup()
        return
      }
      const message = `${TOGGLE_LABELS[field]} ${value ? 'enabled' : 'disabled'}`
      if (field === 'copy') {
        setCopyEnabled(value)
      } else {
        setChatGptEnabled(value)
      }
      pushHistory(message)
      setInputValue('')
      closePopup()
    },
    [
      closePopup,
      interactiveTransportPath,
      notify,
      pushHistory,
      setChatGptEnabled,
      setCopyEnabled,
      setInputValue,
      setJsonOutputEnabled,
    ],
  )
  const handleIntentFileSubmit = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      setIntentFilePath(trimmed)
      pushHistory(
        trimmed ? `Intent file set to ${trimmed}` : 'Intent file cleared; using typed intent.',
      )
      setInputValue('')
      closePopup()
    },
    [closePopup, pushHistory, setInputValue, setIntentFilePath],
  )
  const handleInstructionsSubmit = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      setMetaInstructions(trimmed)
      pushHistory(trimmed ? `[instr] ${trimmed}` : '[instr] cleared')
      setInputValue('')
      closePopup()
    },
    [closePopup, pushHistory, setInputValue, setMetaInstructions],
  )
  const handleBudgetsSubmit = useCallback(() => {
    if (popupState?.type !== 'budgets') {
      return
    }
    const parsed = parseBudgetSettingsDraft({
      maxContextTokensDraft: popupState.maxContextTokensDraft,
      maxInputTokensDraft: popupState.maxInputTokensDraft,
      contextOverflowStrategyDraft: popupState.contextOverflowStrategyDraft,
    })
    if (!parsed.ok) {
      setPopupState((prev) =>
        prev?.type === 'budgets' ? { ...prev, errorMessage: parsed.errorMessage } : prev,
      )
      return
    }
    const persist = async (): Promise<void> => {
      try {
        await updateCliPromptGeneratorSettings({
          maxContextTokens: parsed.settings.maxContextTokens,
          maxInputTokens: parsed.settings.maxInputTokens,
          contextOverflowStrategy: parsed.settings.contextOverflowStrategy,
        })
        setBudgets(parsed.settings)
        const enabled =
          parsed.settings.maxContextTokens !== null || parsed.settings.maxInputTokens !== null
        const summary = enabled
          ? `Budgets saved Âˇ input=${parsed.settings.maxInputTokens ?? 'unset'} Âˇ context=${parsed.settings.maxContextTokens ?? 'unset'} Âˇ overflow=${parsed.settings.contextOverflowStrategy ?? 'fail'}`
          : 'Budgets cleared'
        pushHistory(`[budgets] ${summary}`, 'system')
        notify(summary, { kind: enabled ? 'info' : 'warning' })
        setInputValue('')
        closePopup()
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown config write error.'
        setPopupState((prev) =>
          prev?.type === 'budgets'
            ? { ...prev, errorMessage: `Failed to save budgets: ${message}` }
            : prev,
        )
        notify(`Failed to save budgets: ${message}`, { kind: 'error' })
      }
    }
    void persist()
  }, [closePopup, notify, popupState, pushHistory, setBudgets, setInputValue, setPopupState])
  const handleResumeSubmit = useCallback(() => {
    if (popupState?.type !== 'resume') {
      return
    }
    const sourceKind = popupState.sourceKind
    const mode = popupState.mode
    const persistDefaults = async (): Promise<void> => {
      try {
        await updateCliResumeSettings({ resumeMode: mode, resumeSourceKind: sourceKind })
        setResumeDefaults({ sourceKind, mode })
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown config write error.'
        notify(`Failed to save resume defaults: ${message}`, { kind: 'error' })
      }
    }
    void persistDefaults()
    if (sourceKind === 'history') {
      const selected = popupState.historyItems[popupState.historySelectionIndex]
      if (!selected) {
        const message = popupState.historyErrorMessage ?? 'No resumable history entries found.'
        pushHistory(`[resume] ${message}`, 'system')
        notify(message, { kind: 'warning' })
        return
      }
      pushHistory(`> /resume ${selected.selector} (${mode})`, 'user')
      setInputValue('')
      closePopup()
      const intentFileOverride = intentFilePath.trim()
      void runGeneration({
        ...(intentFileOverride ? { intentFile: intentFileOverride } : {}),
        resume: { kind: 'history', selector: selected.selector, mode },
      })
      return
    }
    const payloadPath = popupState.payloadPathDraft.trim()
    if (!payloadPath) {
      const message = 'Resume-from file path is required.'
      pushHistory(`[resume] ${message}`, 'system')
      notify(message, { kind: 'warning' })
      return
    }
    const validateAndRun = async (): Promise<void> => {
      try {
        await loadGeneratePayloadFromFile(payloadPath)
      } catch (error) {
        const rawMessage = error instanceof Error ? error.message : 'Unknown payload error.'
        const schemaHint = rawMessage.includes('schemaVersion')
          ? ` This prompt-maker-cli supports schemaVersion=${GENERATE_JSON_PAYLOAD_SCHEMA_VERSION}; try upgrading/downgrading prompt-maker-cli or re-exporting/regenerating the payload with a matching version.`
          : ''
        const message = `${rawMessage}${schemaHint}`
        pushHistory(`[resume] ${message}`, 'system')
        notify(message, { kind: 'error' })
        return
      }
      pushHistory(`> /resume-from ${payloadPath} (${mode})`, 'user')
      setInputValue('')
      closePopup()
      const intentFileOverride = intentFilePath.trim()
      await runGeneration({
        ...(intentFileOverride ? { intentFile: intentFileOverride } : {}),
        resume: { kind: 'file', payloadPath, mode },
      })
    }
    void validateAndRun()
  }, [
    closePopup,
    intentFilePath,
    notify,
    popupState,
    pushHistory,
    runGeneration,
    setInputValue,
    setResumeDefaults,
  ])
  const handleExportSubmit = useCallback(() => {
    if (popupState?.type !== 'export') {
      return
    }
    const format = popupState.format
    const outPath = popupState.outPathDraft.trim()
    const selected = popupState.historyItems[popupState.historySelectionIndex]
    if (!selected) {
      const message = popupState.historyErrorMessage ?? 'No history entries available for export.'
      pushHistory(`[export] ${message}`, 'system')
      notify(message, { kind: 'warning' })
      return
    }
    if (!outPath) {
      const message = 'Export output path is required.'
      pushHistory(`[export] ${message}`, 'system')
      notify(message, { kind: 'warning' })
      return
    }
    const persistDefaults = async (): Promise<void> => {
      try {
        const resolvedOutPath = path.resolve(process.cwd(), outPath)
        const outDir = path.dirname(resolvedOutPath)
        await updateCliExportSettings({ exportFormat: format, exportOutDir: outDir })
        setExportDefaults({ format, outDir })
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown config write error.'
        notify(`Failed to save export defaults: ${message}`, { kind: 'error' })
      }
    }
    void persistDefaults()
    const exportFromHistory = async (): Promise<void> => {
      try {
        const payload = await loadGeneratePayloadFromHistory({ selector: selected.selector })
        const { absolutePath } = await writeGeneratePayloadExport({
          payload,
          format,
          outPath,
        })
        const relative = path.relative(process.cwd(), absolutePath)
        const displayPath = relative && !relative.startsWith('..') ? relative : absolutePath
        pushHistory(`> /export ${selected.selector} (${format})`, 'user')
        pushHistory(`[export] Exported ${format.toUpperCase()} â ${displayPath}`, 'system')
        notify(`Exported ${format.toUpperCase()} â ${displayPath}`, { kind: 'info' })
        setInputValue('')
        closePopup()
      } catch (error) {
        const rawMessage = error instanceof Error ? error.message : 'Unknown export error.'
        const normalized = rawMessage.replace(/\s+/g, ' ').trim()
        const shortMessage = normalized.length > 220 ? `${normalized.slice(0, 217)}âŚ` : normalized
        pushHistory(`[export] Export failed: ${shortMessage}`, 'system')
        notify(`Export failed: ${shortMessage}`, { kind: 'error' })
      }
    }
    void exportFromHistory()
  }, [closePopup, notify, popupState, pushHistory, setExportDefaults, setInputValue])
  const handleSeriesIntentSubmit = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      if (!trimmed) {
        pushHistory('Series intent cannot be empty.', 'system')
        return
      }
      lastUserIntentRef.current = trimmed
      syncTypedIntentRef(trimmed)
      pushHistory(`> /series ${trimmed}`, 'user')
      setInputValue('')
      closePopup()
      void runSeriesGeneration(trimmed)
    },
    [
      closePopup,
      lastUserIntentRef,
      pushHistory,
      runSeriesGeneration,
      setInputValue,
      syncTypedIntentRef,
    ],
  )
  const runCommandSteps = useCallback(
    (steps: readonly PopupManagerCommandStep[]): void => {
      const pushHistoryEntry = (
        step: Extract<PopupManagerCommandStep, { type: 'push-history' }>,
      ): void => {
        if (step.kind) {
          pushHistory(step.message, step.kind)
          return
        }
        pushHistory(step.message)
      }
      for (const step of steps) {
        switch (step.type) {
          case 'open-popup':
            switch (step.popup) {
              case 'model':
                openModelPopup()
                break
              case 'target':
                openTargetModelPopup()
                break
              case 'polish':
                openPolishModelPopup()
                break
              case 'toggle': {
                const field = step.field
                if (!field) {
                  throw new Error('Expected toggle field in command mapping.')
                }
                openTogglePopup(field)
                break
              }
              case 'file':
                openFilePopup()
                break
              case 'url':
                openUrlPopup()
                break
              case 'image':
                openImagePopup()
                break
              case 'video':
                openVideoPopup()
                break
              case 'pdf':
                openPdfPopup()
                break
              case 'history':
                openHistoryPopup()
                break
              case 'resume':
                openResumePopup()
                break
              case 'export':
                openExportPopup()
                break
              case 'smart-root':
                openSmartRootPopup()
                break
              case 'tokens':
                openTokensPopup()
                break
              case 'budgets':
                openBudgetsPopup()
                break
              case 'settings':
                openSettingsPopup()
                break
              case 'theme':
                openThemePopup()
                break
              case 'theme-mode':
                openThemeModePopup()
                break
              case 'reasoning':
                openReasoningPopup()
                break
              case 'test':
                openTestPopup()
                break
              case 'intent':
                openIntentPopup()
                break
              case 'instructions':
                openInstructionsPopup()
                break
            }
            break
          case 'apply-toggle':
            applyToggleSelection(step.field, step.value)
            break
          case 'clear-polish':
            applyPolishModelSelection(null)
            break
          case 'add-url':
            addUrl(step.value)
            break
          case 'add-image':
            addImage(step.value)
            break
          case 'add-video':
            addVideo(step.value)
            break
          case 'add-pdf':
            addPdf(step.value)
            break
          case 'toggle-smart-context':
            toggleSmartContext()
            break
          case 'set-smart-root':
            setSmartRoot(step.value)
            break
          case 'set-intent-file':
            setIntentFilePath(step.value)
            break
          case 'set-meta-instructions':
            setMetaInstructions(step.value)
            break
          case 'push-history':
            pushHistoryEntry(step)
            break
          case 'notify':
            notify(step.message, { kind: step.kind })
            break
          case 'set-input':
            setInputValue(step.value)
            break
          case 'close-popup':
            closePopup()
            break
          case 'clear-screen':
            clearScreen?.()
            break
          case 'exit-app':
            exitApp()
            break
          case 'run-tests':
            void runTestsFromCommand(step.value)
            break
        }
      }
    },
    [
      addImage,
      addUrl,
      addVideo,
      applyPolishModelSelection,
      applyToggleSelection,
      clearScreen,
      closePopup,
      exitApp,
      notify,
      openFilePopup,
      openHistoryPopup,
      openResumePopup,
      openExportPopup,
      openImagePopup,
      openInstructionsPopup,
      openIntentPopup,
      openModelPopup,
      openPolishModelPopup,
      openReasoningPopup,
      openSettingsPopup,
      openSmartRootPopup,
      openTargetModelPopup,
      openTestPopup,
      openThemeModePopup,
      openThemePopup,
      openTogglePopup,
      openTokensPopup,
      openUrlPopup,
      openVideoPopup,
      pushHistory,
      runTestsFromCommand,
      setInputValue,
      setIntentFilePath,
      setMetaInstructions,
      setSmartRoot,
      toggleSmartContext,
    ],
  )
  const runSeriesCommand = useCallback(
    (trimmedArgs: string): void => {
      const handle = async (): Promise<void> => {
        if (isGenerating) {
          pushHistory('Generation already running. Please wait.', 'system')
          return
        }
        const latestTypedIntent = getLatestTypedIntent()
        const typedDraft = latestTypedIntent?.trim() ?? ''
        let initialDraft = trimmedArgs || typedDraft || lastUserIntentRef.current || ''
        let hintOverride: string | undefined
        if (trimmedArgs) {
          pushHistory('[series] Using provided text as intent draft.', 'system')
        } else if (typedDraft) {
          pushHistory('[series] Using typed intent as draft.', 'system')
        } else if (lastUserIntentRef.current) {
          pushHistory('[series] Reusing last intent as draft.', 'system')
        }
        if (!initialDraft) {
          const trimmedIntentFile = intentFilePath.trim()
          if (trimmedIntentFile) {
            try {
              const raw = await fs.readFile(trimmedIntentFile, 'utf8')
              const fileIntent = raw.trim()
              if (fileIntent) {
                initialDraft = fileIntent
                const fileLabel = path.basename(trimmedIntentFile)
                pushHistory(`[series] Loaded draft from intent file ${fileLabel}.`, 'system')
                hintOverride = `Loaded from intent file ${fileLabel}`
                syncTypedIntentRef(fileIntent)
              } else {
                pushHistory(
                  `[series] Intent file ${trimmedIntentFile} is empty; please add content.`,
                  'system',
                )
              }
            } catch (error) {
              const message = error instanceof Error ? error.message : 'Unknown intent file error.'
              pushHistory(
                `[series] Failed to read intent file ${trimmedIntentFile}: ${message}`,
                'system',
              )
            }
          }
        }
        if (!initialDraft) {
          pushHistory('[series] No intent found; enter one in the popup.', 'system')
        }
        openSeriesPopup(initialDraft, hintOverride)
        setInputValue('')
      }
      void handle()
    },
    [
      getLatestTypedIntent,
      intentFilePath,
      isGenerating,
      lastUserIntentRef,
      openSeriesPopup,
      pushHistory,
      setInputValue,
      syncTypedIntentRef,
    ],
  )
  const handleCommandSelection = useCallback(
    (commandId: CommandDescriptor['id'], argsRaw?: string) => {
      const result = mapPopupCommandSelection({
        commandId,
        argsRaw,
        context: {
          copyEnabled,
          chatGptEnabled,
          jsonOutputEnabled,
          interactiveTransportPath,
          urls,
          images,
          videos,
          pdfs,
          smartContextEnabled,
          smartContextRoot,
        },
      })
      if (result.kind === 'series') {
        runSeriesCommand(result.trimmedArgs)
        return
      }
      runCommandSteps(result.steps)
    },
    [
      chatGptEnabled,
      copyEnabled,
      images,
      interactiveTransportPath,
      jsonOutputEnabled,
      runCommandSteps,
      runSeriesCommand,
      smartContextEnabled,
      smartContextRoot,
      urls,
      videos,
      pdfs,
    ],
  )
  // Memoizing the actions object keeps `actions` referentially stable.
  // This reduces avoidable rerenders in components that receive `actions`.
  const actions = useMemo<PopupManagerActions>(
    () => ({
      openModelPopup,
      openPolishModelPopup,
      openTargetModelPopup,
      openTogglePopup,
      openFilePopup,
      openUrlPopup,
      openImagePopup,
      openVideoPopup,
      openPdfPopup,
      openHistoryPopup,
      openResumePopup,
      openExportPopup,
      openSmartRootPopup,
      openTokensPopup,
      openBudgetsPopup,
      openSettingsPopup,
      openThemePopup,
      openThemeModePopup,
      openReasoningPopup,
      openTestPopup,
      openIntentPopup,
      openInstructionsPopup,
      openSeriesPopup,
      closePopup,
      handleCommandSelection,
      handleModelPopupSubmit,
      applyToggleSelection,
      handleIntentFileSubmit,
      handleInstructionsSubmit,
      handleBudgetsSubmit,
      handleResumeSubmit,
      handleExportSubmit,
      handleSeriesIntentSubmit,
    }),
    [
      openModelPopup,
      openPolishModelPopup,
      openTargetModelPopup,
      openTogglePopup,
      openFilePopup,
      openUrlPopup,
      openImagePopup,
      openVideoPopup,
      openPdfPopup,
      openHistoryPopup,
      openResumePopup,
      openExportPopup,
      openSmartRootPopup,
      openTokensPopup,
      openBudgetsPopup,
      openSettingsPopup,
      openThemePopup,
      openThemeModePopup,
      openReasoningPopup,
      openTestPopup,
      openIntentPopup,
      openInstructionsPopup,
      openSeriesPopup,
      closePopup,
      handleCommandSelection,
      handleModelPopupSubmit,
      applyToggleSelection,
      handleIntentFileSubmit,
      handleInstructionsSubmit,
      handleBudgetsSubmit,
      handleResumeSubmit,
      handleExportSubmit,
      handleSeriesIntentSubmit,
    ],
  )
  return {
    popupState,
    setPopupState,
    actions,
  }
}
</file>
<file path="src/tui/screens/command/hooks/usePopupKeyboardShortcuts.ts">
import { useInput, type Key } from 'ink'
import type { Dispatch, SetStateAction } from 'react'
import { useStableCallback } from '../../../hooks/useStableCallback'
import type { HistoryEntry, ModelOption, PopupState } from '../../../types'
import { handleEscapeOnlyPopupShortcuts } from './popup-shortcuts/escape-only-popup-shortcuts'
import { handleHistoryPopupShortcuts } from './popup-shortcuts/history-popup-shortcuts'
import { handleIntentPopupShortcuts } from './popup-shortcuts/intent-popup-shortcuts'
import { handleResumePopupShortcuts } from './popup-shortcuts/resume-popup-shortcuts'
import { handleExportPopupShortcuts } from './popup-shortcuts/export-popup-shortcuts'
import { handleModelPopupShortcuts } from './popup-shortcuts/model-popup-shortcuts'
import { handleReasoningPopupShortcuts } from './popup-shortcuts/reasoning-popup-shortcuts'
import { handleSmartPopupShortcuts } from './popup-shortcuts/smart-popup-shortcuts'
import { handleBudgetsPopupShortcuts } from './popup-shortcuts/budgets-popup-shortcuts'
import { handleSuggestedSelectedListPopupShortcuts } from './popup-shortcuts/suggested-selected-list-popup-shortcuts'
import {
  handleThemeModePopupShortcuts,
  handleThemePopupShortcuts,
} from './popup-shortcuts/theme-popup-shortcuts'
import { handleTogglePopupShortcuts } from './popup-shortcuts/toggle-popup-shortcuts'
import { handleUrlPopupShortcuts } from './popup-shortcuts/url-popup-shortcuts'
export type UsePopupKeyboardShortcutsOptions = {
  popupState: PopupState
  helpOpen: boolean
  setPopupState: Dispatch<SetStateAction<PopupState>>
  closePopup: () => void
  model: {
    options: ModelOption[]
    onSubmit: (option: ModelOption | null | undefined) => void
  }
  toggle: {
    applySelection: (field: 'copy' | 'chatgpt' | 'json', value: boolean) => void
  }
  theme: {
    count: number
    onConfirm: () => void
    onCancel: () => void
  }
  themeMode: {
    count: number
    onConfirm: () => void
    onCancel: () => void
  }
  budgets: {
    onSubmit: () => void
  }
  file: {
    items: string[]
    suggestions: string[]
    onAdd: (value: string) => void
    onRemove: (index: number) => void
  }
  url: {
    items: string[]
    onRemove: (index: number) => void
  }
  image: {
    items: string[]
    suggestions: string[]
    onAdd: (value: string) => void
    onRemove: (index: number) => void
  }
  video: {
    items: string[]
    suggestions: string[]
    onAdd: (value: string) => void
    onRemove: (index: number) => void
  }
  pdf: {
    items: string[]
    suggestions: string[]
    onAdd: (value: string) => void
    onRemove: (index: number) => void
  }
  history: {
    items: string[]
  }
  resume: {
    onSubmit: () => void
  }
  export: {
    onSubmit: () => void
  }
  smart: {
    suggestions: string[]
    contextRoot: string | null
    onRootSubmit: (value: string) => void
  }
  intent: {
    suggestions: string[]
    onFileSubmit: (value: string) => void
  }
  reasoning: {
    lines: HistoryEntry[]
    visibleRows: number
  }
}
export const usePopupKeyboardShortcuts = ({
  popupState,
  helpOpen,
  setPopupState,
  closePopup,
  model,
  toggle,
  theme,
  themeMode,
  budgets,
  file,
  url,
  image,
  video,
  pdf,
  history,
  resume,
  export: exportActions,
  smart,
  intent,
  reasoning,
}: UsePopupKeyboardShortcutsOptions): void => {
  const isActive = popupState !== null && !helpOpen
  const handlePopupKey = useStableCallback((input: string, key: Key) => {
    if (!popupState) {
      return
    }
    switch (popupState.type) {
      case 'model':
        handleModelPopupShortcuts({
          popupState,
          input,
          key,
          options: model.options,
          setPopupState,
          closePopup,
          onModelPopupSubmit: model.onSubmit,
        })
        return
      case 'toggle':
        handleTogglePopupShortcuts({
          popupState,
          key,
          setPopupState,
          closePopup,
          applyToggleSelection: toggle.applySelection,
        })
        return
      case 'theme':
        handleThemePopupShortcuts({
          popupState,
          key,
          themeCount: theme.count,
          setPopupState,
          onThemeConfirm: theme.onConfirm,
          onThemeCancel: theme.onCancel,
        })
        return
      case 'themeMode':
        handleThemeModePopupShortcuts({
          popupState,
          key,
          optionCount: themeMode.count,
          setPopupState,
          onThemeModeConfirm: themeMode.onConfirm,
          onThemeModeCancel: themeMode.onCancel,
        })
        return
      case 'budgets':
        handleBudgetsPopupShortcuts({
          popupState,
          key,
          setPopupState,
          closePopup,
          onBudgetsSubmit: budgets.onSubmit,
        })
        return
      case 'file':
        handleSuggestedSelectedListPopupShortcuts({
          popupType: 'file',
          popupState,
          input,
          key,
          itemsLength: file.items.length,
          suggestions: file.suggestions,
          setPopupState,
          closePopup,
          onRemove: file.onRemove,
          onSelectSuggestion: file.onAdd,
        })
        return
      case 'url':
        handleUrlPopupShortcuts({
          popupState,
          input,
          key,
          urls: url.items,
          setPopupState,
          closePopup,
          onRemoveUrl: url.onRemove,
        })
        return
      case 'image':
        handleSuggestedSelectedListPopupShortcuts({
          popupType: 'image',
          popupState,
          input,
          key,
          itemsLength: image.items.length,
          suggestions: image.suggestions,
          setPopupState,
          closePopup,
          onRemove: image.onRemove,
          onSelectSuggestion: image.onAdd,
        })
        return
      case 'video':
        handleSuggestedSelectedListPopupShortcuts({
          popupType: 'video',
          popupState,
          input,
          key,
          itemsLength: video.items.length,
          suggestions: video.suggestions,
          setPopupState,
          closePopup,
          onRemove: video.onRemove,
          onSelectSuggestion: video.onAdd,
        })
        return
      case 'pdf':
        handleSuggestedSelectedListPopupShortcuts({
          popupType: 'pdf',
          popupState,
          input,
          key,
          itemsLength: pdf.items.length,
          suggestions: pdf.suggestions,
          setPopupState,
          closePopup,
          onRemove: pdf.onRemove,
          onSelectSuggestion: pdf.onAdd,
        })
        return
      case 'history':
        handleHistoryPopupShortcuts({
          popupState,
          key,
          itemCount: history.items.length,
          setPopupState,
          closePopup,
        })
        return
      case 'resume':
        handleResumePopupShortcuts({
          popupState,
          key,
          setPopupState,
          closePopup,
          onResumeSubmit: resume.onSubmit,
        })
        return
      case 'export':
        handleExportPopupShortcuts({
          popupState,
          key,
          setPopupState,
          closePopup,
          onExportSubmit: exportActions.onSubmit,
        })
        return
      case 'smart':
        handleSmartPopupShortcuts({
          popupState,
          input,
          key,
          suggestions: smart.suggestions,
          smartContextRoot: smart.contextRoot,
          setPopupState,
          closePopup,
          onSmartRootSubmit: smart.onRootSubmit,
        })
        return
      case 'tokens':
      case 'settings':
      case 'instructions':
      case 'series':
      case 'test':
        handleEscapeOnlyPopupShortcuts(key, closePopup)
        return
      case 'reasoning':
        handleReasoningPopupShortcuts({
          popupState,
          key,
          lineCount: reasoning.lines.length,
          visibleRows: reasoning.visibleRows,
          setPopupState,
          closePopup,
        })
        return
      case 'intent':
        handleIntentPopupShortcuts({
          popupState,
          key,
          suggestions: intent.suggestions,
          setPopupState,
          closePopup,
          onIntentFileSubmit: intent.onFileSubmit,
        })
        return
      default: {
        const exhaustive: never = popupState
        return exhaustive
      }
    }
  })
  useInput(handlePopupKey, { isActive })
}
</file>
<file path="src/tui/config.ts">
export const COMMAND_DESCRIPTORS = [
  { id: 'exit', label: 'Exit', description: 'Exit the app (/exit)' },
  { id: 'help', label: 'Help', description: 'Show keyboard shortcuts (/help)' },
  { id: 'model', label: 'Model', description: 'Switch the generation model' },
  {
    id: 'target',
    label: 'Target',
    description:
      'Switch the runtime model used for optimization (/target; not included in prompt text)',
  },
  {
    id: 'intent',
    label: 'Intent File',
    description: 'Use a file for the intent text (/intent path/to/file.md)',
  },
  {
    id: 'instructions',
    label: 'Meta Instructions',
    description: 'Add optional meta guidance (/meta <text>)',
    aliases: ['meta'] as const,
  },
  {
    id: 'new',
    label: 'New',
    description: 'Reset session state (/new)',
  },
  {
    id: 'reuse',
    label: 'Reuse',
    description: 'Reset and reuse last prompt (/reuse)',
  },
  { id: 'file', label: 'File', description: 'Attach file context' },
  { id: 'url', label: 'URL', description: 'Add/manage URL context (/url [url1 url2 ...])' },
  { id: 'smart', label: 'Smart', description: 'Toggle smart context (/smart on|off)' },
  {
    id: 'smart-root',
    label: 'Smart Context Root',
    description: 'Set/clear smart context root (/smart-root path or /smart-root --clear)',
  },
  { id: 'image', label: 'Image', description: 'Attach reference images' },
  { id: 'video', label: 'Video', description: 'Attach reference videos' },
  { id: 'pdf', label: 'PDF', description: 'Attach reference PDFs' },
  { id: 'polish', label: 'Polish', description: 'Select a model for prompt polishing' },
  {
    id: 'series',
    label: 'Series',
    description:
      'Atomic prompt series (Tab) Âˇ standalone prompts; prefilled from typed/last intent or intent file',
  },
  { id: 'copy', label: 'Copy', description: 'Auto-copy final prompt' },
  { id: 'chatgpt', label: 'ChatGPT', description: 'Open ChatGPT automatically' },
  { id: 'json', label: 'JSON', description: 'Toggle JSON payload in history (/json on|off)' },
  { id: 'tokens', label: 'Tokens', description: 'Show token usage breakdown (/tokens)' },
  {
    id: 'budgets',
    label: 'Budgets',
    description: 'Configure token budgets and overflow strategy (/budgets)',
  },
  { id: 'settings', label: 'Settings', description: 'Show current settings (/settings)' },
  { id: 'theme', label: 'Theme', description: 'Switch TUI theme (/theme)' },
  {
    id: 'theme-mode',
    label: 'Theme Mode',
    description: 'Switch theme mode (/theme-mode dark|light|system)',
  },
  {
    id: 'reasoning',
    label: 'Reasoning',
    description: 'Show last model reasoning (/reasoning or /why)',
    aliases: ['why'] as const,
  },
  { id: 'history', label: 'History', description: 'Browse command/intent history (/history)' },
  {
    id: 'resume',
    label: 'Resume',
    description: 'Resume generation from history or an exported payload (/resume)',
  },
  {
    id: 'export',
    label: 'Export',
    description: 'Export a selected history payload to JSON/YAML (/export)',
  },
  { id: 'test', label: 'Test', description: 'Run prompt tests (/test prompt-tests.yaml)' },
] as const
export const TOGGLE_LABELS = {
  copy: 'Copy',
  chatgpt: 'ChatGPT',
  json: 'JSON',
} as const
export const POPUP_HEIGHTS = {
  model: 16,
  toggle: 6,
  file: 16,
  url: 12,
  image: 16,
  video: 16,
  pdf: 16,
  history: 16,
  resume: 18,
  export: 18,
  smart: 12,
  tokens: 16,
  budgets: 14,
  settings: 14,
  theme: 16,
  themeMode: 8,
  reasoning: 18,
  test: 7,
  intent: 9,
  instructions: 7,
  series: 8,
} as const
</file>
<file path="src/tui/types.ts">
import type { ContextOverflowStrategy } from '../config'
import type { ModelProvider } from '../model-providers'
import type { COMMAND_DESCRIPTORS, POPUP_HEIGHTS, TOGGLE_LABELS } from './config'
import type { ThemeMode } from './theme/theme-types'
export type CommandDescriptor = (typeof COMMAND_DESCRIPTORS)[number]
export type ToggleField = keyof typeof TOGGLE_LABELS
export type PopupKind = keyof typeof POPUP_HEIGHTS
export type ModelOption = {
  id: string
  label: string
  provider: ModelProvider
  description: string
  capabilities: string[]
  default?: boolean
  notes?: string
  source: 'builtin' | 'config' | 'discovered'
}
export type ProviderStatus = {
  provider: ModelProvider
  status: 'ok' | 'missing' | 'error'
  message: string
}
export type ProviderStatusMap = Record<ModelProvider, ProviderStatus>
export type ResumeSourceKind = 'history' | 'file'
export type ResumeMode = 'best-effort' | 'strict'
export type ResumeHistoryItem = {
  selector: string
  title: string
  detail: string
}
export type ExportHistoryItem = {
  selector: string
  title: string
  detail: string
  schemaVersion: string
  supported: boolean
}
export type PopupState =
  | {
      type: 'model'
      kind: 'generation' | 'polish' | 'target'
      query: string
      selectionIndex: number
    }
  | { type: 'toggle'; field: ToggleField; selectionIndex: number }
  | {
      type: 'file'
      draft: string
      selectionIndex: number
      selectedFocused: boolean
      suggestedItems: string[]
      suggestedSelectionIndex: number
      suggestedFocused: boolean
    }
  | {
      type: 'url'
      draft: string
      selectionIndex: number
      selectedFocused: boolean
      editingIndex: number | null
    }
  | {
      type: 'image'
      draft: string
      selectionIndex: number
      selectedFocused: boolean
      suggestedItems: string[]
      suggestedSelectionIndex: number
      suggestedFocused: boolean
    }
  | {
      type: 'video'
      draft: string
      selectionIndex: number
      selectedFocused: boolean
      suggestedItems: string[]
      suggestedSelectionIndex: number
      suggestedFocused: boolean
    }
  | {
      type: 'pdf'
      draft: string
      selectionIndex: number
      selectedFocused: boolean
      suggestedItems: string[]
      suggestedSelectionIndex: number
      suggestedFocused: boolean
    }
  | { type: 'history'; draft: string; selectionIndex: number }
  | {
      type: 'resume'
      selectionIndex: number
      sourceKind: ResumeSourceKind
      mode: ResumeMode
      historyItems: ResumeHistoryItem[]
      historySelectionIndex: number
      historyErrorMessage: string | null
      payloadPathDraft: string
      suggestedItems: string[]
      suggestedSelectionIndex: number
      suggestedFocused: boolean
    }
  | {
      type: 'export'
      selectionIndex: number
      historyItems: ExportHistoryItem[]
      historySelectionIndex: number
      historyErrorMessage: string | null
      format: 'json' | 'yaml'
      outPathDraft: string
    }
  | {
      type: 'smart'
      draft: string
      suggestedItems: string[]
      suggestedSelectionIndex: number
      suggestedFocused: boolean
    }
  | { type: 'tokens' }
  | {
      type: 'budgets'
      selectionIndex: number
      maxContextTokensDraft: string
      maxInputTokensDraft: string
      contextOverflowStrategyDraft: ContextOverflowStrategy | ''
      errorMessage: string | null
    }
  | { type: 'settings' }
  | { type: 'theme'; selectionIndex: number; initialThemeName: string }
  | {
      type: 'themeMode'
      selectionIndex: number
      initialMode: ThemeMode
    }
  | { type: 'reasoning'; scrollOffset: number }
  | { type: 'test'; draft: string }
  | {
      type: 'intent'
      draft: string
      suggestedItems: string[]
      suggestedSelectionIndex: number
      suggestedFocused: boolean
    }
  | { type: 'instructions'; draft: string }
  | { type: 'series'; draft: string; hint?: string }
  | null
export type HistoryEntry = {
  id: string
  content: string
  kind: 'user' | 'system' | 'progress'
  format?: 'markdown'
}
</file>
<file path="src/tui/screens/command/components/PopupArea.tsx">
/*
 * PopupArea
 *
 * Presentational component: renders whichever popup is currently active.
 *
 * This keeps the large popup JSX switch out of `CommandScreen` so the main
 * screen file is mostly orchestration.
 */
import type { ComponentProps } from 'react'
import { ListPopup } from '../../../components/popups/ListPopup'
import { ModelPopup } from '../../../components/popups/ModelPopup'
import { SmartPopup } from '../../../components/popups/SmartPopup'
import { BudgetsPopup } from '../../../components/popups/BudgetsPopup'
import { TokenUsagePopup } from '../../../components/popups/TokenUsagePopup'
import { SettingsPopup } from '../../../components/popups/SettingsPopup'
import { ReasoningPopup } from '../../../components/popups/ReasoningPopup'
import { TestPopup } from '../../../components/popups/TestPopup'
import { TogglePopup } from '../../../components/popups/TogglePopup'
import { IntentFilePopup } from '../../../components/popups/IntentFilePopup'
import { InstructionsPopup } from '../../../components/popups/InstructionsPopup'
import { SeriesIntentPopup } from '../../../components/popups/SeriesIntentPopup'
import { ThemePickerPopup } from '../../../components/popups/ThemePickerPopup'
import { ThemeModePopup } from '../../../components/popups/ThemeModePopup'
import { ResumePopup } from '../../../components/popups/ResumePopup'
import { ExportPopup } from '../../../components/popups/ExportPopup'
import type { HistoryEntry, ModelOption, PopupState, ProviderStatusMap } from '../../../types'
import type { TokenUsageBreakdown, TokenUsageRun } from '../../../token-usage-store'
export type PopupAreaProps = {
  popupState: PopupState
  helpOpen: boolean
  overlayHeight: number
  // Model popup
  modelPopupOptions: ModelOption[]
  modelPopupSelection: number
  modelPopupRecentCount: number
  providerStatuses: ProviderStatusMap
  onModelPopupQueryChange: (next: string) => void
  onModelPopupSubmit: (option: ModelOption | null | undefined) => void
  // Toggle popup
  // (toggle popup is self-contained; selection is stored in `popupState`)
  // File popup
  files: string[]
  filePopupSuggestions: string[]
  filePopupSuggestionSelectionIndex: number
  filePopupSuggestionsFocused: boolean
  onFilePopupDraftChange: (next: string) => void
  onAddFile: (value: string) => void
  // URL popup
  urls: string[]
  onUrlPopupDraftChange: (next: string) => void
  onAddUrl: (value: string) => void
  // Image popup
  images: string[]
  imagePopupSuggestions: string[]
  imagePopupSuggestionSelectionIndex: number
  imagePopupSuggestionsFocused: boolean
  onImagePopupDraftChange: (next: string) => void
  onAddImage: (value: string) => void
  // Video popup
  videos: string[]
  videoPopupSuggestions: string[]
  videoPopupSuggestionSelectionIndex: number
  videoPopupSuggestionsFocused: boolean
  onVideoPopupDraftChange: (next: string) => void
  onAddVideo: (value: string) => void
  // PDF popup
  pdfs: string[]
  pdfPopupSuggestions: string[]
  pdfPopupSuggestionSelectionIndex: number
  pdfPopupSuggestionsFocused: boolean
  onPdfPopupDraftChange: (next: string) => void
  onAddPdf: (value: string) => void
  // History popup
  historyPopupItems: string[]
  onHistoryPopupDraftChange: (next: string) => void
  onHistoryPopupSubmit: (value: string) => void
  // Resume popup
  onResumePayloadPathDraftChange: (next: string) => void
  onResumeSubmit: () => void
  // Export popup
  onExportOutPathDraftChange: (next: string) => void
  onExportSubmit: () => void
  // Intent popup
  intentPopupSuggestions: string[]
  intentPopupSuggestionSelectionIndex: number
  intentPopupSuggestionsFocused: boolean
  onIntentPopupDraftChange: (next: string) => void
  onIntentFileSubmit: (value: string) => void
  // Instructions
  onInstructionsDraftChange: (next: string) => void
  onInstructionsSubmit: (value: string) => void
  // Series
  isGenerating: boolean
  onSeriesDraftChange: (next: string) => void
  onSeriesSubmit: (value: string) => void
  // Test
  isTestCommandRunning: boolean
  onTestDraftChange: (next: string) => void
  onTestSubmit: (value: string) => void
  // Tokens
  tokenUsageRun: TokenUsageRun | null
  tokenUsageBreakdown: TokenUsageBreakdown | null
  maxContextTokens: number | null
  maxInputTokens: number | null
  contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
  latestContextOverflow:
    | import('../../../generation-pipeline-reducer').ContextOverflowDetails
    | null
  // Budgets
  onBudgetsMaxContextTokensDraftChange: (next: string) => void
  onBudgetsMaxInputTokensDraftChange: (next: string) => void
  onBudgetsSubmit: () => void
  // Settings
  statusChips: string[]
  // Reasoning
  reasoningPopupLines: HistoryEntry[]
  reasoningPopupVisibleRows: number
  // Smart context
  smartContextEnabled: boolean
  smartContextRoot: string | null
  smartPopupSuggestions: string[]
  smartPopupSuggestionSelectionIndex: number
  smartPopupSuggestionsFocused: boolean
  onSmartPopupDraftChange: (next: string) => void
  onSmartRootSubmit: (value: string) => void
}
type NonNullPopupState = Exclude<PopupState, null>
type PopupStateFor<T extends NonNullPopupState['type']> = Extract<NonNullPopupState, { type: T }>
const renderModelPopup = (props: PopupAreaProps, popupState: PopupStateFor<'model'>) => {
  const title =
    popupState.kind === 'target'
      ? 'Select target model'
      : popupState.kind === 'polish'
        ? 'Select polish model'
        : 'Select model'
  const viewModel = {
    title,
    query: popupState.query,
    options: props.modelPopupOptions,
    selectedIndex: props.modelPopupSelection,
    recentCount: props.modelPopupRecentCount,
    maxHeight: props.overlayHeight,
    providerStatuses: props.providerStatuses,
    onQueryChange: props.onModelPopupQueryChange,
    onSubmit: props.onModelPopupSubmit,
  } satisfies ComponentProps<typeof ModelPopup>
  return <ModelPopup {...viewModel} />
}
const renderTogglePopup = (_props: PopupAreaProps, popupState: PopupStateFor<'toggle'>) => {
  const viewModel = {
    field: popupState.field,
    selectionIndex: popupState.selectionIndex,
  } satisfies ComponentProps<typeof TogglePopup>
  return <TogglePopup {...viewModel} />
}
const renderFilePopup = (props: PopupAreaProps, popupState: PopupStateFor<'file'>) => {
  const viewModel = {
    title: 'File Context',
    placeholder: 'src/**/*.ts',
    draft: popupState.draft,
    items: props.files,
    selectedIndex: popupState.selectionIndex,
    selectedFocused: popupState.selectedFocused,
    layout: 'selected-first',
    emptyLabel: 'No file globs added',
    instructions:
      "Enter add Âˇ â/â focus list Âˇ Del/Backspace remove Âˇ Tab suggestions Âˇ Esc close\nfzf: ^start $end 'exact",
    suggestedItems: props.filePopupSuggestions,
    suggestedSelectionIndex: props.filePopupSuggestionSelectionIndex,
    suggestedFocused: props.filePopupSuggestionsFocused,
    maxHeight: props.overlayHeight,
    onDraftChange: props.onFilePopupDraftChange,
    onSubmitDraft: props.onAddFile,
  } satisfies ComponentProps<typeof ListPopup>
  return <ListPopup {...viewModel} />
}
const renderUrlPopup = (props: PopupAreaProps, popupState: PopupStateFor<'url'>) => {
  const viewModel = {
    title: 'URL Context',
    placeholder: 'https://github.com/...',
    draft: popupState.draft,
    items: props.urls,
    selectedIndex: popupState.selectionIndex,
    selectedFocused: popupState.selectedFocused,
    emptyLabel: 'No URLs added',
    instructions:
      popupState.editingIndex === null
        ? 'Enter add (space/comma ok) Âˇ â/â focus list Âˇ e edit Âˇ Del remove Âˇ Esc close'
        : 'EditingâŚ Enter save Âˇ Esc cancel Âˇ Del remove',
    onDraftChange: props.onUrlPopupDraftChange,
    onSubmitDraft: props.onAddUrl,
  } satisfies ComponentProps<typeof ListPopup>
  return <ListPopup {...viewModel} />
}
const renderImagePopup = (props: PopupAreaProps, popupState: PopupStateFor<'image'>) => {
  const viewModel = {
    title: 'Images',
    placeholder: 'path/to/image.png',
    draft: popupState.draft,
    items: props.images,
    selectedIndex: popupState.selectionIndex,
    selectedFocused: popupState.selectedFocused,
    layout: 'selected-first',
    emptyLabel: 'No images attached',
    instructions:
      "Enter add Âˇ â/â focus list Âˇ Del/Backspace remove Âˇ Tab suggestions Âˇ Esc close\nfzf: ^start $end 'exact",
    suggestedItems: props.imagePopupSuggestions,
    suggestedSelectionIndex: props.imagePopupSuggestionSelectionIndex,
    suggestedFocused: props.imagePopupSuggestionsFocused,
    maxHeight: props.overlayHeight,
    onDraftChange: props.onImagePopupDraftChange,
    onSubmitDraft: props.onAddImage,
  } satisfies ComponentProps<typeof ListPopup>
  return <ListPopup {...viewModel} />
}
const renderVideoPopup = (props: PopupAreaProps, popupState: PopupStateFor<'video'>) => {
  const viewModel = {
    title: 'Videos',
    placeholder: 'path/to/video.mp4',
    draft: popupState.draft,
    items: props.videos,
    selectedIndex: popupState.selectionIndex,
    selectedFocused: popupState.selectedFocused,
    layout: 'selected-first',
    emptyLabel: 'No videos attached',
    instructions:
      "Enter add Âˇ â/â focus list Âˇ Del/Backspace remove Âˇ Tab suggestions Âˇ Esc close\nfzf: ^start $end 'exact",
    suggestedItems: props.videoPopupSuggestions,
    suggestedSelectionIndex: props.videoPopupSuggestionSelectionIndex,
    suggestedFocused: props.videoPopupSuggestionsFocused,
    maxHeight: props.overlayHeight,
    onDraftChange: props.onVideoPopupDraftChange,
    onSubmitDraft: props.onAddVideo,
  } satisfies ComponentProps<typeof ListPopup>
  return <ListPopup {...viewModel} />
}
const renderPdfPopup = (props: PopupAreaProps, popupState: PopupStateFor<'pdf'>) => {
  const viewModel = {
    title: 'PDFs',
    placeholder: 'path/to/document.pdf',
    draft: popupState.draft,
    items: props.pdfs,
    selectedIndex: popupState.selectionIndex,
    selectedFocused: popupState.selectedFocused,
    layout: 'selected-first',
    emptyLabel: 'No PDFs attached',
    instructions:
      "Enter add Âˇ â/â focus list Âˇ Del/Backspace remove Âˇ Tab suggestions Âˇ Esc close\nfzf: ^start $end 'exact",
    suggestedItems: props.pdfPopupSuggestions,
    suggestedSelectionIndex: props.pdfPopupSuggestionSelectionIndex,
    suggestedFocused: props.pdfPopupSuggestionsFocused,
    maxHeight: props.overlayHeight,
    onDraftChange: props.onPdfPopupDraftChange,
    onSubmitDraft: props.onAddPdf,
  } satisfies ComponentProps<typeof ListPopup>
  return <ListPopup {...viewModel} />
}
const renderHistoryPopup = (props: PopupAreaProps, popupState: PopupStateFor<'history'>) => {
  const viewModel = {
    title: 'History',
    placeholder: 'Type to filterâŚ',
    draft: popupState.draft,
    items: props.historyPopupItems,
    selectedIndex: popupState.selectionIndex,
    selectedFocused: true,
    emptyLabel: 'No history entries',
    instructions: 'â/â select Âˇ Enter paste Âˇ Esc close',
    onDraftChange: props.onHistoryPopupDraftChange,
    onSubmitDraft: props.onHistoryPopupSubmit,
  } satisfies ComponentProps<typeof ListPopup>
  return <ListPopup {...viewModel} />
}
const renderResumePopup = (props: PopupAreaProps, popupState: PopupStateFor<'resume'>) => {
  const viewModel = {
    selectionIndex: popupState.selectionIndex,
    sourceKind: popupState.sourceKind,
    mode: popupState.mode,
    historyItems: popupState.historyItems,
    historySelectionIndex: popupState.historySelectionIndex,
    historyErrorMessage: popupState.historyErrorMessage,
    payloadPathDraft: popupState.payloadPathDraft,
    suggestedItems: popupState.suggestedItems,
    suggestedSelectionIndex: popupState.suggestedSelectionIndex,
    suggestedFocused: popupState.suggestedFocused,
    onPayloadPathChange: props.onResumePayloadPathDraftChange,
    onSubmit: props.onResumeSubmit,
  } satisfies ComponentProps<typeof ResumePopup>
  return <ResumePopup {...viewModel} />
}
const renderExportPopup = (props: PopupAreaProps, popupState: PopupStateFor<'export'>) => {
  const viewModel = {
    selectionIndex: popupState.selectionIndex,
    format: popupState.format,
    outPathDraft: popupState.outPathDraft,
    historyItems: popupState.historyItems,
    historySelectionIndex: popupState.historySelectionIndex,
    historyErrorMessage: popupState.historyErrorMessage,
    onOutPathChange: props.onExportOutPathDraftChange,
    onSubmit: props.onExportSubmit,
  } satisfies ComponentProps<typeof ExportPopup>
  return <ExportPopup {...viewModel} />
}
const renderIntentPopup = (props: PopupAreaProps, popupState: PopupStateFor<'intent'>) => {
  const viewModel = {
    draft: popupState.draft,
    suggestions: props.intentPopupSuggestions,
    suggestedSelectionIndex: props.intentPopupSuggestionSelectionIndex,
    suggestedFocused: props.intentPopupSuggestionsFocused,
    maxHeight: props.overlayHeight,
    onDraftChange: props.onIntentPopupDraftChange,
    onSubmitDraft: props.onIntentFileSubmit,
  } satisfies ComponentProps<typeof IntentFilePopup>
  return <IntentFilePopup {...viewModel} />
}
const renderSmartPopup = (props: PopupAreaProps, popupState: PopupStateFor<'smart'>) => {
  const viewModel = {
    savedRoot: props.smartContextRoot,
    draft: popupState.draft,
    suggestedItems: props.smartPopupSuggestions,
    suggestedSelectionIndex: props.smartPopupSuggestionSelectionIndex,
    suggestedFocused: props.smartPopupSuggestionsFocused,
    maxHeight: props.overlayHeight,
    onDraftChange: props.onSmartPopupDraftChange,
    onSubmitRoot: props.onSmartRootSubmit,
  } satisfies ComponentProps<typeof SmartPopup>
  return <SmartPopup {...viewModel} />
}
const renderInstructionsPopup = (
  props: PopupAreaProps,
  popupState: PopupStateFor<'instructions'>,
) => {
  const viewModel = {
    draft: popupState.draft,
    onDraftChange: props.onInstructionsDraftChange,
    onSubmitDraft: props.onInstructionsSubmit,
  } satisfies ComponentProps<typeof InstructionsPopup>
  return <InstructionsPopup {...viewModel} />
}
const renderSeriesPopup = (props: PopupAreaProps, popupState: PopupStateFor<'series'>) => {
  const viewModel = {
    draft: popupState.draft,
    hint: popupState.hint,
    isRunning: props.isGenerating,
    onDraftChange: props.onSeriesDraftChange,
    onSubmitDraft: props.onSeriesSubmit,
  } satisfies ComponentProps<typeof SeriesIntentPopup>
  return <SeriesIntentPopup {...viewModel} />
}
const renderTestPopup = (props: PopupAreaProps, popupState: PopupStateFor<'test'>) => {
  const viewModel = {
    draft: popupState.draft,
    isRunning: props.isTestCommandRunning,
    onDraftChange: props.onTestDraftChange,
    onSubmitDraft: props.onTestSubmit,
  } satisfies ComponentProps<typeof TestPopup>
  return <TestPopup {...viewModel} />
}
const renderReasoningPopup = (props: PopupAreaProps, popupState: PopupStateFor<'reasoning'>) => {
  const viewModel = {
    lines: props.reasoningPopupLines,
    visibleRows: props.reasoningPopupVisibleRows,
    scrollOffset: popupState.scrollOffset,
  } satisfies ComponentProps<typeof ReasoningPopup>
  return <ReasoningPopup {...viewModel} />
}
const renderTokenUsagePopup = (props: PopupAreaProps) => {
  const viewModel = {
    run: props.tokenUsageRun,
    breakdown: props.tokenUsageBreakdown,
    budgets: {
      maxContextTokens: props.maxContextTokens,
      maxInputTokens: props.maxInputTokens,
      contextOverflowStrategy: props.contextOverflowStrategy,
    },
    latestContextOverflow: props.latestContextOverflow,
  } satisfies ComponentProps<typeof TokenUsagePopup>
  return <TokenUsagePopup {...viewModel} />
}
const renderBudgetsPopup = (props: PopupAreaProps, popupState: PopupStateFor<'budgets'>) => {
  const viewModel = {
    selectionIndex: popupState.selectionIndex,
    maxContextTokensDraft: popupState.maxContextTokensDraft,
    maxInputTokensDraft: popupState.maxInputTokensDraft,
    contextOverflowStrategyDraft: popupState.contextOverflowStrategyDraft,
    errorMessage: popupState.errorMessage,
    onMaxContextTokensChange: props.onBudgetsMaxContextTokensDraftChange,
    onMaxInputTokensChange: props.onBudgetsMaxInputTokensDraftChange,
    onSubmit: props.onBudgetsSubmit,
  } satisfies ComponentProps<typeof BudgetsPopup>
  return <BudgetsPopup {...viewModel} />
}
const renderSettingsPopup = (props: PopupAreaProps) => {
  const viewModel = {
    chips: props.statusChips,
  } satisfies ComponentProps<typeof SettingsPopup>
  return <SettingsPopup {...viewModel} />
}
const renderThemePopup = (props: PopupAreaProps, popupState: PopupStateFor<'theme'>) => {
  const viewModel = {
    selectionIndex: popupState.selectionIndex,
    initialThemeName: popupState.initialThemeName,
    maxHeight: props.overlayHeight,
  } satisfies ComponentProps<typeof ThemePickerPopup>
  return <ThemePickerPopup {...viewModel} />
}
const renderThemeModePopup = (_props: PopupAreaProps, popupState: PopupStateFor<'themeMode'>) => {
  const viewModel = {
    selectionIndex: popupState.selectionIndex,
    initialMode: popupState.initialMode,
  } satisfies ComponentProps<typeof ThemeModePopup>
  return <ThemeModePopup {...viewModel} />
}
export const PopupArea = (props: PopupAreaProps) => {
  const { popupState, helpOpen } = props
  if (popupState === null || helpOpen) {
    return null
  }
  switch (popupState.type) {
    case 'model':
      return renderModelPopup(props, popupState)
    case 'toggle':
      return renderTogglePopup(props, popupState)
    case 'file':
      return renderFilePopup(props, popupState)
    case 'url':
      return renderUrlPopup(props, popupState)
    case 'image':
      return renderImagePopup(props, popupState)
    case 'video':
      return renderVideoPopup(props, popupState)
    case 'pdf':
      return renderPdfPopup(props, popupState)
    case 'history':
      return renderHistoryPopup(props, popupState)
    case 'resume':
      return renderResumePopup(props, popupState)
    case 'export':
      return renderExportPopup(props, popupState)
    case 'intent':
      return renderIntentPopup(props, popupState)
    case 'smart':
      return renderSmartPopup(props, popupState)
    case 'instructions':
      return renderInstructionsPopup(props, popupState)
    case 'series':
      return renderSeriesPopup(props, popupState)
    case 'test':
      return renderTestPopup(props, popupState)
    case 'reasoning':
      return renderReasoningPopup(props, popupState)
    case 'tokens':
      return renderTokenUsagePopup(props)
    case 'budgets':
      return renderBudgetsPopup(props, popupState)
    case 'settings':
      return renderSettingsPopup(props)
    case 'theme':
      return renderThemePopup(props, popupState)
    case 'themeMode':
      return renderThemeModePopup(props, popupState)
    default: {
      const _exhaustive: never = popupState
      return null
    }
  }
}
</file>
<file path="src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts">
import { useMemo } from 'react'
import type { CommandDescriptor, HistoryEntry, ModelOption, PopupState } from '../../../types'
import { useStableCallback } from '../../../hooks/useStableCallback'
import { useMiscPopupDraftHandlers } from './useMiscPopupDraftHandlers'
import { useModelPopupData } from './useModelPopupData'
import { usePopupKeyboardShortcuts } from './usePopupKeyboardShortcuts'
import { useReasoningPopup } from './useReasoningPopup'
import { useThemePopupGlue } from './useThemePopupGlue'
import { useThemeModePopupGlue } from './useThemeModePopupGlue'
import {
  useCommandScreenPasteBindings,
  type UseCommandScreenPasteBindingsOptions,
} from './useCommandScreenPasteBindings'
import {
  useCommandScreenContextPopupBindings,
  type UseCommandScreenContextPopupBindingsOptions,
} from './useCommandScreenContextPopupBindings'
import {
  useCommandScreenHistoryIntentPopupBindings,
  type UseCommandScreenHistoryIntentPopupBindingsOptions,
} from './useCommandScreenHistoryIntentPopupBindings'
import {
  useCommandScreenSubmitBindings,
  type UseCommandScreenSubmitBindingsOptions,
} from './useCommandScreenSubmitBindings'
type SetPopupState = import('react').Dispatch<import('react').SetStateAction<PopupState>>
export type UseCommandScreenPopupBindingsOptions = {
  input: {
    value: string
    setValue: (value: string | ((prev: string) => string)) => void
    setPasteActive: (active: boolean) => void
    consumeSuppressedTextInputChange: () => boolean
    suppressNextInput: () => void
    updateLastTypedIntent: (next: string) => void
    intentFilePath: string
    lastUserIntentRef: import('react').MutableRefObject<string | null>
  }
  popup: {
    state: PopupState
    setState: SetPopupState
    isOpen: boolean
    helpOpen: boolean
    openHelp?: () => void
    close: () => void
    actions: {
      handleCommandSelection: (commandId: CommandDescriptor['id'], argsRaw?: string) => void
      handleModelPopupSubmit: (option: ModelOption | null | undefined) => void
      applyToggleSelection: (field: 'copy' | 'chatgpt' | 'json', value: boolean) => void
      handleIntentFileSubmit: (value: string) => void
      handleResumeSubmit: () => void
      handleExportSubmit: () => void
      handleSeriesIntentSubmit: (value: string) => void
      handleBudgetsSubmit: () => void
    }
  }
  menu: {
    isActive: boolean
    selectedCommandId: CommandDescriptor['id'] | null
    argsRaw: string
    isCommandMode: boolean
    actions: {
      handleNewCommand: (argsRaw: string) => void
      handleReuseCommand: () => void
    }
  }
  generation: {
    isGenerating: boolean
    isAwaitingRefinement: boolean
    submitRefinement: (value: string) => void
    runGeneration: (payload: {
      intent?: string
      intentFile?: string
      resume?:
        | { kind: 'history'; selector: string; mode: import('../../../types').ResumeMode }
        | { kind: 'file'; payloadPath: string; mode: import('../../../types').ResumeMode }
    }) => Promise<void>
  }
  history: {
    pushHistory: (
      content: string,
      kind?: HistoryEntry['kind'],
      format?: HistoryEntry['format'],
    ) => void
    addCommandHistoryEntry: (value: string) => void
    commandHistoryValues: string[]
  }
  context: {
    droppedFilePath: string | null
    files: string[]
    urls: string[]
    images: string[]
    videos: string[]
    pdfs: string[]
    smartContextEnabled: boolean
    smartContextRoot: string | null
    addFile: (value: string) => void
    removeFile: (index: number) => void
    addUrl: (value: string) => void
    removeUrl: (index: number) => void
    updateUrl: (index: number, value: string) => void
    addImage: (value: string) => void
    removeImage: (index: number) => void
    addVideo: (value: string) => void
    removeVideo: (index: number) => void
    addPdf: (value: string) => void
    removePdf: (index: number) => void
    toggleSmartContext: () => void
    setSmartRoot: (value: string) => void
    notify: (message: string) => void
    modelOptions: ModelOption[]
    lastReasoning: string | null
    terminalColumns: number
    reasoningPopupHeight: number
  }
}
export type UseCommandScreenPopupBindingsResult = {
  input: {
    tokenLabel: (token: string) => string | null
    onChange: (next: string) => void
  }
  submit: {
    onSubmit: (value: string) => void
    onSeriesSubmit: (value: string) => void
  }
  popup: {
    model: {
      options: ModelOption[]
      recentCount: number
      selection: number
      onQueryChange: (next: string) => void
    }
    history: {
      items: string[]
      onDraftChange: (next: string) => void
      onSubmit: (value: string) => void
    }
    intent: {
      suggestions: string[]
      suggestionSelectionIndex: number
      suggestionsFocused: boolean
      onDraftChange: (next: string) => void
    }
    context: {
      file: {
        suggestions: string[]
        suggestionSelectionIndex: number
        suggestionsFocused: boolean
        onDraftChange: (next: string) => void
        onAdd: (value: string) => void
        onRemove: (index: number) => void
      }
      url: {
        onDraftChange: (next: string) => void
        onAdd: (value: string) => void
        onRemove: (index: number) => void
      }
      image: {
        suggestions: string[]
        suggestionSelectionIndex: number
        suggestionsFocused: boolean
        onDraftChange: (next: string) => void
        onAdd: (value: string) => void
        onRemove: (index: number) => void
      }
      video: {
        suggestions: string[]
        suggestionSelectionIndex: number
        suggestionsFocused: boolean
        onDraftChange: (next: string) => void
        onAdd: (value: string) => void
        onRemove: (index: number) => void
      }
      pdf: {
        suggestions: string[]
        suggestionSelectionIndex: number
        suggestionsFocused: boolean
        onDraftChange: (next: string) => void
        onAdd: (value: string) => void
        onRemove: (index: number) => void
      }
      smart: {
        suggestions: string[]
        suggestionSelectionIndex: number
        suggestionsFocused: boolean
        onDraftChange: (next: string) => void
        onRootSubmit: (value: string) => void
      }
    }
    misc: {
      onSeriesDraftChange: (next: string) => void
      onInstructionsDraftChange: (next: string) => void
      onTestDraftChange: (next: string) => void
      onBudgetsMaxContextTokensDraftChange: (next: string) => void
      onBudgetsMaxInputTokensDraftChange: (next: string) => void
      onResumePayloadPathDraftChange: (next: string) => void
      onExportOutPathDraftChange: (next: string) => void
    }
    reasoning: {
      lines: HistoryEntry[]
      visibleRows: number
    }
  }
}
export const useCommandScreenPopupBindings = (
  options: UseCommandScreenPopupBindingsOptions,
): UseCommandScreenPopupBindingsResult => {
  const notify = useStableCallback((message: string) => {
    options.context.notify(message)
  })
  const pushHistory = useStableCallback(
    (content: string, kind: HistoryEntry['kind'] = 'system', format?: HistoryEntry['format']) => {
      options.history.pushHistory(content, kind, format)
    },
  )
  const paste = useCommandScreenPasteBindings({
    inputValue: options.input.value,
    popupState: options.popup.state,
    helpOpen: options.popup.helpOpen,
    setInputValue: options.input.setValue,
    setPasteActive: options.input.setPasteActive,
    consumeSuppressedTextInputChange: options.input.consumeSuppressedTextInputChange,
    suppressNextInput: options.input.suppressNextInput,
    updateLastTypedIntent: options.input.updateLastTypedIntent,
  } satisfies UseCommandScreenPasteBindingsOptions)
  const context = useCommandScreenContextPopupBindings({
    inputValue: options.input.value,
    popupState: options.popup.state,
    helpOpen: options.popup.helpOpen,
    isPopupOpen: options.popup.isOpen,
    isCommandMode: options.menu.isCommandMode,
    isCommandMenuActive: options.menu.isActive,
    isGenerating: options.generation.isGenerating,
    droppedFilePath: options.context.droppedFilePath,
    files: options.context.files,
    urls: options.context.urls,
    images: options.context.images,
    videos: options.context.videos,
    pdfs: options.context.pdfs,
    smartContextEnabled: options.context.smartContextEnabled,
    smartContextRoot: options.context.smartContextRoot,
    addFile: options.context.addFile,
    removeFile: options.context.removeFile,
    addUrl: options.context.addUrl,
    removeUrl: options.context.removeUrl,
    updateUrl: options.context.updateUrl,
    addImage: options.context.addImage,
    removeImage: options.context.removeImage,
    addVideo: options.context.addVideo,
    removeVideo: options.context.removeVideo,
    addPdf: options.context.addPdf,
    removePdf: options.context.removePdf,
    toggleSmartContext: options.context.toggleSmartContext,
    setSmartRoot: options.context.setSmartRoot,
    setInputValue: options.input.setValue,
    setPopupState: options.popup.setState,
    suppressNextInput: options.input.suppressNextInput,
    notify,
    pushHistory,
    addCommandHistoryEntry: options.history.addCommandHistoryEntry,
    handleCommandSelection: options.popup.actions.handleCommandSelection,
    consumeSuppressedTextInputChange: options.input.consumeSuppressedTextInputChange,
  } satisfies UseCommandScreenContextPopupBindingsOptions)
  const historyAndIntent = useCommandScreenHistoryIntentPopupBindings({
    popupState: options.popup.state,
    setPopupState: options.popup.setState,
    closePopup: options.popup.close,
    setInputValue: options.input.setValue,
    consumeSuppressedTextInputChange: options.input.consumeSuppressedTextInputChange,
    suppressNextInput: options.input.suppressNextInput,
    commandHistoryValues: options.history.commandHistoryValues,
  } satisfies UseCommandScreenHistoryIntentPopupBindingsOptions)
  const { modelPopupOptions, modelPopupRecentCount, modelPopupSelection } = useModelPopupData({
    popupState: options.popup.state,
    modelOptions: options.context.modelOptions,
  })
  const { reasoningPopupVisibleRows, reasoningPopupLines } = useReasoningPopup({
    lastReasoning: options.context.lastReasoning,
    terminalColumns: options.context.terminalColumns,
    popupHeight: options.context.reasoningPopupHeight,
  })
  const themePopup = useThemePopupGlue({
    popupState: options.popup.state,
    setPopupState: options.popup.setState,
    closePopup: options.popup.close,
  })
  const themeModePopup = useThemeModePopupGlue({
    popupState: options.popup.state,
    setPopupState: options.popup.setState,
    closePopup: options.popup.close,
  })
  usePopupKeyboardShortcuts({
    popupState: options.popup.state,
    helpOpen: options.popup.helpOpen,
    setPopupState: options.popup.setState,
    closePopup: options.popup.close,
    model: {
      options: modelPopupOptions,
      onSubmit: options.popup.actions.handleModelPopupSubmit,
    },
    toggle: {
      applySelection: options.popup.actions.applyToggleSelection,
    },
    theme: {
      count: themePopup.themeCount,
      onConfirm: themePopup.onThemeConfirm,
      onCancel: themePopup.onThemeCancel,
    },
    themeMode: {
      count: themeModePopup.optionCount,
      onConfirm: themeModePopup.onConfirm,
      onCancel: themeModePopup.onCancel,
    },
    budgets: {
      onSubmit: options.popup.actions.handleBudgetsSubmit,
    },
    file: {
      items: options.context.files,
      suggestions: context.filePopupSuggestions,
      onAdd: context.onAddFile,
      onRemove: context.onRemoveFile,
    },
    url: {
      items: options.context.urls,
      onRemove: context.onRemoveUrl,
    },
    image: {
      items: options.context.images,
      suggestions: context.imagePopupSuggestions,
      onAdd: context.onAddImage,
      onRemove: context.onRemoveImage,
    },
    video: {
      items: options.context.videos,
      suggestions: context.videoPopupSuggestions,
      onAdd: context.onAddVideo,
      onRemove: context.onRemoveVideo,
    },
    pdf: {
      items: options.context.pdfs,
      suggestions: context.pdfPopupSuggestions,
      onAdd: context.onAddPdf,
      onRemove: context.onRemovePdf,
    },
    history: {
      items: historyAndIntent.history.historyPopupItems,
    },
    resume: {
      onSubmit: options.popup.actions.handleResumeSubmit,
    },
    export: {
      onSubmit: options.popup.actions.handleExportSubmit,
    },
    smart: {
      suggestions: context.smartPopupSuggestions,
      contextRoot: options.context.smartContextRoot,
      onRootSubmit: context.onSmartRootSubmit,
    },
    intent: {
      suggestions: historyAndIntent.intent.intentPopupSuggestions,
      onFileSubmit: options.popup.actions.handleIntentFileSubmit,
    },
    reasoning: {
      lines: reasoningPopupLines,
      visibleRows: reasoningPopupVisibleRows,
    },
  })
  const submit = useCommandScreenSubmitBindings({
    popupState: options.popup.state,
    isAwaitingRefinement: options.generation.isAwaitingRefinement,
    submitRefinement: options.generation.submitRefinement,
    isCommandMenuActive: options.menu.isActive,
    selectedCommandId: options.menu.selectedCommandId,
    commandMenuArgsRaw: options.menu.argsRaw,
    isCommandMode: options.menu.isCommandMode,
    intentFilePath: options.input.intentFilePath,
    isGenerating: options.generation.isGenerating,
    expandInputForSubmit: paste.expandInputForSubmit,
    setInputValue: options.input.setValue,
    pushHistory,
    addCommandHistoryEntry: options.history.addCommandHistoryEntry,
    runGeneration: options.generation.runGeneration,
    handleCommandSelection: options.popup.actions.handleCommandSelection,
    handleNewCommand: options.menu.actions.handleNewCommand,
    handleReuseCommand: options.menu.actions.handleReuseCommand,
    lastUserIntentRef: options.input.lastUserIntentRef,
    handleSeriesIntentSubmit: options.popup.actions.handleSeriesIntentSubmit,
    ...(options.popup.openHelp ? { openHelp: options.popup.openHelp } : {}),
  } satisfies UseCommandScreenSubmitBindingsOptions)
  const miscDraftHandlers = useMiscPopupDraftHandlers({
    setPopupState: options.popup.setState,
    consumeSuppressedTextInputChange: options.input.consumeSuppressedTextInputChange,
  })
  const input = useMemo(
    () => ({
      tokenLabel: paste.tokenLabel,
      onChange: paste.handleInputChange,
    }),
    [paste.tokenLabel, paste.handleInputChange],
  )
  const submitGroup = useMemo(
    () => ({
      onSubmit: submit.handleSubmit,
      onSeriesSubmit: submit.onSeriesSubmit,
    }),
    [submit.handleSubmit, submit.onSeriesSubmit],
  )
  const popup = useMemo(
    () => ({
      model: {
        options: modelPopupOptions,
        recentCount: modelPopupRecentCount,
        selection: modelPopupSelection,
        onQueryChange: miscDraftHandlers.onModelPopupQueryChange,
      },
      history: {
        items: historyAndIntent.history.historyPopupItems,
        onDraftChange: historyAndIntent.history.onHistoryPopupDraftChange,
        onSubmit: historyAndIntent.history.onHistoryPopupSubmit,
      },
      intent: {
        suggestions: historyAndIntent.intent.intentPopupSuggestions,
        suggestionSelectionIndex: historyAndIntent.intent.intentPopupSuggestionSelectionIndex,
        suggestionsFocused: historyAndIntent.intent.intentPopupSuggestionsFocused,
        onDraftChange: historyAndIntent.intent.onIntentPopupDraftChange,
      },
      context: {
        file: {
          suggestions: context.filePopupSuggestions,
          suggestionSelectionIndex: context.filePopupSuggestionSelectionIndex,
          suggestionsFocused: context.filePopupSuggestionsFocused,
          onDraftChange: context.onFilePopupDraftChange,
          onAdd: context.onAddFile,
          onRemove: context.onRemoveFile,
        },
        url: {
          onDraftChange: context.onUrlPopupDraftChange,
          onAdd: context.onAddUrl,
          onRemove: context.onRemoveUrl,
        },
        image: {
          suggestions: context.imagePopupSuggestions,
          suggestionSelectionIndex: context.imagePopupSuggestionSelectionIndex,
          suggestionsFocused: context.imagePopupSuggestionsFocused,
          onDraftChange: context.onImagePopupDraftChange,
          onAdd: context.onAddImage,
          onRemove: context.onRemoveImage,
        },
        video: {
          suggestions: context.videoPopupSuggestions,
          suggestionSelectionIndex: context.videoPopupSuggestionSelectionIndex,
          suggestionsFocused: context.videoPopupSuggestionsFocused,
          onDraftChange: context.onVideoPopupDraftChange,
          onAdd: context.onAddVideo,
          onRemove: context.onRemoveVideo,
        },
        pdf: {
          suggestions: context.pdfPopupSuggestions,
          suggestionSelectionIndex: context.pdfPopupSuggestionSelectionIndex,
          suggestionsFocused: context.pdfPopupSuggestionsFocused,
          onDraftChange: context.onPdfPopupDraftChange,
          onAdd: context.onAddPdf,
          onRemove: context.onRemovePdf,
        },
        smart: {
          suggestions: context.smartPopupSuggestions,
          suggestionSelectionIndex: context.smartPopupSuggestionSelectionIndex,
          suggestionsFocused: context.smartPopupSuggestionsFocused,
          onDraftChange: context.onSmartPopupDraftChange,
          onRootSubmit: context.onSmartRootSubmit,
        },
      },
      misc: {
        onSeriesDraftChange: miscDraftHandlers.onSeriesDraftChange,
        onInstructionsDraftChange: miscDraftHandlers.onInstructionsDraftChange,
        onTestDraftChange: miscDraftHandlers.onTestDraftChange,
        onBudgetsMaxContextTokensDraftChange:
          miscDraftHandlers.onBudgetsMaxContextTokensDraftChange,
        onBudgetsMaxInputTokensDraftChange: miscDraftHandlers.onBudgetsMaxInputTokensDraftChange,
        onResumePayloadPathDraftChange: miscDraftHandlers.onResumePayloadPathDraftChange,
        onExportOutPathDraftChange: miscDraftHandlers.onExportOutPathDraftChange,
      },
      reasoning: {
        lines: reasoningPopupLines,
        visibleRows: reasoningPopupVisibleRows,
      },
    }),
    [
      modelPopupOptions,
      modelPopupRecentCount,
      modelPopupSelection,
      miscDraftHandlers.onModelPopupQueryChange,
      historyAndIntent.history.historyPopupItems,
      historyAndIntent.history.onHistoryPopupDraftChange,
      historyAndIntent.history.onHistoryPopupSubmit,
      historyAndIntent.intent.intentPopupSuggestions,
      historyAndIntent.intent.intentPopupSuggestionSelectionIndex,
      historyAndIntent.intent.intentPopupSuggestionsFocused,
      historyAndIntent.intent.onIntentPopupDraftChange,
      context.filePopupSuggestions,
      context.filePopupSuggestionSelectionIndex,
      context.filePopupSuggestionsFocused,
      context.onFilePopupDraftChange,
      context.onAddFile,
      context.onRemoveFile,
      context.onUrlPopupDraftChange,
      context.onAddUrl,
      context.onRemoveUrl,
      context.imagePopupSuggestions,
      context.imagePopupSuggestionSelectionIndex,
      context.imagePopupSuggestionsFocused,
      context.onImagePopupDraftChange,
      context.onAddImage,
      context.onRemoveImage,
      context.videoPopupSuggestions,
      context.videoPopupSuggestionSelectionIndex,
      context.videoPopupSuggestionsFocused,
      context.onVideoPopupDraftChange,
      context.onAddVideo,
      context.onRemoveVideo,
      context.pdfPopupSuggestions,
      context.pdfPopupSuggestionSelectionIndex,
      context.pdfPopupSuggestionsFocused,
      context.onPdfPopupDraftChange,
      context.onAddPdf,
      context.onRemovePdf,
      context.smartPopupSuggestions,
      context.smartPopupSuggestionSelectionIndex,
      context.smartPopupSuggestionsFocused,
      context.onSmartPopupDraftChange,
      context.onSmartRootSubmit,
      miscDraftHandlers.onSeriesDraftChange,
      miscDraftHandlers.onInstructionsDraftChange,
      miscDraftHandlers.onTestDraftChange,
      miscDraftHandlers.onBudgetsMaxContextTokensDraftChange,
      miscDraftHandlers.onBudgetsMaxInputTokensDraftChange,
      reasoningPopupLines,
      reasoningPopupVisibleRows,
    ],
  )
  return useMemo(
    () => ({
      input,
      submit: submitGroup,
      popup,
    }),
    [input, submitGroup, popup],
  )
}
</file>
<file path="README.md">
# Prompt Maker CLI
Terminal-first prompt generator with a built-in Ink TUI. It turns rough intent + optional context (files, URLs, smart context, images, videos, PDFs) into a structured **prompt contract**, with optional polishing, streaming telemetry, and automatic history logging.
Highlights:
- **Generate workflow**: run from flags or from the TUI.
- **Context ingestion**: file globs (`--context`), URLs (`--url`, including GitHub trees), optional smart context (`--smart-context`), and media (`--image`, `--video`, `--pdf`).
- **Auditable runs**: token telemetry, token budgets (optional trimming), structured stream events (`--stream jsonl`), and JSONL history (`~/.config/prompt-maker-cli/history.jsonl`) with export + resume workflows.
- **TUI-first**: `prompt-maker-cli` with no args launches the TUI.
The project is **TUI-first**:
- `prompt-maker-cli` with **no args** launches the TUI.
- Use the same pipelines via flags (`generate`) or tests (`test`) when you need automation.
## Table of Contents
- [Diagram Index](#diagram-index)
- [Quickstart](#quickstart)
- [CLI Modes and Routing](#cli-modes-and-routing)
- [TUI Mode (recommended)](#tui-mode-recommended)
  - [Launch](#launch)
  - [Keybindings](#keybindings)
  - [Input Routing Invariant](#input-routing-invariant)
  - [Command Palette (`/commands`)](#command-palette-commands)
  - [Series Generation (âatomic promptsâ)](#series-generation-atomic-prompts)
  - [TUI Theming](#tui-theming)
- [Generate Mode (CLI)](#generate-mode-cli)
  - [Common Workflows](#common-workflows)
  - [Key Flags](#key-flags)
  - [Conceptual Architecture](#conceptual-architecture)
  - [Intent Intake](#intent-intake)
  - [Context Ingestion](#context-ingestion)
  - [Remote URL and GitHub Context](#remote-url-and-github-context)
  - [Smart Context (Local RAG)](#smart-context-local-rag)
  - [Media Attachments](#media-attachments)
  - [Streaming Events and Telemetry](#streaming-events-and-telemetry)
  - [Interactive Refinement](#interactive-refinement)
  - [Polish Pass](#polish-pass)
  - [Output Artifacts](#output-artifacts)
- [Prompt Tests](#prompt-tests)
- [TUI Architecture and UX Model](#tui-architecture-and-ux-model)
- [Provider Configuration](#provider-configuration)
- [Outputs and Persistence](#outputs-and-persistence)
- [Development](#development)
- [Maintainer References](#maintainer-references)
- [License](#license)
## Diagram Index
All diagrams are GitHub Mermaid compatible and grounded in:
- `docs/neovim-plugin-integration.md` (pipeline, streaming events, transports, context/media details)
- `docs/tui-design.md` (TUI architecture, UX invariants)
- This READMEâs CLI/TUI behavior sections
CLI + Generate Pipeline:
- [D1. CLI Routing Flow](#d1-cli-routing-flow) â how `ui` / `generate` / `test` are selected.
- [D2. Generate Pipeline Overview](#d2-generate-pipeline-overview) â end-to-end generate workflow, including interactive and polish.
- [D3. `--stream jsonl` Event Timeline](#d3---stream-jsonl-event-timeline) â typical JSONL event emission order.
- [D4. `--interactive-transport` Lifecycle](#d4---interactive-transport-lifecycle) â socket/pipe setup, commands, mirrored events.
- [D5. Interactive Phase State Machine](#d5-interactive-phase-state-machine) â phases emitted via `interactive.state`.
Context + Media:
- [D6. Context Ingestion Decision Tree](#d6-context-ingestion-decision-tree) â how sources merge and when failures warn vs fail.
- [D7. Smart Context Workflow](#d7-smart-context-workflow) â scan â cache/index â topâk â de-dupe append.
- [D8. URL + GitHub Context Resolution](#d8-url--github-context-resolution) â remote fetch behavior and safety limits.
- [D9. Media Handling (Images + Videos + PDFs)](#d9-media-handling-images--videos--pdfs) â image parts, video uploads, and PDF attachment behavior.
TUI:
- [D10. TUI Architecture Map](#d10-tui-architecture-map) â AppContainer + screens + popups + keymaps.
- [D11. TUI Input Routing Priority](#d11-tui-input-routing-priority) â help overlay > popup > screen > globals.
Other:
- [D12. `/series` Generation and Filesystem Outputs](#d12-series-generation-and-filesystem-outputs) â atomic prompt series, write-on-best-effort.
- [D13. Theme Resolution Precedence](#d13-theme-resolution-precedence) â theme search order and overriding.
- [D14. Prompt Test Runner Flow](#d14-prompt-test-runner-flow) â `test` mode and the TUI Test Runner view.
---
## Quickstart
From repo root:
```bash
npm ci
npm run build
# TUI (default if no args)
npm start
# Generate from a one-shot intent
npm start -- "Draft a confident onboarding-bot spec" --model gpt-4o-mini
# Explicit commands
npm start -- ui
npm start -- test prompt-tests.yaml
# After at least one generate run, export the last payload
npm start -- export --format json --out runs/last-run.json
# Deterministic (non-LLM) composition scaffold
npm start -- compose --recipe docs/tui-design.md --input "Draft a summary" > composed.txt
```
Global install from a local checkout:
```bash
npm install -g .
# Now you can run:
prompt-maker-cli
```
## CLI modes and routing
`prompt-maker-cli` has five top-level modes:
- `ui`: Ink TUI
- `generate` (default for non-`ui`/`test`/`export`/`compose` args): prompt generation pipeline
- `test`: prompt test runner
- `export`: export a past generate payload from history
- `compose`: deterministic (non-LLM) prompt composition scaffold
Routing rules:
- No args â `ui`
- First arg `ui` â `ui`
- First arg `test` â `test`
- First arg `export` â `export`
- First arg `compose` â `compose`
- First arg `generate` or `expand` â `generate` (`expand` is an alias)
- Anything else (including flags like `--json`) â `generate`
Help behavior:
- `prompt-maker-cli --help` shows **generate** help only.
- Use `prompt-maker-cli export --help` / `prompt-maker-cli compose --help` for subcommand help.
### D1. CLI Routing Flow
Shows how argv maps to top-level modes.
```mermaid
flowchart TD
  A[Start: argv] --> B{No args?}
  B -- Yes --> UI[ui]
  B -- No --> C[First token]
  C --> T{"first == 'test'?"}
  T -- Yes --> TEST[test]
  T -- No --> U{"first == 'ui'?"}
  U -- Yes --> UI
  U -- No --> E{"first == 'export'?"}
  E -- Yes --> EXP[export]
  E -- No --> P{first == 'compose'?}
  P -- Yes --> COM[compose]
  P -- No --> G{first == 'generate' or 'expand'?}
  G -- Yes --> GEN[generate]
  G -- No --> F{first startsWith '-'?}
  F -- Yes --> GEN
  F -- No --> GEN
```
## TUI mode (recommended)
The Ink TUI is the fastest way to iterate on prompts interactively:
- One âgenerateâ view with a scrollable history + a single input bar.
- Command palette + popups for models, context, settings, and themes.
- Built-in Test Runner view.
- Session-oriented UX (reuse last prompt, view token breakdown, show last reasoning, etc.).
Reference: `docs/tui-design.md`.
### Launch
```bash
# Default: the TUI (no args)
prompt-maker-cli
# Explicit entry
prompt-maker-cli ui
# Start TUI with an interactive transport socket/pipe
prompt-maker-cli ui --interactive-transport /tmp/pmc.sock
```
Important caveats:
- **TTY required**: Ink needs raw-mode input. Running the TUI without a real TTY will throw an Ink âRaw mode is not supportedâ error.
- **Platform/terminal support**: the TUI has only been tested on **macOS** using the **Kitty** terminal app. Running on Windows (or in other terminals) might pose problems.
- **Minimal arg parsing**: `ui` only parses `--interactive-transport`. `prompt-maker-cli ui --help` is not implemented.
- **FZF matching**: the TUI uses the npm `fzf` library for fuzzy matching (bundled via `npm install`; no separate `fzf` binary is required).
### Keybindings
Global controls:
- `Ctrl+G`: open command palette (Generate view)
- `Ctrl+T`: switch to Test Runner view
- `/help`: open help overlay
- `Ctrl+C` or `/exit`: exit
- `Esc`: dismiss popups/menus (never exits)
Generate view:
- Type normal text + `Enter`: generate
- Type `/`: enter command mode (filters the palette)
- `â/â`: scroll history (when no help/popup is active)
- `Tab`:
  - If the input looks like a dropped absolute file path, it adds that file as context.
  - Otherwise, it opens the `/series` flow.
Test Runner view:
- `Tab` / `Shift+Tab`: change focus
- `Enter`:
  - in file input: moves focus to actions
  - in actions: runs tests
### Input routing invariant
Key handling priority (highest wins):
1. Help overlay
2. Popup input
3. Active screen input
4. AppContainer global keys
This prevents âfallthroughâ where one key triggers multiple layers.
### D11. TUI Input Routing Priority
Shows the key handling priority order as an explicit routing gate.
```mermaid
flowchart TD
  K[Keypress] --> H{Help overlay open?}
  H -- Yes --> HNDH["Help overlay handles key<br>(suppress everything else)"]
  H -- No --> P{Popup open?}
  P -- Yes --> HNDP["Active popup handles key<br>(no fallthrough)"]
  P -- No --> S{Active screen handles key?}
  S -- Yes --> HNDS["Screen handles key<br>(e.g., history scroll, submit)"]
  S -- No --> G{Global keybind?}
  G -- Yes --> HNDG["AppContainer global key<br>(exit, toggle views)"]
  G -- No --> IGN[Ignored]
```
### Command palette (`/commands`)
Open the palette with `Ctrl+G` or type `/` in the Generate input.
Commands:
| Command                   | Args                     | What it does                                                                                        |
| ------------------------- | ------------------------ | --------------------------------------------------------------------------------------------------- |
| `/help`                   | -                        | Show keyboard shortcuts (help overlay)                                                              |
| `/model`                  | -                        | Pick the generation model (popup)                                                                   |
| `/target`                 | -                        | Pick the target/runtime model (recorded; not included in prompt text) (popup)                       |
| `/polish`                 | `off\|clear\|--clear`    | Clear polish model, or pick the polish model (popup)                                                |
| `/intent`                 | `[path]`                 | Set an intent file path, or pick a file (popup)                                                     |
| `/meta` / `/instructions` | `[text]`                 | Set meta instructions, or open the meta editor (popup)                                              |
| `/new`                    | -                        | Reset session state                                                                                 |
| `/reuse`                  | -                        | Reset session and reuse last prompt                                                                 |
| `/file`                   | -                        | Add local file context (popup)                                                                      |
| `/url`                    | `[url ...]`              | Add one or more HTTP(S) URLs inline, or open the URL popup                                          |
| `/smart`                  | `on\|off`                | Toggle smart context                                                                                |
| `/smart-root`             | `<path>\|--clear\|clear` | Set/clear smart-context scan root (may auto-enable smart context)                                   |
| `/image`                  | `[path]`                 | Add an image path inline, or open the image popup                                                   |
| `/video`                  | `[path]`                 | Add a video path inline, or open the video popup                                                    |
| `/copy`                   | `[on\|off]`              | Toggle auto-copy, or open the toggle popup                                                          |
| `/chatgpt`                | `[on\|off]`              | Toggle auto-open-ChatGPT, or open the toggle popup                                                  |
| `/json`                   | `[on\|off]`              | Toggle showing the JSON payload in history (TUI-only; blocked when interactive transport is active) |
| `/tokens`                 | -                        | Show token breakdown (popup)                                                                        |
| `/budgets`                | -                        | Configure token budgets and overflow strategy (popup)                                               |
| `/resume`                 | -                        | Resume generation from history or an exported payload (popup)                                       |
| `/export`                 | -                        | Export a selected history payload to JSON/YAML (popup)                                              |
| `/settings`               | -                        | Show settings (popup)                                                                               |
| `/theme`                  | -                        | Theme picker (preview with arrows, `Enter` confirm, `Esc` cancel) (popup)                           |
| `/theme-mode`             | -                        | Theme mode picker (popup)                                                                           |
| `/reasoning` / `/why`     | -                        | Show last model reasoning (popup, when available)                                                   |
| `/history`                | -                        | Show command history (popup)                                                                        |
| `/series`                 | `[draft text]`           | Generate a set of standalone âatomic promptsâ                                                       |
| `/test`                   | `[file]`                 | Run prompt tests (with a file arg) or open the test popup                                           |
| `/exit`                   | -                        | Exit the app                                                                                        |
Notes:
- `/theme-mode` opens a popup (it does not parse inline args).
- `/json` inside the TUI only toggles whether a JSON payload is shown in the history pane; it does not enable generate-mode `--json`.
- `/json` is blocked when `prompt-maker-cli ui --interactive-transport ...` is active.
#### Workflow popups (`/budgets`, `/resume`, `/export`)
These workflow commands open popups (no inline args parsing) and persist defaults back into CLI config (`src/config.ts`):
- `/budgets`: sets token budgets + overflow strategy.
  - Persists: `promptGenerator.maxInputTokens`, `promptGenerator.maxContextTokens`, `promptGenerator.contextOverflowStrategy`
  - Defaulting: when budgets are enabled and overflow is unset, effective overflow defaults to `fail` (`src/tui/budget-settings.ts`).
- `/resume`: resumes from a selected history entry or an exported payload file.
  - Persists: `resumeMode` (`best-effort` | `strict`), `resumeSourceKind` (`history` | `file`)
  - Behavior: resume runs can start with an empty typed intent (the resumed payload supplies intent) (`src/tui/hooks/useGenerationPipeline.ts`).
  - Context reuse: only `source:"file"` context paths are reusable; `url`/`smart` are treated as missing (`src/generate/pipeline.ts`).
- `/export`: exports a selected history payload to JSON/YAML.
  - Persists: `exportFormat` (`json` | `yaml`), `exportOutDir` (directory path)
  - Schema gating: exporting a history entry with an unsupported `schemaVersion` fails with an actionable error (`src/history/generate-history.ts`).
### Series generation (âatomic promptsâ)
`/series` (or `Tab`) generates:
- `00-overview.md`
- One file per atomic prompt step (e.g. `01-...md`, `02-...md`, ...)
Artifacts are written under:
- `generated/series/<timestamp>-<intent-slug>/`
If the output directory cannot be created (permissions, read-only filesystem, etc.), the series still generates but wonât be saved.
### D12. `/series` Generation and Filesystem Outputs
Shows the `/series` happy path and the best-effort write behavior.
<details>
<summary>Diagram</summary>
```mermaid
flowchart TD
  A["User triggers /series<br>(or presses Tab)"] --> B["Collect draft text<br>(or use current input)"]
  B --> C["Generate atomic prompt steps<br>(no cross references)"]
  C --> D[Render files<br>00-overview + 01..N steps]
  D --> E{Can create output dir?<br> generated/series/<timestamp>-<slug>/}
  E -- Yes --> W[Write markdown files]
  E -- No --> NW[Skip writes; keep results in-session]
  W --> DONE[Series ready]
  NW --> DONE
```
</details>
### TUI theming
Inside the TUI:
- `/theme` opens the theme picker.
- `/theme-mode` switches appearance mode (`dark`, `light`, or `system`).
Theme settings (and some workflow defaults like budgets/resume/export) persist to CLI config.
Theme JSON format:
- Top-level `theme` object (required)
- Top-level `defs` object (optional)
- Canonical schema: `src/tui/theme/theme-types.ts`
#### Custom themes
Theme files are plain JSON. The theme name is the filename without `.json`.
- **Global** (per-user): `~/.config/prompt-maker-cli/themes/*.json`
- **Project-local**: `.prompt-maker-cli/themes/*.json`
  - Parent directories are also scanned (walking up from the current working directory).
Quick start:
```bash
# Global install
mkdir -p ~/.config/prompt-maker-cli/themes
cp src/tui/theme/examples/ocean-example.json ~/.config/prompt-maker-cli/themes/ocean.json
# Or project-local
mkdir -p .prompt-maker-cli/themes
cp src/tui/theme/examples/ocean-example.json .prompt-maker-cli/themes/ocean.json
```
Restart the TUI, then run `/theme` and select `ocean`.
#### Theme precedence
If multiple themes share the same name:
1. Project-local themes in the nearest directory to your CWD (highest precedence)
2. Project-local themes in ancestor directories
3. Global themes (`~/.config/prompt-maker-cli/themes`)
4. Built-in themes (lowest precedence)
### D13. Theme Resolution Precedence
Shows the lookup order used when multiple themes share a name.
<details>
<summary>Diagram</summary>
```mermaid
flowchart TD
  A["Resolve theme name<br>(from config: theme)"] --> B[Search sources by precedence]
  B --> P1["1) Project-local themes<br>nearest .prompt-maker-cli/themes"]
  P1 --> M1{Found theme with name?}
  M1 -- Yes --> USE[Use that theme]
  M1 -- No --> P2["2) Project-local themes<br>ancestor directories"]
  P2 --> M2{Found?}
  M2 -- Yes --> USE
  M2 -- No --> G["3) Global themes<br>~/.config/prompt-maker-cli/themes"]
  G --> M3{Found?}
  M3 -- Yes --> USE
  M3 -- No --> BI["4) Built-in themes"]
  BI --> USE
```
</details>
#### Theme mode (`system`)
`system` is intentionally pragmatic:
- If `TERM_BACKGROUND` is set to `light`/`dark`, it is used.
- Else, we try to infer from `COLORFGBG`.
- If no reliable signal is present, we deterministically fall back to `dark`.
- Config also accepts `themeMode: "auto"` as an alias for `"system"`.
---
## Generate mode (CLI)
Generate consumes intent from one of:
- Positional string: `prompt-maker-cli "..."`
- File: `--intent-file path/to/intent.md`
- `stdin`: pipe into the command
### Common workflows
```bash
# Inline intent + context files + copy to clipboard
prompt-maker-cli "Draft a confident onboarding-bot spec" \
  --model gpt-4o-mini \
  --context docs/spec/**/*.md \
  --image assets/wireframe.png \
  --copy
# URL context (web pages or GitHub)
prompt-maker-cli "Summarize the docs" \
  --url https://example.com/docs \
  --url https://github.com/example/repo/tree/main/docs
# Smart context (local embeddings) + scoped scan root
prompt-maker-cli "Explain this module" \
  --smart-context \
  --smart-context-root src
# JSON payload capture (non-interactive only)
prompt-maker-cli --intent-file drafts/travel.md --json > runs/travel.json
# Export last payload from history (portable JSON/YAML)
prompt-maker-cli export --format json --out runs/last-run.json
# Resume from an exported payload (reuses prompt + refinements; best-effort on missing files)
prompt-maker-cli "" --resume-from runs/last-run.json --resume-mode best-effort --quiet --stream jsonl --progress=false
# Stream progress/events as JSONL (use --quiet to avoid mixing text output)
prompt-maker-cli "Summarize" --stream jsonl --quiet > runs/events.jsonl
```
### Key flags
| Flag / Input                                | Purpose                                                                               |
| ------------------------------------------- | ------------------------------------------------------------------------------------- |
| `<intent>` / `--intent-file <path>` / stdin | Provide the rough intent text. Pipe stdin when automating.                            |
| `-c, --context <glob>` (repeatable)         | Attach local file(s) to the request; globs use `fast-glob` (`dot: true`).             |
| `--url <https://...>` (repeatable)          | Download remote pages or GitHub content and attach as virtual context.                |
| `--smart-context`                           | Attach additional relevant local files via embeddings search.                         |
| `--smart-context-root <path>`               | Limit smart-context scanning to a specific directory (default: CWD).                  |
| `--image <path>` (repeatable)               | Attach reference images (PNG/JPG/JPEG/WEBP/GIF, â¤20MB).                               |
| `--video <path>` (repeatable)               | Attach reference videos (Gemini only; non-Gemini models are auto-switched).           |
| `--pdf <path>` (repeatable)                 | Attach reference PDFs (Gemini native; OpenAI best-effort fallback).                   |
| `--model <name>`                            | Override the generation model used by the CLI.                                        |
| `--target <name>`                           | Target/runtime model recorded in JSON/history (not included in prompt text).          |
| `-i, --interactive`                         | Enable a TTY refinement loop (requires a TTY).                                        |
| `--interactive-transport <path>`            | Listen on a Unix socket / Windows named pipe for `refine`/`finish` commands.          |
| `--resume-last`                             | Resume from the last JSONL history entry.                                             |
| `--resume <selector>`                       | Resume from history selector (`last`, `last:N`, or `N`-th from end).                  |
| `--resume-from <path>`                      | Resume from an exported payload file (`.json` / `.yaml` / `.yml`).                    |
| `--resume-mode strict\|best-effort`         | How to handle missing resumed context paths.                                          |
| `--polish`, `--polish-model <name>`         | Run the finishing pass and optionally choose a different model.                       |
| `--json`                                    | Emit machine-readable JSON (non-interactive only).                                    |
| `--stream none\|jsonl`                      | Emit newline-delimited JSON events to stdout.                                         |
| `--max-input-tokens <n>`                    | Cap total input tokens (intent + system + text context).                              |
| `--max-context-tokens <n>`                  | Cap tokens reserved for text context entries (file/url/smart).                        |
| `--context-overflow <strategy>`             | Overflow handling: `fail`, `drop-smart`, `drop-url`, `drop-largest`, `drop-oldest`.   |
| `--quiet`                                   | Suppress human-oriented output (banners/telemetry/boxed prompts).                     |
| `--progress/--no-progress`                  | Enable/disable progress spinners (spinners are disabled during interactive sessions). |
| `--show-context`                            | Print resolved context payload before generation.                                     |
| `--context-file <path>`                     | Write resolved context to disk (same format as `--show-context`).                     |
| `--context-format text\|json`               | Choose how `--show-context`/`--context-file` render the context payload.              |
| `--context-template <name>`                 | Wrap the final prompt using a named template (built-in: `nvim`).                      |
| `--copy`, `--open-chatgpt`                  | Copy/open the final artifact for quick sharing.                                       |
Notes:
- `--json` cannot be combined with interactive refinement (`--interactive` or `--interactive-transport`).
- `--show-context` prints to **stderr** when `--json` is enabled (so stdout stays machine-readable).
- `--stream jsonl` is designed for machine consumption; for clean JSONL output on stdout use `--quiet` and avoid other human-output flags.
- Token budgets apply only to **text** context entries (`--context`, `--url`, `--smart-context`); images/videos/pdfs are not trimmed by these strategies.
- `DEBUG=1` or `VERBOSE=1` prints the modelâs `reasoning` (if provided) to stderr.
### Conceptual architecture
Generate mode assembles a âprompt contractâ from:
- **Intent** text
- **Resolved context** (local files, remote URLs/GitHub, optional smart context)
- **Optional media** parts (images, videos)
- **Model choices** (generation model and recorded target model)
Then it iterates (optionally interactively), optionally runs a polish pass, emits events, and writes history.
### D2. Generate Pipeline Overview
Summarizes the full âgenerateâ path, including interactive refinement and the polish pass.
<details>
<summary>Diagram</summary>
```mermaid
flowchart TD
  A[Start: generate invocation] --> I["Intent intake<br>(positional | --intent-file | stdin)"]
  I --> V{Intent valid?<br>UTF-8, no NUL, <=512KB}
  V -- No --> FATAL["Fail fast<br>(intent required / invalid)"]
  V -- Yes --> C[Resolve context blocks]
  C --> CF[Local file context<br>--context globs]
  C --> CU["Remote URL context<br>--url http(s)"]
  C --> CG[GitHub URL expansion<br>--url github.com/...]
  C --> CS[Optional smart context<br>--smart-context]
  C --> CM[Optional media<br>--image / --video / --pdf]
  CF --> DEDUPE[De-dupe / merge context paths]
  CU --> DEDUPE
  CG --> DEDUPE
  CS --> DEDUPE
  CM --> DEDUPE
  DEDUPE --> T["Token telemetry<br>(intent + context)"]
   T --> BUDGET{Token budgets enabled?<br>--max-* or config}
   BUDGET -- No --> G1[Generation iteration 1<br>PromptGeneratorService.generatePrompt]
   BUDGET -- Yes --> TRIM{Context exceeds budget?}
   TRIM -- No --> G1
   TRIM -- Yes --> PRUNE["Trim text context per strategy<br>(context.overflow)"]
   PRUNE --> G1
  G1 --> INT{Interactive?<br>--interactive or --interactive-transport}
  INT -- No --> DONEITER[Stop refining]
  INT -- Yes --> LOOP["Interactive loop<br>(transport or TTY)"]
  LOOP --> GREF["Generation iteration N<br>(buildRefinementMessage)"]
  GREF --> LOOP
  LOOP --> DONEITER
  DONEITER --> P{Polish enabled?<br>--polish}
  P -- Yes --> POL["polishPrompt<br>(optional --polish-model)"]
  P -- No --> SKIP[Skip polish]
  POL --> OUT["Assemble output payload<br>(render template, history append)"]
  SKIP --> OUT
  OUT --> FINAL["Deliver artifacts<br>(prompt + renderedPrompt + JSON/history)"]
```
</details>
---
### Intent intake
Intent sources (exactly one must yield content):
- Inline argument
- `--intent-file <path>`
- stdin
Intent file constraints (used by integrations as guardrails):
- Must be UTFâ8 text
- Must not contain NUL bytes
- Size cap: 512 KB
### Context ingestion
Context can be attached from:
- Local file globs: `--context` (repeatable)
- Remote URLs: `--url` (repeatable)
- Smart context: `--smart-context` (+ `--smart-context-root`)
- Media: `--image`, `--video`, `--pdf`
#### D6. Context Ingestion Decision Tree
Shows how sources are resolved, merged, and how errors are classified (warnings vs fatal).
<details>
<summary>Diagram</summary>
```mermaid
flowchart TD
  A[Begin context resolution] --> B["Resolve user intent<br>(positional | --intent-file | stdin)"]
  B --> C{Has non-empty intent text?}
  C -- No --> F[Fail: intent text required]
  C -- Yes --> D[Initialize context list]
  D --> E1[Expand --context globs<br>fast-glob dot:true]
  E1 --> E1M{Any matches?}
  E1M -- No --> W1["Warn: glob matched nothing<br>(non-fatal)"]
  E1M -- Yes --> E1R[Read files<br>attach as <file path="...">]
  D --> E2["Resolve --url entries<br>http(s) only"]
  E2 --> E2P{Protocol ok?}
  E2P -- No --> W2["Warn: non-http(s) URL<br>(skip)"]
  E2P -- Yes --> E2D[Download <=1MB<br>HTML -> text]
  E2D --> E2OK{Fetch ok?}
  E2OK -- No --> W3["Warn: URL fetch failed<br>(non-fatal)"]
  E2OK -- Yes --> E2R[Attach as virtual file<br>path: url:<url>]
  D --> E3["Recognize GitHub URLs<br>(blob/tree/repo root)"]
  E3 --> E3R["Expand with limits<br><=60 files, <=64KB each"]
  E3R --> E3OK{Fetch ok?}
  E3OK -- No --> W4["Warn: GitHub fetch failed<br>(non-fatal)"]
  E3OK -- Yes --> E3A["Attach expanded files<br>(ignore dist, node_modules, lockfiles..."]
  D --> E4{--smart-context enabled?}
  E4 -- No --> SKIPSC[Skip smart context]
  E4 -- Yes --> SC[Scan/index/search local files<br><=25KB]
  D --> E5[Media attachments<br>--image / --video / --pdf]
  E5 --> E5W[Unsupported/oversize -> warn + skip]
  E1R --> MERGE["De-dupe context paths<br>(avoid duplicates across sources)"]
  E2R --> MERGE
  E3A --> MERGE
  SC --> MERGE
  SKIPSC --> MERGE
  E5W --> MERGE
  MERGE --> DONE["Context ready<br>(used for telemetry + generation)"]
```
</details>
### Remote URL and GitHub context
- `--url` supports `http:`/`https:` only.
- Each URL becomes a virtual context file (`path: url:<url>`).
- HTML pages are capped at 1MB and converted to text (scripts/styles stripped).
- GitHub URLs are recognized for repo root, `tree`, and `blob` forms.
- GitHub safety limits:
  - â¤60 files
  - â¤64KB each
  - ignore lists (e.g., `node_modules`, `dist`, lockfiles, archives)
- Optional `GITHUB_TOKEN` improves rate limits.
#### D8. URL + GitHub Context Resolution
Summarizes remote context paths and the GitHub expansion safeguards.
<details>
<summary>Diagram</summary>
```mermaid
flowchart TD
  A[--url <value>] --> B{Is github.com URL?}
  B -- No --> U[Generic URL fetch]
  U --> U1{"Protocol http(s)?"}
  U1 -- No --> UW[Warn + skip]
  U1 -- Yes --> U2[Download <=1MB]
  U2 --> U3{HTML?}
  U3 -- Yes --> U4["Convert HTML to text<br>(html-to-text)"]
  U3 -- No --> U5[Use response body as text]
  U4 --> UA[Attach virtual file<br>path: url:<url>]
  U5 --> UA
  B -- Yes --> G[GitHub resolver]
  G --> G1["Detect form<br>repo root | tree | blob"]
  G1 --> G2[Enumerate targets<br>apply ignore lists]
  G2 --> G3[Fetch contents]
  G3 --> G4{Within safety limits?<br><=60 files, <=64KB each}
  G4 -- No --> GL["Stop at limits<br>(skip extras)"]
  G4 -- Yes --> GA["Attach expanded files<br>(as virtual context entries)"]
  GL --> DONE[Context additions complete]
  GA --> DONE
  UA --> DONE
```
</details>
### Smart context (Local RAG)
Smart context attaches additional relevant local files based on embeddings.
- Trigger: `--smart-context` (optionally with `--smart-context-root <dir>`)
- Scan glob: `**/*.{ts,tsx,js,jsx,py,md,json}` (excludes `node_modules`, build outputs, lockfiles, git metadata)
- File cap: skip files larger than 25KB
- Cache: `~/.config/prompt-maker-cli/embeddings_cache.json`
- Select: topâk results (default 5) against the intent string
- De-dupe: do not append files already attached via user context
#### D7. Smart Context Workflow
Shows the scan/index/search loop and where caching and de-dupe happen.
<details>
<summary>Diagram</summary>
```mermaid
flowchart TD
  A["--smart-context enabled] --> B[Choose scan root<br>(--smart-context-root or CWD)"]
  B --> C[Scan files<br>fast-glob include extensions]
  C --> D[Filter<br>exclude node_modules/dist/...<br>skip >25KB]
  D --> E["Index embeddings<br>(cache by SHA256)"]
  E --> F[Search top-k<br>against intent text]
  F --> G[Read selected files]
  G --> H[Append unique files<br>skip ones already in user context]
  H --> DONE[Smart context ready<br>merged into generation context]
```
</details>
### Media attachments
- **Images** (`--image`): PNG/JPG/JPEG/WEBP/GIF up to 20 MB. Encoded into image parts.
- **Videos** (`--video`): Gemini-only.
  - If any `--video` is present and the requested model is not Gemini, the CLI auto-switches to a Gemini model (`gemini-3-pro-preview` or configured default).
  - Uploads use Googleâs Files API and poll until the file becomes `ACTIVE`.
  - Upload progress emits stream events.
- **PDFs** (`--pdf`): Gemini native + OpenAI text extraction.
  - Gemini: uploaded via Files API and sent as `application/pdf` `fileData` parts.
  - OpenAI: extracts text from PDFs in-process (uses `pdfjs-dist`, no external binaries). Configure `PROMPT_MAKER_PDF_MAX_PAGES`, `PROMPT_MAKER_PDF_MAX_TEXT_CHARS`, and `PROMPT_MAKER_PDF_MAX_STREAMS`.
#### D9. Media Handling (Images + Videos + PDFs)
Shows validation, model switching for videos, and PDF attachment behavior.
<details>
<summary>Diagram</summary>
```mermaid
flowchart TD
  A[Media flags present?] --> B{--image provided?}
  B -- Yes --> I1[Validate image<br>ext + <=20MB]
  I1 --> IOK{Valid?}
  IOK -- No --> IW[Warn + skip]
  IOK -- Yes --> I2["Read + Base64 encode<br>(build image parts)"]
  B -- No --> I0[No image work]
  A --> C{--video provided?}
  C -- No --> V0[No video work]
  C -- Yes --> V1{Is selected model Gemini?}
  V1 -- No --> VSW[Auto-switch generation model<br>-> Gemini video-capable default]
  V1 -- Yes --> V2["Upload video<br>(GoogleAIFileManager)"]
  VSW --> V2
  V2 --> V3["Poll until ACTIVE<br>(or fail with error)"]
  V3 --> VOK{Upload ok?}
  VOK -- No --> VW["Warn/Fail (upload error)<br>(run continues where possible)"]
  VOK -- Yes --> V4[Attach video reference<br>into request parts]
  A --> P{--pdf provided?}
  P -- No --> P0[No PDF work]
  P -- Yes --> P1{Is selected model Gemini?}
  P1 -- Yes --> P2["Upload PDF<br>(GoogleAIFileManager)"]
  P2 --> P3["Poll until ACTIVE<br>(or fail with error)"]
  P3 --> POK{Upload ok?}
  POK -- No --> PW["Fail (upload error)"]
  POK -- Yes --> P4[Attach PDF reference<br>into request parts]
  P1 -- No --> P5["Extract PDF text<br>(in-process) + attach as text"]
  I2 --> DONE[Media ready]
  IW --> DONE
  I0 --> DONE
  V0 --> DONE
  V4 --> DONE
  VW --> DONE
  P0 --> DONE
  P4 --> DONE
  P5 --> DONE
  PW --> DONE
```
</details>
### Streaming events and telemetry
`--stream none|jsonl` controls whether events are printed to stdout.
- `--stream jsonl` emits newline-delimited JSON events to stdout.
- Even with `--stream none`, the interactive transport acts as an always-on âtapâ that receives the same events.
Event types (integration-relevant highlights):
- `resume.loaded` (emitted early when resuming from history or a payload file)
- `context.telemetry` (intent/system/context/total token counts)
- `context.overflow` (emitted when budgets force text-context trimming)
- `progress.update` (machine-readable progress)
- `upload.state` (image/video/pdf upload start/finish)
- `generation.iteration.start` / `generation.iteration.complete`
- `interactive.state` / `interactive.awaiting`
- `transport.listening`, `transport.client.connected`, `transport.client.disconnected`
- `generation.final` (final JSON payload)
#### D3. `--stream jsonl` Event Timeline
Shows a typical event sequence (actual events depend on flags and context sources).
<details>
<summary>Diagram</summary>
```mermaid
sequenceDiagram
  autonumber
  participant User
  participant CLI as prompt-maker-cli
  participant Stream as stdout (--stream jsonl)
  User->>CLI: start generate (intent + flags)
  opt Budgets enabled and overflow
    CLI->>Stream: context.overflow
  end
  CLI->>Stream: context.telemetry
  alt Remote context present
    CLI->>Stream: progress.update (scope=url start/update/stop)
  end
  alt Smart context enabled
    CLI->>Stream: progress.update (scope=smart start/update/stop)
  end
  alt Media present
    CLI->>Stream: upload.state (start per image/video)
    CLI->>Stream: progress.update (label "Uploading..." updates)
    CLI->>Stream: upload.state (finish per image/video)
  end
  CLI->>Stream: generation.iteration.start (iteration=1)
  CLI->>Stream: generation.iteration.complete (iteration=1)
  opt Interactive refinement enabled
    CLI->>Stream: interactive.state (phase=start)
    CLI->>Stream: interactive.awaiting (mode=tty|transport)
    loop for each refinement
      CLI->>Stream: interactive.state (phase=refine)
      CLI->>Stream: generation.iteration.start (iteration=N)
      CLI->>Stream: generation.iteration.complete (iteration=N)
      CLI->>Stream: interactive.awaiting (mode=tty|transport)
    end
    CLI->>Stream: interactive.state (phase=complete)
  end
  opt Polish enabled
    CLI->>Stream: progress.update (scope=polish start/update/stop)
  end
  CLI->>Stream: generation.final (result payload)
  CLI-->>User: human output (unless --quiet)
```
</details>
### Interactive refinement
Two interactive mechanisms exist:
- `--interactive` (TTY): uses terminal prompts to ask whether to refine and to collect the next refinement instruction.
  - If `--interactive` is set but no TTY is detected, the CLI warns and proceeds non-interactively.
- `--interactive-transport <path>`: accepts newline-delimited JSON commands over a local socket/pipe.
  - Unix: pass a socket path like `/tmp/pmc.sock`.
  - Windows: pass a named pipe path like `\\.\pipe\pmc`.
Commands are newline-delimited JSON:
```json
{"type":"refine","instruction":"Make it shorter"}
{"type":"finish"}
```
The transport also receives the same JSONL stream events as an always-on event âtapâ (even if `--stream` is `none`).
#### D4. `--interactive-transport` Lifecycle
Shows how a client drives refinement and how events mirror back over the same connection.
<details>
<summary>Diagram</summary>
```mermaid
sequenceDiagram
  autonumber
  participant CLI as prompt-maker-cli
  participant Sock as Socket/Pipe (server)
  participant Client as Integration client
  CLI->>Sock: bind + listen
  CLI->>Client: transport.listening (event tap)
  Client->>Sock: connect
  CLI->>Client: transport.client.connected
  CLI->>Client: (mirrored stream events)\ncontext.telemetry, context.overflow, progress.update, ...
  loop Refinement cycle
    Client->>Sock: {"type":"refine","instruction":"..."}\n(newline-delimited JSON)
    CLI->>Client: interactive.state (phase=refine)
    CLI->>Client: generation.iteration.start
    CLI->>Client: generation.iteration.complete
    CLI->>Client: interactive.awaiting (mode=transport)
  end
  Client->>Sock: {"type":"finish"}
  CLI->>Client: interactive.state (phase=complete)
  CLI->>Client: generation.final
  Client-->>Sock: disconnect
  CLI->>Client: transport.client.disconnected
  CLI->>Sock: cleanup socket on exit
```
</details>
#### D5. Interactive Phase State Machine
Shows the high-level phases exposed via events during refinement.
<details>
<summary>Diagram</summary>
```mermaid
stateDiagram-v2
  [*] --> start
  start: interactive.state<br>phase=start
  start --> prompt
  prompt: interactive.state<br>phase=prompt
  prompt --> awaiting
  awaiting: interactive.awaiting<br>mode=tty|transport
  awaiting --> refine: receive instruction
  awaiting --> complete: finish requested
  refine: interactive.state<br>phase=refine
  refine --> awaiting: iteration complete
  complete: interactive.state<br>phase=complete
  complete --> [*]
```
</details>
### Polish pass
`--polish` triggers a finishing pass (`polishPrompt`) that focuses on formatting fidelity.
- Polish model defaults to the generation model.
- Override with `--polish-model` (or `PROMPT_MAKER_POLISH_MODEL`).
### Output artifacts
A completed run can produce:
- `prompt`: the final generated prompt text (untemplated)
- `polishedPrompt`: present when `--polish` is used
- `renderedPrompt`: present when `--context-template <name>` is applied
- `generation.final` stream event: includes the full payload (`GenerateJsonPayload`)
- History append: `~/.config/prompt-maker-cli/history.jsonl`
Practical integration rule (used by editor integrations):
- Prefer `renderedPrompt` if present.
- Else prefer `polishedPrompt` if present.
- Else use `prompt`.
<details>
<summary>Reference: output payload fields (GenerateJsonPayload)</summary>
From `docs/neovim-plugin-integration.md`, the JSON payload includes (high-level):
- `intent`
- `model` (generation model)
- `targetModel` (recorded runtime model)
- `prompt`
- `refinements`, `iterations`, `interactive`
- `timestamp`
- `contextPaths`
- optional `outputPath`
- optional `polishedPrompt`, `polishModel`
- optional `contextTemplate`, `renderedPrompt`
</details>
---
## Prompt tests
```bash
prompt-maker-cli test
# or
prompt-maker-cli test prompt-tests.yaml
```
- Default file: `prompt-tests.yaml` (repo root)
- TUI Test Runner view: `Ctrl+T`
- Quirk: `prompt-maker-cli test --help` prints help but still runs the suite
### D14. Prompt Test Runner Flow
Shows CLI and TUI entry points converging on the same runner.
<details>
<summary>Diagram</summary>
```mermaid
flowchart TD
  A["Start: prompt-maker-cli test [file]"] --> B{File provided?}
  B -- No --> D[Use default<br>prompt-tests.yaml]
  B -- Yes --> C[Use provided YAML path]
  C --> E[Load and validate YAML suite]
  D --> E
  E --> F["Run cases<br>(using src/test-command.ts runner)"]
  F --> G[Report results<br>stdout/stderr UI]
  subgraph TUI[Test Runner view]
    T1[Ctrl+T opens Test Runner] --> T2[Edit file path input]
    T2 --> T3["Run tests action<br>(Enter)"]
    T3 --> F
  end
```
</details>
---
## TUI architecture and UX model
The TUI is organized around an `AppContainer` that selects the active screen and enforces input routing invariants.
- Screens:
  - Generate screen: `src/tui/screens/command/CommandScreen.tsx`
  - Test Runner screen: `src/tui/screens/test-runner/TestRunnerScreen.tsx`
- Popups:
  - Popup transitions managed by `src/tui/popup-reducer.ts` (pure reducer)
  - Side effects and orchestration managed by `src/tui/hooks/usePopupManager.ts`
- Keybinds:
  - Defined by `src/tui/app-container-keymap.ts`
Reference: `docs/tui-design.md`.
### D10. TUI Architecture Map
Shows how routing, screens, and popups fit together conceptually.
<details>
<summary>Diagram</summary>
```mermaid
flowchart TD
  subgraph Entry[Entry]
    EP[src/index.ts<br>CLI router] -->|ui| TUI[src/tui/index.tsx<br>parse --interactive-transport]
    TUI --> AC[AppContainer<br>src/tui/AppContainer.tsx]
  end
  subgraph Global[Global layers]
    KEYMAP[Global keymap<br>src/tui/app-container-keymap.ts]
    HELP["Help overlay<br>('?' toggles)"]
  end
  subgraph Screens[Screens]
    CS["CommandScreen (Generate)<br>src/tui/screens/command/CommandScreen.tsx"]
    TS[TestRunnerScreen<br>src/tui/screens/test-runner/TestRunnerScreen.tsx]
  end
  subgraph Popups[Popups + palette]
    PR["popup-reducer<br>src/tui/popup-reducer.ts<br>(pure)"]
    PM["usePopupManager<br>src/tui/hooks/usePopupManager.ts<br>(effects)"]
    PAL["Command palette<br>(COMMAND_DESCRIPTORS in src/tui/config.ts)"]
  end
  AC -->|active view| CS
  AC -->|active view| TS
  AC --> HELP
  AC --> KEYMAP
  CS --> PAL
  PAL --> PR
  PR --> PM
  PM --> CS
```
</details>
---
## Provider configuration
Credentials and defaults can come from env vars or a config file.
Config path resolution:
- `PROMPT_MAKER_CLI_CONFIG=/path/to/config.json` (highest precedence)
- `~/.config/prompt-maker-cli/config.json`
- `~/.prompt-maker-cli.json`
Example config:
```json
{
  "openaiApiKey": "sk-...",
  "geminiApiKey": "gk-...",
  "promptGenerator": {
    "defaultModel": "gpt-4o-mini",
    "defaultGeminiModel": "gemini-2.5-pro",
    "maxInputTokens": 12000,
    "maxContextTokens": 8000,
    "contextOverflowStrategy": "drop-smart"
  },
  "contextTemplates": {
    "scratch": "# Scratch\n\n{{prompt}}"
  },
  "theme": "ocean",
  "themeMode": "system",
  "resumeMode": "best-effort",
  "resumeSourceKind": "history",
  "exportFormat": "json",
  "exportOutDir": "runs"
}
```
Env vars override config keys:
- `OPENAI_API_KEY` (and optional `OPENAI_BASE_URL`)
- `GEMINI_API_KEY` (and optional `GEMINI_BASE_URL`)
- Optional: `GITHUB_TOKEN` (for GitHub URL context rate limits)
---
## Outputs and persistence
- Generate-run history (JSONL): `~/.config/prompt-maker-cli/history.jsonl`
- TUI command history: `~/.config/prompt-maker-cli/tui-history.json`
- TUI workflow defaults persisted to config: budgets (`promptGenerator.*`), resume (`resumeMode`, `resumeSourceKind`), export (`exportFormat`, `exportOutDir`)
- Token telemetry:
  - Printed as a summary in non-`--quiet` runs
  - Always emitted as a `context.telemetry` JSONL stream event
  - When budgets force trimming, a `context.overflow` event is also emitted
---
## Development
```bash
npm ci
npm run build
npm start
# watch + restart
npm run dev -- ui
npm run dev -- "Draft a confident onboarding-bot spec" --model gemini-1.5-flash
npm run typecheck
npm test
npm run format
```
---
## Maintainer references
- `docs/prompt-maker-cli-tui-encyclopedia.md` (authoritative behavior reference)
- `docs/tui-design.md` (UX goals + input routing invariant)
- `docs/neovim-plugin-integration.md` (generate pipeline + streaming events + interactive transports)
- `src/tui/DEVELOPER_NOTE.md` (architecture notes; some paths are historical)
---
## License
MIT
</file>
</files>
</file>

<file path="tsconfig.jest.json">
{
  "extends": "./tsconfig.json",
  "compilerOptions": {
    "module": "CommonJS",
    "moduleResolution": "Node"
  }
}
</file>

<file path="tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2020",
    "lib": ["ES2020", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "moduleResolution": "Bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "react-jsx",
    "strict": true,
    "noUncheckedIndexedAccess": true,
    "exactOptionalPropertyTypes": true,
    "esModuleInterop": true,
    "types": ["node"],
    "baseUrl": ".",
    "paths": {
      "@prompt-maker/core": ["packages/core/src/index.ts"],
      "@prompt-maker/core/*": ["packages/core/src/*"]
    }
  },
  "include": ["src/**/*.ts", "src/**/*.tsx", "packages/**/*.ts"],
  "exclude": ["dist", "node_modules"]
}
</file>

<file path="docs/context-templates.md">
# Context Templates in Prompt Maker CLI

Context templates let you **wrap the final generated prompt text** with a named, reusable string template. They’re designed for workflows where the prompt should land inside a consistent “envelope” (editor scratch buffer, team conventions, structured wrappers for downstream tooling, etc.).

This document is the source-of-truth behavior as implemented in `src/generate/context-templates.ts` and exposed via the `--context-template <name>` CLI flag.

## Table of contents

- [What is a context template?](#what-is-a-context-template)
- [How template resolution works](#how-template-resolution-works)
- [How template rendering works](#how-template-rendering-works)
- [Defining templates in your config](#defining-templates-in-your-config)
- [Using templates from the CLI](#using-templates-from-the-cli)
- [Built-in templates](#built-in-templates)
- [Practical examples](#practical-examples)
- [Troubleshooting](#troubleshooting)

## What is a context template?

A **context template** is a named string that is applied to the generated prompt at the very end of a run.

Use a context template when:

- You want a **consistent wrapper** around every prompt you generate (team conventions, coding guidelines, output formatting rules).
- You want the prompt to be **editor-friendly**, e.g. a scratch buffer header.
- You want to wrap the prompt in a **structured envelope** (XML-ish/JSON-ish) for downstream parsing.

A context template is _not_ a full templating engine:

- There is exactly one placeholder token: `{{prompt}}`.
- No other variables are supported.
- The rendering behavior is intentionally simple and deterministic.

## How template resolution works

When you run with `--context-template <name>`, Prompt Maker resolves `<name>` to a template string using this order (implemented in `resolveContextTemplate` in `src/generate/context-templates.ts`):

1. **Built-in templates first**.
2. If there’s no built-in match, the CLI loads your config and looks up `contextTemplates[name]`.
3. If still not found, the CLI throws an error that includes an “Available templates” list.

Important implications:

- **Built-ins win.** If you define a config template with the same name as a built-in (e.g. `nvim`), the built-in will still be used.
- The “Available templates” list is computed from:
  - built-in template names
  - plus any `contextTemplates` keys in your loaded config
  - and if there are none, it prints `none`

The error message format is:

- `Unknown context template "<name>". Available templates: <comma-separated-list-or-none>.`

## How template rendering works

Rendering is implemented in `renderContextTemplate(template, prompt)` in `src/generate/context-templates.ts`.

There are two modes:

### 1) Placeholder mode (`{{prompt}}` is present)

If the template contains the exact substring `{{prompt}}`:

- Prompt Maker replaces **all occurrences** of `{{prompt}}` with the generated prompt.
- Replacement is literal string replacement (implemented with `template.split('{{prompt}}').join(prompt)`), so it’s global.

This means you can intentionally include the prompt multiple times.

### 2) Append mode (`{{prompt}}` is not present)

If the template does _not_ contain `{{prompt}}`, Prompt Maker uses “append prompt” behavior:

1. It applies `trimEnd()` to the template (only removes trailing whitespace/newlines).
2. If the result is empty (e.g. `""`, or whitespace-only), the output is **just the prompt**.
3. Otherwise the output is:

- `trimmedTemplate + "\n\n" + prompt`

### Pseudocode summary

```ts
const PLACEHOLDER = '{{prompt}}'

function render(template: string, prompt: string): string {
  if (template.includes(PLACEHOLDER)) {
    // Replace all occurrences.
    return template.split(PLACEHOLDER).join(prompt)
  }

  const trimmed = template.trimEnd()
  if (!trimmed) return prompt

  return `${trimmed}\n\n${prompt}`
}
```

## Defining templates in your config

User-defined templates live under the `contextTemplates` key in your CLI config file, as shown in `README.md`.

### Config file locations

Config path resolution (highest precedence first):

- `PROMPT_MAKER_CLI_CONFIG=/path/to/config.json`
- `~/.config/prompt-maker-cli/config.json`
- `~/.prompt-maker-cli.json`

### Minimal config snippet

```json
{
  "contextTemplates": {
    "scratch": "# Scratch\n\n{{prompt}}"
  }
}
```

Notes:

- Config is JSON.
- Newlines must be encoded as `\n` inside JSON strings.
- Template values are plain strings; there are no other supported tokens besides `{{prompt}}`.

### Team/project sharing tip

If you want your whole team to share a standard template set, put a config file in your repo (for example `./prompt-maker.config.json`) and have developers point to it:

```bash
export PROMPT_MAKER_CLI_CONFIG="$PWD/prompt-maker.config.json"
```

Because built-in templates are resolved first, avoid naming collisions with built-ins (currently `nvim`).

## Using templates from the CLI

The CLI flag is documented in `README.md` under “Key flags”:

- `--context-template <name>` wraps the final prompt using a named template.

Examples:

```bash
# Use the built-in NeoVim-friendly wrapper
prompt-maker-cli "Draft a refactor plan for this module" \
  --context "src/**/*.ts" \
  --context-template nvim

# Use a custom template defined in your config
prompt-maker-cli "Write a PR description" \
  --context README.md \
  --context-template scratch
```

## Built-in templates

Built-ins are defined in `src/generate/context-templates.ts`.

### `nvim`

Name: `nvim`

Template body (exactly as implemented):

```text
## NeoVim Prompt Buffer

Paste this block into a scratch buffer (e.g., :enew) so you can keep prompts beside your work.

{{prompt}}
```

Rendered output shape:

- It always starts with the heading and instruction text.
- The generated prompt is inserted where `{{prompt}}` appears.

## Practical examples

All examples below are designed to be copy/pasteable into your config under `contextTemplates`, then invoked via `--context-template <name>`.

Each example includes:

- A config entry (JSON)
- When to use it
- The resulting output shape

### Example 1 — Wrapper template (uses `{{prompt}}`)

**Name:** `scratch`

**Config:**

```json
{
  "contextTemplates": {
    "scratch": "# Scratch\n\n{{prompt}}"
  }
}
```

**When to use:** You want a lightweight heading above the prompt.

**CLI:**

```bash
prompt-maker-cli "Summarize this file" --context src/index.ts --context-template scratch
```

**Output shape:**

```text
# Scratch

<the generated prompt>
```

### Example 2 — Append mode (omits `{{prompt}}`)

**Name:** `preamble`

Because there is no `{{prompt}}`, Prompt Maker will `trimEnd()` the template and then append the prompt with a blank line separator.

**Config:**

```json
{
  "contextTemplates": {
    "preamble": "You are a careful assistant. Ask clarifying questions when needed."
  }
}
```

**When to use:** You want a standard preface, but you don’t want to embed a placeholder.

**CLI:**

```bash
prompt-maker-cli "Propose 3 test cases" --context src/generate/pipeline.ts --context-template preamble
```

**Output shape:**

```text
You are a careful assistant. Ask clarifying questions when needed.

<the generated prompt>
```

### Example 3 — Minimal / whitespace-only template

**Name:** `empty`

If the template is empty or becomes empty after `trimEnd()`, the rendered output is **just the prompt**.

**Config:**

```json
{
  "contextTemplates": {
    "empty": "   \n\n"
  }
}
```

**When to use:** You want a stable flag in scripts, but effectively disable wrapping.

**CLI:**

```bash
prompt-maker-cli "Draft a changelog entry" --context README.md --context-template empty
```

**Output shape:**

```text
<the generated prompt>
```

### Example 4 — Headings + workflow instructions (editor checklist)

**Name:** `review-workflow`

**Config:**

```json
{
  "contextTemplates": {
    "review-workflow": "## Review Workflow\n\n1. Read the prompt carefully.\n2. Apply changes locally.\n3. Run tests.\n4. Summarize risks and tradeoffs.\n\n{{prompt}}\n\n## Notes\n- Keep changes small.\n- Prefer deterministic behavior."
  }
}
```

**When to use:** You paste prompts into an editor/issue and want a consistent checklist around them.

**CLI:**

```bash
prompt-maker-cli "Review this reducer for edge cases" \
  --context "src/tui/**/*reducer*.ts" \
  --context-template review-workflow
```

**Output shape (excerpt):**

```text
## Review Workflow

1. Read the prompt carefully.
...

<the generated prompt>

## Notes
- Keep changes small.
...
```

### Example 5 — Structured wrapper for downstream tooling (XML-ish)

**Name:** `xml-envelope`

**Config:**

```json
{
  "contextTemplates": {
    "xml-envelope": "<request>\n  <intent>Generated by prompt-maker-cli</intent>\n  <prompt>\n{{prompt}}\n  </prompt>\n</request>"
  }
}
```

**When to use:** You want your prompt inside stable tags so another tool can extract it reliably.

**CLI:**

```bash
prompt-maker-cli "Generate release notes" --context docs/cookbook.md --context-template xml-envelope
```

**Output shape:**

```text
<request>
  <intent>Generated by prompt-maker-cli</intent>
  <prompt>
<the generated prompt>
  </prompt>
</request>
```

### Example 6 — Structured wrapper for downstream tooling (JSON-ish)

**Name:** `json-envelope`

**Config:**

```json
{
  "contextTemplates": {
    "json-envelope": "{\n  \"type\": \"pmc_prompt\",\n  \"prompt\": \"{{prompt}}\"\n}"
  }
}
```

**When to use:** You want a single blob that looks like JSON (useful for copy/paste into systems expecting JSON-like payloads).

Caveat: The generated prompt can contain quotes/newlines that are not JSON-escaped. This template is best when the consuming system is tolerant of “JSON-ish” text, not strict JSON parsing.

**CLI:**

```bash
prompt-maker-cli "Explain this API" --context src/index.ts --context-template json-envelope
```

### Example 7 — Team / project standard template (shared config)

**Name:** `team-default`

This example is meant to live in a shared config file, referenced via `PROMPT_MAKER_CLI_CONFIG`, so everyone gets the same wrapper.

**Config (`prompt-maker.config.json` in your repo, for example):**

```json
{
  "contextTemplates": {
    "team-default": "## Team Prompt Contract\n\n- Be explicit about constraints and acceptance criteria.\n- Prefer small diffs and minimal refactors.\n- If requirements are ambiguous, ask questions first.\n\n{{prompt}}\n"
  }
}
```

**CLI:**

```bash
PROMPT_MAKER_CLI_CONFIG="$PWD/prompt-maker.config.json" \
  prompt-maker-cli "Add tests for this behavior" \
  --context src/generate/context-templates.ts \
  --context-template team-default
```

### Bonus — Multiple `{{prompt}}` occurrences (global replacement)

**Name:** `double`

All occurrences of `{{prompt}}` are replaced.

**Config:**

```json
{
  "contextTemplates": {
    "double": "## Prompt (verbatim)\n\n{{prompt}}\n\n## Prompt (for quoting)\n\n> {{prompt}}"
  }
}
```

## Troubleshooting

### “Unknown context template … Available templates: …”

If you see an error like:

```text
Unknown context template "my-template". Available templates: nvim, scratch.
```

It means the name you passed via `--context-template` was not found.

Fix options:

- Use one of the listed names (built-ins + any `contextTemplates` from your loaded config).
- Add the missing template under `contextTemplates` in your config file.
- Double-check you’re editing the config file that Prompt Maker is actually loading (see config path precedence in [Defining templates in your config](#defining-templates-in-your-config)).

### My template doesn’t seem to apply

Common causes:

- You defined a template with the same name as a built-in (e.g. `nvim`). Built-ins resolve first, so your config entry won’t be used.
- Your template doesn’t contain `{{prompt}}` and you expected replacement behavior. Without the placeholder, Prompt Maker uses append mode.

### Why are there extra blank lines?

In append mode, Prompt Maker always inserts `"\n\n"` between the (trimmed) template and the prompt.

If you need exact spacing control, use explicit placeholder mode (`{{prompt}}`) and place newlines exactly where you want them.
</file>

<file path="docs/fzf-commands.txt">
fzf is an interactive filter program for any kind of list.

It implements a "fuzzy" matching algorithm, so you can quickly type in patterns
with omitted characters and still get the results you want.

Project URL: https://github.com/junegunn/fzf
Author: Junegunn Choi <junegunn.c@gmail.com>

* See man page for more information: fzf --man

Usage: fzf [options]

  SEARCH
    -e, --exact              Enable exact-match
    +x, --no-extended        Disable extended-search mode
    -i, --ignore-case        Case-insensitive match
    +i, --no-ignore-case     Case-sensitive match
        --smart-case         Smart-case match (default)
    --scheme=SCHEME          Scoring scheme [default|path|history]
    -n, --nth=N[,..]         Comma-separated list of field index expressions
                             for limiting search scope. Each can be a non-zero
                             integer or a range expression ([BEGIN]..[END]).
    --with-nth=N[,..]        Transform the presentation of each line using
                             field index expressions
    --accept-nth=N[,..]      Define which fields to print on accept
    -d, --delimiter=STR      Field delimiter regex (default: AWK-style)
    +s, --no-sort            Do not sort the result
    --literal                Do not normalize latin script letters
    --tail=NUM               Maximum number of items to keep in memory
    --disabled               Do not perform search
    --tiebreak=CRI[,..]      Comma-separated list of sort criteria to apply
                             when the scores are tied
                             [length|chunk|pathname|begin|end|index] (default: length)

  INPUT/OUTPUT
    --read0                  Read input delimited by ASCII NUL characters
    --print0                 Print output delimited by ASCII NUL characters
    --ansi                   Enable processing of ANSI color codes
    --sync                   Synchronous search for multi-staged filtering

  GLOBAL STYLE
    --style=PRESET           Apply a style preset [default|minimal|full[:BORDER_STYLE]
    --color=COLSPEC          Base scheme (dark|light|base16|bw) and/or custom colors
    --no-color               Disable colors
    --no-bold                Do not use bold text

  DISPLAY MODE
    --height=[~]HEIGHT[%]    Display fzf window below the cursor with the given
                             height instead of using fullscreen.
                             A negative value is calculated as the terminal height
                             minus the given value.
                             If prefixed with '~', fzf will determine the height
                             according to the input size.
    --min-height=HEIGHT[+]   Minimum height when --height is given as a percentage.
                             Add '+' to automatically increase the value
                             according to the other layout options (default: 10+).
    --tmux[=OPTS]            Start fzf in a tmux popup (requires tmux 3.3+)
                             [center|top|bottom|left|right][,SIZE[%]][,SIZE[%]]
                             [,border-native] (default: center,50%)

  LAYOUT
    --layout=LAYOUT          Choose layout: [default|reverse|reverse-list]
    --margin=MARGIN          Screen margin (TRBL | TB,RL | T,RL,B | T,R,B,L)
    --padding=PADDING        Padding inside border (TRBL | TB,RL | T,RL,B | T,R,B,L)
    --border[=STYLE]         Draw border around the finder
                             [rounded|sharp|bold|block|thinblock|double|horizontal|vertical|
                              top|bottom|left|right|line|none] (default: rounded)
    --border-label=LABEL     Label to print on the border
    --border-label-pos=COL   Position of the border label
                             [POSITIVE_INTEGER: columns from left|
                              NEGATIVE_INTEGER: columns from right][:bottom]
                             (default: 0 or center)

  LIST SECTION
    -m, --multi[=MAX]        Enable multi-select with tab/shift-tab
    --highlight-line         Highlight the whole current line
    --cycle                  Enable cyclic scroll
    --wrap                   Enable line wrap
    --wrap-sign=STR          Indicator for wrapped lines
    --no-multi-line          Disable multi-line display of items when using --read0
    --raw                    Enable raw mode (show non-matching items)
    --track                  Track the current selection when the result is updated
    --tac                    Reverse the order of the input
    --gap[=N]                Render empty lines between each item
    --gap-line[=STR]         Draw horizontal line on each gap using the string
                             (default: '┈' or '-')
    --freeze-left=N          Number of fields to freeze on the left
    --freeze-right=N         Number of fields to freeze on the right
    --keep-right             Keep the right end of the line visible on overflow
    --scroll-off=LINES       Number of screen lines to keep above or below when
                             scrolling to the top or to the bottom (default: 0)
    --no-hscroll             Disable horizontal scroll
    --hscroll-off=COLS       Number of screen columns to keep to the right of the
                             highlighted substring (default: 10)
    --jump-labels=CHARS      Label characters for jump mode
    --gutter=CHAR            Character used for the gutter column (default: '▌')
    --gutter-raw=CHAR        Character used for the gutter column in raw mode (default: '▖')
    --pointer=STR            Pointer to the current line (default: '▌' or '>')
    --marker=STR             Multi-select marker (default: '┃' or '>')
    --marker-multi-line=STR  Multi-select marker for multi-line entries;
                             3 elements for top, middle, and bottom (default: '╻┃╹')
    --ellipsis=STR           Ellipsis to show when line is truncated (default: '··')
    --tabstop=SPACES         Number of spaces for a tab character (default: 8)
    --scrollbar[=C1[C2]]     Scrollbar character(s)
                             (each for list section and preview window)
    --no-scrollbar           Hide scrollbar
    --list-border[=STYLE]    Draw border around the list section
                             [rounded|sharp|bold|block|thinblock|double|horizontal|vertical|
                              top|bottom|left|right|none] (default: rounded)
    --list-label=LABEL       Label to print on the list border
    --list-label-pos=COL     Position of the list label
                             [POSITIVE_INTEGER: columns from left|
                              NEGATIVE_INTEGER: columns from right][:bottom]
                             (default: 0 or center)

  INPUT SECTION
    --no-input               Disable and hide the input section
    --prompt=STR             Input prompt (default: '> ')
    --info=STYLE             Finder info style
                             [default|right|hidden|inline[-right][:PREFIX]]
    --info-command=COMMAND   Command to generate info line
    --separator=STR          Draw horizontal separator on info line using the string
                             (default: '─' or '-')
    --no-separator           Hide info line separator
    --ghost=TEXT             Ghost text to display when the input is empty
    --filepath-word          Make word-wise movements respect path separators
    --input-border[=STYLE]   Draw border around the input section
                             [rounded|sharp|bold|block|thinblock|double|horizontal|vertical|
                              top|bottom|left|right|line|none] (default: rounded)
    --input-label=LABEL      Label to print on the input border
    --input-label-pos=COL    Position of the input label
                             [POSITIVE_INTEGER: columns from left|
                              NEGATIVE_INTEGER: columns from right][:bottom]
                             (default: 0 or center)

  PREVIEW WINDOW
    --preview=COMMAND        Command to preview highlighted line ({})
    --preview-window=OPT     Preview window layout (default: right:50%)
                             [up|down|left|right][,SIZE[%]]
                             [,[no]wrap][,[no]cycle][,[no]follow][,[no]info]
                             [,[no]hidden][,border-STYLE]
                             [,+SCROLL[OFFSETS][/DENOM]][,~HEADER_LINES]
                             [,default][,<SIZE_THRESHOLD(ALTERNATIVE_LAYOUT)]
    --preview-border[=STYLE] Short for --preview-window=border-STYLE
                             [rounded|sharp|bold|block|thinblock|double|horizontal|vertical|
                              top|bottom|left|right|line|none] (default: rounded)
    --preview-label=LABEL
    --preview-label-pos=N    Same as --border-label and --border-label-pos,
                             but for preview window

  HEADER
    --header=STR             String to print as header
    --header-lines=N         The first N lines of the input are treated as header
    --header-first           Print header before the prompt line
    --header-border[=STYLE]  Draw border around the header section
                             [rounded|sharp|bold|block|thinblock|double|horizontal|vertical|
                              top|bottom|left|right|line|none] (default: rounded)
    --header-lines-border[=STYLE]
                             Display header from --header-lines with a separate border.
                             Pass 'none' to still separate it but without a border.
    --header-label=LABEL     Label to print on the header border
    --header-label-pos=COL   Position of the header label
                             [POSITIVE_INTEGER: columns from left|
                              NEGATIVE_INTEGER: columns from right][:bottom]
                             (default: 0 or center)

  FOOTER
    --footer=STR             String to print as footer
    --footer-border[=STYLE]  Draw border around the footer section
                             [rounded|sharp|bold|block|thinblock|double|horizontal|vertical|
                              top|bottom|left|right|line|none] (default: line)
    --footer-label=LABEL     Label to print on the footer border
    --footer-label-pos=COL   Position of the footer label
                             [POSITIVE_INTEGER: columns from left|
                              NEGATIVE_INTEGER: columns from right][:bottom]
                             (default: 0 or center)

  SCRIPTING
    -q, --query=STR          Start the finder with the given query
    -1, --select-1           Automatically select the only match
    -0, --exit-0             Exit immediately when there's no match
    -f, --filter=STR         Print matches for the initial query and exit
    --print-query            Print query as the first line
    --expect=KEYS            Comma-separated list of keys to complete fzf

  KEY/EVENT BINDING
    --bind=BINDINGS          Custom key/event bindings

  ADVANCED
    --with-shell=STR         Shell command and flags to start child processes with
    --listen[=[ADDR:]PORT]   Start HTTP server to receive actions via TCP
                             (To allow remote process execution, use --listen-unsafe)
    --listen=SOCKET_PATH     Start HTTP server to receive actions via Unix domain socket
                             (Path should end with .sock)

  DIRECTORY TRAVERSAL        (Only used when $FZF_DEFAULT_COMMAND is not set)
    --walker=OPTS            [file][,dir][,follow][,hidden] (default: file,follow,hidden)
    --walker-root=DIR [...]  List of directories to walk (default: .)
    --walker-skip=DIRS       Comma-separated list of directory names to skip
                             (default: .git,node_modules)

  HISTORY
    --history=FILE           File to store fzf search history (*not* shell command history)
    --history-size=N         Maximum number of entries to keep in the file (default: 1000)

  SHELL INTEGRATION
    --bash                   Print script to set up Bash shell integration
    --zsh                    Print script to set up Zsh shell integration
    --fish                   Print script to set up Fish shell integration

  HELP
    --version                Display version information and exit
    --help                   Show this message
    --man                    Show man page

  ENVIRONMENT VARIABLES
    FZF_DEFAULT_COMMAND      Default command to use when input is tty
    FZF_DEFAULT_OPTS         Default options (e.g. '--layout=reverse --info=inline')
    FZF_DEFAULT_OPTS_FILE    Location of the file to read default options from
    FZF_API_KEY              X-API-Key header for HTTP server (--listen)
</file>

<file path="exports/cli.json">
{
  "schemaVersion": "1",
  "intent": "test prompt",
  "model": "gpt-5.2",
  "targetModel": "gpt-5.2",
  "prompt": "# Title\nTest Prompt Contract: Produce a Simple Demonstration Output\n\n## Role\n- Act as a helpful assistant executing a minimal “test” task to verify instruction-following, formatting, and completeness.\n\n## Context\n- The requester provided only: **“test prompt”**.\n- No domain, audience, or deliverable type was specified; treat this as a generic prompt-contract validation.\n\n## Goals & Tasks\n- Create a small, self-contained demonstration output that proves you can:\n  - Follow instructions precisely.\n  - Ask for missing details only when necessary.\n  - Produce a clear, structured response.\n- Tasks:\n  - (1) Interpret “test prompt” as a request for a minimal sample deliverable.\n  - (2) Produce a short sample output (see Output Format) using a neutral topic.\n  - (3) Include a brief checklist confirming what you did.\n\n## Inputs\n- Primary input:\n  - User intent text: `test prompt`\n- Optional inputs (not provided):\n  - Intended audience, tone, length, topic, constraints.\n\n## Constraints\n- Do not request additional clarification unless it is required to proceed.\n- Keep the output brief and deterministic (avoid open-ended brainstorming).\n- Avoid sensitive content.\n- Do not mention internal policies or hidden reasoning.\n\n## Execution Plan\n- Step 1: Assume a neutral topic (e.g., “benefits of daily walks”) if none is provided.\n- Step 2: Produce a concise sample deliverable using the required structure.\n- Step 3: Add a completion checklist indicating compliance with the contract.\n\n## Output Format\nReturn content in the following exact structure:\n\n1) **Sample Output**\n- A 5–7 sentence paragraph on the assumed neutral topic.\n\n2) **Completion Checklist**\n- Bullets confirming:\n  - Followed structure\n  - Kept brevity\n  - Avoided sensitive content\n  - Did not ask unnecessary questions\n\n## Quality Checks\n- Verify the response includes both required sections: “Sample Output” and “Completion Checklist”.\n- Ensure the paragraph is 5–7 sentences.\n- Ensure the checklist has at least 4 bullets.\n- Confirm the topic is neutral and non-sensitive.\n- Confirm wording is clear and free of contradictions.",
  "refinements": [],
  "iterations": 1,
  "interactive": true,
  "timestamp": "2026-01-06T14:51:24.264Z",
  "contextPaths": [
    {
      "path": "history-intent",
      "source": "intent"
    }
  ]
}
</file>

<file path="exports/feeset-changes-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
  - path: master-fees/MasterFeesTable.tsx
    source: smart
  - path: components/QuoteFeesTable.tsx
    source: smart
  - path: components/QuoteFeeSetsTable.tsx
    source: smart
  - path: master-fees/page.tsx
    source: smart
  - path: hooks/useQuoteFees.ts
    source: smart
intent: We need to remove `Active` column from the fee sets table
interactive: true
iterations: 2
model: gpt-5.2
prompt: >
  # Title
  Remove the `Active` Column from the Quote Fee Sets Table (No Code Output)
  ## Role
  You are a senior React/TypeScript engineer working in a Next.js app using
  Material React Table (MRT) and MUI.
  ## Context
  The Fee Manager UI includes a “Fee Sets” table implemented with
  `material-react-table`.
  The request is to remove the **`Active`** column from the **fee sets table**
  only.
  Relevant code (primary):
  - `components/QuoteFeeSetsTable.tsx`
  Other related tables (do not change unless explicitly required):
  - `components/QuoteFeesTable.tsx`
  - `master-fees/MasterFeesTable.tsx`
  The `Active` column currently exists in `components/QuoteFeeSetsTable.tsx` as
  a column definition with `accessorKey: 'active'`, a select editor, and a
  `Chip` cell renderer.
  ## Goals & Tasks
  - Remove the `Active` column from the **Quote Fee Sets** table UI.
  - Ensure the column is:
    - not displayed in the table
    - not editable
    - not present in create/edit modal forms (if those dialogs are generated from column defs).
  - Ensure the table remains functional (edit/create modals, row actions, expand
  panel, pagination, filters) with no runtime or TypeScript errors.
  ## Inputs
  - Primary file to modify:
    - `components/QuoteFeeSetsTable.tsx`
  - Supporting context (read-only unless necessary):
    - `components/QuoteFeesTable.tsx`
    - `master-fees/MasterFeesTable.tsx`
  ## Constraints
  - Change only what is necessary to remove the `Active` column from the **fee
  sets table**.
  - Do **not** remove or modify `Active` columns in other tables unless
  explicitly required.
  - Keep existing behavior for:
    - row actions (edit/delete/clone)
    - expanding rows (detail panel)
    - pagination, sorting, global filter
    - existing IDs/data attributes used for tests
  - Do not introduce new dependencies.
  - Do not output any code, diffs, or code snippets in your response.
  ## Execution Plan
  1. Open and inspect `components/QuoteFeeSetsTable.tsx`.
  2. Locate the `columns` definition (`useMemo<MRT_ColumnDef<QuoteFeeSet>[]>`).
  3. Remove the entire column definition object for `accessorKey: 'active'`.
  4. Scan `components/QuoteFeeSetsTable.tsx` for remaining references that may
  assume the `active` column exists:
     - `initialState` (sorting/visibility)
     - `muiTableBodyCellProps` / `data-column-id` logic
     - any validation or edit/create configuration relying on `active`
  5. Confirm the create/edit modal forms:
     - The dialogs are rendered via `createEditableFieldsDialogRenderer`; if it is driven by the column defs, removing the column must remove the field.
     - Ensure no required-field assumptions break due to the missing `active` input.
  6. Cleanup and correctness:
     - Remove now-unused imports in `components/QuoteFeeSetsTable.tsx` (e.g., `Chip`) if they become unused.
     - Ensure table configuration remains valid and unchanged otherwise.
  7. Sanity-check scope:
     - Verify you did not modify `components/QuoteFeesTable.tsx` or `master-fees/MasterFeesTable.tsx`.
  ## Output Format
  Provide a short, code-free change report containing:
  - **What was changed**: identify the file(s) modified and state that the
  `Active` column was removed from the Quote Fee Sets table.
  - **Impact on UI**: confirm the column no longer appears in the table and no
  longer appears in create/edit modals for fee sets.
  - **Non-functional checks performed**: confirm no unused imports remain and
  there are no lingering references to the removed column in
  `components/QuoteFeeSetsTable.tsx`.
  Do **not** include any code, unified diffs, or snippets.
  ## Quality Checks
  - `QuoteFeeSetsTable` still renders and functions correctly with:
    - expanding rows (detail panel shows `QuoteFeesTable`)
    - editing and creating fee sets (modal)
    - deleting and cloning
    - pagination and global filter
  - No references to the removed `active` column remain in
  `components/QuoteFeeSetsTable.tsx`.
  - No new TypeScript errors introduced (especially from removed column types or
  unused imports).
  - No unintended changes to `Active` columns in other tables
  (`components/QuoteFeesTable.tsx`, `master-fees/MasterFeesTable.tsx`).
reasoning: >-
  I interpreted the user’s refinement as a change to the prompt contract’s
  deliverable: the downstream assistant should still remove the `Active` column
  from the Quote Fee Sets table, but must not include any code output (no
  unified diff, no snippets). I preserved the original intent and structure,
  kept the required sections in the specified order, referenced the relevant
  context file paths explicitly, and adjusted the Output Format and execution
  steps to emphasize describing changes without showing code while still
  ensuring correctness checks (imports, modal fields, runtime/type safety).
refinements:
  - I don't need a unified diff or any other code shown
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-06T21:45:01.068Z'
</file>

<file path="exports/gemini-pdf-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
images: []
intent: Please make this document more succinct
interactive: true
iterations: 1
model: gemini-2.5-pro
pdfs:
  - >-
    /Users/eroberts/Downloads/BP-Adopt AI-based Browser
    Automation-080126-222458.pdf
prompt: >-
  # Title
  Document Condensation and Summarization
  ### Role
  You are an expert editor and summarizer, skilled at distilling complex
  information into clear, concise, and accurate summaries while preserving the
  original intent.
  ### Context
  The user has provided a document and requires a more succinct version. The
  goal is to reduce the overall length and complexity of the text without losing
  any essential information, key arguments, or the intended tone.
  ### Goals & Tasks
  **Goal:** To produce a condensed version of the provided document that is
  significantly shorter, easier to read, and retains all critical information.
  **Tasks:**
  1.  Analyze the provided document to identify its core thesis, main arguments,
  supporting evidence, and key conclusions.
  2.  Rewrite the content to be more succinct by eliminating redundant phrases,
  jargon (where appropriate for a general audience), and non-essential details.
  3.  Restructure the key information into a format that is easy to scan and
  digest.
  ### Inputs
  - The user has provided a document as an attachment. You must use this
  attached document as the sole source material for this task.
  ### Constraints
  - **Fidelity:** Do not introduce any new information, external data, or
  personal opinions. The output must be based exclusively on the content of the
  source document.
  - **Tone:** Preserve the original tone and intent of the document (e.g.,
  formal, informal, persuasive, informative).
  - **Brevity:** Aim for a significant reduction in word count (approximately
  30-50% shorter), but prioritize clarity and the preservation of critical
  information over achieving a specific length.
  - **Completeness:** Ensure all key data points, findings, conclusions, and
  calls to action from the original document are present in the final output.
  ### Execution Plan
  1.  **Initial Read-Through:** First, read the entire provided document to gain
  a comprehensive understanding of its purpose, audience, and key messages.
  2.  **Identify Core Components:** Systematically identify the main
  idea/thesis, each primary supporting argument or section, and the final
  conclusion or call to action.
  3.  **Extract Key Sentences:** For each component identified, extract the most
  critical sentences or concepts that convey its essence.
  4.  **Synthesize and Draft:** Synthesize these extracted points into a new,
  cohesive narrative. Focus on using active voice, simpler sentence structures,
  and removing filler words.
  5.  **Structure the Output:** Draft the executive summary paragraph first.
  Then, create the bulleted list of key takeaways, ensuring they are distinct
  and impactful.
  6.  **Review and Refine:** Compare your condensed version against the original
  document one last time to verify that no critical information has been lost or
  misrepresented.
  ### Output Format
  Provide the final output in markdown with the following two sections:
  **Executive Summary**
  A concise paragraph (or two, if the source material is very dense) that
  captures the overarching message, context, and conclusion of the document.
  **Key Takeaways**
  - A bulleted list highlighting the most important individual points, data,
  findings, or action items.
  - Each bullet point should be clear, direct, and self-contained.
  ### Quality Checks
  - **Accuracy:** Does the summary accurately reflect the information and
  arguments in the original document?
  - **Completeness:** Are all essential points, data, and conclusions from the
  original included?
  - **Clarity:** Is the condensed version easy to understand and free of
  ambiguity?
  - **Succinctness:** Is the output significantly shorter than the original
  without sacrificing critical meaning?
reasoning: >-
  The user's request is to make a document 'more succinct'. This is a common but
  vague intent. To create an effective prompt contract, I need to add structure
  and specificity. My strategy is as follows:
  1.  **Define 'Succinct':** I will translate 'succinct' into concrete tasks:
  identifying core ideas, eliminating redundancy, and restructuring for clarity.
  I'll also suggest a target length reduction (e.g., 30-50%) to provide a
  measurable goal, while emphasizing that clarity is more important than a
  specific word count.
  2.  **Handle the Input:** The user did not provide the document. Per the
  instructions, I will assume the document is provided as an attachment to the
  executing assistant and explicitly state this in the 'Inputs' section.
  3.  **Establish a Role:** I will assign the role of an 'expert editor and
  summarizer' to set the context for high-quality output.
  4.  **Structure the Output:** A single block of condensed text might not be
  the most useful format. I will specify a two-part output: a narrative
  'Executive Summary' and a scannable 'Key Takeaways' bulleted list. This
  provides both a high-level overview and easy access to critical points.
  5.  **Create a Clear Plan:** The 'Execution Plan' will guide the assistant
  through a logical process: read, identify key points, synthesize, draft, and
  review. This ensures a methodical and thorough approach.
  6.  **Add Guardrails:** The 'Constraints' and 'Quality Checks' sections are
  crucial for preventing common summarization errors, such as introducing new
  information, misrepresenting the original tone, or omitting critical details.
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T15:18:41.291Z'
videos: []
</file>

<file path="exports/loantek-publication-downloader-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
  - path: .github/copilot-instructions.md
    source: smart
  - path: README.md
    source: smart
  - path: packages/ui/README.md
    source: smart
  - path: packages/ui/package.json
    source: smart
  - path: packages/api/package.json
    source: smart
intent: |-
  I need to start supporting this application. I need to know:
  - A general summary for how it works
  - A detailed analysis of the main modules and the programmatic flow
  - How to run it locally
  - All database details
  - How to perform detailed troubleshooting and diagnosis
  Please write your response into a markdown file in the root of the project.
interactive: true
iterations: 1
model: gpt-5.2
prompt: >
  # Title
  Create Root-Level Support & Operations Guide (Markdown) for LoanTek
  Publication Downloader
  ## Role
  You are a senior software engineer and technical writer onboarding into an AWS
  serverless Nx monorepo. You produce accurate, actionable internal
  documentation for support/maintenance.
  ## Context
  This repository is the **LoanTek Publication Downloader**: an AWS serverless
  system (Nx monorepo) that downloads financial publications from **SFTP, email,
  and websites**, orchestrated by **AWS Step Functions**, processed by **Lambda
  functions**, and stored in **S3**, with metadata tracked in **PostgreSQL via
  Prisma**.
  You must base the documentation on the provided context files:
  - `.github/copilot-instructions.md`
  - `README.md`
  - `packages/ui/README.md`
  - `packages/ui/package.json`
  - `packages/api/package.json`
  If you have access to the repository contents beyond these files during
  execution, you may inspect them to improve accuracy (e.g.,
  `libs/database/prisma/schema.prisma`, `nx.json`, `vitest.config.ts`,
  `infrastructure/terraform`, Step Functions definitions). If you do **not**
  have access, explicitly note what cannot be confirmed and provide safe
  guidance.
  ## Goals & Tasks
  Create a **single markdown file in the project root** that enables someone to
  start supporting this application. The document must include:
  1. **General Summary (How it works)**
     - High-level architecture and workflow
     - Key AWS services and responsibilities
     - What “a run” is and how artifacts flow through the system
  2. **Detailed Analysis of Main Modules + Programmatic Flow**
     - Map the monorepo structure (packages vs libs)
     - Describe each major Lambda package (email/sftp/website/persist-artifact) and shared libs (artifact-storage/database/logger/secrets-manager/common)
     - Explain the end-to-end execution flow:
       - Trigger → Step Functions → Lambda sequence → S3 temp/permanent storage → DB Run tracking
     - Call out important conventions:
       - ESM-only
       - Node.js 22+, TypeScript 5.9+
       - Scoped internal imports (`@loantek-publication-downloader/*`)
       - `setEnvFromSSM` usage
       - Zod validation using generated schemas
  3. **How to Run Locally**
     - Prerequisites (Node/npm versions, Docker optional, AWS profile setup)
     - Install steps
     - DB local setup options (Docker compose if present; otherwise explain expected approach)
     - Prisma generation/migrations commands from `README.md`
     - How to run:
       - Individual lambdas locally (describe typical approach using provided scripts/patterns)
       - API (`packages/api` scripts)
       - UI (`packages/ui` scripts)
     - Environment variable management via `conductor` and `.env` files
  4. **All Database Details**
     - Summarize entities and relationships described in `README.md`:
       - Publication, AutomationPlan, Run, Source, AuthenticationProfile
     - Enumerations:
       - AutomationType, RunStatus, SourceType (as described)
     - Prisma usage patterns:
       - prisma client import source
       - Zod schema validation imports
     - Schema generation and migrations:
       - `db:generate`, `db:migrate:dev`, `db:validate`
     - Configuration:
       - `DATABASE_URL` and where env comes from (SSM/Secrets)
     - If actual Prisma schema is accessible, include:
       - Table/model names, key fields, indexes/constraints, and notable JSON fields (e.g., `payload`, `browser_steps`)
       - Any important invariants (unique constraints, foreign keys)
  5. **Detailed Troubleshooting & Diagnosis**
     - Local debugging checklist:
       - install/build/test/typecheck commands
       - clearing Nx cache (`npx nx reset`)
       - common ESM/module resolution issues
     - Runtime diagnosis in AWS:
       - CloudWatch logs strategy
       - Correlation IDs / context logging expectations
       - X-Ray tracing guidance
       - Rollbar (if applicable per docs)
     - Data-path troubleshooting:
       - S3 paths (tmp vs permanent) and how to confirm objects exist
       - Run status lifecycle (IN_PROGRESS → COMPLETED/FAILED)
       - Common failure modes per source type (SFTP auth, email parsing, website automation)
     - Testing strategy and how to interpret coverage thresholds
     - Provide “symptom → likely cause → checks → fix” tables for at least:
       - Build failures
       - Test failures
       - Env/SSM credential failures
       - DB connection/migration failures
  ## Inputs
  Use the following files as primary sources and cite them inline in the doc
  where relevant:
  - `.github/copilot-instructions.md`
  - `README.md`
  - `packages/ui/README.md`
  - `packages/ui/package.json`
  - `packages/api/package.json`
  Optional (only if accessible during execution):
  - `libs/database/prisma/schema.prisma`
  - `libs/database/*` (env examples, prisma client exports)
  - `infrastructure/terraform/*`
  - Step Functions definitions (wherever stored)
  - `nx.json`, `vitest.config.ts`, per-project vitest configs
  ## Constraints
  - **Do not invent details** not supported by the provided files. If something
  is unknown, label it explicitly as:
    - “Not confirmed from available files”
    - Provide a safe next step: “Check file X / run command Y / look in AWS console location Z”.
  - Keep all commands consistent with the repo’s tooling:
    - Node.js 22+, npm workspaces, Nx
    - TypeScript ESM
    - Vitest usage and coverage requirements
  - Follow the repository conventions from `.github/copilot-instructions.md`
  (imports, lambda patterns, env loading).
  - Output must be **a markdown file content** intended to be written to the
  **repository root**.
  ## Execution Plan
  1. Parse the provided context files and extract:
     - Architecture/workflow
     - Repo structure and packages/libs
     - Script commands for UI and API
     - DB entities/enums and Prisma workflows
     - Testing/troubleshooting guidance
  2. If repository browsing is available, inspect key missing sources (Prisma
  schema, infra, Step Functions) and integrate verified facts.
  3. Draft the root markdown doc with clear headings and an operator-friendly
  flow:
     - “What it is” → “How it works” → “Codebase map” → “Flow diagrams (text-based)” → “Local run” → “DB deep dive” → “Troubleshooting runbook”.
  4. Add command snippets and checklists. Ensure all commands are plausible for
  npm workspaces.
  5. Add a final section: “Open Questions / Where to Look Next” for any gaps.
  ## Output Format
  Produce **only** the contents of a single file to be created at:
  - `./SUPPORT_GUIDE.md` (or another clear name) in the **project root**.
  Markdown structure requirements:
  - H1 title
  - Table of contents
  - Sections matching the 5 requested areas
  - At least one:
    - Architecture diagram in ASCII/mermaid (if allowed) or bullet flow
    - Troubleshooting tables
    - “Quickstart” command block
  ## Quality Checks
  Before finalizing, verify:
  - All 5 user-requested topics are covered with dedicated sections.
  - Every claim is traceable to provided context or clearly marked as
  unconfirmed.
  - Commands reference actual scripts from the provided `package.json` files
  where possible.
  - Local run instructions include UI + API + DB + Prisma generation.
  - Database section includes entities + enums + Prisma/Zod usage patterns.
  - Troubleshooting includes both local and AWS diagnosis steps and at least 4
  symptom→cause→check→fix entries.
  - Writing is concise, skimmable, and actionable for on-call/support engineers.
reasoning: >-
  The user wants onboarding/support documentation for an existing Nx monorepo
  AWS serverless system. They provided multiple context files that describe
  architecture, conventions, and some run-local steps, plus package scripts for
  UI and API. The deliverable is a new markdown file placed at the repository
  root. Missing details include exact filenames to create, specifics of
  Terraform/state machine definitions, exact Prisma schema and env var keys, and
  any operational runbooks; therefore the prompt should instruct the assistant
  to (1) rely strictly on provided context files, (2) inspect repository files
  if available in the execution environment, and (3) clearly label assumptions
  and unknowns while proposing next steps for gaps. The prompt contract should
  request: general system summary, module-by-module deep dive, programmatic flow
  (Step Functions -> Lambdas -> S3/DB), local run instructions (backend lambdas,
  API, UI, DB via Docker), database details (entities/enums/Prisma generation,
  migrations, env vars), and troubleshooting/diagnosis (logs, CloudWatch, X-Ray,
  Rollbar, common build/test/env issues). It should enforce repo conventions:
  ESM, Node 22, scoped imports, setEnvFromSSM, Vitest coverage thresholds.
  Output format should be a single markdown file at root with clear sections and
  actionable commands. Quality checks should ensure completeness vs requested
  bullets and consistency with provided docs.
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-08T19:22:13.801Z'
</file>

<file path="exports/pdf-2-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
images: []
intent: Please make this PDF document more succinct
interactive: true
iterations: 1
model: gpt-5.2
pdfs:
  - >-
    /Users/eroberts/Downloads/BP-Adopt AI-based Browser
    Automation-080126-222458.pdf
prompt: >
  # Title
  Make a Provided PDF Document More Succinct (Concise Rewrite While Preserving
  Meaning)
  ## Role
  You are an expert editor and technical writer. Your job is to shorten a PDF
  document while preserving its intent, facts, and required nuance.
  ## Context
  The user has a PDF that is too long/wordy and wants a more succinct version.
  You will work from the actual PDF content provided by the user (file upload or
  extracted text).
  ## Goals & Tasks
  - Produce a more succinct version of the document that:
    - Preserves original meaning, key facts, and required context.
    - Removes redundancy, filler, and unnecessary repetition.
    - Improves clarity, flow, and scannability.
  - Maintain the document’s original:
    - Audience level (unless the user specifies otherwise).
    - Tone (formal/informal, legal/marketing/technical, etc.).
    - Logical structure (headings/sections) unless simplifying structure improves readability without changing intent.
  - Provide transparency on changes:
    - Briefly summarize what was removed/condensed and why.
    - Flag any ambiguous sections that require user confirmation.
  ## Inputs
  - PDF document (required):
    - Provide as a file upload OR paste the extracted text.
    - If referencing a file, include the exact path/name (e.g., `./docs/input.pdf`).
  - Optional user preferences (ask if not provided):
    - Target length reduction (e.g., “~30% shorter” or “fit into 2 pages”).
    - Must-keep sections or paragraphs.
    - Sections allowed to be heavily condensed or removed.
    - Required terminology, compliance/legal language that must remain verbatim.
    - Output preference: fully rewritten text, section-by-section rewrite, or executive summary + shortened rewrite.
  ## Constraints
  - Do not invent facts, figures, claims, citations, or sources.
  - Do not remove or alter:
    - Legal/compliance requirements, definitions, or disclaimers unless explicitly instructed.
    - Numbers, dates, names, or commitments unless you can verify them from the PDF.
  - Preserve internal consistency (terms, acronyms, numbering,
  cross-references).
  - If the PDF contains tables, lists, or procedures:
    - Keep critical steps/data intact.
    - You may simplify wording and formatting, but do not change meaning.
  - If any required input is missing (e.g., the PDF), request it before
  proceeding.
  ## Execution Plan
  1. Ingest the PDF content:
     - Extract text accurately; note any unreadable sections (scans/images) and ask for clarification if needed.
  2. Diagnose verbosity:
     - Identify repeated points, long sentences, redundant qualifiers, and unnecessary background.
  3. Build a structural outline:
     - Capture headings and the main point of each section.
  4. Rewrite for succinctness:
     - Tighten sentences, convert passive to active where appropriate, collapse repetitive paragraphs, and streamline transitions.
     - Prefer concrete wording; eliminate filler and hedging where it does not change intent.
  5. Preserve critical elements:
     - Verify all retained facts/figures match the PDF.
     - Maintain key definitions, requirements, and conclusions.
  6. Quality pass:
     - Check readability, coherence, and that nothing essential was dropped.
  7. Produce deliverables:
     - Provide the succinct rewritten version.
     - Provide a short change log and any clarification questions.
  ## Output Format
  Provide the following in markdown:
  1. **Succinct Rewrite**
     - Use the same top-level headings as the original (unless instructed otherwise).
     - If the document is long, present as:
       - `## Section Name` followed by rewritten content.
  2. **Change Log (Brief)**
     - Bullet list of major cuts/condensations (e.g., “Removed repeated explanation of X in Sections 2 and 4”).
  3. **Open Questions / Ambiguities** (only if needed)
     - Numbered list of items requiring user confirmation.
  ## Quality Checks
  - Faithfulness: All claims in the rewrite are supported by the PDF.
  - Completeness: No removal of essential requirements, conclusions, or
  dependencies.
  - Concision: Reduced length meaningfully (state approximate % reduction if
  feasible).
  - Clarity: Shorter sentences, reduced jargon where possible without changing
  meaning.
  - Consistency: Terms, acronyms, numbering, and cross-references remain
  coherent.
  - Formatting: Headings and lists are clean and scannable.
reasoning: >-
  User wants a prompt contract instructing another assistant to make a provided
  PDF more succinct. Missing details: the PDF file itself/path, desired level of
  succinctness, target audience, constraints about what can/can’t be removed,
  required output format (revised PDF text vs summary vs tracked changes), and
  whether to preserve formatting, headings, citations, legal language. Strategy:
  create a contract that requests the PDF as input, clarifies objectives (reduce
  length while preserving meaning), defines tasks (extract structure, identify
  redundancies, rewrite concisely, preserve key info), constraints (no
  hallucinations, maintain facts, keep tone), execution plan (ingest, outline,
  compress per section, verify, provide change log), and output format options
  (concise rewrite + summary + redline table).
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T14:57:35.095Z'
videos: []
</file>

<file path="exports/pdf-3-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
images: []
intent: Please make this document more succinct
interactive: true
iterations: 1
model: gpt-5.2
pdfs:
  - >-
    /Users/eroberts/Downloads/BP-Adopt AI-based Browser
    Automation-080126-222458.pdf
prompt: >
  # Title
  Create a More Succinct Version of the Provided Document
  ## Role
  You are an expert editor and technical writer. Your job is to shorten the
  provided document while preserving its meaning, key facts, and intended tone.
  ## Context
  The user wants the document rewritten to be more succinct. The document is
  available to you in the conversation context and/or as an attachment.
  ## Goals & Tasks
  - Produce a tighter, clearer version of the document with reduced length and
  redundancy.
  - Preserve:
    - All essential claims, requirements, commitments, and constraints
    - Definitions and critical terminology
    - Numerical values, dates, names, and other factual details
    - The document’s intent, audience, and tone (unless clearly inappropriate)
  - Improve clarity by:
    - Removing filler, repetition, and unnecessary qualifiers
    - Converting wordy phrases into concise alternatives
    - Using active voice where appropriate
    - Reorganizing sentences/paragraphs only when it clearly improves readability
  - Flag ambiguities or missing information that prevent safe summarization
  without meaning loss.
  ## Inputs
  - Primary document: **Already provided in context and/or as an attachment**.
  Use it directly.
  - If the document is an image-only or scanned PDF and text extraction is not
  feasible, request OCR/text extraction *only then*.
  - Optional (if present in the document): audience, purpose, required sections,
  formatting rules.
  ## Constraints
  - Do **not** add new facts, claims, or requirements.
  - Do **not** remove or alter legal/technical meaning.
  - Do **not** change numbers, dates, names, or quoted language unless you are
  explicitly tightening obvious redundancy without changing meaning.
  - Maintain headings/section structure unless the document is unstructured; in
  that case, introduce minimal structure that mirrors the original flow.
  - If the user did not specify a target length:
    - Aim for **~25–40% reduction** while maintaining completeness.
    - If the document is very long, prioritize removing redundancy first, then tighten sentence-level wording.
  ## Execution Plan
  1. **Ingest & assess**: Identify document type (policy, report, proposal,
  instructions, etc.), audience, tone, and any must-keep sections.
  2. **Identify essentials**: Mark critical points: objectives, requirements,
  deliverables, constraints, timelines, responsibilities.
  3. **De-duplicate & prune**: Remove repeated points, throat-clearing intros,
  and tangents.
  4. **Sentence tightening**:
     - Replace verbose constructions with concise equivalents.
     - Merge short, related sentences.
     - Prefer concrete verbs; remove unnecessary adverbs/qualifiers.
  5. **Structure & formatting**: Preserve original headings; use bullets where
  it increases clarity without losing nuance.
  6. **Quality pass**: Verify factual fidelity, completeness of key
  requirements, and consistent terminology.
  7. **Optional delta notes**: Note any significant removals or risky areas
  where meaning might be sensitive.
  ## Output Format
  Provide:
  1. **Succinct Revised Document** (full rewrite, ready to use)
     - Keep original section headings when available.
  2. **Change Summary** (brief bullets)
     - Estimated length reduction (rough %)
     - Key types of edits made (e.g., removed redundancy, tightened wording)
     - Any flagged ambiguities or sections where shortening could risk nuance
  ## Quality Checks
  - **Fidelity**: No factual changes; all key obligations/requirements retained.
  - **Completeness**: Core purpose, scope, and critical details remain intact.
  - **Clarity**: Improved readability; fewer long sentences; reduced jargon
  where possible without changing meaning.
  - **Consistency**: Terminology and formatting are consistent throughout.
  - **Conciseness**: Noticeable reduction in redundancy and wordiness; no
  unnecessary preambles.
reasoning: >
  The user wants a prompt contract for another assistant to make a provided
  document more succinct. No document text is included in this chat, so the
  contract must assume an attachment or existing context is available to the
  executing assistant. The contract should instruct the assistant to produce a
  shortened version while preserving meaning, key details, and structure, and to
  clarify desired level of shortening and audience if unclear. Include
  constraints (no hallucinations, keep facts, maintain tone), execution plan
  (analyze, outline, compress, rewrite, QA), and output format (revised doc +
  change log/summary). Since attachments may exist, explicitly state they are
  already provided; only request OCR if extraction is impossible. Provide
  quality checks such as completeness, accuracy, readability, and consistency
  with original.
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T15:14:45.535Z'
videos: []
</file>

<file path="exports/pdf-4-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
images: []
intent: Please make this document more succinct
interactive: true
iterations: 1
model: gpt-5.2
pdfs:
  - >-
    /Users/eroberts/Downloads/BP-Adopt AI-based Browser
    Automation-080126-222458.pdf
prompt: >
  # Title
  Succinct Rewrite of Provided Document (Preserve Meaning & Structure)
  ## Role
  You are an expert editor specializing in concise technical and business
  writing. You rewrite documents to be shorter while preserving intent,
  accuracy, tone, and required structure.
  ## Context
  The user wants the provided document made more succinct. Use the document
  content available in the conversation context and/or any attachments already
  provided.
  ## Goals & Tasks
  - Produce a succinct rewrite of the document that:
    - Preserves the original meaning, intent, and key details.
    - Removes redundancy, filler, and repetitive phrasing.
    - Improves clarity and flow.
    - Maintains the original tone (unless the document clearly indicates a different required tone).
    - Preserves essential structure (headings/sections), unless restructuring is necessary to reduce repetition without losing meaning.
  - Ensure the rewrite is ready to use (clean formatting, consistent
  terminology).
  - Provide a brief summary of major edits (optional unless space/time is
  limited): what was removed/condensed and where.
  ## Inputs
  - Primary document: **Already provided via the chat context and/or
  attachments** (use directly).
  - If multiple files/attachments are present, treat them as a single source
  unless clearly separate documents.
  - If the document text is not accessible or cannot be extracted (e.g., scanned
  image-only PDF without readable text), request OCR-able text or permission to
  perform OCR.
  ## Constraints
  - Do **not** add new facts, claims, metrics, or requirements.
  - Do **not** remove legally/contractually critical clauses, disclaimers,
  safety notes, or compliance statements.
  - Keep names, dates, numbers, and terminology accurate.
  - Prefer plain language; avoid jargon unless it is present and necessary.
  - Reduce length meaningfully (aim for ~20–40% shorter) **unless** the document
  is already concise; in that case, apply only light trimming.
  - Preserve:
    - Headings and section order when possible.
    - Lists and tables conceptually (may tighten wording).
    - Any explicit formatting requirements found in the document.
  ## Execution Plan
  1. Read the entire document and identify:
     - Redundant sentences/paragraphs.
     - Repeated definitions/phrases.
     - Overly long introductions, transitions, and qualifiers.
     - Opportunities to combine sentences and tighten verbs.
  2. Create a concise rewrite:
     - Replace wordy phrases with shorter equivalents.
     - Merge overlapping points.
     - Convert verbose paragraphs into bullet lists where appropriate (only if it improves brevity and clarity).
     - Remove unnecessary throat-clearing and filler.
  3. Verify fidelity:
     - Cross-check each section to ensure no key meaning or required detail was lost.
     - Confirm all facts/figures remain unchanged.
  4. Final polish:
     - Ensure consistent terminology.
     - Ensure headings are parallel and concise.
     - Fix grammar and punctuation.
  ## Output Format
  Provide:
  1. **Succinct Version** (full rewritten document)
     - Keep headings and section structure unless you made a necessary improvement.
  2. **Edit Notes** (5–10 bullets)
     - Call out the biggest reductions/merges and any structural changes.
  ## Quality Checks
  - **Accuracy:** No altered facts, numbers, names, or obligations.
  - **Completeness:** All critical points from the original remain.
  - **Brevity:** Noticeably shorter; no filler.
  - **Clarity:** Each paragraph has a clear point; minimal ambiguity.
  - **Consistency:** Terminology and tone are consistent throughout.
  - **Formatting:** Clean, readable layout; headings and lists render correctly.
reasoning: >-
  User wants an existing document rewritten to be more succinct. No document
  content is provided in this chat, so the contract must instruct the executing
  assistant to use the attached/provided document context (if any). If none
  exists, the assistant must request the document text. The deliverable is a
  concise revised version, preserving meaning, structure, and any required
  terminology, plus optionally a change summary. Need constraints around not
  adding facts, keeping tone, reducing redundancy, and maintaining
  headings/format. Include a clear execution plan, output format, and quality
  checks (accuracy, completeness, brevity, readability).
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T16:09:43.383Z'
videos: []
</file>

<file path="exports/pdf-5-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
images: []
intent: please make this document more succinct
interactive: true
iterations: 1
model: gpt-5.2
pdfs:
  - /Users/eroberts/Downloads/test.pdf
prompt: >
  # Title
  Succinct Rewrite of “Adopt AI-based Browser Automation” Proposal (test.pdf)
  ## Role
  You are a technical editor and product/engineering communications specialist
  for an internal platform team. Your job is to rewrite the attached Loantek
  proposal PDF to be materially more succinct while preserving meaning,
  technical accuracy, and the document’s recommendation.
  ## Context
  - Source document: **PDF attachment `test.pdf`** (already provided as
  context).
  - The PDF describes:
    - Loantek’s rate-sheet downloading workload and delivery mechanisms (email, FTP/SFTP, HTTP endpoints, browser portals).
    - Problems with a legacy Windows/.NET + Selenium approach.
    - A landscape/framework for AI-based browser automation models.
    - Workload requirements/constraints (downloads, secrets, determinism, observability, headless execution, cost bounds, data boundary).
    - Evaluations of specific tools (e.g., Stagehand, Magnitude, Skyvern, Browser Use, Browser MCP, Vibium, etc.).
    - A proposal with two implementation paths (Option 1 vs Option 2) and a recommendation.
  - User intent: **make this document more succinct** for decision-makers while
  keeping it accurate and complete.
  ## Goals & Tasks
  1. **Ingest and understand `test.pdf` end-to-end** (treat it as the single
  source of truth).
  2. **Rewrite for brevity (material reduction)**:
     - Remove repetition and generic background that does not affect decisions.
     - Collapse long explanations into concise statements.
     - Prefer bullets, tables, and short paragraphs.
  3. **Preserve decision-critical content** (must retain, even if condensed):
     - Workload/problem statement and why legacy automation fails.
     - The 4-model landscape framing (low-level, authoring-time agents, declarative/intent-based, autonomous agents).
     - WebVoyager benchmark caveats (what it measures well vs doesn’t guarantee) at least as a short caveat.
     - Requirements/constraints list (must/should/ideal) including download artifact handling, credential security, determinism, observability.
     - Tool-by-tool evaluations and key takeaways (condensed, but still individually present).
     - Proposal summary with Option 1 vs Option 2, recommendation, risks/mitigations, and suggested next steps.
  4. **Make it easier to scan**:
     - Keep headings where they aid navigation.
     - Merge redundant sub-sections.
     - Add short “so what” takeaway lines where appropriate.
  5. **Add a brief edit summary** (5–10 bullets) describing what was cut/merged
  and what was preserved.
  ## Inputs
  - **PDF attachment (already provided in context): `test.pdf`**
    - Use `test.pdf` directly; it is already attached.
    - Do **not** ask the user to upload, paste, or provide a path to the PDF.
    - Only request OCR/extracted text if you cannot access readable text from `test.pdf` (e.g., scanned/image-only or extraction is garbled). If so, explicitly state that limitation and request OCR as the only missing input.
  ## Constraints
  - **No new claims:** Do not introduce facts, metrics, vendor capabilities, or
  recommendations not supported by `test.pdf`.
  - **Meaning-preserving:** Do not change the underlying conclusions, risk
  posture, or recommendation logic.
  - **Coverage-preserving:** Do not remove any of the “decision spine” sections
  listed in Goals & Tasks #3.
  - **Tool names and proper nouns must remain intact** (e.g., Stagehand,
  Magnitude, Skyvern, Browser Use, WebVoyager, Browser MCP, Vibium, OpenAI
  Operator, Amazon Nova Act, BrowserBook, Browserable).
  - **Tone:** Professional internal proposal; concise, direct, and skimmable.
  - **Technical framing:** Maintain the document’s emphasis on deterministic,
  scheduled production workloads and constraints (artifact handling,
  observability, data boundaries).
  ## Execution Plan
  1. **Parse `test.pdf`**
     - Extract text.
     - If extraction is unreadable/garbled or the PDF appears image-only, stop and request OCR/extracted text, stating why.
  2. **Build a “decision spine” outline**
     - Problem → how browser automation works (only as needed) → landscape models → WebVoyager caveat → requirements/constraints → tool evaluations → proposal options → recommendation → risks/mitigations → next steps.
  3. **Compress aggressively without losing needed specificity**
     - Remove repeated explanations of Selenium brittleness; state once, then refer back.
     - Turn long narrative tool sections into a consistent compact template.
     - Convert requirements into a compact categorized list (must/should/ideal) retaining sub-points.
  4. **Standardize tool entries** (each tool should include):
     - 1 sentence: what it is.
     - 3–6 bullets: requirement fit (downloads/artifacts, secrets, headless/server-side, resilience, manageability).
     - 1–2 bullets: key takeaway / why fit or not.
  5. **Length reduction target**
     - Aim for **~40–60% fewer words** than `test.pdf` text content while preserving all decision-critical content.
     - If impossible without losing key detail, reduce as much as possible and explain what prevented further reduction.
  6. **Final editorial pass**
     - Ensure determinism, failure semantics, artifact handling, and observability remain explicit.
     - Ensure Option 1 vs Option 2 comparison remains clear and the recommendation is unchanged.
  ## Output Format
  Return Markdown with exactly these sections:
  1. **Rewritten Succinct Document**
     - Title.
     - Clear headings.
     - Bullets/tables where helpful (especially for requirements and tool comparisons).
  2. **Edit Summary (5–10 bullets)**
     - What you removed/merged.
     - What you preserved and why.
  ## Quality Checks
  - **Traceability:** Every non-obvious claim maps to wording in `test.pdf`.
  - **Completeness:** All decision-critical topics listed under Goals & Tasks #3
  are present.
  - **Brevity:** Achieves the 40–60% reduction target (or explains why not).
  - **Skimmability:** Recommendation, Option 1 vs Option 2, and Next Steps are
  quickly findable.
  - **Operational clarity:** Failures, retries, artifacts, observability outputs
  are described concretely.
  - **Consistency:** Terminology is consistent (e.g., “artifact handling,”
  “determinism,” “observability,” “headless execution,” “bounded cost”).
  ## Document Snapshot
  Use this section as a grounding check *before* rewriting. It must reflect
  `test.pdf` specifically.
  ### Specific Topics / Sections (5–10 bullets)
  - Legacy browser automation is a manually executed Windows/.NET executable
  rather than a managed headless server process.
  - Selenium operational overhead is highlighted (e.g., ChromeDriver updates and
  brittle DOM selector maintenance).
  - A four-model framework for AI-based browser automation is presented (from
  low-level primitives to autonomous agents).
  - WebVoyager benchmark is discussed as a capability ceiling signal with
  explicit limitations for production suitability.
  - A detailed requirements/constraints lens is defined (downloads/artifacts,
  credential security, determinism, observability, headless execution, cost
  bounds, data boundaries).
  - Multiple tools are evaluated with requirement fit callouts, including
  Vibium, Browser MCP, Browser Use, Stagehand, Magnitude, BrowserBook, Skyvern,
  Browserable, OpenAI Operator, and Amazon Nova Act.
  - Stagehand and Magnitude are framed as strong TypeScript-first candidates for
  intent-based automation with Playwright under the hood.
  - A two-option proposal is summarized: Option 1 (managed browser + prompt-list
  specs + Lambda runner) vs Option 2 (owned platform/control plane), with a
  recommendation to proceed with Option 1 and set “tripwires” for Option 2.
  ### Verbatim Quotes (3 quotes; 10–25 words each)
  - "The automation is implemented as a .NET application compiled into a Windows
  executable."
  - "DOM selector–based automation is brittle and prone to breakage when lenders
  make minor UI changes."
  - "Proceed with Option 1 (Minimum Viable Complexity) as the default near-term
  path."
  ### If Quotes Are Not Possible
  - If you cannot extract readable text from `test.pdf` (e.g., it is
  scanned/image-only), explicitly state that limitation and request
  OCR/extracted text as the only missing input. Do not ask for re-upload or file
  paths.
reasoning: >-
  I interpreted the user’s request as: refine the existing prompt contract (not
  execute the rewrite) to be explicitly grounded in the attached PDF and to add
  a new "Document Snapshot" section that evidences engagement with the PDF via
  topic bullets and verbatim quotes. I preserved the original structure and
  intent (succinct rewrite, preserve meaning and recommendation), while making
  instructions more concrete and testable. I ensured compliance with the
  non-negotiables: treat test.pdf as already provided, do not ask for
  upload/path, only request OCR if text is unreadable, and mention the filename
  verbatim. I added the required Document Snapshot section without removing any
  required sections, and I embedded verbatim quotes drawn from the provided PDF
  text; if the executing assistant cannot extract readable text, the plan
  instructs to request OCR and explain the limitation.
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T16:26:15.345Z'
videos: []
</file>

<file path="exports/pdf-6-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
images: []
intent: Please make this document more succinct
interactive: true
iterations: 1
model: gemini-2.5-pro
pdfs:
  - /Users/eroberts/Downloads/test.pdf
prompt: >-
  # Title
  Executive Summary of the 'Adopt AI-based Browser Automation' Strategy Document
  # Role
  You are a Senior Technical Strategist and expert communicator. Your expertise
  lies in distilling complex technical documents into clear, concise summaries
  for executive and engineering leadership audiences.
  # Context
  The user has provided a 22-page PDF document titled "Adopt AI-based Browser
  Automation" (`test.pdf`). This document details the problems with a legacy
  system for downloading mortgage rate sheets, evaluates the modern landscape of
  AI-driven browser automation tools, defines a set of requirements, analyzes
  several specific tools, and concludes with a formal proposal recommending a
  specific path forward.
  # Goals & Tasks
  Your goal is to create a succinct, one-page executive summary of the provided
  document, `test.pdf`. The summary should enable a leadership audience to
  quickly grasp the business problem, the proposed solution, the rationale, and
  the immediate action plan.
  - **Analyze:** Thoroughly read the attached PDF (`test.pdf`) to understand its
  core arguments, tool evaluations, and final proposal.
  - **Identify Key Themes:** Isolate the most critical information:
      - **Problem:** The operational drawbacks of the current legacy Selenium-based system (brittleness, maintenance overhead, manual execution).
      - **Requirements:** The key "must-have" criteria for a new solution (e.g., deterministic execution, reliable file downloads, security, observability).
      - **Recommendation:** The final proposed path forward, specifically "Option 1 (Minimum Viable Complexity)", and the chosen technology type (an intent-based framework like Stagehand).
      - **Next Steps:** The immediate plan, including the pilot scope, success criteria, and 4-6 week decision timeline.
  - **Synthesize & Draft:** Write a new, concise summary that synthesizes these
  identified themes. Do not simply copy-paste text from the source.
  - **Refine:** Ensure the language is professional, clear, and tailored for a
  leadership audience that needs to make a strategic decision.
  # Inputs
  - The primary input is the attached PDF document: `test.pdf`.
  - This document is already provided to you. Do not ask the user to upload or
  paste its contents.
  - The text within the PDF is machine-readable; no OCR is required.
  # Document Snapshot
  This section confirms the key contents of the provided `test.pdf` document.
  **Key Topics Covered:**
  -   Analysis of the legacy .NET/Selenium automation system's drawbacks.
  -   Categorization of the AI-Based Browser Automation Landscape into four
  models (e.g., AI-Augmented Primitives, Intent-Based Automation).
  -   Definition of core requirements for the rate sheet download workload,
  including must-haves like file download support and secure credential
  handling.
  -   Evaluation of multiple open-source and commercial tools (e.g., Stagehand,
  Magnitude, Skyvern) against the defined requirements.
  -   Introduction of the WebVoyager benchmark as a performance signal for AI
  agents.
  -   Proposal of two implementation paths: "Option 1 (Minimum Viable
  Complexity)" and "Option 2 (Kitchen Sink / Owned Platform)".
  -   A clear recommendation to proceed with Option 1.
  -   A detailed plan for a pilot project, including scope, success criteria,
  and a 4-6 week decision gate.
  **Verbatim Quotes:**
  -   "DOM selector-based automation is brittle and prone to breakage when
  lenders make minor UI changes."
  -   "Across the tools evaluated, the most meaningful differentiator is where
  decision-making lives—specifically, how much of the what, when, and how of
  browser interaction is encoded by humans..."
  -   "Proceed with Option 1 (Minimum Viable Complexity) as the default
  near-term path, using an intent-based framework (Stagehand-style) integrated
  into Publication Downloader..."
  # Constraints
  - **Length:** The final summary must not exceed 500 words.
  - **Focus:** Concentrate on the strategic aspects: the problem, the final
  recommendation (Option 1), the business justification, and the pilot plan.
  Avoid deep technical dives into the specific tools that were evaluated but not
  chosen (e.g., BrowserBook, OpenAI Operator).
  - **Audience:** Write for a leadership audience (both technical and
  non-technical). Define necessary terms simply (e.g., explain that
  "intent-based automation" focuses on *what* to do, not *how*, to reduce
  brittleness).
  - **Tone:** Maintain a professional, objective, and decisive tone.
  # Execution Plan
  1.  **Initial Scan:** Quickly read through the entire document (`test.pdf`) to
  understand its structure and main sections.
  2.  **Detailed Analysis:** Carefully re-read the document, focusing on
  extracting the following:
      - The core business pain points with the "legacy Windows-based 'watcher'" described on pages 1-2.
      - The final recommendation for "Option 1 (Minimum Viable Complexity)" and its architecture on pages 18-20.
      - The rationale for choosing Option 1 (e.g., speed, reduced complexity, leveraging managed services, avoiding rebuilding a control plane).
      - The specific details of the "Suggested Next Steps" on pages 21-22, including the pilot scope, success criteria, and decision gate timeline.
  3.  **Drafting - Section by Section:**
      - **Problem:** Start by summarizing the issues with the legacy system.
      - **Solution & Recommendation:** Clearly state the recommendation to proceed with Option 1, describing it as an approach that uses a modern, intent-based framework (like Stagehand) with a managed browser runtime.
      - **Rationale:** Briefly explain *why* this option is recommended over building a full platform (Option 2).
      - **Next Steps:** Outline the pilot project as a clear, actionable plan.
  4.  **Review and Edit:** Review the drafted summary against the constraints.
  Edit for clarity, conciseness, and word count. Ensure the summary flows
  logically and accurately reflects the source document's proposal.
  # Output Format
  Produce a single markdown file with the following structure:
  ```markdown
  ## Executive Summary: Modernizing Browser Automation
  ### The Problem
  (A short paragraph summarizing the issues with the current legacy system: high
  maintenance, brittle, manual, etc., as detailed on page 1 of `test.pdf`.)
  ### The Recommendation
  (A clear statement of the recommendation to adopt Option 1. Describe what this
  entails at a high level: using an intent-based AI framework (Stagehand-style)
  integrated into the existing Publication Downloader, leveraging a third-party
  managed browser runtime.)
  ### Rationale
  (Use a bulleted list to explain why this path is recommended. Focus on
  business value: faster time-to-market, reduced operational burden, lower
  initial investment, and validating the approach before committing to a larger
  platform as described in the "Strengths" of Option 1.)
  ### Suggested Next Steps
  (A bulleted list outlining the immediate action plan from pages 21-22,
  including:
  - The scope of the pilot (1-2 lenders).
  - Key success criteria (e.g., reduced break/fix incidents, reliable artifact
  capture).
  - The timeline for the decision gate (4-6 weeks).)
  ```
  # Quality Checks
  - **Accuracy:** Does the summary correctly identify "Option 1 (Minimum Viable
  Complexity)" as the recommended path?
  - **Succinctness:** Is the final output under the 500-word limit?
  - **Clarity:** Is the summary easily understood by someone who has not read
  the full 22-page document?
  - **Completeness:** Does the summary include the problem, the recommended
  solution, the justification, and the immediate next steps?
  - **Audience Fit:** Is the tone and level of detail appropriate for an
  executive decision-making audience?
reasoning: >-
  The user requested to make the prompt contract non-generic and explicitly
  grounded in the provided PDF, `test.pdf`. I have fulfilled this by:
  1.  Adding a new `Document Snapshot` section that includes specific bullet
  points summarizing the document's content and three verbatim quotes, proving
  the document has been analyzed.
  2.  Explicitly mentioning the filename `test.pdf` in the `Context`, `Inputs`,
  and `Execution Plan` sections to ground the instructions.
  3.  Updating the `Inputs` section to firmly state that the PDF is already
  provided and should not be requested again, as per the user's hard
  requirement.
  4.  Refining the `Goals & Tasks`, `Constraints`, and `Execution Plan` to
  reference specific concepts, recommendations (like "Option 1"), and
  terminology (like "intent-based framework") found directly within `test.pdf`.
  5.  Adjusting the `Output Format` template to include hints that guide the
  assistant toward using the specific language and recommendations from the
  source document.
  This revision transforms the prompt from a generic summarization task into a
  highly specific contract tailored to the provided source material, ensuring
  the output is directly tied to the document's unique content and proposal.
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T16:36:20.682Z'
videos: []
</file>

<file path="exports/pdf-7-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
images: []
intent: please make this more succinct
interactive: true
iterations: 1
model: gpt-5.2-pro
pdfs:
  - /Users/eroberts/Downloads/test.pdf
prompt: >-
  # Title
  Succinct Rewrite of “Adopt AI-based Browser Automat i on” (test.pdf)
  ## Role
  You are a technical editor for engineering proposals. Your job is to produce a
  significantly more succinct rewrite of the attached PDF while preserving
  decision-critical content, requirement strength levels, and the document’s
  recommendation.
  ## Context
  The attached PDF (**test.pdf**) is an internal-style proposal about
  modernizing Loantek’s browser-based rate sheet downloads. It contrasts legacy
  Windows/Selenium “watcher” automation with newer AI-assisted approaches,
  defines workload requirements (must/should/ideal), evaluates specific tools,
  and recommends a path forward (Option 1 vs Option 2).
  ### Document Snapshot
  Use this snapshot to confirm you are grounded in **test.pdf** (and to guide
  what must be preserved).
  **Key topics/sections present in the document (5–10 bullets):**
  - Loantek’s current rate sheet ingestion context and why browser portals are
  needed (email/FTP/SFTP/HTTP + scripted portals).
  - Legacy browser automation drawbacks: Windows executable/manual VDI run,
  hardcoded lender steps requiring redeploys, Selenium/ChromeDriver drift,
  brittle selectors.
  - The “Publication Downloader” as a modernization inflection point prompting a
  re-evaluation of browser automation.
  - How traditional browser automation works (Selenium/Playwright): DOM
  selectors, scripted actions, waits/retries; brittleness under UI changes.
  - AI-based browser automation landscape framed as 4 models: low-level AI
  primitives; prompt-assisted authoring; declarative/intent-based automation;
  autonomous agent frameworks.
  - WebVoyager benchmark: what it measures well vs what it does not guarantee;
  used as capability ceiling signal.
  - Workload requirements & constraints with explicit **must/should/ideal**
  labels, including deterministic outcomes, observability, data boundaries, and
  headless/server suitability.
  - Tool evaluations and takeaways for: Vibium, Browser MCP, Browser Use,
  Stagehand, Magnitude, BrowserBook, Skyvern, Browserable, OpenAI Operator,
  Amazon Nova Act.
  - Proposal summary with two implementation paths (Option 1 outsource runtime
  vs Option 2 owned platform), trade-offs/risks, mitigations, and next steps
  (pilot + decision gate).
  **Verbatim quotes (10–25 words each; keep exactly as written):**
  - "The Loantek platform i s respons i ble for download i ng rate sheets from
  mortgage lenders ."
  - "The automat i on i s i mplemented as a . NET appl i cat i on comp i led i
  nto a W i ndows executable ."
  - "Proceed w i th Opt i on 1 (M i n i mum V i able Complex i ty) as the
  default near-term path"
  If you cannot access readable embedded text in **test.pdf** (e.g.,
  image-only/scanned PDF), state that explicitly and request OCR/extracted text
  as the only missing input.
  ## Goals & Tasks
  - Read **test.pdf** end-to-end and rewrite it to be materially more succinct
  while keeping the proposal’s technical and operational meaning intact.
  - Remove repetition and “throat-clearing” while preserving:
    - The problem statement and why the legacy “watcher” approach is risky/expensive.
    - The 4-model landscape (and the axis framing: authoring/execution/ownership).
    - The WebVoyager interpretation caveats.
    - The complete requirements list with original numbering and labels (**must/should/ideal**, including **5** and **5a**).
    - Each evaluated tool’s overview + requirements satisfaction indicators (✅/⚠/❌) + key takeaways (no meaning drift).
    - The proposal’s two options, the recommendation to pursue Option 1 near-term, “tripwires” for Option 2, risks/mitigations, and suggested next steps.
  - Fix obvious PDF text artifacts (e.g., broken spacing: “automat i on”, “w i
  th”) to normal English **without changing meaning**.
  - Keep the tone: engineering proposal (clear, operationally grounded), not
  marketing.
  ## Inputs
  - PDF attachment already provided as context: **test.pdf**
    - Reference path (context only): `/Users/eroberts/Downloads/test.pdf`
  - Use the attached PDF directly as the source of truth.
  - Do **not** ask the user to paste/re-upload/provide the PDF or its path.
  - Only request OCR/extracted text if you cannot read usable text from the PDF;
  explicitly state that limitation.
  ## Constraints
  - Do **not** introduce new claims, requirements, tools, benchmark results,
  pricing, or vendor capabilities not supported by **test.pdf**.
  - Do **not** delete or weaken security/compliance/data-boundary requirements
  (e.g., credential handling, observability artifacts, deterministic outcomes).
  - Preserve the semantic strength of requirement keywords exactly: **must /
  should / ideal**.
  - Preserve the document’s internal consistency, especially:
    - Which tools do/do not support download artifact capture.
    - Which tools are TypeScript-first vs Python-first.
    - Which options are recommended and why (Option 1 default near-term; Option 2 later if tripwires hit).
    - Any licensing note present (e.g., AGPL-3.0 mention around Skyvern) and its framing as a trade-off.
  - Prefer concise sentences, active voice, and de-duplicated phrasing.
  ## Execution Plan
  1. **Parse the document structure** from **test.pdf**:
     - List the major headings and ensure you keep them in roughly the same narrative order.
  2. **Compress section-by-section**:
     - Remove repeated rationale and overlong explanations.
     - Convert eligible paragraphs into bullet lists (especially requirements, tool comparisons, risks/mitigations, next steps).
  3. **Normalize and clean text**:
     - Repair broken words/spacing from PDF extraction.
     - Standardize headings, numbering, and ✅/⚠/❌ indicators.
  4. **Preserve decision-critical fidelity**:
     - Keep requirements numbering and labels.
     - Keep each tool’s “Requirements Satisfied” content aligned with the PDF.
     - Keep proposal details: Option 1 architecture elements (managed browser runtime, prompt-list specs, Step Functions/Lambdas runner) and Option 2 concept; retain “tripwires.”
  5. **Run a coverage/consistency pass** using the Quality Checks section.
  ## Output Format
  Provide **two deliverables** in Markdown.
  1) **Succinct Rewrite (Primary Output)**
  - Use clear headings and compact prose.
  - Maintain the document’s general flow (do not reorganize into a totally new
  paper), covering:
    - Context + objective
    - How browser automation works today
    - AI-based browser automation landscape (4 models)
    - WebVoyager notes (what it measures / doesn’t)
    - Workload requirements & constraints (must/should/ideal)
    - Research overview + per-tool summaries (Vibium, Browser MCP, Browser Use, Stagehand, Magnitude, BrowserBook, Skyvern, Browserable, OpenAI Operator, Amazon Nova Act)
    - Proposal summary (Option 1 vs Option 2), recommendation, risks/mitigations, suggested next steps
  2) **One-Page Executive Summary (Optional but Recommended)**
  - 8–15 bullets maximum.
  - Must include:
    - Why the legacy watcher/Selenium approach is failing operationally
    - The top workload requirements (must-haves)
    - The best-fitting candidates and why (as stated in the PDF)
    - The recommended path (Option 1) + pilot scope + decision gate timing
  ## Quality Checks
  - **Succinctness target:** reduce word count by ~40–60% vs **test.pdf** while
  retaining decision-critical content.
  - **Requirements integrity:** all numbered requirements remain, including
  **5** and **5a**, with correct **must/should/ideal** labels.
  - **Tool fidelity:** for each evaluated tool, ✅/⚠/❌ judgments remain
  consistent with **test.pdf** (especially download artifact access, execution
  model, and manageability).
  - **Recommendation preserved:** the rewrite still recommends Option 1 as the
  default near-term approach and retains the “tripwires” framing for Option 2.
  - **Operational clarity retained:** deterministic outcomes/failure semantics,
  observability artifacts (logs/screenshots/page snapshots),
  credential/security, data boundary/compliance, cost/runtime bounds are
  explicit.
  - **Text cleanup correctness:** spacing/broken-word artifacts are fixed
  without altering meaning; headings and lists render cleanly.
  - **No extraneous additions:** no new vendors, no unreferenced benchmark
  claims, no speculative capabilities beyond the PDF.
reasoning: >-
  I preserved the original prompt contract structure and required section order,
  while making it explicitly grounded in the provided attachment. To satisfy the
  user’s added “Document Snapshot” requirement without violating the mandated
  section order, I embedded a clearly titled “Document Snapshot” subsection
  inside the Context section. I referenced the attachment by filename (test.pdf)
  and treated it as already-available input, only permitting an OCR request if
  the PDF text is unreadable. I also tightened tasks and checks to be concrete
  and testable against the PDF’s specific content: Loantek’s legacy “watcher,”
  Publication Downloader, the 4-model landscape, WebVoyager caveats, numbered
  requirements (including 5 and 5a), specific tool set, and the Option 1 vs
  Option 2 proposal with “tripwires.”
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T17:26:32.606Z'
videos: []
</file>

<file path="exports/pdf-document-prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
intent: please make this document more succinct
interactive: true
iterations: 1
model: gpt-5.2
prompt: >-
  # Title
  Succinct Rewrite of Provided Document (Preserve Meaning)
  ## Role
  You are an expert editor specializing in concise rewriting. Your job is to
  shorten the provided document while preserving its meaning, accuracy, and
  required details.
  ## Context
  The user has a document that is too long or wordy and wants a more succinct
  version. The rewrite must maintain the original intent and any critical
  information, while improving clarity and reducing redundancy.
  ## Goals & Tasks
  - Produce a more succinct version of the document.
  - Preserve:
    - Core meaning, claims, and factual content
    - Necessary nuance, definitions, and any required terminology
    - Logical structure where helpful (unless reorganizing improves concision without losing meaning)
  - Improve:
    - Redundancy and repetition
    - Wordiness, filler, and indirect phrasing
    - Sentence length and readability
  - Optional (only if helpful and safe):
    - Convert verbose passages into bullets
    - Tighten headings and transitions
  ## Inputs
  Request/Use the following inputs from the user:
  - **Document text** to edit (paste in full).
  - Any **must-keep** sections, sentences, terms, or legal/compliance language.
  - Target constraints (if any):
    - Desired length (e.g., “cut by 30%”, “max 400 words”, “one page”)
    - Audience (executives, customers, internal team, academic, etc.)
    - Tone (formal, friendly, neutral) and voice (first person vs. third person)
    - Formatting requirements (keep headings, markdown, doc style, etc.)
  If constraints are not provided, default to:
  - Maintain original tone and formatting as much as possible.
  - Aim for a **~25–40% reduction** in word count without losing meaning.
  ## Constraints
  - Do **not** introduce new facts, claims, metrics, or promises.
  - Do **not** remove critical caveats, safety/compliance statements, or
  definitions.
  - Do **not** change meaning, scope, or intent.
  - Keep proper nouns, product names, and key terms consistent.
  - If a section is ambiguous about what can be removed, **flag it** rather than
  guessing.
  ## Execution Plan
  1. Read the entire document to understand purpose, audience, and key points.
  2. Identify redundancies, repeated ideas, filler phrases, and overly long
  sentences.
  3. Tighten language:
     - Prefer active voice when it reduces words without changing tone.
     - Replace phrases with single precise words.
     - Remove throat-clearing intros and obvious statements.
  4. Restructure minimally when it improves concision (e.g., merge similar
  paragraphs, use bullets).
  5. Verify fidelity: confirm all key points remain and nothing new was added.
  6. Provide the revised document and a brief summary of major edits.
  ## Output Format
  Return:
  1. **Succinct Version (Final)**: Provide the rewritten document in the same
  general format as the original (headings/bullets preserved unless improved).
  2. **Change Summary (Brief)**:
     - Bullet list of the main categories of reductions (e.g., removed redundancy, simplified sentences, condensed examples).
  3. **Metrics (if feasible)**:
     - Approx. original word count vs. revised word count and % reduction.
  4. **Questions/Flags (only if needed)**:
     - Any passages where removing text risked changing meaning; ask targeted clarification questions.
  ## Quality Checks
  Before finalizing, ensure:
  - Meaning and key details are preserved; no new information added.
  - The revised text is clearly shorter and more readable.
  - No critical disclaimers/requirements were dropped.
  - Terminology, names, and numbers (if present) match the original.
  - Formatting is clean and consistent.
  - Any necessary clarifications are explicitly flagged.
reasoning: >-
  User wants an optimized prompt contract for another assistant to make a
  document more succinct. Missing key input: the document itself and any
  constraints (target length, audience, tone, must-keep sections, formatting).
  Strategy: create a contract that requests the document and preferences,
  defines succinctness goals, preserves meaning, and provides an output format
  with a revised version plus a change summary. Include constraints to avoid
  adding new facts and to maintain required terminology. Provide an execution
  plan and quality checks for concision, fidelity, and readability.
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T14:37:00.263Z'
</file>

<file path="exports/pdf-issue.yaml">
contextPaths:
  - path: inline-intent
    source: intent
intent: Please make this information more succinct for me
interactive: true
iterations: 1
model: gpt-5.2
prompt: >-
  # Title
  Succinct Rewrite of Provided Information
  ## Role
  You are a concise editor and summarizer. Your job is to rewrite user-provided
  content to be more succinct while preserving meaning and key details.
  ## Context
  The user wants a shorter, clearer version of some information. The source text
  may be any length or format (paragraphs, notes, email draft, report excerpt,
  etc.). If the source text is missing, you must request it.
  ## Goals & Tasks
  - Produce a more succinct version of the provided text.
  - Preserve:
    - Core message and intent
    - Key facts, figures, dates, names, and constraints
    - Required nuance (e.g., caveats, exceptions) when it materially affects meaning
  - Improve clarity by:
    - Removing redundancies and filler
    - Simplifying wording without changing meaning
    - Tightening sentences and structure
  - If the user specifies a target length, audience, tone, or format, follow it.
  ## Inputs
  - **Source text to condense**: {{SOURCE_TEXT}}
  - **Optional preferences** (use if provided; otherwise infer conservatively):
    - Target length (e.g., “cut by 30%”, “<=120 words”, “one paragraph”): {{TARGET_LENGTH}}
    - Audience (e.g., internal team, executives, customers): {{AUDIENCE}}
    - Tone (e.g., neutral, formal, friendly, direct): {{TONE}}
    - Required format (e.g., bullets, email, memo, slide notes): {{FORMAT}}
    - Must-keep items (specific lines/terms that must remain): {{MUST_KEEP}}
  ## Constraints
  - Do **not** add new facts, claims, or assumptions.
  - Do **not** omit critical details that change decisions, obligations, or
  outcomes.
  - Maintain proper nouns and numbers accurately.
  - If the source is ambiguous or missing essential context, ask up to **3**
  clarifying questions before rewriting.
  - If {{SOURCE_TEXT}} is empty or not provided, request it and stop.
  ## Execution Plan
  1. Verify {{SOURCE_TEXT}} is present; if not, ask the user to paste it.
  2. Identify the core purpose (what the text is trying to communicate) and the
  must-keep details (facts, dates, actions, constraints).
  3. Remove repetition, throat-clearing, and low-information phrases.
  4. Reconstruct the text with tighter phrasing and clearer structure.
  5. Ensure fidelity: compare against the original for meaning, key facts, and
  completeness.
  6. Confirm compliance with any provided length/tone/format constraints.
  ## Output Format
  Provide:
  1. **Succinct version** (primary deliverable) in the requested {{FORMAT}}; if
  none, return as a single tight paragraph.
  2. **Key changes (optional, brief)**: 3–6 bullets noting major cuts/rewrites
  only if the user asked for explanation; otherwise omit.
  ## Quality Checks
  - **Fidelity**: No meaning drift; all critical facts retained.
  - **Brevity**: Noticeably shorter; no filler.
  - **Clarity**: Plain language; strong verbs; minimal jargon.
  - **Correctness**: Names, numbers, dates, and requirements match the source.
  - **Constraint adherence**: Meets target length/tone/format if provided.
reasoning: >-
  The user wants help making “this information” more succinct, but they did not
  provide the actual text to condense. So the prompt contract must (a)
  request/accept the source text as an input, (b) define how to summarize
  (length targets, preserve meaning, keep key facts, remove redundancies), (c)
  specify optional preferences (audience, tone, maximum length, format), and (d)
  ensure the assistant only performs the succincting task when the text is
  provided. I will draft a prompt contract that can be executed later, with
  clear goals, constraints (no new info, maintain critical details), execution
  steps, output format (succinct version + optional bullets of key points), and
  quality checks (coverage, fidelity, length compliance).
refinements: []
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-08T22:26:25.496Z'
</file>

<file path="exports/prompt-export.json">
{
  "schemaVersion": "1",
  "intent": "test prompt",
  "model": "gpt-5.2",
  "targetModel": "gpt-5.2",
  "prompt": "# Title\nTest Prompt Contract: Produce a Simple Demonstration Output\n\n## Role\n- Act as a helpful assistant executing a minimal “test” task to verify instruction-following, formatting, and completeness.\n\n## Context\n- The requester provided only: **“test prompt”**.\n- No domain, audience, or deliverable type was specified; treat this as a generic prompt-contract validation.\n\n## Goals & Tasks\n- Create a small, self-contained demonstration output that proves you can:\n  - Follow instructions precisely.\n  - Ask for missing details only when necessary.\n  - Produce a clear, structured response.\n- Tasks:\n  - (1) Interpret “test prompt” as a request for a minimal sample deliverable.\n  - (2) Produce a short sample output (see Output Format) using a neutral topic.\n  - (3) Include a brief checklist confirming what you did.\n\n## Inputs\n- Primary input:\n  - User intent text: `test prompt`\n- Optional inputs (not provided):\n  - Intended audience, tone, length, topic, constraints.\n\n## Constraints\n- Do not request additional clarification unless it is required to proceed.\n- Keep the output brief and deterministic (avoid open-ended brainstorming).\n- Avoid sensitive content.\n- Do not mention internal policies or hidden reasoning.\n\n## Execution Plan\n- Step 1: Assume a neutral topic (e.g., “benefits of daily walks”) if none is provided.\n- Step 2: Produce a concise sample deliverable using the required structure.\n- Step 3: Add a completion checklist indicating compliance with the contract.\n\n## Output Format\nReturn content in the following exact structure:\n\n1) **Sample Output**\n- A 5–7 sentence paragraph on the assumed neutral topic.\n\n2) **Completion Checklist**\n- Bullets confirming:\n  - Followed structure\n  - Kept brevity\n  - Avoided sensitive content\n  - Did not ask unnecessary questions\n\n## Quality Checks\n- Verify the response includes both required sections: “Sample Output” and “Completion Checklist”.\n- Ensure the paragraph is 5–7 sentences.\n- Ensure the checklist has at least 4 bullets.\n- Confirm the topic is neutral and non-sensitive.\n- Confirm wording is clear and free of contradictions.",
  "reasoning": "User provided minimal intent: \"test prompt\" and asked for a structured prompt contract. No concrete deliverable domain is specified, so create a generic test prompt contract that another assistant can execute, focusing on demonstrating compliance with the required sections, clarity, and testability. Include placeholders for missing inputs and a simple, safe task (e.g., produce a brief sample output and explain steps) without actually executing it. Ensure the prompt text contains the exact required sections in order and begins with a # Title. Return JSON with exactly two keys.",
  "refinements": [],
  "iterations": 1,
  "interactive": true,
  "timestamp": "2026-01-06T14:50:14.538Z",
  "contextPaths": [
    {
      "path": "inline-intent",
      "source": "intent"
    }
  ]
}
</file>

<file path="packages/core/src/index.ts">
export { callLLM, getEmbedding } from './lib/llm'
export type { Message, MessageContent, TextPart, ImagePart, VideoPart, PdfPart } from './lib/llm'
</file>

<file path="runs/prompt-export.yaml">
contextPaths:
  - path: inline-intent
    source: intent
  - path: packages/ui/README.md
    source: smart
  - path: packages/ui/package.json
    source: smart
  - path: packages/ui/src/app/layout.tsx
    source: smart
  - path: packages/api/package.json
    source: smart
  - path: vitest.config.ts
    source: smart
images: []
intent: >-
  I need to know how to run the UI for this project locally.
  I have a `.env` file setup in the `packages/ui/env` folder, but I'm not sure
  this is the correct place. Please let me know if it is correct and if it is
  missing any information, just to make sure I have everything setup.
interactive: true
iterations: 2
model: gpt-5.2
pdfs: []
prompt: >
  # Title
  Local Development Runbook for LoanTek Publication Downloader UI + `.env`
  Placement Validation + AWS Connectivity Setup (Step-by-Step)
  ## Role
  You are a senior full-stack engineer experienced with Next.js (App Router),
  Node/TypeScript monorepos, and AWS (SSM Parameter Store, IAM, AWS CLI/SSO,
  credential resolution).
  ## Context
  The repository contains a UI package at `packages/ui`.
  You must use the provided context files to determine:
  - How the UI is intended to be run locally
  - How environment variables are loaded (local `.env`, generated `.env`, and/or
  AWS SSM)
  - Whether placing a `.env` file inside `packages/ui/env` is correct, and what
  the correct location should be
  - What AWS settings/steps are required locally to successfully connect to AWS
  services used by the UI (especially SSM Parameter Store) and how to validate
  connectivity
  Relevant files already provided (assume you can inspect them directly as
  context):
  - `packages/ui/README.md`
  - `packages/ui/package.json`
  - `packages/ui/src/app/layout.tsx`
  - `vitest.config.ts`
  Key known facts to leverage explicitly:
  - `packages/ui/README.md` documents running locally and AWS_PROFILE usage,
  plus SSM path convention `/qa/loantek/publication-downloader-ui/` derived from
  `APP_ENV`.
  - `packages/ui/package.json` defines scripts: `dev`, `start`, `env-sync`,
  `generate-env`.
  - `packages/ui/src/app/layout.tsx` calls `await loadEnv()` at runtime,
  implying server-side env loading.
  ## Goals & Tasks
  1. Provide step-by-step instructions to run the UI locally using the repo’s
  documented scripts.
  2. Determine and explain the correct `.env` file location(s) for local UI
  development.
     - Explicitly address the user’s current setup: `.env` inside `packages/ui/env`.
  3. Explain how env vars are expected to be sourced in this repo:
     - Next.js env loading expectations (app root env files)
     - From generated env via `npm run generate-env`
     - From AWS SSM via `npm run env-sync` and/or runtime loading mentioned in the README
  4. Add an ordered, “basic principles” AWS connectivity setup section with
  exact steps to make AWS calls succeed locally.
     - Cover credential acquisition (AWS SSO or access keys), profile selection, region configuration, and verification.
     - Cover required access to SSM Parameter Store paths used by this UI and how to confirm permissions.
     - Tie steps to the repo’s conventions: `APP_ENV`, SSM path prefix, and `AWS_PROFILE` usage.
  5. Identify what information might be missing from the user’s env setup.
     - Provide a concrete checklist of likely required variables and how to discover them from code (without inventing names).
  6. Include troubleshooting steps for common failures:
     - Missing env, wrong env file location, missing AWS credentials, wrong profile, wrong region
     - SSM permission errors, parameter path mismatch, APP_ENV mismatch
     - Next dev server issues, New Relic preload issues from `NODE_OPTIONS` in scripts
  ## Inputs
  - Context files (use directly):
    - `packages/ui/README.md`
    - `packages/ui/package.json`
    - `packages/ui/src/app/layout.tsx`
    - `vitest.config.ts`
  - No additional user attachments are required.
  ## Constraints
  - Do not ask the user to re-upload or paste any of the provided files.
  - Only ask clarifying questions if the answer truly depends on unknowns not
  inferable from the provided context (e.g., whether their org uses AWS SSO vs
  access keys; which AWS account/role; intended APP_ENV).
  - Keep recommendations consistent with scripts and conventions shown in
  `packages/ui/package.json` and `packages/ui/README.md`.
  - Do not invent specific environment variable names unless you can justify
  them from code found in the provided files. If uncertain, provide a precise
  method to discover them.
  - Provide “exact steps, in order” for AWS connectivity from basics
  (credentials → config → verify identity → verify SSM access → run UI).
  ## Execution Plan
  1. Extract official local run steps from `packages/ui/README.md` and reflect
  them verbatim where applicable.
  2. Enumerate and explain UI scripts from `packages/ui/package.json`:
     - What `dev` does (including `NODE_OPTIONS` preload)
     - What `start` does
     - What `env-sync` does (SSM path argument pattern)
     - What `generate-env` does (source template location under `packages/ui/env`)
  3. Use `packages/ui/src/app/layout.tsx` to highlight runtime env loading via
  `loadEnv()` and explain implications (server-side execution during app
  startup).
  4. Determine correct `.env` placement based on Next.js conventions:
     - Explain which directory Next uses as the “project root” (the Next app directory `packages/ui/`) and which `.env*` filenames are loaded.
     - Compare against `packages/ui/env/.env` and state whether it will be auto-loaded by Next.
  5. Provide **AWS connectivity setup (ordered)** from first principles:
     - Step 1: Confirm AWS tooling presence (AWS CLI) and baseline environment (Node/npm).
     - Step 2: Choose credential method:
       - If AWS SSO: configure SSO profile and login.
       - If access keys: configure credentials file and default/selected profile.
     - Step 3: Ensure region is set (env var or config), explain why region matters for SSM.
     - Step 4: Verify credentials resolution:
       - Command to confirm identity (e.g., STS caller identity) and interpret output.
     - Step 5: Verify SSM access:
       - Determine expected parameter path prefix from README: `/${APP_ENV}/loantek/publication-downloader-ui/`.
       - Provide commands to test listing/getting parameters under that path.
       - If denied, explain required IAM permissions at a high level (actions and resource patterns) without fabricating account IDs.
     - Step 6: Apply repo workflow:
       - Option A: Run `npm run env-sync` to populate `packages/ui/.env`.
       - Option B: Rely on runtime loading (as described in README) and ensure `APP_ENV` and `AWS_PROFILE` are set.
     - Step 7: Run the UI and validate expected behavior.
  6. Provide “missing env var discovery” method:
     - Direct the assistant to search for `process.env` usage and inspect `packages/ui/src/lib/serverInit` (mentioned in `layout.tsx` import) if available; otherwise clearly state the limitation and provide search instructions.
  7. Troubleshooting matrix:
     - Symptom → likely cause → exact fix steps (including AWS_PROFILE/APP_ENV/region, SSM permissions, New Relic preload issues, wrong `.env` location).
  ## Output Format
  Provide a single markdown response with these sections (in this order):
  1. **Quick Start (Local UI)** — exact commands to run; include prerequisites.
  2. **Where the `.env` should live (and what `packages/ui/env` is for)** —
  explicit answer about `packages/ui/env/.env` vs `packages/ui/.env*` and how
  `generate-env` relates.
  3. **AWS Connectivity Setup (Exact Steps, In Order)**
     - Subsections: Credentials, Profile selection (`AWS_PROFILE`), Region, Verify identity, Verify SSM path access, Sync/load env, Run UI.
     - Include concrete terminal commands and what success/failure looks like.
  4. **Recommended Env Workflows** — (A) local-only, (B) AWS SSM-backed, (C)
  template-based `generate-env`.
  5. **How to Verify If Your `.env` Is Missing Anything** — concrete steps; how
  to locate required vars from code.
  6. **Troubleshooting** — table or bullet list: symptom → cause → fix.
  7. **Clarifying Questions (only if needed)** — maximum 3 short questions.
  ## Quality Checks
  - All commands must match scripts shown in `packages/ui/package.json` (e.g.,
  `npm run dev`, `npm run env-sync`, `npm run generate-env`).
  - Must explicitly reference and align with guidance in
  `packages/ui/README.md`:
    - `AWS_PROFILE=... npm run dev`
    - `APP_ENV` usage
    - SSM path convention `/qa/loantek/publication-downloader-ui/` and generalized `/${APP_ENV}/loantek/publication-downloader-ui/`.
  - Must directly answer whether `packages/ui/env/.env` is the correct place and
  explain what Next.js loads by default.
  - AWS section must be “from basic principles” and strictly ordered with
  verification steps and expected outcomes.
  - Must not claim specific env variable names unless verified from provided
  code; otherwise provide a method to discover.
  - Ensure the runbook is usable end-to-end for a developer on a fresh clone.
reasoning: >-
  I interpreted the refinement instruction as requiring the prompt contract to
  explicitly add an ordered, from-first-principles AWS setup path for local UI
  execution, including credentials sourcing (AWS_PROFILE), region,
  permissions/SSM path, and verification steps, while still grounding everything
  in the provided repo context (README/package scripts/layout loadEnv). I
  preserved the existing structure and goals, expanded the AWS section into
  concrete, sequential tasks (configure AWS CLI/SSO, set region, validate
  identity, validate SSM access, then run env-sync or rely on runtime loading),
  and added constraints to avoid inventing env var names while still providing
  exact operational steps and checks. No task execution is performed; it remains
  a contract for another assistant and references the provided context files by
  path.
refinements:
  - >-
    I also need to understand any AWS settings that I might need to add/change
    in order to get a connection. I need exact steps, in order, from basic
    principles
schemaVersion: '1'
targetModel: gpt-5.2
timestamp: '2026-01-09T18:57:39.623Z'
videos: []
</file>

<file path="src/__tests__/__fixtures__/themes/adapt/opencode.json">
{
  "defs": {
    "surface": "#0b2a32"
  },
  "theme": {
    "background": "#001219",
    "text": "#e9d8a6",
    "textMuted": "#94d2bd",
    "backgroundPanel": "surface",
    "backgroundElement": "#005f73",
    "primary": "#0a9396",
    "border": "#005f73",
    "warning": "#ee9b00",
    "error": "#ae2012",
    "success": "#2a9d8f"
  }
}
</file>

<file path="src/__tests__/generate/interactive.test.ts">
import { runGenerationWorkflow } from '../../generate/interactive'
import type { StreamDispatcher } from '../../generate/stream'
import type { TokenTelemetry } from '../../generate/types'
describe('runGenerationWorkflow (interactive)', () => {
  it('passes pdfs through to PromptGenerationRequest', async () => {
    const generator = {
      generatePromptDetailed: jest.fn().mockResolvedValue({ prompt: 'ok', reasoning: 'r' }),
    }
    const stream: StreamDispatcher = {
      mode: 'none',
      emit: jest.fn(),
    }
    const telemetry: TokenTelemetry = {
      files: [],
      intentTokens: 0,
      fileTokens: 0,
      systemTokens: 0,
      totalTokens: 0,
    }
    await runGenerationWorkflow({
      service: generator as never,
      context: {
        intent: 'intent',
        refinements: [],
        model: 'gpt-4o-mini',
        targetModel: 'gpt-5.2',
        fileContext: [],
        images: [],
        videos: [],
        pdfs: ['/tmp/doc.pdf'],
        metaInstructions: '',
      },
      telemetry,
      interactiveMode: 'none',
      display: false,
      stream,
    })
    expect(generator.generatePromptDetailed).toHaveBeenCalledWith(
      expect.objectContaining({ pdfs: ['/tmp/doc.pdf'] }),
    )
  })
})
</file>

<file path="src/__tests__/generate/payload-io.test.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
import {
  GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
  type GenerateJsonPayload,
} from '../../generate/types'
import { loadGeneratePayloadFromFile, serializeGeneratePayload } from '../../generate/payload-io'
const createTempDir = async (): Promise<string> =>
  fs.mkdtemp(path.join(os.tmpdir(), 'prompt-maker-payload-io-'))
const SAMPLE_PAYLOAD: GenerateJsonPayload = {
  schemaVersion: GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
  intent: 'Build a payload loader',
  model: 'gpt-4.1',
  targetModel: 'gpt-4.1',
  prompt: 'Hello world',
  refinements: ['Be concise'],
  iterations: 1,
  interactive: false,
  timestamp: '1970-01-01T00:00:00.000Z',
  contextPaths: [{ path: 'README.md', source: 'file' }],
}
describe('payload-io', () => {
  it('round-trips JSON payloads', async () => {
    const dir = await createTempDir()
    try {
      const filePath = path.join(dir, 'payload.json')
      await fs.writeFile(filePath, serializeGeneratePayload(SAMPLE_PAYLOAD, 'json'), 'utf8')
      await expect(loadGeneratePayloadFromFile(filePath)).resolves.toEqual(SAMPLE_PAYLOAD)
    } finally {
      await fs.rm(dir, { recursive: true, force: true })
    }
  })
  it('round-trips YAML payloads', async () => {
    const dir = await createTempDir()
    try {
      const filePath = path.join(dir, 'payload.yaml')
      await fs.writeFile(filePath, serializeGeneratePayload(SAMPLE_PAYLOAD, 'yaml'), 'utf8')
      await expect(loadGeneratePayloadFromFile(filePath)).resolves.toEqual(SAMPLE_PAYLOAD)
    } finally {
      await fs.rm(dir, { recursive: true, force: true })
    }
  })
  it('throws on invalid JSON content', async () => {
    const dir = await createTempDir()
    try {
      const filePath = path.join(dir, 'payload.json')
      await fs.writeFile(filePath, '{ not-valid-json', 'utf8')
      await expect(loadGeneratePayloadFromFile(filePath)).rejects.toThrow('Failed to parse JSON')
    } finally {
      await fs.rm(dir, { recursive: true, force: true })
    }
  })
  it('throws on invalid payload shape', async () => {
    const dir = await createTempDir()
    try {
      const filePath = path.join(dir, 'payload.yml')
      await fs.writeFile(filePath, 'intent: hello\n', 'utf8')
      await expect(loadGeneratePayloadFromFile(filePath)).rejects.toThrow(
        'Invalid generate payload',
      )
    } finally {
      await fs.rm(dir, { recursive: true, force: true })
    }
  })
})
</file>

<file path="src/__tests__/generate/resume-loaded-stream-event.test.ts">
import type { StreamEventInput } from '../../generate/types'
test('resume.loaded is a valid StreamEventInput', () => {
  const event = {
    event: 'resume.loaded',
    source: 'history',
    reusedContextPaths: [{ path: 'docs/context-templates.md', source: 'file' }],
    missingContextPaths: [{ path: 'https://example.com', source: 'url' }],
  } satisfies StreamEventInput
  expect(event.event).toBe('resume.loaded')
})
</file>

<file path="src/__tests__/history/generate-history.test.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
import {
  loadGeneratePayloadFromHistory,
  parseFromHistorySelector,
} from '../../history/generate-history'
import {
  GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
  type GenerateJsonPayload,
} from '../../generate/types'
describe('generate-history helpers', () => {
  const tempRoots: string[] = []
  afterEach(async () => {
    await Promise.all(
      tempRoots.map(async (root) => {
        await fs.rm(root, { recursive: true, force: true })
      }),
    )
    tempRoots.splice(0, tempRoots.length)
  })
  const writeHistory = async (lines: string[]): Promise<string> => {
    const root = await fs.mkdtemp(path.join(os.tmpdir(), 'pm-generate-history-'))
    tempRoots.push(root)
    const historyPath = path.join(root, 'history.jsonl')
    await fs.writeFile(historyPath, `${lines.join('\n')}\n`, 'utf8')
    return historyPath
  }
  const createPayload = (overrides: Partial<GenerateJsonPayload>): GenerateJsonPayload => ({
    schemaVersion: GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
    intent: 'intent',
    model: 'model',
    targetModel: 'target-model',
    prompt: 'prompt',
    refinements: [],
    iterations: 1,
    interactive: false,
    timestamp: new Date(0).toISOString(),
    contextPaths: [{ path: 'file.txt', source: 'file' }],
    ...overrides,
  })
  test('parseFromHistorySelector parses last/last:N/N', () => {
    expect(parseFromHistorySelector('last')).toEqual({ fromEnd: 1, label: 'last' })
    expect(parseFromHistorySelector('last:2')).toEqual({ fromEnd: 2, label: 'last:2' })
    expect(parseFromHistorySelector('3')).toEqual({ fromEnd: 3, label: '3' })
  })
  test('parseFromHistorySelector rejects invalid selectors', () => {
    expect(() => parseFromHistorySelector('last:0')).toThrow(/Invalid --from-history selector/)
    expect(() => parseFromHistorySelector('wat')).toThrow(/Invalid --from-history selector/)
  })
  test('loadGeneratePayloadFromHistory enforces selector bounds', async () => {
    const payload = createPayload({ intent: 'only' })
    const historyPath = await writeHistory([JSON.stringify(payload)])
    await expect(loadGeneratePayloadFromHistory({ selector: '2', historyPath })).rejects.toThrow(
      /History selector is out of range/,
    )
  })
  test('loadGeneratePayloadFromHistory refuses unsupported schemaVersion', async () => {
    const supported = createPayload({ intent: 'supported' })
    const unsupported = { ...supported, schemaVersion: '999' }
    const historyPath = await writeHistory([JSON.stringify(supported), JSON.stringify(unsupported)])
    await expect(loadGeneratePayloadFromHistory({ selector: 'last', historyPath })).rejects.toThrow(
      /Unsupported history payload schemaVersion=999/,
    )
    await expect(loadGeneratePayloadFromHistory({ selector: '2', historyPath })).resolves.toEqual(
      supported,
    )
  })
})
</file>

<file path="src/__tests__/tui/budgets-persistence.test.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
const readJson = async (filePath: string): Promise<unknown> => {
  const contents = await fs.readFile(filePath, 'utf8')
  return JSON.parse(contents) as unknown
}
const writeJson = async (filePath: string, value: unknown): Promise<void> => {
  await fs.mkdir(path.dirname(filePath), { recursive: true })
  await fs.writeFile(filePath, `${JSON.stringify(value, null, 2)}\n`, 'utf8')
}
describe('budget settings persistence', () => {
  const envBefore = { ...process.env }
  afterEach(() => {
    process.env = { ...envBefore }
  })
  test('saving updates promptGenerator without rewriting other fields', async () => {
    const tempRoot = await fs.mkdtemp(path.join(os.tmpdir(), 'pm-budgets-persist-'))
    const configPath = path.join(tempRoot, 'config.json')
    await writeJson(configPath, {
      openaiApiKey: 'keep-me',
      promptGenerator: { defaultModel: 'gpt-4o', maxInputTokens: 500 },
    })
    process.env.PROMPT_MAKER_CLI_CONFIG = configPath
    jest.resetModules()
    const { updateCliPromptGeneratorSettings } = await import('../../config')
    await updateCliPromptGeneratorSettings({
      maxInputTokens: 123,
      maxContextTokens: 456,
      contextOverflowStrategy: 'drop-oldest',
    })
    const updated = await readJson(configPath)
    expect(updated).toMatchObject({
      openaiApiKey: 'keep-me',
      promptGenerator: {
        defaultModel: 'gpt-4o',
        maxInputTokens: 123,
        maxContextTokens: 456,
        contextOverflowStrategy: 'drop-oldest',
      },
    })
  })
  test('null patch deletes budget fields', async () => {
    const tempRoot = await fs.mkdtemp(path.join(os.tmpdir(), 'pm-budgets-persist-'))
    const configPath = path.join(tempRoot, 'config.json')
    await writeJson(configPath, {
      promptGenerator: {
        defaultModel: 'gpt-4o',
        maxInputTokens: 123,
        maxContextTokens: 456,
        contextOverflowStrategy: 'drop-oldest',
      },
    })
    process.env.PROMPT_MAKER_CLI_CONFIG = configPath
    jest.resetModules()
    const { updateCliPromptGeneratorSettings } = await import('../../config')
    await updateCliPromptGeneratorSettings({
      maxInputTokens: null,
      maxContextTokens: null,
      contextOverflowStrategy: null,
    })
    const updated = await readJson(configPath)
    expect(updated).toMatchObject({
      promptGenerator: { defaultModel: 'gpt-4o' },
    })
    expect(updated).not.toMatchObject({
      promptGenerator: {
        maxInputTokens: expect.anything(),
      },
    })
  })
})
</file>

<file path="src/__tests__/tui/budgets-settings.test.ts">
import { parseBudgetSettingsDraft } from '../../tui/budget-settings'
describe('budget settings draft parsing', () => {
  test('empty fields disable budgets', () => {
    expect(
      parseBudgetSettingsDraft({
        maxContextTokensDraft: '',
        maxInputTokensDraft: '',
        contextOverflowStrategyDraft: '',
      }),
    ).toEqual({
      ok: true,
      settings: {
        maxContextTokens: null,
        maxInputTokens: null,
        contextOverflowStrategy: null,
      },
    })
  })
  test('rejects non-integer tokens', () => {
    const result = parseBudgetSettingsDraft({
      maxContextTokensDraft: 'abc',
      maxInputTokensDraft: '',
      contextOverflowStrategyDraft: '',
    })
    expect(result.ok).toBe(false)
    if (result.ok) {
      throw new Error('Expected error result')
    }
    expect(result.errorMessage).toBe('Max context tokens must be a positive integer.')
  })
  test('defaults overflow to fail when budgets enabled', () => {
    expect(
      parseBudgetSettingsDraft({
        maxContextTokensDraft: '100',
        maxInputTokensDraft: '',
        contextOverflowStrategyDraft: '',
      }),
    ).toEqual({
      ok: true,
      settings: {
        maxContextTokens: 100,
        maxInputTokens: null,
        contextOverflowStrategy: 'fail',
      },
    })
  })
  test('preserves selected overflow strategy', () => {
    expect(
      parseBudgetSettingsDraft({
        maxContextTokensDraft: '100',
        maxInputTokensDraft: '200',
        contextOverflowStrategyDraft: 'drop-oldest',
      }),
    ).toEqual({
      ok: true,
      settings: {
        maxContextTokens: 100,
        maxInputTokens: 200,
        contextOverflowStrategy: 'drop-oldest',
      },
    })
  })
  test('allows setting overflow strategy without budgets', () => {
    expect(
      parseBudgetSettingsDraft({
        maxContextTokensDraft: '',
        maxInputTokensDraft: '',
        contextOverflowStrategyDraft: 'drop-url',
      }),
    ).toEqual({
      ok: true,
      settings: {
        maxContextTokens: null,
        maxInputTokens: null,
        contextOverflowStrategy: 'drop-url',
      },
    })
  })
})
</file>

<file path="src/__tests__/tui/context-popup-state-mutations.test.ts">
import {
  defocusSuggestionsIfEmpty,
  deriveSuggestionsViewModel,
  getAutoAddAbsolutePathCandidate,
} from '../../tui/screens/command/hooks/context-popup-glue/popup-state-mutations'
describe('context popup state mutations', () => {
  describe('deriveSuggestionsViewModel', () => {
    it('clamps suggested selection index and preserves focus when suggestions exist', () => {
      const model = deriveSuggestionsViewModel({
        draft: 'a',
        suggestedItems: ['a', 'b', 'c'],
        suggestedSelectionIndex: 99,
        suggestedFocused: true,
        filterSuggestions: (_query, suggestions) => suggestions,
      })
      expect(model.suggestions).toEqual(['a', 'b', 'c'])
      expect(model.selectionIndex).toBe(2)
      expect(model.focused).toBe(true)
      expect(model.shouldDefocus).toBe(false)
    })
    it('defocuses suggestions when the filtered list is empty', () => {
      const model = deriveSuggestionsViewModel({
        draft: 'a',
        suggestedItems: ['a', 'b'],
        suggestedSelectionIndex: 1,
        suggestedFocused: true,
        filterSuggestions: () => [],
      })
      expect(model.suggestions).toEqual([])
      expect(model.selectionIndex).toBe(0)
      expect(model.focused).toBe(false)
      expect(model.shouldDefocus).toBe(true)
    })
  })
  describe('defocusSuggestionsIfEmpty', () => {
    it('clears focus and resets selection when empty', () => {
      const prev = { suggestedFocused: true, suggestedSelectionIndex: 5, extra: 'x' }
      const next = defocusSuggestionsIfEmpty(prev, 0)
      expect(next).toEqual({
        suggestedFocused: false,
        suggestedSelectionIndex: 0,
        extra: 'x',
      })
    })
    it('does not change state when suggestions exist', () => {
      const prev = { suggestedFocused: true, suggestedSelectionIndex: 1 }
      expect(defocusSuggestionsIfEmpty(prev, 2)).toBe(prev)
    })
    it('does not change state when suggestions are already unfocused', () => {
      const prev = { suggestedFocused: false, suggestedSelectionIndex: 1 }
      expect(defocusSuggestionsIfEmpty(prev, 0)).toBe(prev)
    })
  })
  describe('getAutoAddAbsolutePathCandidate', () => {
    it('returns an absolute path candidate when it parses and is allowed', () => {
      expect(getAutoAddAbsolutePathCandidate('/Users/alice/file.txt', () => true)).toBe(
        '/Users/alice/file.txt',
      )
    })
    it('returns null when the candidate fails validation', () => {
      expect(getAutoAddAbsolutePathCandidate('/Users/alice/file.txt', () => false)).toBeNull()
    })
    it('returns null when the draft is not an absolute path', () => {
      expect(getAutoAddAbsolutePathCandidate('src/index.ts', () => true)).toBeNull()
    })
    it('strips quotes from paths and returns the normalized candidate', () => {
      expect(getAutoAddAbsolutePathCandidate('"/Users/alice/My File.md"', () => true)).toBe(
        '/Users/alice/My File.md',
      )
    })
  })
})
</file>

<file path="src/__tests__/tui/generation-pipeline-reducer.test.ts">
import {
  INITIAL_GENERATION_PIPELINE_STATE,
  generationPipelineReducer,
  type ContextOverflowDetails,
  type ResumeLoadedDetails,
} from '../../tui/generation-pipeline-reducer'
describe('generationPipelineReducer', () => {
  it('stores latest resume.loaded details', () => {
    const details = {
      event: 'resume.loaded',
      source: 'history',
      reusedContextPaths: [{ path: 'src/a.ts', source: 'file' }],
      missingContextPaths: [{ path: 'https://example.com', source: 'url' }],
    } satisfies ResumeLoadedDetails
    const next = generationPipelineReducer(INITIAL_GENERATION_PIPELINE_STATE, {
      type: 'set-resume-loaded',
      details,
    })
    expect(next.latestResumeLoaded).toEqual(details)
    expect(next.latestContextOverflow).toBeNull()
  })
  it('stores latest context.overflow details', () => {
    const before = {
      files: [{ path: 'src/a.ts', tokens: 120 }],
      intentTokens: 10,
      fileTokens: 120,
      systemTokens: 5,
      totalTokens: 135,
    }
    const after = {
      files: [],
      intentTokens: 10,
      fileTokens: 0,
      systemTokens: 5,
      totalTokens: 15,
    }
    const details = {
      event: 'context.overflow',
      strategy: 'drop-largest',
      before,
      after,
      droppedPaths: [{ path: 'src/a.ts', source: 'file' }],
    } satisfies ContextOverflowDetails
    const next = generationPipelineReducer(INITIAL_GENERATION_PIPELINE_STATE, {
      type: 'set-context-overflow',
      details,
    })
    expect(next.latestContextOverflow).toEqual(details)
    expect(next.latestResumeLoaded).toBeNull()
  })
  it('clears overflow/resume details on generation-start', () => {
    const resumeDetails = {
      event: 'resume.loaded',
      source: 'file',
      reusedContextPaths: [{ path: 'src/a.ts', source: 'file' }],
      missingContextPaths: [],
    } satisfies ResumeLoadedDetails
    const overflowDetails = {
      event: 'context.overflow',
      strategy: 'drop-oldest',
      before: {
        files: [{ path: 'src/a.ts', tokens: 120 }],
        intentTokens: 10,
        fileTokens: 120,
        systemTokens: 5,
        totalTokens: 135,
      },
      after: {
        files: [],
        intentTokens: 10,
        fileTokens: 0,
        systemTokens: 5,
        totalTokens: 15,
      },
      droppedPaths: [{ path: 'src/a.ts', source: 'file' }],
    } satisfies ContextOverflowDetails
    const withResume = generationPipelineReducer(INITIAL_GENERATION_PIPELINE_STATE, {
      type: 'set-resume-loaded',
      details: resumeDetails,
    })
    const withOverflow = generationPipelineReducer(withResume, {
      type: 'set-context-overflow',
      details: overflowDetails,
    })
    const next = generationPipelineReducer(withOverflow, {
      type: 'generation-start',
      statusMessage: 'Preparing generation…',
    })
    expect(next.isGenerating).toBe(true)
    expect(next.latestTelemetry).toBeNull()
    expect(next.latestResumeLoaded).toBeNull()
    expect(next.latestContextOverflow).toBeNull()
  })
})
</file>

<file path="src/__tests__/tui/help-command.test.ts">
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import type { MutableRefObject } from 'react'
import { useIntentSubmitHandler } from '../../tui/screens/command/hooks/useIntentSubmitHandler'
const dom = new JSDOM('<!doctype html><html><body></body></html>')
const globalScope = globalThis as typeof globalThis & {
  window: Window & typeof globalThis
  document: Document
  navigator: Navigator
}
globalScope.window = dom.window
globalScope.document = dom.window.document
globalScope.navigator = dom.window.navigator
describe('/help command', () => {
  it('opens the help overlay instead of running a popup command', () => {
    const openHelp = jest.fn()
    const handleCommandSelection = jest.fn()
    const lastUserIntentRef: MutableRefObject<string | null> = { current: null }
    const { result } = renderHook(() =>
      useIntentSubmitHandler({
        popupState: null,
        isAwaitingRefinement: false,
        submitRefinement: jest.fn(),
        isCommandMenuActive: true,
        selectedCommandId: 'help',
        commandMenuArgsRaw: '',
        isCommandMode: true,
        intentFilePath: '',
        isGenerating: false,
        expandInputForSubmit: (value) => value,
        setInputValue: jest.fn(),
        pushHistory: jest.fn(),
        addCommandHistoryEntry: jest.fn(),
        runGeneration: jest.fn(async () => {}),
        handleCommandSelection,
        handleNewCommand: jest.fn(),
        handleReuseCommand: jest.fn(),
        lastUserIntentRef,
        openHelp,
      }),
    )
    act(() => {
      result.current('/help')
    })
    expect(openHelp).toHaveBeenCalledTimes(1)
    expect(handleCommandSelection).not.toHaveBeenCalled()
  })
})
</file>

<file path="src/__tests__/tui/list-popup-model.test.ts">
import { buildListPopupModel } from '../../tui/components/popups/list-popup-model'
const makeItems = (count: number, prefix = 'item'): string[] =>
  Array.from({ length: count }, (_, index) => `${prefix}${index}`)
const getBlockTypes = (model: ReturnType<typeof buildListPopupModel>): string[] =>
  model.blocks.map((block) => block.type)
describe('buildListPopupModel', () => {
  it('builds a free-height selected section when suggestions are absent', () => {
    const model = buildListPopupModel({
      items: makeItems(10),
      selectedIndex: 5,
      emptyLabel: '(none)',
      instructions: 'Use arrows',
      layout: 'input-first',
      popupHeight: 16,
    })
    expect(model.hasSuggestions).toBe(false)
    expect(model.input).toEqual({ variant: 'titled', title: 'Add new', focus: true })
    expect(model.selectedSection.fixedRowCount).toBeUndefined()
    expect(model.selectedSection.rows.map((row) => row.label)).toEqual([
      '… earlier entries …',
      '4. item3',
      '5. item4',
      '6. item5',
      '7. item6',
      '8. item7',
      '9. item8',
      '… later entries …',
    ])
    const selectedRow = model.selectedSection.rows.find((row) => row.selection !== 'none')
    expect(selectedRow).toBeDefined()
    expect(selectedRow?.label).toBe('6. item5')
    expect(selectedRow?.selection).toBe('focused')
    expect(getBlockTypes(model)).toEqual([
      'spacer',
      'input',
      'spacer',
      'section',
      'spacer',
      'instructions',
    ])
  })
  it('builds fixed-height selected/suggestion sections when suggestions are present', () => {
    const model = buildListPopupModel({
      items: makeItems(10),
      selectedIndex: 5,
      emptyLabel: '(none)',
      instructions: 'Use arrows',
      layout: 'input-first',
      popupHeight: 16,
      suggestedItems: makeItems(10, 's'),
      suggestedSelectionIndex: 5,
      suggestedFocused: true,
    })
    expect(model.hasSuggestions).toBe(true)
    expect(model.input).toEqual({ variant: 'inline', label: 'Add:', focus: false })
    expect(model.selectedSection.fixedRowCount).toBe(3)
    expect(model.selectedSection.rows.map((row) => row.label)).toEqual([
      '… earlier entries …',
      '6. item5',
      '… later entries …',
    ])
    expect(model.suggestionsSection?.fixedRowCount).toBe(4)
    expect(model.suggestionsSection?.rows.map((row) => row.label)).toEqual([
      '… earlier suggestions …',
      's4',
      's5',
      '… later suggestions …',
    ])
    const suggestedRow = model.suggestionsSection?.rows.find((row) => row.selection !== 'none')
    expect(suggestedRow?.label).toBe('s5')
    expect(suggestedRow?.selection).toBe('focused')
    expect(getBlockTypes(model)).toEqual(['input', 'section', 'section', 'instructions'])
  })
  it('clamps the suggested selection index', () => {
    const model = buildListPopupModel({
      items: [],
      selectedIndex: 0,
      emptyLabel: '(none)',
      instructions: 'Use arrows',
      layout: 'input-first',
      popupHeight: 16,
      suggestedItems: ['a', 'b', 'c'],
      suggestedSelectionIndex: 99,
    })
    expect(model.safeSuggestedSelection).toBe(2)
    const selectedSuggestion = model.suggestionsSection?.rows.find(
      (row) => row.selection !== 'none',
    )
    expect(selectedSuggestion?.label).toBe('c')
  })
  it('reflects focus switching between suggested and selected lists', () => {
    const model = buildListPopupModel({
      items: makeItems(10),
      selectedIndex: 5,
      emptyLabel: '(none)',
      instructions: 'Use arrows',
      layout: 'input-first',
      popupHeight: 16,
      suggestedItems: makeItems(10, 's'),
      suggestedSelectionIndex: 5,
      suggestedFocused: true,
      selectedFocused: false,
    })
    expect(model.input).toEqual({ variant: 'inline', label: 'Add:', focus: false })
    const selectedRow = model.selectedSection.rows.find((row) => row.selection !== 'none')
    expect(selectedRow?.label).toBe('6. item5')
    expect(selectedRow?.selection).toBe('unfocused')
    const suggestedRow = model.suggestionsSection?.rows.find((row) => row.selection !== 'none')
    expect(suggestedRow?.label).toBe('s5')
    expect(suggestedRow?.selection).toBe('focused')
  })
})
</file>

<file path="src/__tests__/tui/markdown-highlight.test.ts">
import {
  resolveMarkdownSlotColor,
  tokenizeMarkdownLines,
} from '../../tui/markdown/markdown-highlight'
import type { ResolvedTheme } from '../../tui/theme/theme-types'
const makeBaseTheme = (): ResolvedTheme => ({
  background: '#000000',
  text: '#ffffff',
  mutedText: '#888888',
  border: '#444444',
  accent: '#00ffff',
  accentText: '#000000',
  warning: '#ffff00',
  error: '#ff0000',
  success: '#00ff00',
  panelBackground: '#111111',
  popupBackground: '#111111',
  selectionBackground: '#333333',
  selectionText: '#ffffff',
  chipBackground: '#222222',
  chipText: '#ffffff',
  chipMutedText: '#aaaaaa',
})
describe('markdown-highlight', () => {
  test('tokenizes common markdown constructs into theme slots', () => {
    const lines = [
      '# Heading',
      '> Blockquote with `code`',
      '- Bullet item',
      '1. Enumerated item',
      '---',
      '[Link text](https://example.com)',
      '`inline`',
      '**strong** and *emph*',
      '![alt text](image.png)',
      '```ts',
      'const x = 1',
      '```',
    ]
    const tokenized = tokenizeMarkdownLines(lines)
    expect(tokenized[0]?.[0]?.slot).toBe('markdownHeading')
    expect(tokenized[1]?.[0]?.slot).toBe('markdownBlockQuote')
    expect(tokenized[2]?.some((span) => span.slot === 'markdownListItem')).toBe(true)
    expect(tokenized[3]?.some((span) => span.slot === 'markdownListEnumeration')).toBe(true)
    expect(tokenized[4]?.[0]?.slot).toBe('markdownHorizontalRule')
    const linkLine = tokenized[5] ?? []
    expect(linkLine.some((span) => span.slot === 'markdownLinkText' && span.underline)).toBe(true)
    expect(linkLine.some((span) => span.slot === 'markdownLink')).toBe(true)
    expect(tokenized[6]?.[0]?.slot).toBe('markdownCode')
    const strongLine = tokenized[7] ?? []
    expect(strongLine.some((span) => span.slot === 'markdownStrong' && span.bold)).toBe(true)
    expect(strongLine.some((span) => span.slot === 'markdownEmph' && span.italic)).toBe(true)
    const imageLine = tokenized[8] ?? []
    expect(imageLine.some((span) => span.slot === 'markdownImageText')).toBe(true)
    expect(imageLine.some((span) => span.slot === 'markdownImage')).toBe(true)
    expect(tokenized[9]?.[0]?.slot).toBe('markdownCodeBlock')
    expect(tokenized[10]?.[0]?.slot).toBe('markdownCodeBlock')
    expect(tokenized[11]?.[0]?.slot).toBe('markdownCodeBlock')
  })
  test('resolves markdown slot colors with fallbacks', () => {
    const base = makeBaseTheme()
    expect(resolveMarkdownSlotColor(base, 'markdownText')).toBe(base.text)
    expect(resolveMarkdownSlotColor(base, 'markdownHeading')).toBe(base.accent)
    expect(resolveMarkdownSlotColor(base, 'markdownHorizontalRule')).toBe(base.border)
    const overridden: ResolvedTheme = {
      ...base,
      markdownHeading: '#123456',
      markdownLink: undefined,
    }
    expect(resolveMarkdownSlotColor(overridden, 'markdownHeading')).toBe('#123456')
    expect(resolveMarkdownSlotColor(overridden, 'markdownLink')).toBeUndefined()
  })
})
</file>

<file path="src/__tests__/tui/notifier-dedupe.test.ts">
import React from 'react'
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import { ToastProvider, useNotifier } from '../../tui/notifier'
const dom = new JSDOM('<!doctype html><html><body></body></html>')
const globalEnv = globalThis as typeof globalThis & {
  window: Window & typeof globalThis
  document: Document
  navigator: Navigator
}
globalEnv.window = dom.window as typeof globalEnv.window
globalEnv.document = dom.window.document as Document
globalEnv.navigator = dom.window.navigator
const createWrapper = (options: Omit<React.ComponentProps<typeof ToastProvider>, 'children'>) => {
  const Wrapper: React.FC<{ children: React.ReactNode }> = ({ children }) =>
    React.createElement(ToastProvider, { ...options, children })
  return Wrapper
}
describe('toast provider notifier dedupe', () => {
  beforeEach(() => {
    jest.useFakeTimers()
  })
  afterEach(() => {
    jest.useRealTimers()
  })
  it('reuses latest active toast when message+kind match', () => {
    const wrapper = createWrapper({ exitAnimationMs: 10 })
    const { result } = renderHook(() => useNotifier({ autoDismissMs: 1_000 }), { wrapper })
    let firstId: number | null = null
    let secondId: number | null = null
    act(() => {
      firstId = result.current.showToast('Same', { kind: 'info', autoDismissMs: 100 })
    })
    act(() => {
      jest.advanceTimersByTime(60)
    })
    act(() => {
      secondId = result.current.showToast('Same', { kind: 'info', autoDismissMs: 500 })
    })
    expect(firstId).not.toBeNull()
    expect(secondId).toBe(firstId)
    expect(result.current.toasts).toHaveLength(1)
    expect(result.current.toasts[0]?.message).toBe('Same')
    // NOTE: the toast item is not updated on reuse.
    expect(result.current.toasts[0]?.autoDismissMs).toBe(100)
    act(() => {
      jest.advanceTimersByTime(100)
    })
    // The dismiss timer is reset on reuse.
    expect(result.current.toasts[0]?.isExiting).toBe(false)
    act(() => {
      jest.advanceTimersByTime(400)
    })
    expect(result.current.toasts[0]?.isExiting).toBe(true)
    act(() => {
      jest.advanceTimersByTime(10)
    })
    expect(result.current.toasts).toHaveLength(0)
  })
  it('does not reuse a toast that is already exiting', () => {
    const wrapper = createWrapper({ exitAnimationMs: 10 })
    const { result } = renderHook(() => useNotifier({ autoDismissMs: 100_000 }), { wrapper })
    let firstId: number | null = null
    let secondId: number | null = null
    act(() => {
      firstId = result.current.showToast('Same', { kind: 'info', autoDismissMs: 100_000 })
    })
    if (firstId === null) {
      throw new Error('Expected toast id')
    }
    const firstToastId = firstId
    act(() => {
      result.current.dismissToast(firstToastId)
    })
    expect(result.current.toasts[0]?.isExiting).toBe(true)
    act(() => {
      secondId = result.current.showToast('Same', { kind: 'info', autoDismissMs: 100_000 })
    })
    expect(secondId).not.toBeNull()
    expect(secondId).not.toBe(firstToastId)
    expect(result.current.toasts).toHaveLength(2)
    expect(result.current.toasts[0]?.isExiting).toBe(true)
    expect(result.current.toasts[1]?.isExiting).toBe(false)
  })
  it('only reuses the latest active toast (not an older matching one)', () => {
    const wrapper = createWrapper({ exitAnimationMs: 10 })
    const { result } = renderHook(() => useNotifier({ autoDismissMs: 100_000 }), { wrapper })
    let firstId: number | null = null
    let thirdId: number | null = null
    act(() => {
      firstId = result.current.showToast('A', { kind: 'info', autoDismissMs: 100_000 })
      result.current.showToast('B', { kind: 'info', autoDismissMs: 100_000 })
    })
    act(() => {
      thirdId = result.current.showToast('A', { kind: 'info', autoDismissMs: 100_000 })
    })
    expect(firstId).not.toBeNull()
    expect(thirdId).not.toBeNull()
    expect(thirdId).not.toBe(firstId)
    expect(result.current.toasts.map((toast) => toast.message)).toEqual(['A', 'B', 'A'])
  })
})
</file>

<file path="src/__tests__/tui/popup-scan-orchestrator.test.ts">
import type { PopupAction } from '../../tui/popup-reducer'
import { INITIAL_POPUP_MANAGER_STATE, popupReducer } from '../../tui/popup-reducer'
import { createPopupScanOrchestrator } from '../../tui/hooks/popup-manager/scan-orchestrator'
type Deferred<T> = {
  promise: Promise<T>
  resolve: (value: T) => void
  reject: (reason?: unknown) => void
}
const createDeferred = <T>(): Deferred<T> => {
  let resolve: (value: T) => void = (_value) => undefined
  let reject: (reason?: unknown) => void = (_reason) => undefined
  const promise = new Promise<T>((promiseResolve, promiseReject) => {
    resolve = promiseResolve
    reject = promiseReject
  })
  return { promise, resolve, reject }
}
describe('popup scan orchestrator', () => {
  it('keeps scanId monotonic and ignores stale results', async () => {
    const scanIdRef = { current: 0 }
    const actions: PopupAction[] = []
    const dispatch = (action: PopupAction): void => {
      actions.push(action)
    }
    const pushHistory = jest.fn()
    const orchestrator = createPopupScanOrchestrator({ scanIdRef, dispatch, pushHistory })
    const firstScan = createDeferred<string[]>()
    const secondScan = createDeferred<string[]>()
    orchestrator.runSuggestionScan({
      kind: 'file',
      open: (scanId) => ({ type: 'open-file', scanId }),
      scan: () => firstScan.promise,
    })
    orchestrator.runSuggestionScan({
      kind: 'file',
      open: (scanId) => ({ type: 'open-file', scanId }),
      scan: () => secondScan.promise,
    })
    expect(actions[0]).toEqual({ type: 'open-file', scanId: 1 })
    expect(actions[1]).toEqual({ type: 'open-file', scanId: 2 })
    firstScan.resolve(['stale'])
    await firstScan.promise
    await Promise.resolve()
    let state = INITIAL_POPUP_MANAGER_STATE
    for (const action of actions) {
      state = popupReducer(state, action)
    }
    if (state.popupState?.type !== 'file') {
      throw new Error('Expected file popup')
    }
    expect(state.popupState.suggestedItems).toEqual([])
    secondScan.resolve(['fresh'])
    await secondScan.promise
    await Promise.resolve()
    state = INITIAL_POPUP_MANAGER_STATE
    for (const action of actions) {
      state = popupReducer(state, action)
    }
    if (state.popupState?.type !== 'file') {
      throw new Error('Expected file popup')
    }
    expect(state.popupState.suggestedItems).toEqual(['fresh'])
  })
  it('does not apply file suggestions after switching popup types', async () => {
    const scanIdRef = { current: 0 }
    const actions: PopupAction[] = []
    const dispatch = (action: PopupAction): void => {
      actions.push(action)
    }
    const pushHistory = jest.fn()
    const orchestrator = createPopupScanOrchestrator({ scanIdRef, dispatch, pushHistory })
    const fileScan = createDeferred<string[]>()
    const imageScan = createDeferred<string[]>()
    orchestrator.runSuggestionScan({
      kind: 'file',
      open: (scanId) => ({ type: 'open-file', scanId }),
      scan: () => fileScan.promise,
    })
    orchestrator.runSuggestionScan({
      kind: 'image',
      open: (scanId) => ({ type: 'open-image', scanId }),
      scan: () => imageScan.promise,
    })
    fileScan.resolve(['file.txt'])
    await fileScan.promise
    await Promise.resolve()
    let state = INITIAL_POPUP_MANAGER_STATE
    for (const action of actions) {
      state = popupReducer(state, action)
    }
    expect(state.popupState?.type).toBe('image')
    imageScan.resolve(['img.png'])
    await imageScan.promise
    await Promise.resolve()
    state = INITIAL_POPUP_MANAGER_STATE
    for (const action of actions) {
      state = popupReducer(state, action)
    }
    if (state.popupState?.type !== 'image') {
      throw new Error('Expected image popup')
    }
    expect(state.popupState.suggestedItems).toEqual(['img.png'])
  })
})
</file>

<file path="src/__tests__/tui/popup-shortcuts.test.ts">
import type { PopupState } from '../../tui/types'
import {
  getSuggestedSelectedListPopupEffect,
  type SuggestedSelectedListPopupState,
} from '../../tui/screens/command/hooks/popup-shortcuts/suggested-selected-list'
import {
  getSuggestedInputPopupEffect,
  type SuggestedInputPopupState,
} from '../../tui/screens/command/hooks/popup-shortcuts/suggested-input'
describe('popup shortcut reducers', () => {
  describe('getSuggestedSelectedListPopupEffect', () => {
    const createFileState = (overrides: Partial<SuggestedSelectedListPopupState> = {}) =>
      ({
        type: 'file',
        draft: '',
        selectionIndex: 0,
        selectedFocused: false,
        suggestedItems: [],
        suggestedSelectionIndex: 0,
        suggestedFocused: false,
        ...overrides,
      }) satisfies PopupState
    it('moves focus from suggestions to selected list on Up at top', () => {
      const state = createFileState({ suggestedFocused: true, suggestedSelectionIndex: 0 })
      const effect = getSuggestedSelectedListPopupEffect({
        popupType: 'file',
        popupState: state,
        itemsLength: 2,
        suggestions: ['a', 'b'],
        key: { upArrow: true },
        isBackspace: false,
      })
      expect(effect.type).toBe('set')
      if (effect.type !== 'set') {
        throw new Error('Expected set effect')
      }
      const next = effect.updater(state as PopupState)
      expect(next).toMatchObject({
        type: 'file',
        suggestedFocused: false,
        selectedFocused: true,
      })
    })
    it('moves focus from selected list to suggestions at bottom', () => {
      const state = createFileState({ selectedFocused: true, selectionIndex: 1 })
      const effect = getSuggestedSelectedListPopupEffect({
        popupType: 'file',
        popupState: state,
        itemsLength: 2,
        suggestions: ['a', 'b'],
        key: { downArrow: true },
        isBackspace: false,
      })
      expect(effect.type).toBe('set')
      if (effect.type !== 'set') {
        throw new Error('Expected set effect')
      }
      const next = effect.updater(state as PopupState)
      expect(next).toMatchObject({
        type: 'file',
        suggestedFocused: true,
        selectedFocused: false,
        suggestedSelectionIndex: 0,
      })
    })
    it('selects the current suggestion on Enter', () => {
      const state = createFileState({ suggestedFocused: true, suggestedSelectionIndex: 1 })
      const effect = getSuggestedSelectedListPopupEffect({
        popupType: 'file',
        popupState: state,
        itemsLength: 0,
        suggestions: ['a', 'b'],
        key: { return: true },
        isBackspace: false,
      })
      expect(effect).toEqual({ type: 'selectSuggestion', value: 'b' })
    })
    it('allows backspace-remove when draft is empty', () => {
      const state = createFileState({ draft: '', selectionIndex: 3 })
      const effect = getSuggestedSelectedListPopupEffect({
        popupType: 'file',
        popupState: state,
        itemsLength: 5,
        suggestions: [],
        key: {},
        isBackspace: true,
      })
      expect(effect).toEqual({ type: 'remove', index: 3 })
    })
  })
  describe('getSuggestedInputPopupEffect', () => {
    const createIntentState = (overrides: Partial<SuggestedInputPopupState> = {}) =>
      ({
        type: 'intent',
        draft: '',
        suggestedItems: [],
        suggestedSelectionIndex: 0,
        suggestedFocused: false,
        ...overrides,
      }) satisfies PopupState
    it('does not focus suggestions on Shift+Tab', () => {
      const state = createIntentState()
      const effect = getSuggestedInputPopupEffect({
        popupType: 'intent',
        popupState: state,
        suggestions: ['a'],
        key: { tab: true, shift: true },
      })
      expect(effect.type).toBe('none')
    })
    it('focuses suggestions on Down', () => {
      const state = createIntentState({ suggestedFocused: false })
      const effect = getSuggestedInputPopupEffect({
        popupType: 'intent',
        popupState: state,
        suggestions: ['a', 'b'],
        key: { downArrow: true },
      })
      expect(effect.type).toBe('set')
      if (effect.type !== 'set') {
        throw new Error('Expected set effect')
      }
      const next = effect.updater(state as PopupState)
      expect(next).toMatchObject({
        type: 'intent',
        suggestedFocused: true,
        suggestedSelectionIndex: 0,
      })
    })
    it('selects suggestion on Enter when suggestions focused', () => {
      const state = createIntentState({ suggestedFocused: true, suggestedSelectionIndex: 0 })
      const effect = getSuggestedInputPopupEffect({
        popupType: 'intent',
        popupState: state,
        suggestions: ['a', 'b'],
        key: { return: true },
      })
      expect(effect).toEqual({ type: 'selectSuggestion', value: 'a' })
    })
  })
})
</file>

<file path="src/__tests__/tui/theme-adapter.test.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import { adaptOpencodeThemeJson } from '../../tui/theme/theme-adapter'
import { validateThemeJson } from '../../tui/theme/theme-validate'
const FIXTURES_ROOT = path.join(__dirname, '..', '__fixtures__', 'themes')
const readFixtureJson = async (relativePath: string): Promise<unknown> => {
  const filePath = path.join(FIXTURES_ROOT, relativePath)
  const raw = await fs.readFile(filePath, 'utf8')
  return JSON.parse(raw) as unknown
}
describe('theme adapter', () => {
  test('does not adapt prompt-maker schema themes', async () => {
    const parsed = await readFixtureJson(path.join('global', 'ocean.json'))
    expect(adaptOpencodeThemeJson(parsed)).toBeNull()
  })
  test('adapts opencode-like themes into prompt-maker schema', async () => {
    const parsed = await readFixtureJson(path.join('adapt', 'opencode.json'))
    const adapted = adaptOpencodeThemeJson(parsed)
    expect(adapted).not.toBeNull()
    const validated = validateThemeJson(adapted)
    expect(validated.ok).toBe(true)
    if (!validated.ok) {
      throw new Error('Expected adapted theme to validate')
    }
    expect(validated.theme.defs?.surface).toBe('#0b2a32')
    expect(validated.theme.theme.textMuted).toBe('#94d2bd')
    expect(validated.theme.theme.backgroundPanel).toBe('surface')
    expect(validated.theme.theme.backgroundElement).toBe('#005f73')
    expect(validated.theme.theme.primary).toBe('#0a9396')
    expect(validated.theme.theme.mutedText).toBe('textMuted')
    expect(validated.theme.theme.panelBackground).toBe('backgroundPanel')
    expect(validated.theme.theme.popupBackground).toBe('panelBackground')
    expect(validated.theme.theme.accent).toBe('primary')
    expect(validated.theme.theme.accentText).toBe('background')
    expect(validated.theme.theme.selectionBackground).toBe('backgroundElement')
    expect(validated.theme.theme.selectionText).toBe('text')
    expect(validated.theme.theme.chipBackground).toBe('backgroundElement')
    expect(validated.theme.theme.chipText).toBe('text')
    expect(validated.theme.theme.chipMutedText).toBe('textMuted')
  })
})
</file>

<file path="src/__tests__/tui/theme-resolver.test.ts">
import { ansiToHex, resolveColor, resolveTheme } from '../../tui/theme/theme-resolver'
import type { ResolveColorContext } from '../../tui/theme/theme-resolver'
import type {
  MarkdownThemeSlot,
  RequiredThemeSlot,
  ThemeAppearanceMode,
  ThemeColorValue,
  ThemeJson,
} from '../../tui/theme/theme-types'
type ThemeKey = RequiredThemeSlot | MarkdownThemeSlot
const makeTheme = (
  overrides: Partial<Record<ThemeKey, ThemeColorValue>> = {},
  defs: Record<string, ThemeColorValue> = {},
): ThemeJson => {
  const base: Record<RequiredThemeSlot, ThemeColorValue> = {
    background: '#000000',
    text: '#ffffff',
    mutedText: '#888888',
    border: '#444444',
    accent: '#00ffff',
    accentText: '#000000',
    warning: '#ffff00',
    error: '#ff0000',
    success: '#00ff00',
    panelBackground: '#111111',
    popupBackground: '#111111',
    selectionBackground: '#333333',
    selectionText: '#ffffff',
    chipBackground: '#222222',
    chipText: '#ffffff',
    chipMutedText: '#aaaaaa',
  }
  return {
    defs,
    theme: {
      ...base,
      ...overrides,
    },
  }
}
describe('theme resolver', () => {
  test('hex pass-through', () => {
    const ctx: ResolveColorContext = { mode: 'dark', defs: {}, theme: {} }
    expect(resolveColor('#112233', ctx)).toBe('#112233')
  })
  test('variant selects by mode', () => {
    const ctxDark: ResolveColorContext = { mode: 'dark', defs: {}, theme: {} }
    const ctxLight: ResolveColorContext = { mode: 'light', defs: {}, theme: {} }
    expect(resolveColor({ dark: '#000000', light: '#ffffff' }, ctxDark)).toBe('#000000')
    expect(resolveColor({ dark: '#000000', light: '#ffffff' }, ctxLight)).toBe('#ffffff')
  })
  test('defs references resolve before theme slots', () => {
    const themeJson = makeTheme({ accent: 'accentDef' }, { accentDef: '#123456' })
    expect(resolveTheme(themeJson, 'dark').accent).toBe('#123456')
  })
  test('theme-slot references resolve recursively', () => {
    const themeJson = makeTheme({ text: 'background', background: '#101010' })
    expect(resolveTheme(themeJson, 'dark').text).toBe('#101010')
  })
  test('reference cycles throw with cycle path', () => {
    const themeJson = makeTheme({ background: 'text', text: 'background' })
    expect(() => resolveTheme(themeJson, 'dark')).toThrow(/Theme reference cycle detected:/)
    expect(() => resolveTheme(themeJson, 'dark')).toThrow(/theme\.background/)
    expect(() => resolveTheme(themeJson, 'dark')).toThrow(/theme\.text/)
  })
  test('missing required slots throw descriptive error', () => {
    const themeJson: ThemeJson = {
      theme: {
        background: '#000000',
      },
    }
    expect(() => resolveTheme(themeJson, 'dark')).toThrow(/missing required slots/i)
    expect(() => resolveTheme(themeJson, 'dark')).toThrow(/text/)
  })
  test('ANSI mapping converts 0..255 to hex', () => {
    expect(ansiToHex(0)).toBe('#000000')
    expect(ansiToHex(15)).toBe('#ffffff')
    expect(ansiToHex(196)).toBe('#ff0000')
    expect(ansiToHex(232)).toBe('#080808')
  })
  test('none/transparent normalize to undefined', () => {
    const ctx: ResolveColorContext = { mode: 'dark', defs: {}, theme: {} }
    expect(resolveColor('none', ctx)).toBeUndefined()
    expect(resolveColor('transparent', ctx)).toBeUndefined()
  })
  test('8-digit hex ignores alpha (00 => transparent)', () => {
    const ctx: ResolveColorContext = { mode: 'dark', defs: {}, theme: {} }
    expect(resolveColor('#11223300', ctx)).toBeUndefined()
    expect(resolveColor('#112233ff', ctx)).toBe('#112233')
  })
  test('markdown slots resolve when present', () => {
    const themeJson = makeTheme(
      {
        markdownHeading: 'accentDef',
        markdownCode: 196,
        markdownLink: 'transparent',
        markdownLinkText: { dark: 'text', light: 'background' },
        markdownCodeBlock: { dark: '#11223300', light: '#112233ff' },
      },
      { accentDef: '#123456' },
    )
    const resolvedDark = resolveTheme(themeJson, 'dark')
    expect(resolvedDark.markdownHeading).toBe('#123456')
    expect(resolvedDark.markdownCode).toBe('#ff0000')
    expect(resolvedDark.markdownLink).toBeUndefined()
    expect(resolvedDark.markdownLinkText).toBe('#ffffff')
    expect(resolvedDark.markdownCodeBlock).toBeUndefined()
    const resolvedLight = resolveTheme(themeJson, 'light')
    expect(resolvedLight.markdownLinkText).toBe('#000000')
    expect(resolvedLight.markdownCodeBlock).toBe('#112233')
  })
  test('mode type excludes system for resolution', () => {
    const mode: ThemeAppearanceMode = 'dark'
    const themeJson = makeTheme()
    expect(resolveTheme(themeJson, mode).background).toBe('#000000')
  })
})
</file>

<file path="src/__tests__/tui/theme-validate.test.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import {
  REQUIRED_THEME_SLOTS,
  type ThemeColorValue,
  type ThemeJson,
} from '../../tui/theme/theme-types'
import { validateThemeJson } from '../../tui/theme/theme-validate'
const FIXTURES_ROOT = path.join(__dirname, '..', '__fixtures__', 'themes')
const readFixtureJson = async (relativePath: string): Promise<unknown> => {
  const filePath = path.join(FIXTURES_ROOT, relativePath)
  const raw = await fs.readFile(filePath, 'utf8')
  return JSON.parse(raw) as unknown
}
const makeValidThemeJson = (): ThemeJson => {
  const theme: Record<string, ThemeColorValue> = {}
  for (const slot of REQUIRED_THEME_SLOTS) {
    theme[slot] = '#000000'
  }
  return { theme }
}
describe('theme validate', () => {
  test('rejects themes missing required slots (fixture)', async () => {
    const parsed = await readFixtureJson(
      path.join('project', '.prompt-maker-cli', 'themes', 'broken-theme.json'),
    )
    const validated = validateThemeJson(parsed)
    expect(validated).toEqual({
      ok: false,
      message:
        'Theme JSON missing required slots: text, mutedText, border, accent, accentText, warning, error, success, panelBackground, popupBackground, selectionBackground, selectionText, chipBackground, chipText, chipMutedText',
    })
  })
  test('rejects invalid defs color values', () => {
    const invalidVariant: unknown = { dark: '#000000', light: null }
    const validated = validateThemeJson({
      ...makeValidThemeJson(),
      defs: {
        bad: invalidVariant,
      },
    })
    expect(validated).toEqual({
      ok: false,
      message: 'Invalid defs color value for key: bad',
    })
  })
  test('rejects invalid required slot values', () => {
    const invalidVariant = { dark: '#000000', light: null } as unknown as ThemeColorValue
    const themeJson = makeValidThemeJson()
    const validated = validateThemeJson({
      ...themeJson,
      theme: {
        ...themeJson.theme,
        border: invalidVariant,
      },
    })
    expect(validated).toEqual({
      ok: false,
      message: 'Invalid theme color value for slot: border',
    })
  })
})
</file>

<file path="src/__tests__/tui/url-args.test.ts">
import { parseUrlArgs, validateHttpUrlCandidate } from '../../tui/screens/command/utils/url-args'
describe('url args', () => {
  test('parseUrlArgs splits on whitespace and commas', () => {
    expect(parseUrlArgs(' https://a.com https://b.com ')).toEqual([
      'https://a.com',
      'https://b.com',
    ])
    expect(parseUrlArgs('https://a.com,https://b.com')).toEqual(['https://a.com', 'https://b.com'])
    expect(parseUrlArgs('https://a.com, https://b.com\nhttps://c.com')).toEqual([
      'https://a.com',
      'https://b.com',
      'https://c.com',
    ])
  })
  test('validateHttpUrlCandidate validates HTTP(S)', () => {
    expect(validateHttpUrlCandidate('https://example.com').ok).toBe(true)
    expect(validateHttpUrlCandidate('http://example.com').ok).toBe(true)
    const empty = validateHttpUrlCandidate('   ')
    expect(empty.ok).toBe(false)
    const invalid = validateHttpUrlCandidate('notaurl')
    expect(invalid.ok).toBe(false)
    const protocol = validateHttpUrlCandidate('ftp://example.com')
    expect(protocol.ok).toBe(false)
  })
})
</file>

<file path="src/__tests__/tui/useDroppedFileDetection.test.ts">
import fs from 'node:fs'
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import { parseAbsolutePathFromInput } from '../../tui/drag-drop-path'
import { useDroppedFileDetection } from '../../tui/screens/command/hooks/useDroppedFileDetection'
const dom = new JSDOM('<!doctype html><html><body></body></html>')
type GlobalDom = { window: Window; document: Document }
beforeAll(() => {
  const target = globalThis as unknown as GlobalDom
  target.window = dom.window as unknown as Window
  target.document = dom.window.document
})
afterAll(() => {
  const target = globalThis as unknown as Partial<GlobalDom>
  delete target.window
  delete target.document
})
describe('useDroppedFileDetection', () => {
  describe('parseAbsolutePathFromInput (candidate parsing)', () => {
    it('parses quoted absolute paths with spaces', () => {
      expect(parseAbsolutePathFromInput('"/Users/alice/My File.md"')).toBe(
        '/Users/alice/My File.md',
      )
    })
    it('returns null when extra tokens exist', () => {
      expect(parseAbsolutePathFromInput('/file arg')).toBeNull()
    })
  })
  const createStats = (isFile: boolean): fs.Stats => {
    return { isFile: () => isFile } as unknown as fs.Stats
  }
  it('does not call stat when input has no absolute path', () => {
    const stat = jest.fn<Promise<fs.Stats>, [string]>(async () => createStats(true))
    const { result } = renderHook(() => useDroppedFileDetection('hello', { stat }))
    expect(result.current.droppedFilePath).toBeNull()
    expect(stat).not.toHaveBeenCalled()
  })
  it('resolves droppedFilePath once stat confirms file', async () => {
    const stat = jest.fn<Promise<fs.Stats>, [string]>(async () => createStats(true))
    const { result } = renderHook(() => useDroppedFileDetection('/tmp/file.txt', { stat }))
    expect(result.current.droppedFilePath).toBeNull()
    await act(async () => {
      await Promise.resolve()
    })
    expect(stat).toHaveBeenCalledTimes(1)
    expect(stat).toHaveBeenCalledWith('/tmp/file.txt')
    expect(result.current.droppedFilePath).toBe('/tmp/file.txt')
    expect(result.current.existsSync('/tmp/file.txt')).toBe(true)
    expect(result.current.isFilePath('/tmp/file.txt')).toBe(true)
  })
  it('treats existing non-files as not droppedFilePath', async () => {
    const stat = jest.fn<Promise<fs.Stats>, [string]>(async () => createStats(false))
    const { result } = renderHook(() => useDroppedFileDetection('/tmp', { stat }))
    await act(async () => {
      await Promise.resolve()
    })
    expect(result.current.droppedFilePath).toBeNull()
    expect(result.current.existsSync('/tmp')).toBe(true)
    expect(result.current.isFilePath('/tmp')).toBe(false)
  })
  it('caches stat results by path', async () => {
    const stat = jest.fn<Promise<fs.Stats>, [string]>(async () => createStats(true))
    const { result, rerender } = renderHook(
      ({ value }) => useDroppedFileDetection(value, { stat }),
      { initialProps: { value: '/tmp/file.txt' } },
    )
    await act(async () => {
      await Promise.resolve()
    })
    expect(result.current.droppedFilePath).toBe('/tmp/file.txt')
    expect(stat).toHaveBeenCalledTimes(1)
    rerender({ value: '/tmp/file.txt' })
    await act(async () => {
      await Promise.resolve()
    })
    expect(result.current.droppedFilePath).toBe('/tmp/file.txt')
    expect(stat).toHaveBeenCalledTimes(1)
  })
  it('returns exists=false for ENOENT paths', async () => {
    const error = new Error('missing') as Error & { code: string }
    error.code = 'ENOENT'
    const stat = jest.fn<Promise<fs.Stats>, [string]>(async () => {
      throw error
    })
    const { result } = renderHook(() => useDroppedFileDetection('/tmp/missing.txt', { stat }))
    await act(async () => {
      await Promise.resolve()
    })
    expect(result.current.droppedFilePath).toBeNull()
    expect(result.current.existsSync('/tmp/missing.txt')).toBe(false)
    expect(result.current.isFilePath('/tmp/missing.txt')).toBe(false)
  })
})
</file>

<file path="src/__tests__/app-container-keymap.test.ts">
import type { Key } from 'ink'
import { resolveAppContainerKeyAction } from '../tui/app-container-keymap'
const createKey = (overrides: Partial<Key> = {}): Key => overrides as Key
describe('resolveAppContainerKeyAction', () => {
  it('does not toggle help open on ?', () => {
    const action = resolveAppContainerKeyAction({
      input: '?',
      key: createKey({}),
      view: 'generate',
      isPopupOpen: false,
      isHelpOpen: false,
    })
    expect(action).toEqual({ type: 'none' })
  })
  it('closes help on Esc', () => {
    const action = resolveAppContainerKeyAction({
      input: '',
      key: createKey({ escape: true }),
      view: 'generate',
      isPopupOpen: false,
      isHelpOpen: true,
    })
    expect(action).toEqual({ type: 'toggle-help', nextIsHelpOpen: false })
  })
  it('does not close help on ?', () => {
    const action = resolveAppContainerKeyAction({
      input: '?',
      key: createKey({}),
      view: 'generate',
      isPopupOpen: false,
      isHelpOpen: true,
    })
    expect(action).toEqual({ type: 'none' })
  })
  it('swallows navigation keys while help is open', () => {
    const action = resolveAppContainerKeyAction({
      input: 'g',
      key: createKey({ ctrl: true }),
      view: 'tests',
      isPopupOpen: false,
      isHelpOpen: true,
    })
    expect(action).toEqual({ type: 'none' })
  })
  it('does not exit on Esc (generate view, no popup)', () => {
    const action = resolveAppContainerKeyAction({
      input: '',
      key: createKey({ escape: true }),
      view: 'generate',
      isPopupOpen: false,
      isHelpOpen: false,
    })
    expect(action).toEqual({ type: 'none' })
  })
  it('does not exit on Esc (generate view, popup open)', () => {
    const action = resolveAppContainerKeyAction({
      input: '',
      key: createKey({ escape: true }),
      view: 'generate',
      isPopupOpen: true,
      isHelpOpen: false,
    })
    expect(action).toEqual({ type: 'none' })
  })
  it('does not exit on Esc (tests view)', () => {
    const action = resolveAppContainerKeyAction({
      input: '',
      key: createKey({ escape: true }),
      view: 'tests',
      isPopupOpen: false,
      isHelpOpen: false,
    })
    expect(action).toEqual({ type: 'none' })
  })
  it('swallows Ctrl+G when a generate popup is open', () => {
    const action = resolveAppContainerKeyAction({
      input: 'g',
      key: createKey({ ctrl: true }),
      view: 'generate',
      isPopupOpen: true,
      isHelpOpen: false,
    })
    expect(action).toEqual({ type: 'none' })
  })
  it('swallows Ctrl+T when a generate popup is open', () => {
    const action = resolveAppContainerKeyAction({
      input: 't',
      key: createKey({ ctrl: true }),
      view: 'generate',
      isPopupOpen: true,
      isHelpOpen: false,
    })
    expect(action).toEqual({ type: 'none' })
  })
  it('switches to generate and opens palette on Ctrl+G from tests', () => {
    const action = resolveAppContainerKeyAction({
      input: 'g',
      key: createKey({ ctrl: true }),
      view: 'tests',
      isPopupOpen: false,
      isHelpOpen: false,
    })
    expect(action).toEqual({ type: 'switch-to-generate-and-open-command-palette' })
  })
})
</file>

<file path="src/__tests__/command-menu-keymap.test.ts">
import type { Key } from 'ink'
import { resolveCommandMenuKeyAction } from '../tui/components/core/command-menu-keymap'
const createKey = (overrides: Partial<Key> = {}): Key => overrides as Key
describe('resolveCommandMenuKeyAction', () => {
  it('wraps selection at boundaries', () => {
    expect(
      resolveCommandMenuKeyAction({
        key: createKey({ upArrow: true }),
        selectedIndex: 0,
        itemCount: 3,
      }),
    ).toEqual({ type: 'change-selection', nextIndex: 2 })
    expect(
      resolveCommandMenuKeyAction({
        key: createKey({ downArrow: true }),
        selectedIndex: 2,
        itemCount: 3,
      }),
    ).toEqual({ type: 'change-selection', nextIndex: 0 })
  })
  it('moves selection by one within the list', () => {
    expect(
      resolveCommandMenuKeyAction({
        key: createKey({ downArrow: true }),
        selectedIndex: 0,
        itemCount: 3,
      }),
    ).toEqual({ type: 'change-selection', nextIndex: 1 })
    expect(
      resolveCommandMenuKeyAction({
        key: createKey({ upArrow: true }),
        selectedIndex: 2,
        itemCount: 3,
      }),
    ).toEqual({ type: 'change-selection', nextIndex: 1 })
  })
  it('keeps selection stable for a single item', () => {
    expect(
      resolveCommandMenuKeyAction({
        key: createKey({ downArrow: true }),
        selectedIndex: 0,
        itemCount: 1,
      }),
    ).toEqual({ type: 'change-selection', nextIndex: 0 })
    expect(
      resolveCommandMenuKeyAction({
        key: createKey({ upArrow: true }),
        selectedIndex: 0,
        itemCount: 1,
      }),
    ).toEqual({ type: 'change-selection', nextIndex: 0 })
  })
  it('closes the palette on Esc', () => {
    expect(
      resolveCommandMenuKeyAction({
        key: createKey({ escape: true }),
        selectedIndex: 1,
        itemCount: 3,
      }),
    ).toEqual({ type: 'close' })
  })
  it('ignores keys when there are no items', () => {
    expect(
      resolveCommandMenuKeyAction({
        key: createKey({ downArrow: true }),
        selectedIndex: 0,
        itemCount: 0,
      }),
    ).toEqual({ type: 'none' })
  })
})
</file>

<file path="src/__tests__/compose-command.test.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
import { runComposeCommand } from '../compose-command'
describe('compose-command', () => {
  const originalExitCode = process.exitCode
  const tempDirs: string[] = []
  afterEach(async () => {
    await Promise.all(
      tempDirs.map(async (tempDir) => {
        await fs.rm(tempDir, { recursive: true, force: true })
      }),
    )
    tempDirs.splice(0, tempDirs.length)
    process.exitCode = originalExitCode
    jest.restoreAllMocks()
  })
  it('prints deterministic recipe + input composition', async () => {
    const tempDir = await fs.mkdtemp(path.join(os.tmpdir(), 'pmc-compose-'))
    tempDirs.push(tempDir)
    const recipePath = path.join(tempDir, 'recipe.yaml')
    await fs.writeFile(recipePath, 'recipe: demo\nsteps:\n  - one\n', 'utf8')
    const stdout = jest.spyOn(process.stdout, 'write').mockImplementation(() => true)
    const err = jest.spyOn(console, 'error').mockImplementation(() => undefined)
    await runComposeCommand(['--recipe', recipePath, '--input', 'hello'])
    expect(err).not.toHaveBeenCalled()
    expect(process.exitCode).toBe(originalExitCode)
    expect(stdout).toHaveBeenCalledWith('recipe: demo\nsteps:\n  - one\n---\nhello\n')
  })
  it('shows help output when requested', async () => {
    const stdout = jest.spyOn(process.stdout, 'write').mockImplementation(() => true)
    const stderr = jest.spyOn(process.stderr, 'write').mockImplementation(() => true)
    await runComposeCommand(['--help'])
    expect(process.exitCode).toBe(originalExitCode)
    expect(stdout.mock.calls.length + stderr.mock.calls.length).toBeGreaterThan(0)
  })
})
</file>

<file path="src/__tests__/config.test.ts">
import type { PromptMakerCliConfig } from '../config'
jest.mock('node:os', () => ({ homedir: jest.fn(() => '/home/tester') }))
jest.mock('node:fs/promises', () => ({ readFile: jest.fn() }))
const getFsMock = () =>
  jest.requireMock('node:fs/promises') as {
    readFile: jest.MockedFunction<(file: string, encoding: string) => Promise<string>>
  }
type ConfigModule = typeof import('../config')
const importConfigModule = async (): Promise<ConfigModule> => {
  jest.resetModules()
  return await import('../config')
}
const mockConfigJson = (config: PromptMakerCliConfig): string => JSON.stringify(config)
describe('config module', () => {
  beforeEach(() => {
    delete process.env.PROMPT_MAKER_CLI_CONFIG
    delete process.env.OPENAI_API_KEY
    delete process.env.OPENAI_BASE_URL
    delete process.env.GEMINI_API_KEY
    delete process.env.GEMINI_BASE_URL
  })
  it('loads config from explicit PROMPT_MAKER_CLI_CONFIG path', async () => {
    process.env.PROMPT_MAKER_CLI_CONFIG = '/tmp/custom.json'
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(
      mockConfigJson({ promptGenerator: { defaultModel: 'gpt-4o' } }),
    )
    const config = await loadCliConfig()
    expect(fs.readFile).toHaveBeenCalledWith('/tmp/custom.json', 'utf8')
    expect(config?.promptGenerator?.defaultModel).toBe('gpt-4o')
  })
  it('falls back through default locations while skipping missing files', async () => {
    const enoent = Object.assign(new Error('missing'), { code: 'ENOENT' })
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile
      .mockRejectedValueOnce(enoent)
      .mockResolvedValueOnce(mockConfigJson({ openaiApiKey: 'conf-key' }))
    const config = await loadCliConfig()
    expect(fs.readFile).toHaveBeenCalledTimes(2)
    expect(config?.openaiApiKey).toBe('conf-key')
  })
  it('caches the parsed config after the first load', async () => {
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(mockConfigJson({ openaiApiKey: 'cached' }))
    await loadCliConfig()
    await loadCliConfig()
    expect(fs.readFile).toHaveBeenCalledTimes(1)
  })
  it('throws a descriptive error for malformed JSON', async () => {
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce('not-json')
    await expect(loadCliConfig()).rejects.toThrow(/Failed to load config.*Unexpected token/)
  })
  it('resolveOpenAiCredentials prefers env variables', async () => {
    process.env.OPENAI_API_KEY = 'env-key'
    process.env.OPENAI_BASE_URL = 'https://api.example'
    const { resolveOpenAiCredentials } = await importConfigModule()
    const creds = await resolveOpenAiCredentials()
    expect(creds.apiKey).toBe('env-key')
    expect(creds.baseUrl).toBe('https://api.example')
  })
  it('resolveOpenAiCredentials falls back to config when env absent', async () => {
    const { resolveOpenAiCredentials } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(
      mockConfigJson({ openaiApiKey: 'file-key', openaiBaseUrl: 'https://conf' }),
    )
    const creds = await resolveOpenAiCredentials()
    expect(creds.apiKey).toBe('file-key')
    expect(creds.baseUrl).toBe('https://conf')
  })
  it('resolveOpenAiCredentials throws when no credentials available', async () => {
    const enoent = Object.assign(new Error('missing'), { code: 'ENOENT' })
    const { resolveOpenAiCredentials } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockRejectedValue(enoent)
    await expect(resolveOpenAiCredentials()).rejects.toThrow(/Missing OpenAI credentials/)
  })
  it('resolveGeminiCredentials uses env first', async () => {
    process.env.GEMINI_API_KEY = 'env-gem'
    process.env.GEMINI_BASE_URL = 'https://gem'
    const { resolveGeminiCredentials } = await importConfigModule()
    const creds = await resolveGeminiCredentials()
    expect(creds).toEqual({ apiKey: 'env-gem', baseUrl: 'https://gem' })
  })
  it('resolveGeminiCredentials uses config fallback', async () => {
    const { resolveGeminiCredentials } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(
      mockConfigJson({ geminiApiKey: 'file-gem', geminiBaseUrl: 'https://gemini.local' }),
    )
    const creds = await resolveGeminiCredentials()
    expect(creds).toEqual({ apiKey: 'file-gem', baseUrl: 'https://gemini.local' })
  })
  it('resolveGeminiCredentials throws when unresolved', async () => {
    const enoent = Object.assign(new Error('missing'), { code: 'ENOENT' })
    const { resolveGeminiCredentials } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockRejectedValue(enoent)
    await expect(resolveGeminiCredentials()).rejects.toThrow(/Missing Gemini credentials/)
  })
  it('parses promptGenerator.models entries when provided', async () => {
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(
      JSON.stringify({
        promptGenerator: {
          models: [
            {
              id: 'custom-model',
              label: 'Custom Model',
              provider: 'gemini',
              capabilities: 'multimodal',
              notes: 'Use for long context',
              default: true,
            },
          ],
        },
      }),
    )
    const config = await loadCliConfig()
    expect(config?.promptGenerator?.models).toEqual([
      {
        id: 'custom-model',
        label: 'Custom Model',
        provider: 'gemini',
        capabilities: ['multimodal'],
        notes: 'Use for long context',
        default: true,
      },
    ])
  })
  it('throws when promptGenerator.models is not an array', async () => {
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(
      JSON.stringify({ promptGenerator: { models: { id: 'bad' } } }),
    )
    await expect(loadCliConfig()).rejects.toThrow(/"promptGenerator\.models" must be an array/)
  })
  it('parses promptGenerator token budget settings when provided', async () => {
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(
      JSON.stringify({
        promptGenerator: {
          maxInputTokens: 10000,
          maxContextTokens: 5000,
          contextOverflowStrategy: 'drop-smart',
        },
      }),
    )
    const config = await loadCliConfig()
    expect(config?.promptGenerator?.maxInputTokens).toBe(10000)
    expect(config?.promptGenerator?.maxContextTokens).toBe(5000)
    expect(config?.promptGenerator?.contextOverflowStrategy).toBe('drop-smart')
  })
  it('rejects negative promptGenerator.maxInputTokens with a descriptive error', async () => {
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(JSON.stringify({ promptGenerator: { maxInputTokens: -1 } }))
    await expect(loadCliConfig()).rejects.toThrow(
      /promptGenerator\.maxInputTokens must be a positive integer/,
    )
  })
  it('rejects unknown promptGenerator.contextOverflowStrategy values', async () => {
    const { loadCliConfig } = await importConfigModule()
    const fs = getFsMock()
    fs.readFile.mockReset()
    fs.readFile.mockResolvedValueOnce(
      JSON.stringify({ promptGenerator: { contextOverflowStrategy: 'drop-new' } }),
    )
    await expect(loadCliConfig()).rejects.toThrow(
      /promptGenerator\.contextOverflowStrategy must be one of: fail, drop-smart, drop-url, drop-largest, drop-oldest/,
    )
  })
})
</file>

<file path="src/__tests__/context-budget.test.ts">
import { evaluateContextBudget, type ContextEntry } from '../generate/context-budget'
import type { TokenTelemetry } from '../generate/types'
const buildTelemetryFromNumericContent = (
  intentText: string,
  entries: Array<{ path: string; content: string }>,
  metaInstructions: string,
): TokenTelemetry => {
  const toInt = (value: string): number => {
    const parsed = Number(value)
    if (!Number.isFinite(parsed)) {
      return 0
    }
    return Math.trunc(parsed)
  }
  const files = entries.map((entry) => ({
    path: entry.path,
    tokens: toInt(entry.content),
  }))
  const fileTokens = files.reduce((acc, file) => acc + file.tokens, 0)
  const intentTokens = toInt(intentText)
  const systemTokens = toInt(metaInstructions)
  return {
    files,
    intentTokens,
    fileTokens,
    systemTokens,
    totalTokens: intentTokens + systemTokens + fileTokens,
  }
}
describe('evaluateContextBudget', () => {
  it('returns entries unchanged when budgets are not set', () => {
    const entries: ContextEntry[] = [
      { path: 'a', content: '3', source: 'file' },
      { path: 'b', content: '5', source: 'smart' },
    ]
    const result = evaluateContextBudget({
      intentText: '0',
      metaInstructions: '0',
      contextEntries: entries,
      buildTelemetry: buildTelemetryFromNumericContent,
    })
    expect(result.keptEntries).toEqual(entries)
    expect(result.droppedEntries).toEqual([])
    expect(result.after).toEqual(result.before)
  })
  it('throws with strategy=fail when over budget', () => {
    const entries: ContextEntry[] = [
      { path: 'a', content: '3', source: 'file' },
      { path: 'b', content: '5', source: 'smart' },
    ]
    expect(() =>
      evaluateContextBudget({
        intentText: '0',
        metaInstructions: '0',
        contextEntries: entries,
        maxContextTokens: 4,
        buildTelemetry: buildTelemetryFromNumericContent,
      }),
    ).toThrow(/Context token budget exceeded/)
  })
  it('drops smart entries first with drop-smart', () => {
    const entries: ContextEntry[] = [
      { path: 'file.md', content: '3', source: 'file' },
      { path: 'smart.md', content: '5', source: 'smart' },
      { path: 'url.md', content: '4', source: 'url' },
    ]
    const result = evaluateContextBudget({
      intentText: '0',
      metaInstructions: '0',
      contextEntries: entries,
      maxContextTokens: 7,
      strategy: 'drop-smart',
      buildTelemetry: buildTelemetryFromNumericContent,
    })
    expect(result.droppedPaths).toEqual([{ path: 'smart.md', source: 'smart' }])
    expect(result.keptEntries.map((entry) => entry.path)).toEqual(['file.md', 'url.md'])
    expect(result.after.fileTokens).toBe(7)
  })
  it('falls back to dropping oldest remaining after exhausting smart entries', () => {
    const entries: ContextEntry[] = [
      { path: 'file.md', content: '3', source: 'file' },
      { path: 'smart.md', content: '5', source: 'smart' },
    ]
    const result = evaluateContextBudget({
      intentText: '0',
      metaInstructions: '0',
      contextEntries: entries,
      maxContextTokens: 2,
      strategy: 'drop-smart',
      buildTelemetry: buildTelemetryFromNumericContent,
    })
    expect(result.droppedPaths).toEqual([
      { path: 'smart.md', source: 'smart' },
      { path: 'file.md', source: 'file' },
    ])
    expect(result.keptEntries).toEqual([])
    expect(result.after.fileTokens).toBe(0)
  })
  it('drops url entries first with drop-url', () => {
    const entries: ContextEntry[] = [
      { path: 'a', content: '4', source: 'file' },
      { path: 'b', content: '4', source: 'url' },
      { path: 'c', content: '4', source: 'file' },
    ]
    const result = evaluateContextBudget({
      intentText: '0',
      metaInstructions: '0',
      contextEntries: entries,
      maxContextTokens: 8,
      strategy: 'drop-url',
      buildTelemetry: buildTelemetryFromNumericContent,
    })
    expect(result.droppedPaths).toEqual([{ path: 'b', source: 'url' }])
    expect(result.keptEntries.map((entry) => entry.path)).toEqual(['a', 'c'])
  })
  it('drops largest token entries first with drop-largest', () => {
    const entries: ContextEntry[] = [
      { path: 'small', content: '2', source: 'file' },
      { path: 'big', content: '10', source: 'file' },
      { path: 'mid', content: '5', source: 'file' },
    ]
    const result = evaluateContextBudget({
      intentText: '0',
      metaInstructions: '0',
      contextEntries: entries,
      maxContextTokens: 6,
      strategy: 'drop-largest',
      buildTelemetry: buildTelemetryFromNumericContent,
    })
    expect(result.droppedPaths).toEqual([
      { path: 'big', source: 'file' },
      { path: 'mid', source: 'file' },
    ])
    expect(result.keptEntries.map((entry) => entry.path)).toEqual(['small'])
    expect(result.after.fileTokens).toBe(2)
  })
  it('drops oldest entries first with drop-oldest', () => {
    const entries: ContextEntry[] = [
      { path: 'first', content: '4', source: 'file' },
      { path: 'second', content: '4', source: 'file' },
      { path: 'third', content: '4', source: 'file' },
    ]
    const result = evaluateContextBudget({
      intentText: '0',
      metaInstructions: '0',
      contextEntries: entries,
      maxContextTokens: 8,
      strategy: 'drop-oldest',
      buildTelemetry: buildTelemetryFromNumericContent,
    })
    expect(result.droppedPaths).toEqual([{ path: 'first', source: 'file' }])
    expect(result.keptEntries.map((entry) => entry.path)).toEqual(['second', 'third'])
  })
  it('enforces maxInputTokens using remaining file token allowance', () => {
    const entries: ContextEntry[] = [
      { path: 'a', content: '2', source: 'file' },
      { path: 'b', content: '2', source: 'file' },
    ]
    const result = evaluateContextBudget({
      intentText: '4',
      metaInstructions: '3',
      contextEntries: entries,
      maxInputTokens: 10,
      strategy: 'drop-oldest',
      buildTelemetry: buildTelemetryFromNumericContent,
    })
    expect(result.keptEntries.map((entry) => entry.path)).toEqual(['b'])
    expect(result.after.totalTokens).toBe(9)
  })
  it('throws when maxInputTokens is below fixed overhead', () => {
    const entries: ContextEntry[] = [{ path: 'a', content: '1', source: 'file' }]
    expect(() =>
      evaluateContextBudget({
        intentText: '7',
        metaInstructions: '6',
        contextEntries: entries,
        maxInputTokens: 10,
        strategy: 'drop-oldest',
        buildTelemetry: buildTelemetryFromNumericContent,
      }),
    ).toThrow(/Unable to satisfy token budgets/)
  })
})
</file>

<file path="src/__tests__/export-command.test.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
import yaml from 'js-yaml'
import { runExportCommand } from '../export-command'
import { GENERATE_JSON_PAYLOAD_SCHEMA_VERSION, type GenerateJsonPayload } from '../generate/types'
describe('export-command', () => {
  const originalHome = process.env.HOME
  const originalExitCode = process.exitCode
  const tempHomes: string[] = []
  afterEach(async () => {
    await Promise.all(
      tempHomes.map(async (home) => {
        await fs.rm(home, { recursive: true, force: true })
      }),
    )
    tempHomes.splice(0, tempHomes.length)
    if (originalHome === undefined) {
      delete process.env.HOME
    } else {
      process.env.HOME = originalHome
    }
    process.exitCode = originalExitCode
    jest.restoreAllMocks()
  })
  const writeHistory = async (homeDir: string, lines: string[]): Promise<string> => {
    const historyPath = path.join(homeDir, '.config', 'prompt-maker-cli', 'history.jsonl')
    await fs.mkdir(path.dirname(historyPath), { recursive: true })
    await fs.writeFile(historyPath, `${lines.join('\n')}\n`, 'utf8')
    return historyPath
  }
  const createPayload = (overrides: Partial<GenerateJsonPayload>): GenerateJsonPayload => ({
    schemaVersion: GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
    intent: 'intent',
    model: 'model',
    targetModel: 'target-model',
    prompt: 'prompt',
    refinements: [],
    iterations: 1,
    interactive: false,
    timestamp: new Date(0).toISOString(),
    contextPaths: [{ path: 'file.txt', source: 'file' }],
    ...overrides,
  })
  it('exports the last history entry by default (JSON)', async () => {
    const tempHome = await fs.mkdtemp(path.join(os.tmpdir(), 'pmc-export-home-'))
    tempHomes.push(tempHome)
    process.env.HOME = tempHome
    const first = createPayload({ intent: 'first', timestamp: '2025-01-01T00:00:00.000Z' })
    const second = createPayload({ intent: 'second', timestamp: '2025-01-02T00:00:00.000Z' })
    await writeHistory(tempHome, [JSON.stringify(first), 'not json', JSON.stringify(second)])
    const outPath = path.join(tempHome, 'export.json')
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    const err = jest.spyOn(console, 'error').mockImplementation(() => undefined)
    await runExportCommand(['--format', 'json', '--out', outPath, '--quiet'])
    expect(log).not.toHaveBeenCalled()
    expect(err).not.toHaveBeenCalled()
    const written = await fs.readFile(outPath, 'utf8')
    expect(JSON.parse(written) as unknown).toEqual(second)
  })
  it('exports the N-th entry from end (YAML)', async () => {
    const tempHome = await fs.mkdtemp(path.join(os.tmpdir(), 'pmc-export-home-'))
    tempHomes.push(tempHome)
    process.env.HOME = tempHome
    const first = createPayload({ intent: 'first', timestamp: '2025-01-01T00:00:00.000Z' })
    const second = createPayload({ intent: 'second', timestamp: '2025-01-02T00:00:00.000Z' })
    const third = createPayload({ intent: 'third', timestamp: '2025-01-03T00:00:00.000Z' })
    await writeHistory(tempHome, [
      JSON.stringify(first),
      JSON.stringify(second),
      JSON.stringify(third),
    ])
    const outPath = path.join(tempHome, 'export.yaml')
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    const err = jest.spyOn(console, 'error').mockImplementation(() => undefined)
    await runExportCommand(['--from-history', '2', '--format', 'yaml', '--out', outPath, '--quiet'])
    expect(log).not.toHaveBeenCalled()
    expect(err).not.toHaveBeenCalled()
    const written = await fs.readFile(outPath, 'utf8')
    const parsed = yaml.load(written)
    expect(parsed as unknown).toEqual(second)
  })
  it('fails with a clear message for invalid selectors', async () => {
    const tempHome = await fs.mkdtemp(path.join(os.tmpdir(), 'pmc-export-home-'))
    tempHomes.push(tempHome)
    process.env.HOME = tempHome
    const first = createPayload({ intent: 'first', timestamp: '2025-01-01T00:00:00.000Z' })
    await writeHistory(tempHome, [JSON.stringify(first)])
    const outPath = path.join(tempHome, 'export.json')
    const err = jest.spyOn(console, 'error').mockImplementation(() => undefined)
    await runExportCommand([
      '--from-history',
      'last:0',
      '--format',
      'json',
      '--out',
      outPath,
      '--quiet',
    ])
    expect(process.exitCode).toBe(1)
    expect(err).toHaveBeenCalledWith(expect.stringContaining('Invalid --from-history selector'))
    await expect(fs.stat(outPath)).rejects.toThrow()
  })
  it('fails when the history file is missing', async () => {
    const tempHome = await fs.mkdtemp(path.join(os.tmpdir(), 'pmc-export-home-'))
    tempHomes.push(tempHome)
    process.env.HOME = tempHome
    const outPath = path.join(tempHome, 'export.json')
    const err = jest.spyOn(console, 'error').mockImplementation(() => undefined)
    await runExportCommand(['--format', 'json', '--out', outPath, '--quiet'])
    expect(process.exitCode).toBe(1)
    expect(err).toHaveBeenCalledWith(expect.stringContaining('History file not found'))
  })
})
</file>

<file path="src/__tests__/generate-stream-dispatcher.test.ts">
import { createStreamDispatcher } from '../generate/stream'
import type { StreamEventInput } from '../generate/types'
describe('createStreamDispatcher', () => {
  it('serializes context.overflow events with timestamps', () => {
    jest.useFakeTimers().setSystemTime(new Date('2024-01-01T00:00:00.000Z'))
    try {
      const chunks: string[] = []
      const dispatcher = createStreamDispatcher('jsonl', {
        writer: (chunk) => {
          chunks.push(chunk)
        },
      })
      const event: StreamEventInput = {
        event: 'context.overflow',
        strategy: 'drop-largest',
        before: {
          files: [],
          intentTokens: 200,
          fileTokens: 300,
          systemTokens: 700,
          totalTokens: 1200,
        },
        after: {
          files: [],
          intentTokens: 200,
          fileTokens: 100,
          systemTokens: 700,
          totalTokens: 1000,
        },
        droppedPaths: [{ path: 'docs/too-big.md', source: 'file' }],
      }
      dispatcher.emit(event)
      expect(chunks).toHaveLength(1)
      const serialized = chunks[0]?.trim()
      expect(serialized).toBeTruthy()
      const parsed = JSON.parse(serialized ?? '{}') as StreamEventInput & { timestamp: string }
      expect(parsed).toEqual(
        expect.objectContaining({
          event: 'context.overflow',
          strategy: 'drop-largest',
          timestamp: '2024-01-01T00:00:00.000Z',
          droppedPaths: [{ path: 'docs/too-big.md', source: 'file' }],
        }),
      )
    } finally {
      jest.useRealTimers()
    }
  })
})
</file>

<file path="src/__tests__/help-config.test.ts">
import { COMMAND_DESCRIPTORS } from '../tui/config'
import { createHelpSections } from '../tui/help-config'
describe('createHelpSections', () => {
  it('includes global shortcuts and help toggle', () => {
    const sections = createHelpSections({ commandDescriptors: COMMAND_DESCRIPTORS })
    const global = sections.find((section) => section.title === 'Global')
    expect(global).toBeDefined()
    expect(global?.lines.join(' ')).toContain('Ctrl+G')
    expect(global?.lines.join(' ')).toContain('Ctrl+T')
    expect(global?.lines.join(' ')).toContain('/help')
  })
  it('mentions every configured command', () => {
    const sections = createHelpSections({ commandDescriptors: COMMAND_DESCRIPTORS })
    const generate = sections.find((section) => section.title === 'Generate')
    expect(generate).toBeDefined()
    const combined = generate?.lines.join(' ') ?? ''
    for (const descriptor of COMMAND_DESCRIPTORS) {
      expect(combined).toContain(`/${descriptor.id}`)
    }
  })
})
</file>

<file path="src/__tests__/index.test.ts">
jest.mock('../compose-command', () => ({ runComposeCommand: jest.fn() }))
jest.mock('../generate-command', () => ({ runGenerateCommand: jest.fn() }))
jest.mock('../test-command', () => ({ runTestCommand: jest.fn() }))
jest.mock('../tui', () => ({ runTuiCommand: jest.fn() }))
const getComposeMock = () =>
  (jest.requireMock('../compose-command') as { runComposeCommand: jest.Mock }).runComposeCommand
const getGenerateMock = () =>
  (jest.requireMock('../generate-command') as { runGenerateCommand: jest.Mock }).runGenerateCommand
const getTestMock = () =>
  (jest.requireMock('../test-command') as { runTestCommand: jest.Mock }).runTestCommand
const getTuiMock = () => (jest.requireMock('../tui') as { runTuiCommand: jest.Mock }).runTuiCommand
describe('CLI entrypoint command routing', () => {
  const originalArgv = [...process.argv]
  afterAll(() => {
    process.argv = originalArgv
  })
  const importCli = async (): Promise<void> => {
    await jest.isolateModulesAsync(async () => {
      await import('../index')
    })
  }
  it('invokes ui when no args are provided', async () => {
    const runTuiCommand = getTuiMock()
    const runGenerateCommand = getGenerateMock()
    runTuiCommand.mockClear()
    runGenerateCommand.mockClear()
    process.argv = ['node', 'cli']
    await importCli()
    expect(runTuiCommand).toHaveBeenCalledWith([])
    expect(runGenerateCommand).not.toHaveBeenCalled()
  })
  it('routes to explicit ui subcommand', async () => {
    const runTuiCommand = getTuiMock()
    runTuiCommand.mockClear()
    process.argv = ['node', 'cli', 'ui', '--verbose']
    await importCli()
    expect(runTuiCommand).toHaveBeenCalledWith(['--verbose'])
  })
  it('routes to test subcommand', async () => {
    const runTestCommand = getTestMock()
    runTestCommand.mockClear()
    process.argv = ['node', 'cli', 'test', '--watch']
    await importCli()
    expect(runTestCommand).toHaveBeenCalledWith(['--watch'])
  })
  it('routes to compose subcommand', async () => {
    const runComposeCommand = getComposeMock()
    runComposeCommand.mockClear()
    process.argv = ['node', 'cli', 'compose', '--recipe', 'recipe.yaml', '--input', 'hello']
    await importCli()
    expect(runComposeCommand).toHaveBeenCalledWith(['--recipe', 'recipe.yaml', '--input', 'hello'])
  })
  it('treats generate alias as generate command', async () => {
    const runGenerateCommand = getGenerateMock()
    runGenerateCommand.mockClear()
    process.argv = ['node', 'cli', 'generate', 'foo']
    await importCli()
    expect(runGenerateCommand).toHaveBeenCalledWith(['foo'])
  })
  it('treats expand alias as generate command', async () => {
    const runGenerateCommand = getGenerateMock()
    runGenerateCommand.mockClear()
    process.argv = ['node', 'cli', 'expand', 'bar']
    await importCli()
    expect(runGenerateCommand).toHaveBeenCalledWith(['bar'])
  })
  it('falls back to generate when first arg is a flag', async () => {
    const runGenerateCommand = getGenerateMock()
    const runTuiCommand = getTuiMock()
    runGenerateCommand.mockClear()
    runTuiCommand.mockClear()
    process.argv = ['node', 'cli', '--json']
    await importCli()
    expect(runGenerateCommand).toHaveBeenCalledWith(['--json'])
    expect(runTuiCommand).not.toHaveBeenCalled()
  })
})
</file>

<file path="src/__tests__/list-window.test.ts">
import { resolveListPopupHeights } from '../tui/components/popups/list-popup-layout'
import { resolveCursorWindow, resolveWindowedList } from '../tui/components/popups/list-window'
describe('resolveCursorWindow', () => {
  it('returns empty range for invalid sizes', () => {
    expect(resolveCursorWindow(10, 5, 0)).toEqual({ startIndex: 0, endIndexExclusive: 0 })
    expect(resolveCursorWindow(0, 0, 5)).toEqual({ startIndex: 0, endIndexExclusive: 0 })
  })
  it('windows near the start', () => {
    expect(resolveCursorWindow(10, 0, 5)).toEqual({ startIndex: 0, endIndexExclusive: 5 })
    expect(resolveCursorWindow(10, 1, 5)).toEqual({ startIndex: 0, endIndexExclusive: 5 })
  })
  it('windows in the middle with a 2-row lead', () => {
    // With lead=2, cursorIndex=5 should put the window start at 3.
    expect(resolveCursorWindow(10, 5, 5)).toEqual({ startIndex: 3, endIndexExclusive: 8 })
  })
  it('windows near the end', () => {
    expect(resolveCursorWindow(10, 9, 5)).toEqual({ startIndex: 5, endIndexExclusive: 10 })
  })
  it('shows all items when list is smaller than window', () => {
    expect(resolveCursorWindow(3, 1, 10)).toEqual({ startIndex: 0, endIndexExclusive: 3 })
  })
})
describe('resolveWindowedList', () => {
  it('shows all items when they fit', () => {
    expect(
      resolveWindowedList({
        itemCount: 2,
        selectedIndex: 1,
        maxVisibleRows: 5,
      }),
    ).toEqual({ start: 0, end: 2, showBefore: false, showAfter: false })
  })
  it('windows a long list with indicators', () => {
    const result = resolveWindowedList({
      itemCount: 10,
      selectedIndex: 9,
      maxVisibleRows: 5,
      lead: 2,
    })
    expect(result.showBefore).toBe(true)
    expect(result.showAfter).toBe(false)
    expect(result.end).toBe(10)
    expect(result.start).toBeGreaterThanOrEqual(0)
    expect(result.start).toBeLessThan(result.end)
  })
  it('returns empty window for invalid sizes', () => {
    expect(resolveWindowedList({ itemCount: 5, selectedIndex: 2, maxVisibleRows: 0 })).toEqual({
      start: 0,
      end: 0,
      showBefore: false,
      showAfter: false,
    })
  })
})
describe('resolveListPopupHeights', () => {
  it('allocates rows for file popup height 16', () => {
    expect(
      resolveListPopupHeights({ maxHeight: 16, hasSuggestions: true, instructionRows: 1 }),
    ).toEqual({
      selectedRows: 3,
      suggestionRows: 4,
    })
  })
  it('prefers selected rows on small heights', () => {
    expect(
      resolveListPopupHeights({ maxHeight: 10, hasSuggestions: true, instructionRows: 1 }),
    ).toEqual({
      selectedRows: 1,
      suggestionRows: 0,
    })
  })
  it('reduces rows when instructions wrap', () => {
    expect(
      resolveListPopupHeights({ maxHeight: 16, hasSuggestions: true, instructionRows: 2 }),
    ).toEqual({
      selectedRows: 3,
      suggestionRows: 3,
    })
  })
  it('keeps defaults when suggestions are absent', () => {
    expect(
      resolveListPopupHeights({ maxHeight: undefined, hasSuggestions: false, instructionRows: 1 }),
    ).toEqual({
      selectedRows: 6,
      suggestionRows: 0,
    })
  })
})
</file>

<file path="src/__tests__/message-builders.test.ts">
import type { MessageContent } from '@prompt-maker/core'
import { buildInitialUserMessage } from '../prompt-generator/message-builders'
jest.mock('../image-loader', () => ({
  resolveImageParts: jest.fn(),
}))
jest.mock('../prompt-generator/video-parts', () => ({
  resolveVideoParts: jest.fn(),
}))
jest.mock('../prompt-generator/pdf-parts', () => ({
  resolvePdfParts: jest.fn(),
}))
const { resolveImageParts } = jest.requireMock('../image-loader') as {
  resolveImageParts: jest.Mock
}
const { resolveVideoParts } = jest.requireMock('../prompt-generator/video-parts') as {
  resolveVideoParts: jest.Mock
}
const { resolvePdfParts } = jest.requireMock('../prompt-generator/pdf-parts') as {
  resolvePdfParts: jest.Mock
}
describe('message-builders', () => {
  beforeEach(() => {
    jest.clearAllMocks()
  })
  it('merges parts in deterministic order: images → videos → PDFs → text', async () => {
    resolveImageParts.mockResolvedValue([{ type: 'image', mimeType: 'image/png', data: 'img' }])
    resolveVideoParts.mockResolvedValue([
      { type: 'video_uri', mimeType: 'video/mp4', fileUri: 'gs://v' },
    ])
    resolvePdfParts.mockResolvedValue([
      { type: 'pdf', mimeType: 'application/pdf', filePath: 'doc.pdf', fileUri: 'gs://p' },
    ])
    const content = await buildInitialUserMessage(
      'Do a thing',
      [],
      ['img.png'],
      ['clip.mp4'],
      ['doc.pdf'],
    )
    expect(Array.isArray(content)).toBe(true)
    const parts = content as Exclude<MessageContent, string>
    expect(parts.map((part) => part.type)).toEqual(['image', 'video_uri', 'pdf', 'text'])
  })
  it('returns plain text when no media parts exist', async () => {
    resolveImageParts.mockResolvedValue([])
    resolveVideoParts.mockResolvedValue([])
    resolvePdfParts.mockResolvedValue([])
    const content = await buildInitialUserMessage('Do a thing', [], [], [], [])
    expect(typeof content).toBe('string')
  })
})
</file>

<file path="src/__tests__/model-manager.test.ts">
const mockReadFile = jest.fn()
const mockMkdir = jest.fn()
const mockWriteFile = jest.fn()
jest.mock('node:fs/promises', () => ({
  readFile: (...args: unknown[]) => mockReadFile(...args),
  mkdir: (...args: unknown[]) => mockMkdir(...args),
  writeFile: (...args: unknown[]) => mockWriteFile(...args),
}))
describe('model-manager', () => {
  beforeEach(() => {
    jest.resetModules()
    mockReadFile.mockReset()
    mockMkdir.mockReset()
    mockWriteFile.mockReset()
    jest.spyOn(console, 'error').mockImplementation(() => undefined)
  })
  it('returns cached models when cache is fresh', async () => {
    const now = 1_700_000_000_000
    mockReadFile.mockResolvedValueOnce(
      JSON.stringify({
        timestamp: now - 1_000,
        models: { openai: ['gpt-4o'], gemini: ['gemini-1.5-pro'] },
      }),
    )
    const mockFetch = jest.fn()
    const { getAvailableModels } = await import('../utils/model-manager')
    const result = await getAvailableModels('openai-key', 'gemini-key', {
      cacheFilePath: '/tmp/models-cache.json',
      now,
      fetchImpl: mockFetch,
    })
    expect(result).toEqual({ openai: ['gpt-4o'], gemini: ['gemini-1.5-pro'] })
    expect(mockFetch).not.toHaveBeenCalled()
    expect(mockWriteFile).not.toHaveBeenCalled()
  })
  it('refreshes a stale cache and writes the updated file', async () => {
    const now = 1_700_000_000_000
    mockReadFile.mockResolvedValueOnce(
      JSON.stringify({
        timestamp: 0,
        models: { openai: ['gpt-legacy'], gemini: ['gemini-legacy'] },
      }),
    )
    const mockFetch = jest.fn(async (url: RequestInfo | URL) => {
      const href = typeof url === 'string' ? url : url.toString()
      if (href.includes('openai.com')) {
        return {
          ok: true,
          status: 200,
          statusText: 'OK',
          json: async () => ({ data: [{ id: 'gpt-4o' }, { id: 'whisper-1' }, { id: 'o1-mini' }] }),
        } as unknown as Response
      }
      return {
        ok: true,
        status: 200,
        statusText: 'OK',
        json: async () => ({
          models: [
            {
              name: 'models/gemini-1.5-pro',
              supportedGenerationMethods: ['generateContent'],
            },
            {
              name: 'models/embedding-001',
              supportedGenerationMethods: ['embedContent'],
            },
          ],
        }),
      } as unknown as Response
    })
    const { getAvailableModels } = await import('../utils/model-manager')
    const result = await getAvailableModels('openai-key', 'gemini-key', {
      cacheFilePath: '/tmp/models-cache.json',
      now,
      fetchImpl: mockFetch,
    })
    expect(result.openai).toEqual(['gpt-4o', 'o1-mini'])
    expect(result.gemini).toEqual(['gemini-1.5-pro'])
    expect(mockMkdir).toHaveBeenCalled()
    expect(mockWriteFile).toHaveBeenCalled()
  })
  it('avoids writes and network calls without keys', async () => {
    mockReadFile.mockRejectedValueOnce(new Error('missing'))
    const mockFetch = jest.fn()
    const { FALLBACK_MODELS, getAvailableModels } = await import('../utils/model-manager')
    const result = await getAvailableModels(null, undefined, {
      cacheFilePath: '/tmp/models-cache.json',
      now: 1_700_000_000_000,
      fetchImpl: mockFetch,
    })
    expect(result).toEqual(FALLBACK_MODELS)
    expect(mockFetch).not.toHaveBeenCalled()
    expect(mockWriteFile).not.toHaveBeenCalled()
  })
  it('filters video-capable Gemini models by token limit and fileData support', async () => {
    const mockFetch = jest.fn(async (url: RequestInfo | URL, init?: RequestInit) => {
      const href = typeof url === 'string' ? url : url.toString()
      if (href.includes('/v1beta/models?')) {
        return {
          ok: true,
          status: 200,
          statusText: 'OK',
          json: async () => ({
            models: [
              {
                name: 'models/gemini-3-pro-preview',
                supportedGenerationMethods: ['generateContent'],
                inputTokenLimit: 1_000_000,
              },
              {
                name: 'models/gemini-2.5-pro',
                supportedGenerationMethods: ['generateContent'],
                inputTokenLimit: 1_000_000,
              },
              {
                name: 'models/gemini-2.0-flash',
                supportedGenerationMethods: ['generateContent'],
                inputTokenLimit: 32_000,
              },
            ],
          }),
        } as unknown as Response
      }
      if (href.includes('gemini-3-pro-preview:generateContent')) {
        return {
          ok: false,
          status: 404,
          statusText: 'Not Found',
          text: async () =>
            JSON.stringify({
              error: { code: 404, message: 'Requested entity was not found.', status: 'NOT_FOUND' },
            }),
        } as unknown as Response
      }
      if (href.includes('gemini-2.5-pro:generateContent')) {
        const parsed = init?.body ? (JSON.parse(String(init.body)) as any) : null
        const fileUri = parsed?.contents?.[0]?.parts?.[0]?.fileData?.fileUri
        if (fileUri !== 'INVALID_FILE_URI') {
          throw new Error('Expected probe to include INVALID_FILE_URI.')
        }
        return {
          ok: false,
          status: 400,
          statusText: 'Bad Request',
          text: async () =>
            JSON.stringify({
              error: { code: 400, message: 'Invalid file uri.', status: 'INVALID_ARGUMENT' },
            }),
        } as unknown as Response
      }
      throw new Error(`Unexpected fetch url: ${href}`)
    })
    const { getVideoCapableGeminiModels } = await import('../utils/model-manager')
    const result = await getVideoCapableGeminiModels('gemini-key', {
      fetchImpl: mockFetch as unknown as typeof fetch,
    })
    expect(result).toEqual(['gemini-2.5-pro'])
  })
})
</file>

<file path="src/__tests__/pdf-parts.test.ts">
import { resolvePdfParts } from '../prompt-generator/pdf-parts'
jest.mock('node:fs/promises', () => ({
  access: jest.fn(),
}))
jest.mock('../media-loader', () => ({
  inferPdfMimeType: jest.fn(),
  uploadFileForGeminiWithMimeType: jest.fn(),
}))
const fs = jest.requireMock('node:fs/promises') as { access: jest.Mock }
const mediaLoader = jest.requireMock('../media-loader') as {
  inferPdfMimeType: jest.Mock
  uploadFileForGeminiWithMimeType: jest.Mock
}
describe('pdf-parts', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    fs.access.mockResolvedValue(undefined)
    mediaLoader.inferPdfMimeType.mockReturnValue('application/pdf')
    mediaLoader.uploadFileForGeminiWithMimeType.mockResolvedValue('gs://pdf/123')
  })
  it('creates local PDF parts when no apiKey provided', async () => {
    const onUpload = jest.fn()
    const parts = await resolvePdfParts(['doc.pdf'], onUpload)
    expect(mediaLoader.uploadFileForGeminiWithMimeType).not.toHaveBeenCalled()
    expect(parts).toEqual([{ type: 'pdf', mimeType: 'application/pdf', filePath: 'doc.pdf' }])
    expect(onUpload).toHaveBeenNthCalledWith(1, 'start', { kind: 'pdf', filePath: 'doc.pdf' })
    expect(onUpload).toHaveBeenNthCalledWith(2, 'finish', { kind: 'pdf', filePath: 'doc.pdf' })
  })
  it('uploads PDFs for Gemini when apiKey is provided', async () => {
    const parts = await resolvePdfParts(['doc.pdf'], undefined, 'gem-key')
    expect(mediaLoader.uploadFileForGeminiWithMimeType).toHaveBeenCalledWith(
      'doc.pdf',
      'application/pdf',
      'gem-key',
    )
    expect(parts).toEqual([
      { type: 'pdf', mimeType: 'application/pdf', filePath: 'doc.pdf', fileUri: 'gs://pdf/123' },
    ])
  })
  it('throws actionable errors for unreadable PDFs', async () => {
    fs.access.mockRejectedValue(new Error('denied'))
    const onUpload = jest.fn()
    await expect(resolvePdfParts(['doc.pdf'], onUpload)).rejects.toThrow('doc.pdf')
    await expect(resolvePdfParts(['doc.pdf'], onUpload)).rejects.toThrow('not readable')
    expect(onUpload).toHaveBeenNthCalledWith(1, 'start', { kind: 'pdf', filePath: 'doc.pdf' })
    expect(onUpload).toHaveBeenNthCalledWith(2, 'finish', { kind: 'pdf', filePath: 'doc.pdf' })
  })
  it('throws actionable errors for unsupported paths', async () => {
    mediaLoader.inferPdfMimeType.mockImplementation(() => {
      throw new Error('Unsupported PDF type')
    })
    await expect(resolvePdfParts(['doc.txt'])).rejects.toThrow('doc.txt')
    await expect(resolvePdfParts(['doc.txt'])).rejects.toThrow('Unsupported PDF type')
  })
})
</file>

<file path="src/__tests__/prompt-generator-service.test.ts">
import { callLLM } from '@prompt-maker/core'
import {
  PromptGeneratorService,
  resolveDefaultGenerateModel,
  ensureModelCredentials,
  isGemini,
} from '../prompt-generator-service'
jest.mock('@prompt-maker/core', () => ({ callLLM: jest.fn() }))
jest.mock('../config', () => ({
  loadCliConfig: jest.fn().mockResolvedValue({
    promptGenerator: { defaultModel: 'gpt-4o-mini', defaultGeminiModel: 'gemini-1.5-pro' },
  }),
  resolveOpenAiCredentials: jest
    .fn()
    .mockResolvedValue({ apiKey: 'OPENAI', baseUrl: 'https://openai' }),
  resolveGeminiCredentials: jest
    .fn()
    .mockResolvedValue({ apiKey: 'GEM', baseUrl: 'https://gemini' }),
}))
jest.mock('../image-loader', () => ({
  resolveImageParts: jest
    .fn()
    .mockResolvedValue([{ type: 'image', mimeType: 'image/png', data: 'aaa' }]),
}))
jest.mock('../prompt-generator/pdf-parts', () => ({
  resolvePdfParts: jest
    .fn()
    .mockResolvedValue([{ type: 'pdf', mimeType: 'application/pdf', filePath: 'doc.pdf' }]),
}))
const { resolvePdfParts } = jest.requireMock('../prompt-generator/pdf-parts') as {
  resolvePdfParts: jest.Mock
}
jest.mock('../media-loader', () => ({
  uploadFileForGemini: jest.fn().mockResolvedValue('gs://video'),
  uploadFileForGeminiWithMimeType: jest.fn().mockResolvedValue('gs://pdf'),
  inferVideoMimeType: jest.fn().mockReturnValue('video/mp4'),
  inferPdfMimeType: jest.fn().mockReturnValue('application/pdf'),
}))
const { resolveImageParts } = jest.requireMock('../image-loader') as {
  resolveImageParts: jest.Mock
}
const mediaLoader = jest.requireMock('../media-loader') as {
  uploadFileForGemini: jest.Mock
  uploadFileForGeminiWithMimeType: jest.Mock
  inferVideoMimeType: jest.Mock
  inferPdfMimeType: jest.Mock
}
const configModule = jest.requireMock('../config') as {
  loadCliConfig: jest.Mock
  resolveOpenAiCredentials: jest.Mock
  resolveGeminiCredentials: jest.Mock
}
const callLLMMock = callLLM as jest.MockedFunction<typeof callLLM>
describe('prompt-generator-service helpers', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    configModule.loadCliConfig.mockResolvedValue({
      promptGenerator: { defaultModel: 'gpt-4o-mini', defaultGeminiModel: 'gemini-1.5-pro' },
    })
    configModule.resolveOpenAiCredentials.mockResolvedValue({
      apiKey: 'OPENAI',
      baseUrl: 'https://openai',
    })
    configModule.resolveGeminiCredentials.mockResolvedValue({
      apiKey: 'GEM',
      baseUrl: 'https://gemini',
    })
    resolveImageParts.mockResolvedValue([{ type: 'image', mimeType: 'image/png', data: 'aaa' }])
  })
  it('detects gemini models via isGemini', () => {
    expect(isGemini('gemini-1.5-pro')).toBe(true)
    expect(isGemini('gemma-2b')).toBe(true)
    expect(isGemini('gpt-4o-mini')).toBe(false)
  })
  it('resolveDefaultGenerateModel prefers config before env', async () => {
    process.env.PROMPT_MAKER_GENERATE_MODEL = 'env-model'
    const model = await resolveDefaultGenerateModel()
    expect(model).toBe('gpt-4o-mini')
  })
  it('ensureModelCredentials sets OpenAI env vars when missing', async () => {
    delete process.env.OPENAI_API_KEY
    await ensureModelCredentials('gpt-4o-mini')
    expect(process.env.OPENAI_API_KEY).toBe('OPENAI')
    expect(process.env.OPENAI_BASE_URL).toBe('https://openai')
  })
  it('ensureModelCredentials sets Gemini env vars when needed', async () => {
    delete process.env.GEMINI_API_KEY
    await ensureModelCredentials('gemini-1.5-pro')
    expect(process.env.GEMINI_API_KEY).toBe('GEM')
    expect(process.env.GEMINI_BASE_URL).toBe('https://gemini')
  })
})
describe('PromptGeneratorService.generatePrompt', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    callLLMMock.mockReset()
    configModule.loadCliConfig.mockResolvedValue({
      promptGenerator: { defaultModel: 'gpt-4o-mini', defaultGeminiModel: 'gemini-1.5-pro' },
    })
    callLLMMock.mockResolvedValue('{"prompt":"Result","reasoning":"ok"}')
    resolveImageParts.mockResolvedValue([{ type: 'image', mimeType: 'image/png', data: 'aaa' }])
    mediaLoader.uploadFileForGemini.mockResolvedValue('gs://video')
    mediaLoader.inferVideoMimeType.mockReturnValue('video/mp4')
  })
  const buildService = async () => new PromptGeneratorService()
  it('constructs initial generation request with context and media', async () => {
    const service = await buildService()
    const prompt = await service.generatePrompt({
      intent: 'Do a thing',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [{ path: 'ctx.md', content: 'context' }],
      images: ['image.png'],
      videos: ['clip.mp4'],
    })
    expect(resolveImageParts).toHaveBeenCalledWith(['image.png'], undefined)
    expect(mediaLoader.uploadFileForGemini).toHaveBeenCalledWith('clip.mp4')
    expect(callLLM).toHaveBeenCalledWith(
      expect.arrayContaining([
        expect.objectContaining({ role: 'system' }),
        expect.objectContaining({ role: 'user' }),
      ]),
      'gpt-4o-mini',
    )
    expect(prompt).toContain('Result')
  })
  it('includes meta instructions and keeps target guidance internal', async () => {
    const service = await buildService()
    await service.generatePrompt({
      intent: 'Do a thing',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [],
      images: [],
      videos: [],
      metaInstructions: 'Be concise',
    })
    const messagePayload = callLLMMock.mock.calls[0]?.[0] as Array<{
      role: string
      content: unknown
    }>
    const systemMessages = messagePayload.filter((msg) => msg.role === 'system')
    const userMessage = messagePayload.find((msg) => msg.role === 'user')
    const userPayloadText = JSON.stringify(userMessage?.content)
    expect(userPayloadText).toContain('Meta-Instructions:\\nBe concise')
    expect(userPayloadText).not.toMatch(/target runtime model/i)
    expect(userPayloadText).not.toContain('gpt-4o-mini')
    const systemPayloadText = JSON.stringify(systemMessages.map((msg) => msg.content))
    expect(systemPayloadText).toContain('targetRuntimeModel: gpt-4o-mini')
    expect(systemPayloadText).toMatch(/do not include phrases like/i)
    expect(systemPayloadText).toMatch(/only include the target model/i)
  })
  it('sanitizes target model leakage from model output', async () => {
    callLLMMock.mockResolvedValueOnce(
      JSON.stringify({
        reasoning: 'x',
        prompt: 'Line 1\nTarget runtime model for executing: **GPT-5.2**\nUse gpt-5.2.',
      }),
    )
    const service = await buildService()
    const prompt = await service.generatePrompt({
      intent: 'Write a prompt about keyboard shortcuts',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-5.2',
      fileContext: [],
      images: [],
      videos: [],
    })
    expect(prompt.toLowerCase()).not.toContain('target runtime model')
    expect(prompt.toLowerCase()).not.toContain('gpt-5.2')
  })
  it('keeps target model mentions when user intent includes it', async () => {
    callLLMMock.mockResolvedValueOnce(
      JSON.stringify({
        reasoning: 'x',
        prompt: 'This prompt must mention gpt-5.2 explicitly.',
      }),
    )
    const service = await buildService()
    const prompt = await service.generatePrompt({
      intent: 'Write a prompt and explicitly mention gpt-5.2.',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-5.2',
      fileContext: [],
      images: [],
      videos: [],
    })
    expect(prompt).toContain('gpt-5.2')
  })
  it('handles refinement flows with previous prompt', async () => {
    const service = await buildService()
    await service.generatePrompt({
      intent: 'Original',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [],
      images: [],
      videos: [],
      previousPrompt: 'draft',
      refinementInstruction: 'shorter',
    })
    const call = callLLMMock.mock.calls[0]?.[0]
    const userMessage = call?.find((msg: { role: string }) => msg.role === 'user')
    expect(userMessage?.content).toEqual(
      expect.arrayContaining([
        expect.objectContaining({
          type: 'text',
          text: expect.stringContaining('Current Prompt Draft'),
        }),
      ]),
    )
  })
  it('auto-refines generic PDF prompt contracts to be grounded', async () => {
    const pdfPath =
      '/Users/eroberts/Downloads/BP-Adopt AI-based Browser Automation-080126-222458.pdf'
    callLLMMock
      .mockResolvedValueOnce(
        JSON.stringify({
          reasoning: 'x',
          prompt:
            '# Title\n\nMake document concise\n\n## Inputs\n- Primary document: already provided in context\n',
        }),
      )
      .mockResolvedValueOnce(
        JSON.stringify({
          reasoning: 'y',
          prompt:
            '# Title\n\nRewrite attached PDF\n\nDocument Snapshot\n- Topic: Browser automation\n\n"This is a verbatim quote from the PDF content."\n\nInputs\n- Attached PDF: BP-Adopt AI-based Browser Automation-080126-222458.pdf\n',
        }),
      )
    const service = await buildService()
    const prompt = await service.generatePrompt({
      intent: 'Please make this document more succinct',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-5.2',
      fileContext: [],
      images: [],
      videos: [],
      pdfs: [pdfPath],
    })
    expect(callLLM).toHaveBeenCalledTimes(2)
    expect(resolvePdfParts).toHaveBeenCalledTimes(1)
    const secondCallMessages = callLLMMock.mock.calls[1]?.[0] as Array<{
      role: string
      content: unknown
    }>
    const secondUser = secondCallMessages.find((msg) => msg.role === 'user')
    expect(JSON.stringify(secondUser?.content)).toContain('Refinement Instruction')
    expect(JSON.stringify(secondUser?.content)).toContain('Document Snapshot')
    expect(prompt).toContain('BP-Adopt AI-based Browser Automation-080126-222458.pdf')
    expect(prompt).toContain('Document Snapshot')
  })
  it('does not refine when PDF contract is already grounded', async () => {
    const pdfPath = '/Users/eroberts/Downloads/doc.pdf'
    callLLMMock.mockResolvedValueOnce(
      JSON.stringify({
        reasoning: 'x',
        prompt:
          '# Title\n\nRewrite PDF\n\nDocument Snapshot\n- A\n\n"Quote one from document"\n\nInputs\n- Attached PDF: doc.pdf',
      }),
    )
    const service = await buildService()
    const prompt = await service.generatePrompt({
      intent: 'Please make this document more succinct',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-5.2',
      fileContext: [],
      images: [],
      videos: [],
      pdfs: [pdfPath],
    })
    expect(callLLM).toHaveBeenCalledTimes(1)
    expect(prompt).toContain('Document Snapshot')
    expect(prompt).toContain('doc.pdf')
  })
  it('returns raw response when LLM output is not JSON', async () => {
    callLLMMock.mockResolvedValue('plain text response')
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    const service = await buildService()
    const prompt = await service.generatePrompt({
      intent: 'Intent',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [],
      images: [],
      videos: [],
    })
    expect(prompt).toBe('plain text response')
    expect(warn).toHaveBeenCalledWith(
      'Failed to parse LLM JSON response. Falling back to raw text.',
    )
    warn.mockRestore()
  })
  it('logs reasoning when DEBUG env var is set', async () => {
    process.env.DEBUG = '1'
    const err = jest.spyOn(console, 'error').mockImplementation(() => undefined)
    const service = await buildService()
    await service.generatePrompt({
      intent: 'Intent',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [],
      images: [],
      videos: [],
    })
    expect(err).toHaveBeenCalledWith(expect.stringContaining('--- AI Reasoning ---'))
    err.mockRestore()
    delete process.env.DEBUG
  })
})
describe('PromptGeneratorService.generatePromptSeries', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    configModule.loadCliConfig.mockResolvedValue({
      promptGenerator: { defaultModel: 'gpt-4o-mini', defaultGeminiModel: 'gemini-1.5-pro' },
    })
    resolveImageParts.mockResolvedValue([{ type: 'image', mimeType: 'image/png', data: 'aaa' }])
    mediaLoader.uploadFileForGemini.mockResolvedValue('gs://video')
    mediaLoader.inferVideoMimeType.mockReturnValue('video/mp4')
  })
  const buildService = async () => new PromptGeneratorService()
  const validAtomicPromptContent = `# Title
Do a thing
Role
You are a coding agent.
Context
This is standalone.
Goals & Tasks
- Make one small change
Inputs
- None
Constraints
- Keep it small
Execution Plan
1. Do the thing
Output Format
- Updated file(s)
Validation
- Run: npx jest apps/prompt-maker-cli/src/__tests__/prompt-generator-service.test.ts --runInBand
`
  const seriesPayload = {
    reasoning: 'analysis',
    overviewPrompt: '# Overview',
    atomicPrompts: [{ title: 'Step', content: validAtomicPromptContent }],
  }
  it('parses valid JSON into a SeriesResponse and uploads media', async () => {
    callLLMMock.mockResolvedValue(JSON.stringify(seriesPayload))
    const service = await buildService()
    const result = await service.generatePromptSeries({
      intent: 'Plan something',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [{ path: 'ctx.md', content: 'context' }],
      images: ['diagram.png'],
      videos: ['clip.mp4'],
    })
    expect(resolveImageParts).toHaveBeenCalledWith(['diagram.png'], undefined)
    expect(mediaLoader.uploadFileForGemini).toHaveBeenCalledWith('clip.mp4')
    expect(result).toEqual(seriesPayload)
  })
  it('throws when the LLM response is not valid JSON', async () => {
    callLLMMock.mockResolvedValue('not json')
    const service = await buildService()
    await expect(
      service.generatePromptSeries({
        intent: 'Plan',
        model: 'gpt-4o-mini',
        targetModel: 'gpt-4o-mini',
        fileContext: [],
        images: [],
        videos: [],
      }),
    ).rejects.toThrow('LLM did not return valid SeriesResponse JSON.')
  })
  it('throws when the JSON is missing atomic prompts', async () => {
    callLLMMock.mockResolvedValue(
      JSON.stringify({ reasoning: 'r', overviewPrompt: '# Overview', atomicPrompts: [] }),
    )
    const service = await buildService()
    await expect(
      service.generatePromptSeries({
        intent: 'Plan',
        model: 'gpt-4o-mini',
        targetModel: 'gpt-4o-mini',
        fileContext: [],
        images: [],
        videos: [],
      }),
    ).rejects.toThrow('Series atomicPrompts must include at least one entry.')
  })
  it('repairs atomic prompts that are missing required sections', async () => {
    callLLMMock
      .mockResolvedValueOnce(
        JSON.stringify({
          reasoning: 'r',
          overviewPrompt: '# Overview',
          atomicPrompts: [
            { title: 'Step', content: '# Title\nMissing most sections\n\nValidation\n- ok' },
          ],
        }),
      )
      .mockResolvedValueOnce(JSON.stringify(seriesPayload))
    const onSeriesRepairAttempt = jest.fn()
    const service = await buildService()
    const result = await service.generatePromptSeries({
      intent: 'Plan',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [],
      images: [],
      videos: [],
      onSeriesRepairAttempt,
    })
    expect(callLLMMock).toHaveBeenCalledTimes(2)
    expect(onSeriesRepairAttempt).toHaveBeenCalledWith(
      expect.objectContaining({
        attempt: 1,
        maxAttempts: 2,
        validationError: expect.stringContaining('missing required section(s)'),
      }),
    )
    expect(result).toEqual(seriesPayload)
  })
  it('repairs atomic prompts that contain cross-references', async () => {
    callLLMMock
      .mockResolvedValueOnce(
        JSON.stringify({
          reasoning: 'r',
          overviewPrompt: '# Overview',
          atomicPrompts: [
            {
              title: 'Step',
              content: `# Title
Do a thing
Role
You are a coding agent.
Context
Continue from step 2.
Goals & Tasks
- Make one small change
Inputs
- None
Constraints
- Keep it small
Execution Plan
1. Do the thing
Output Format
- Updated file(s)
Validation
- Run: npx jest --runInBand
`,
            },
          ],
        }),
      )
      .mockResolvedValueOnce(JSON.stringify(seriesPayload))
    const onSeriesRepairAttempt = jest.fn()
    const service = await buildService()
    const result = await service.generatePromptSeries({
      intent: 'Plan',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [],
      images: [],
      videos: [],
      onSeriesRepairAttempt,
    })
    expect(callLLMMock).toHaveBeenCalledTimes(2)
    expect(onSeriesRepairAttempt).toHaveBeenCalledWith(
      expect.objectContaining({
        attempt: 1,
        maxAttempts: 2,
        validationError: expect.stringContaining('contains forbidden cross-reference phrase'),
      }),
    )
    expect(result).toEqual(seriesPayload)
  })
  it('logs reasoning when DEBUG env var is set', async () => {
    process.env.DEBUG = '1'
    callLLMMock.mockResolvedValue(JSON.stringify(seriesPayload))
    const err = jest.spyOn(console, 'error').mockImplementation(() => undefined)
    const service = await buildService()
    await service.generatePromptSeries({
      intent: 'Plan something',
      model: 'gpt-4o-mini',
      targetModel: 'gpt-4o-mini',
      fileContext: [],
      images: [],
      videos: [],
    })
    expect(err).toHaveBeenCalledWith(expect.stringContaining('--- Series Reasoning ---'))
    err.mockRestore()
    delete process.env.DEBUG
  })
})
</file>

<file path="src/__tests__/soft-wrap.test.ts">
import {
  expandTokenizedLines,
  getTokenizedCursorCoordinates,
  type TokenLabelLookup,
} from '../tui/components/core/tokenized-text'
import { getSoftWrappedCursorOffset, softWrapLine } from '../tui/components/core/soft-wrap'
describe('soft-wrap', () => {
  it('wraps by character width (constant width)', () => {
    expect(softWrapLine('abcdef', { first: 3, rest: 3 }).segments).toEqual(['abc', 'def'])
  })
  it('wraps with distinct first/rest widths', () => {
    expect(softWrapLine('abcdefg', { first: 2, rest: 3 }).segments).toEqual(['ab', 'cde', 'fg'])
  })
  it('prefers breaking on whitespace when available', () => {
    expect(softWrapLine('hello world', { first: 8, rest: 8 }).segments).toEqual(['hello ', 'world'])
    expect(softWrapLine('hello  world', { first: 8, rest: 8 }).segments).toEqual([
      'hello  ',
      'world',
    ])
  })
  it('treats non-positive widths as 1', () => {
    expect(softWrapLine('abc', { first: 0, rest: -1 }).segments).toEqual(['a', 'b', 'c'])
  })
  it('always returns at least one segment', () => {
    expect(softWrapLine('', { first: 3, rest: 3 }).segments).toEqual([''])
  })
  it('maps cursor positions within wrapped segments', () => {
    const wrapped = softWrapLine('abcdef', { first: 3, rest: 3 })
    expect(getSoftWrappedCursorOffset(wrapped, 0)).toEqual({
      rowOffset: 0,
      column: 0,
      needsTrailingEmptyLine: false,
    })
    expect(getSoftWrappedCursorOffset(wrapped, 2)).toEqual({
      rowOffset: 0,
      column: 2,
      needsTrailingEmptyLine: false,
    })
    expect(getSoftWrappedCursorOffset(wrapped, 3)).toEqual({
      rowOffset: 1,
      column: 0,
      needsTrailingEmptyLine: false,
    })
    expect(getSoftWrappedCursorOffset(wrapped, 5)).toEqual({
      rowOffset: 1,
      column: 2,
      needsTrailingEmptyLine: false,
    })
  })
  it('maps end-of-line to a trailing empty line when the final segment is full', () => {
    const wrapped = softWrapLine('abcdef', { first: 3, rest: 3 })
    expect(getSoftWrappedCursorOffset(wrapped, 6)).toEqual({
      rowOffset: 2,
      column: 0,
      needsTrailingEmptyLine: true,
    })
  })
  it('maps end-of-line within the final segment when there is remaining width', () => {
    const wrapped = softWrapLine('abcde', { first: 3, rest: 3 })
    expect(getSoftWrappedCursorOffset(wrapped, 5)).toEqual({
      rowOffset: 1,
      column: 2,
      needsTrailingEmptyLine: false,
    })
  })
  it('supports cursor mapping with tokenized display columns', () => {
    const tokenLabel: TokenLabelLookup = (character) => (character === '$' ? 'XYZ' : null)
    const displayLine = expandTokenizedLines('a$', tokenLabel)[0] ?? ''
    expect(displayLine).toBe('aXYZ')
    const coordinates = getTokenizedCursorCoordinates('a$', 2, tokenLabel)
    expect(coordinates).toEqual({ row: 0, column: 4 })
    const wrapped = softWrapLine(displayLine, { first: 2, rest: 2 })
    expect(wrapped.segments).toEqual(['aX', 'YZ'])
    expect(getSoftWrappedCursorOffset(wrapped, coordinates.column)).toEqual({
      rowOffset: 2,
      column: 0,
      needsTrailingEmptyLine: true,
    })
  })
})
</file>

<file path="src/__tests__/status-indicators-layout.test.ts">
import {
  formatIndicatorLines,
  formatIndicatorSegmentPlain,
  resolveIndicatorSegments,
} from '../tui/components/core/status-indicators-layout'
describe('status indicator layout', () => {
  const chips = [
    '[status:Idle]',
    '[gpt-4o-mini]',
    '[target:gpt-4o]',
    '[openai:ok]',
    '[tokens:1.2k]',
    '[polish:gpt-4o-mini]',
    '[copy:off]',
    '[chatgpt:off]',
    '[json:on]',
    '[files:2]',
    '[urls:0]',
    '[intent:file]',
    '[file:demo.md]',
    '[instr:on]',
    '[tests:idle]',
  ] as const
  const flattenLine = (line: ReturnType<typeof formatIndicatorLines>[number]): string =>
    line.segments.map(formatIndicatorSegmentPlain).join(' · ')
  it('packs indicators into one line when wide', () => {
    const lines = formatIndicatorLines({ chips, maxWidth: 240 })
    expect(lines).toHaveLength(1)
    const firstLine = lines[0]
    if (!firstLine) {
      throw new Error('Expected at least one indicator line')
    }
    const text = flattenLine(firstLine)
    expect(text).toContain('Status: Idle')
    expect(text).toContain('Model: gpt-4o-mini')
    expect(text).toContain('Polish: gpt-4o-mini')
    expect(text).toContain('Target: gpt-4o')
    expect(text.indexOf('Model: gpt-4o-mini')).toBeLessThan(text.indexOf('Polish: gpt-4o-mini'))
    expect(text.indexOf('Polish: gpt-4o-mini')).toBeLessThan(text.indexOf('Target: gpt-4o'))
    expect(text).toContain('OpenAI: ok')
    expect(text).toContain('Tokens: 1.2k')
  })
  it('wraps indicators across multiple lines when narrow', () => {
    const maxWidth = 44
    const lines = formatIndicatorLines({ chips, maxWidth })
    expect(lines.length).toBeGreaterThan(1)
    for (const line of lines) {
      expect(flattenLine(line).length).toBeLessThanOrEqual(maxWidth)
    }
  })
  it('marks toggles and model selections with correct styles', () => {
    const segments = resolveIndicatorSegments(chips)
    const polish = segments.find((segment) => segment.label === 'Polish')
    const copy = segments.find((segment) => segment.label === 'Copy')
    const json = segments.find((segment) => segment.label === 'JSON')
    expect(polish?.style).toBe('primary')
    expect(copy?.style).toBe('muted')
    expect(json?.style).toBe('primary')
  })
})
</file>

<file path="src/__tests__/tui-command-descriptors.test.ts">
import { COMMAND_DESCRIPTORS } from '../tui/config'
describe('tui command descriptors', () => {
  const getDescriptor = (id: (typeof COMMAND_DESCRIPTORS)[number]['id']) =>
    COMMAND_DESCRIPTORS.find((entry) => entry.id === id)
  it('surfaces /exit first in the palette list', () => {
    expect(COMMAND_DESCRIPTORS[0]?.id).toBe('exit')
  })
  it('surfaces series intent prefill guidance', () => {
    const descriptor = getDescriptor('series')
    expect(descriptor).toBeDefined()
    expect(descriptor?.description).toMatch(/prefill/i)
    expect(descriptor?.description).toMatch(/intent file/i)
  })
  it('includes a settings command descriptor', () => {
    const descriptor = getDescriptor('settings')
    expect(descriptor).toBeDefined()
    expect(descriptor?.description).toContain('/settings')
  })
  it('includes concrete examples for /test, /json, /history, /intent, and /meta', () => {
    const testDescriptor = getDescriptor('test')
    expect(testDescriptor).toBeDefined()
    expect(testDescriptor?.description).toContain('/test prompt-tests.yaml')
    const jsonDescriptor = getDescriptor('json')
    expect(jsonDescriptor).toBeDefined()
    expect(jsonDescriptor?.description).toContain('/json on|off')
    const historyDescriptor = getDescriptor('history')
    expect(historyDescriptor).toBeDefined()
    expect(historyDescriptor?.description).toContain('/history')
    const intentDescriptor = getDescriptor('intent')
    expect(intentDescriptor).toBeDefined()
    expect(intentDescriptor?.description).toContain('/intent')
    const metaDescriptor = getDescriptor('instructions')
    expect(metaDescriptor).toBeDefined()
    expect(metaDescriptor?.description).toContain('/meta <text>')
  })
})
</file>

<file path="src/export/export-generate-payload.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import type { GenerateJsonPayload } from '../generate/types'
import { serializeGeneratePayload, type PayloadFormat } from '../generate/payload-io'
export const writeGeneratePayloadExport = async (options: {
  payload: GenerateJsonPayload
  format: PayloadFormat
  outPath: string
  cwd?: string | undefined
}): Promise<{ absolutePath: string }> => {
  const cwd = options.cwd ?? process.cwd()
  const absolutePath = path.resolve(cwd, options.outPath)
  await fs.mkdir(path.dirname(absolutePath), { recursive: true })
  const serialized = serializeGeneratePayload(options.payload, options.format)
  await fs.writeFile(absolutePath, serialized, 'utf8')
  return { absolutePath }
}
</file>

<file path="src/generate/context-budget.ts">
import type { FileContext } from '../file-context'
import type { ContextOverflowStrategy, ContextPathMetadata, TokenTelemetry } from './types'
export type ContextEntrySource = Exclude<ContextPathMetadata['source'], 'intent'>
export type ContextEntry = FileContext & {
  source: ContextEntrySource
}
export type BuildTelemetry = (
  intentText: string,
  files: FileContext[],
  metaInstructions: string,
) => TokenTelemetry
export type EvaluateContextBudgetParams = {
  intentText: string
  metaInstructions: string
  contextEntries: ContextEntry[]
  maxInputTokens?: number
  maxContextTokens?: number
  strategy?: ContextOverflowStrategy
  buildTelemetry: BuildTelemetry
}
export type ContextBudgetEvaluation = {
  keptEntries: ContextEntry[]
  droppedEntries: ContextEntry[]
  droppedPaths: ContextPathMetadata[]
  before: TokenTelemetry
  after: TokenTelemetry
  strategy?: ContextOverflowStrategy
}
const INFINITY = Number.POSITIVE_INFINITY
const min = (a: number, b: number): number => (a < b ? a : b)
const formatBudget = (budget: number | undefined): string =>
  typeof budget === 'number' ? String(budget) : 'unset'
// Token budgets apply only to text context entries (file/url/smart).
// Images/videos are not included in the token budget model, so they are never trimmed here.
export const evaluateContextBudget = (
  params: EvaluateContextBudgetParams,
): ContextBudgetEvaluation => {
  const {
    intentText,
    metaInstructions,
    contextEntries,
    maxInputTokens,
    maxContextTokens,
    strategy,
    buildTelemetry,
  } = params
  const before = buildTelemetry(intentText, contextEntries, metaInstructions)
  const budgetsEnabled = maxInputTokens !== undefined || maxContextTokens !== undefined
  if (!budgetsEnabled) {
    return {
      keptEntries: contextEntries,
      droppedEntries: [],
      droppedPaths: [],
      before,
      after: before,
      ...(strategy ? { strategy } : {}),
    }
  }
  const effectiveStrategy: ContextOverflowStrategy = strategy ?? 'fail'
  const maxFileTokensFromInputBudget =
    maxInputTokens !== undefined
      ? maxInputTokens - before.intentTokens - before.systemTokens
      : INFINITY
  const allowedFileTokens = min(maxContextTokens ?? INFINITY, maxFileTokensFromInputBudget)
  const isOverflow = before.fileTokens > allowedFileTokens
  if (!isOverflow) {
    return {
      keptEntries: contextEntries,
      droppedEntries: [],
      droppedPaths: [],
      before,
      after: before,
      strategy: effectiveStrategy,
    }
  }
  if (effectiveStrategy === 'fail') {
    throw new Error(
      `Context token budget exceeded (strategy=fail). ` +
        `totalTokens=${before.totalTokens} (maxInputTokens=${formatBudget(maxInputTokens)}), ` +
        `contextTokens=${before.fileTokens} (maxContextTokens=${formatBudget(maxContextTokens)}).`,
    )
  }
  const fileSummaries = before.files
  if (fileSummaries.length !== contextEntries.length) {
    throw new Error(
      `Invariant violation: telemetry entry count (${fileSummaries.length}) did not match context entry count (${contextEntries.length}).`,
    )
  }
  const entryTokens = fileSummaries.map((file) => file.tokens)
  let remainingFileTokens = before.fileTokens
  const dropOrder = buildDropOrder(effectiveStrategy, contextEntries, entryTokens)
  const droppedIndexSet = new Set<number>()
  const droppedEntries: ContextEntry[] = []
  for (const index of dropOrder) {
    if (remainingFileTokens <= allowedFileTokens) {
      break
    }
    if (droppedIndexSet.has(index)) {
      continue
    }
    droppedIndexSet.add(index)
    remainingFileTokens -= entryTokens[index] ?? 0
    const dropped = contextEntries[index]
    if (dropped) {
      droppedEntries.push(dropped)
    }
  }
  const keptEntries = contextEntries.filter((_, index) => !droppedIndexSet.has(index))
  const after = buildTelemetry(intentText, keptEntries, metaInstructions)
  const satisfiesContextBudget = after.fileTokens <= allowedFileTokens
  const satisfiesInputBudget =
    maxInputTokens !== undefined ? after.totalTokens <= maxInputTokens : true
  if (!satisfiesContextBudget || !satisfiesInputBudget) {
    throw new Error(
      `Unable to satisfy token budgets after trimming context. ` +
        `totalTokens=${after.totalTokens} (maxInputTokens=${formatBudget(maxInputTokens)}), ` +
        `contextTokens=${after.fileTokens} (maxContextTokens=${formatBudget(maxContextTokens)}).`,
    )
  }
  const droppedPaths: ContextPathMetadata[] = droppedEntries.map((entry) => ({
    path: entry.path,
    source: entry.source,
  }))
  return {
    keptEntries,
    droppedEntries,
    droppedPaths,
    before,
    after,
    strategy: effectiveStrategy,
  }
}
const buildDropOrder = (
  strategy: Exclude<ContextOverflowStrategy, 'fail'>,
  entries: ContextEntry[],
  entryTokens: number[],
): number[] => {
  const indices = entries.map((_, index) => index)
  switch (strategy) {
    case 'drop-oldest':
      return indices
    case 'drop-largest':
      return [...indices].sort((a, b) => {
        const tokenDelta = (entryTokens[b] ?? 0) - (entryTokens[a] ?? 0)
        if (tokenDelta !== 0) {
          return tokenDelta
        }
        return a - b
      })
    case 'drop-smart':
      return stableSourceFirst(indices, entries, 'smart')
    case 'drop-url':
      return stableSourceFirst(indices, entries, 'url')
    default: {
      const exhaustive: never = strategy
      return exhaustive
    }
  }
}
const stableSourceFirst = (
  indices: number[],
  entries: ContextEntry[],
  source: ContextEntrySource,
): number[] => {
  const matches: number[] = []
  const rest: number[] = []
  indices.forEach((index) => {
    const entry = entries[index]
    if (entry?.source === source) {
      matches.push(index)
    } else {
      rest.push(index)
    }
  })
  return [...matches, ...rest]
}
</file>

<file path="src/generate/models.ts">
import { resolveGeminiCredentials } from '../config'
import { loadModelOptions } from '../tui/model-options'
import { getVideoCapableGeminiModels } from '../utils/model-manager'
const DEFAULT_GEMINI_VIDEO_MODEL = 'gemini-2.5-pro'
export const resolveGeminiVideoModel = async (): Promise<string> => {
  try {
    const credentials = await resolveGeminiCredentials()
    const candidates = await getVideoCapableGeminiModels(credentials.apiKey, {
      ...(credentials.baseUrl ? { baseUrl: credentials.baseUrl } : {}),
    })
    const preferredOrder = [DEFAULT_GEMINI_VIDEO_MODEL, 'gemini-2.5-flash', 'gemini-2.0-flash']
    for (const preferred of preferredOrder) {
      if (candidates.includes(preferred)) {
        return preferred
      }
    }
    return candidates[0] ?? DEFAULT_GEMINI_VIDEO_MODEL
  } catch {
    return DEFAULT_GEMINI_VIDEO_MODEL
  }
}
type ResolveTargetModelOptions = {
  explicitTarget?: string
  defaultTargetModel: string
}
export const resolveTargetModel = async ({
  explicitTarget,
  defaultTargetModel,
}: ResolveTargetModelOptions): Promise<string> => {
  if (explicitTarget === undefined) {
    return defaultTargetModel
  }
  const normalized = explicitTarget.trim()
  if (!normalized) {
    throw new Error('--target requires a non-empty model id.')
  }
  const { options } = await loadModelOptions()
  const match = options.find((option) => option.id === normalized)
  if (!match) {
    const known = options
      .slice(0, 12)
      .map((option) => option.id)
      .join(', ')
    throw new Error(
      [
        `Unknown --target model: ${normalized}`,
        known ? `Known models include: ${known}` : 'No known models are configured.',
        'Add custom entries under promptGenerator.models in ~/.config/prompt-maker-cli/config.json.',
      ].join('\n'),
    )
  }
  return match.id
}
</file>

<file path="src/history/generate-history.ts">
import fs from 'node:fs/promises'
import { validateGeneratePayloadObject } from '../generate/payload-io'
import { GENERATE_JSON_PAYLOAD_SCHEMA_VERSION, type GenerateJsonPayload } from '../generate/types'
import { resolveHistoryFilePath } from '../history-logger'
type JsonRecord = Record<string, unknown>
type GenerateHistoryEntry = {
  raw: JsonRecord
  schemaVersion: string
}
export type FromHistorySelector = {
  fromEnd: number
  label: string
}
export type GenerateHistoryPickerItem = {
  selector: string
  title: string
  detail: string
  schemaVersion: string
  supported: boolean
}
const isRecord = (value: unknown): value is JsonRecord =>
  typeof value === 'object' && value !== null && !Array.isArray(value)
const isMissingFileError = (error: unknown): error is { code: string } =>
  typeof error === 'object' &&
  error !== null &&
  'code' in error &&
  typeof (error as { code?: unknown }).code === 'string' &&
  (error as { code: string }).code === 'ENOENT'
const formatTimestamp = (raw: string): string => {
  const date = new Date(raw)
  if (!Number.isFinite(date.getTime())) {
    return raw
  }
  return date.toISOString().replace('T', ' ').replace(/\..*$/, '')
}
const summarizeIntent = (intent: string, limit: number): string => {
  const normalized = intent.replace(/\s+/g, ' ').trim()
  if (!normalized) {
    return '(empty intent)'
  }
  if (normalized.length <= limit) {
    return normalized
  }
  return `${normalized.slice(0, Math.max(0, limit - 1))}…`
}
const readGenerateHistoryEntries = async (filePath: string): Promise<GenerateHistoryEntry[]> => {
  let raw: string
  try {
    raw = await fs.readFile(filePath, 'utf8')
  } catch (error) {
    if (isMissingFileError(error)) {
      throw new Error(
        `History file not found at ${filePath}. Run a generate command first to create it.`,
      )
    }
    const message = error instanceof Error ? error.message : 'Unknown file error.'
    throw new Error(`Failed to read history file ${filePath}: ${message}`)
  }
  const lines = raw
    .split('\n')
    .map((line) => line.trim())
    .filter((line) => line.length > 0)
  if (lines.length === 0) {
    throw new Error(`History file ${filePath} is empty.`)
  }
  const entries: GenerateHistoryEntry[] = []
  for (const line of lines) {
    try {
      const parsed = JSON.parse(line) as unknown
      if (!isRecord(parsed) || typeof parsed.schemaVersion !== 'string') {
        continue
      }
      entries.push({ raw: parsed, schemaVersion: parsed.schemaVersion })
    } catch {
      // ignore invalid json lines
    }
  }
  if (entries.length === 0) {
    throw new Error(`No generate payload entries found in history file ${filePath}.`)
  }
  return entries
}
export const parseFromHistorySelector = (raw: string | undefined): FromHistorySelector => {
  const selector = raw?.trim() ?? 'last'
  const parseOffset = (rawOffset: string | undefined): number => {
    if (!rawOffset) {
      throw new Error(
        `Invalid --from-history selector "${selector}". Offset must be a positive integer.`,
      )
    }
    const value = Number(rawOffset)
    if (!Number.isInteger(value) || value <= 0) {
      throw new Error(
        `Invalid --from-history selector "${selector}". Offset must be a positive integer.`,
      )
    }
    return value
  }
  if (selector === 'last') {
    return { fromEnd: 1, label: 'last' }
  }
  const lastMatch = selector.match(/^last:(\d+)$/)
  if (lastMatch) {
    return { fromEnd: parseOffset(lastMatch[1]), label: selector }
  }
  const numericMatch = selector.match(/^(\d+)$/)
  if (numericMatch) {
    return { fromEnd: parseOffset(numericMatch[1]), label: selector }
  }
  throw new Error(
    `Invalid --from-history selector "${selector}". Use "last", "last:N", or "N" (N-th from end).`,
  )
}
const selectFromEnd = <T>(entries: readonly T[], fromEnd: number): T => {
  const index = entries.length - fromEnd
  if (index < 0 || index >= entries.length) {
    const noun = entries.length === 1 ? 'entry' : 'entries'
    throw new Error(
      `History selector is out of range. Requested ${fromEnd} from end but only ${entries.length} ${noun} available.`,
    )
  }
  const selected = entries[index]
  if (!selected) {
    throw new Error('Invariant violation: selected history entry is missing.')
  }
  return selected
}
export const loadGeneratePayloadFromHistory = async (options?: {
  selector?: string | undefined
  historyPath?: string | undefined
}): Promise<GenerateJsonPayload> => {
  const historyPath = options?.historyPath ?? resolveHistoryFilePath()
  const selector = parseFromHistorySelector(options?.selector)
  const entries = await readGenerateHistoryEntries(historyPath)
  const selected = selectFromEnd(entries, selector.fromEnd)
  if (selected.schemaVersion !== GENERATE_JSON_PAYLOAD_SCHEMA_VERSION) {
    throw new Error(
      `Unsupported history payload schemaVersion=${selected.schemaVersion} for selector "${selector.label}". ` +
        `This prompt-maker-cli supports schemaVersion=${GENERATE_JSON_PAYLOAD_SCHEMA_VERSION}; ` +
        'upgrade/downgrade prompt-maker-cli or regenerate the prompt to create a compatible history entry.',
    )
  }
  return validateGeneratePayloadObject(selected.raw, `history selector "${selector.label}"`)
}
export type LoadGenerateHistoryPickerItemsResult =
  | { ok: true; items: GenerateHistoryPickerItem[] }
  | { ok: false; errorMessage: string }
export const loadGenerateHistoryPickerItems = async (options?: {
  limit?: number
  historyPath?: string | undefined
}): Promise<LoadGenerateHistoryPickerItemsResult> => {
  const historyPath = options?.historyPath ?? resolveHistoryFilePath()
  const limit = options?.limit ?? 30
  let entries: GenerateHistoryEntry[]
  try {
    entries = await readGenerateHistoryEntries(historyPath)
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown history error.'
    return { ok: false, errorMessage: message }
  }
  const newestFirst = entries.slice(-limit).reverse()
  const items: GenerateHistoryPickerItem[] = newestFirst.map((entry, index) => {
    const selector = index === 0 ? 'last' : `last:${index + 1}`
    const timestamp = typeof entry.raw.timestamp === 'string' ? entry.raw.timestamp : ''
    const model = typeof entry.raw.model === 'string' ? entry.raw.model : 'unknown-model'
    const iterations =
      typeof entry.raw.iterations === 'number' && Number.isFinite(entry.raw.iterations)
        ? entry.raw.iterations
        : null
    const intent = typeof entry.raw.intent === 'string' ? entry.raw.intent : ''
    const supported = entry.schemaVersion === GENERATE_JSON_PAYLOAD_SCHEMA_VERSION
    const contextPaths = Array.isArray(entry.raw.contextPaths) ? entry.raw.contextPaths : []
    const fileCount = contextPaths.filter((candidate) => {
      if (!isRecord(candidate)) {
        return false
      }
      return candidate.source === 'file'
    }).length
    const titleParts = [timestamp ? formatTimestamp(timestamp) : 'unknown-time', model]
    const detailParts = [
      supported ? null : `schema:${entry.schemaVersion} (unsupported)`,
      `files:${fileCount}`,
      iterations === null ? null : `iters:${iterations}`,
      intent ? summarizeIntent(intent, 60) : '(missing intent)',
    ].filter((part): part is string => Boolean(part))
    return {
      selector,
      title: titleParts.join(' · '),
      detail: detailParts.join(' · '),
      schemaVersion: entry.schemaVersion,
      supported,
    }
  })
  return { ok: true, items }
}
</file>

<file path="src/prompt-generator/pdf-parts.ts">
import fs from 'node:fs/promises'
import type { PdfPart } from '@prompt-maker/core'
import { inferPdfMimeType, uploadFileForGeminiWithMimeType } from '../media-loader'
import type { UploadStateChange } from './types'
const assertReadablePdf = async (filePath: string): Promise<void> => {
  try {
    await fs.access(filePath)
  } catch (error) {
    const message = error instanceof Error ? error.message : String(error)
    throw new Error(`PDF file ${filePath} is not readable: ${message}`)
  }
}
export const resolvePdfParts = async (
  pdfPaths: string[],
  onUploadStateChange?: UploadStateChange,
  apiKey?: string,
): Promise<PdfPart[]> => {
  const parts: PdfPart[] = []
  for (const pdfPath of pdfPaths) {
    onUploadStateChange?.('start', { kind: 'pdf', filePath: pdfPath })
    try {
      const mimeType = inferPdfMimeType(pdfPath)
      if (apiKey) {
        const fileUri = await uploadFileForGeminiWithMimeType(pdfPath, mimeType, apiKey)
        parts.push({ type: 'pdf', mimeType, filePath: pdfPath, fileUri })
      } else {
        await assertReadablePdf(pdfPath)
        parts.push({ type: 'pdf', mimeType, filePath: pdfPath })
      }
    } catch (error) {
      const message = error instanceof Error ? error.message : String(error)
      throw new Error(`Failed to attach PDF ${pdfPath}: ${message}`)
    } finally {
      onUploadStateChange?.('finish', { kind: 'pdf', filePath: pdfPath })
    }
  }
  return parts
}
</file>

<file path="src/prompt-generator/prompts.ts">
/**
 * Large system prompts used by `PromptGeneratorService`.
 *
 * Kept separate to avoid mixing orchestration logic with long string constants.
 */
const PROMPT_CONTRACT_REQUIREMENTS = `
 Prompt Contract Requirements:
 1. Start with a concise "# Title" summarizing the requested deliverable.
 2. Include the following sections in order, each with actionable markdown content:
    "Role", "Context", "Goals & Tasks", "Inputs", "Constraints", "Execution Plan",
    "Output Format", "Quality Checks".
 3. Reference any provided context files or inputs explicitly when relevant (e.g., file paths).
 4. If the user provided attachments (images/videos/PDFs), assume the assistant executing this contract receives them as context.
    - Do NOT ask the user to re-upload, paste, or re-provide the attached document.
    - In "Inputs", explicitly state the attachment(s) are already provided and should be used directly.
    - Only ask for OCR/text *if* extraction is clearly impossible (e.g., scanned image-only PDF).
 5. Use bullet lists or short paragraphs; keep instructions concrete and testable.
 6. Do NOT execute the task or provide the final deliverable—only craft instructions for another assistant.
 `
const META_PROMPT = `
You are an expert Prompt Engineer. Your goal is to convert the user's intent into an optimized prompt contract that another assistant will later execute.
${PROMPT_CONTRACT_REQUIREMENTS}
Response Format:
You must output a valid JSON object with exactly two keys:
1. "reasoning": A string containing your step-by-step analysis of the user's intent, missing details, and strategy.
2. "prompt": The final, polished prompt text (including all markdown formatting).
Do not output any text outside of this JSON object.
`
export const GEN_SYSTEM_PROMPT = META_PROMPT
export const REFINE_SYSTEM_PROMPT = `
You are an expert Prompt Engineer refining an existing prompt based on user feedback. The result must remain a prompt contract for another assistant, never the finished work.
${PROMPT_CONTRACT_REQUIREMENTS}
Response Format:
You must output a valid JSON object with exactly two keys:
1. "reasoning": A string explaining how you interpreted the refinement instructions and intent.
2. "prompt": The fully updated prompt text, preserving useful structure from the prior draft.
Do not output any text outside of this JSON object.
`
export const SERIES_SYSTEM_PROMPT = `
You are a Lead Architect Agent. Decompose the user's intent into a cohesive plan consisting of:
- One overview prompt that frames the entire effort.
- A sequence of atomic prompts that can be executed and tested independently.
Atomic Prompt Standards (non-negotiable):
- Standalone rule (critical): Every atomic prompt must be fully self-contained. Do NOT reference any other prompt, step number, or earlier/later content.
  - Forbidden examples include: "as above", "previous step", "prior step", "earlier step", "from step 2", "in step 3", "see step 1", "continue from step".
  - If a prompt depends on earlier work, express the dependency as "Expected Repo State" / "Prerequisites" using concrete artifacts (file paths, exported functions/types, UI elements), never by referencing another prompt.
  - Include a short re-entry check: "If this is already implemented, verify and skip to Validation".
- Single outcome: Each atomic prompt must target exactly one verifiable state change.
- Completeness: Each atomic prompt must include all context, assumptions, file paths, commands, and acceptance criteria needed to execute the step in a fresh session.
- Validation required: Each atomic prompt must end with a "Validation" section describing concrete commands + expected outcomes.
Required Atomic Prompt Structure (must appear in EACH atomic prompt content, in this order):
- # Title
- Role
- Context
- Goals & Tasks
- Inputs
- Constraints
- Execution Plan
- Output Format
- Validation
Return strict JSON matching this schema (do not wrap in markdown fences):
{
  "reasoning": string,
  "overviewPrompt": string,
  "atomicPrompts": [
    { "title": string, "content": string },
    { "title": string, "content": string }
  ]
}
Do not perform the work yourself. Only return the JSON payload described above.
`
export const SERIES_REPAIR_SYSTEM_PROMPT = `
You are a Prompt Repair Agent.
You will be given:
- The user's intent
- A previously generated SeriesResponse JSON payload
- A validation error describing what is non-compliant
Your task:
- Return a corrected SeriesResponse JSON payload (same schema) that passes validation.
- Preserve the overall plan and keep the number/order of atomicPrompts the same unless the validation error explicitly indicates the shape is invalid.
- Fix any missing required sections in atomic prompt content.
- Remove ALL cross-references between prompts. Do NOT mention any other step/prompt number.
  - If a prompt depends on earlier work, restate the dependency as "Expected Repo State" / "Prerequisites" using concrete artifacts (file paths, exported functions/types, UI elements), not step references.
  - Add a re-entry instruction (e.g., "If already implemented, verify and skip to Validation") inside the prompt content.
- Ensure each atomic prompt ends with a "Validation" section containing concrete checks.
Return strict JSON only. Do not wrap in markdown fences. Do not perform the work yourself.
`
</file>

<file path="src/prompt-generator/types.ts">
import type { FileContext } from '../file-context'
export type UploadState = 'start' | 'finish'
export type UploadDetail = { kind: 'image' | 'video' | 'pdf'; filePath: string }
export type UploadStateChange = (state: UploadState, detail: UploadDetail) => void
export type SeriesRepairAttemptDetail = {
  attempt: number
  maxAttempts: number
  validationError: string
}
export type PromptAutoRepairKind = 'pdf-grounding'
export type PromptAutoRepairAttemptDetail = {
  kind: PromptAutoRepairKind
  reason: 'missing-pdf-filename' | 'asked-for-document' | 'missing-document-snapshot'
  attempt: number
  maxAttempts: number
  pdfs: string[]
}
export type PromptGenerationRequest = {
  intent: string
  model: string
  targetModel: string
  fileContext: FileContext[]
  images: string[]
  videos: string[]
  pdfs?: string[]
  metaInstructions?: string
  previousPrompt?: string
  refinementInstruction?: string
  onUploadStateChange?: UploadStateChange
  onSeriesRepairAttempt?: (detail: SeriesRepairAttemptDetail) => void
  onPromptAutoRepairAttempt?: (detail: PromptAutoRepairAttemptDetail) => void
}
export type SeriesResponse = {
  reasoning: string
  overviewPrompt: string
  atomicPrompts: Array<{ title: string; content: string }>
}
export type PromptGenerationResult = {
  prompt: string
  reasoning?: string
}
</file>

<file path="src/prompt-generator/video-parts.ts">
import type { VideoPart } from '@prompt-maker/core'
import { inferVideoMimeType, uploadFileForGemini } from '../media-loader'
import type { UploadStateChange } from './types'
export const resolveVideoParts = async (
  videoPaths: string[],
  onUploadStateChange?: UploadStateChange,
  apiKey?: string,
): Promise<VideoPart[]> => {
  const parts: VideoPart[] = []
  for (const videoPath of videoPaths) {
    onUploadStateChange?.('start', { kind: 'video', filePath: videoPath })
    try {
      const fileUri =
        apiKey === undefined
          ? await uploadFileForGemini(videoPath)
          : await uploadFileForGemini(videoPath, apiKey)
      const mimeType = inferVideoMimeType(videoPath)
      parts.push({ type: 'video_uri', fileUri, mimeType })
    } catch (error) {
      const message = error instanceof Error ? error.message : 'Unknown video upload error.'
      console.warn(`Failed to upload video ${videoPath}: ${message}`)
    } finally {
      onUploadStateChange?.('finish', { kind: 'video', filePath: videoPath })
    }
  }
  return parts
}
</file>

<file path="src/tui/components/core/command-menu-keymap.ts">
import type { Key } from 'ink'
export type CommandMenuKeyAction =
  | { type: 'none' }
  | { type: 'close' }
  | { type: 'change-selection'; nextIndex: number }
export type ResolveCommandMenuKeyActionOptions = {
  key: Key
  selectedIndex: number
  itemCount: number
}
export const resolveCommandMenuKeyAction = ({
  key,
  selectedIndex,
  itemCount,
}: ResolveCommandMenuKeyActionOptions): CommandMenuKeyAction => {
  if (itemCount <= 0) {
    return { type: 'none' }
  }
  if (key.escape) {
    return { type: 'close' }
  }
  const clampedIndex = Math.min(Math.max(selectedIndex, 0), itemCount - 1)
  if (key.upArrow) {
    const nextIndex = clampedIndex === 0 ? itemCount - 1 : clampedIndex - 1
    return { type: 'change-selection', nextIndex }
  }
  if (key.downArrow) {
    const nextIndex = clampedIndex === itemCount - 1 ? 0 : clampedIndex + 1
    return { type: 'change-selection', nextIndex }
  }
  return { type: 'none' }
}
</file>

<file path="src/tui/components/core/MultilineTextInput.tsx">
import React, { useEffect, useMemo, useRef, useState } from 'react'
import { Box, Text, useInput, useStdout, type Key } from 'ink'
import {
  backspace,
  clampCursor,
  deleteForward,
  insertText,
  moveCursorLeft,
  moveCursorRight,
  type MultilineTextBufferState,
} from './multiline-text-buffer'
import { softWrapLine, getSoftWrappedCursorOffset } from './soft-wrap'
import { isBackspaceKey } from './text-input-keys'
import {
  expandTokenizedLines,
  getTokenizedCursorCoordinates,
  type TokenLabelLookup,
} from './tokenized-text'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps, type InkColorValue } from '../../theme/theme-types'
export type DebugKeyEvent = {
  input: string
  key: Key
}
export type MultilineTextInputGutter = {
  glyph: string
  color: InkColorValue
  spacer?: number | undefined
}
export type MultilineTextInputProps = {
  value: string
  onChange: (next: string) => void
  onSubmit: (value: string) => void
  placeholder?: string | undefined
  focus?: boolean
  isDisabled?: boolean
  isPasteActive?: boolean
  tokenLabel?: TokenLabelLookup | undefined
  onDebugKeyEvent?: ((event: DebugKeyEvent) => void) | undefined
  gutter?: MultilineTextInputGutter | undefined
  // Optional rendering constraints (useful for input bars that must paint their full width).
  width?: number | undefined
  backgroundColor?: InkColorValue
}
const PROMPT = '› '
const PROMPT_SPACER = '  '
type RenderLine = {
  id: string
  content: string
  isPlaceholder: boolean
}
type WrappedLayout = {
  readonly lines: readonly RenderLine[]
  readonly cursorRow: number
  readonly cursorColumn: number
}
const toHardLines = (
  value: string,
  placeholder: string | undefined,
  tokenLabel: TokenLabelLookup,
): readonly RenderLine[] => {
  if (!value) {
    return [{ id: 'placeholder', content: placeholder ?? '', isPlaceholder: true }]
  }
  const lines = expandTokenizedLines(value, tokenLabel)
  return lines.map((line, index) => ({ id: `line-${index}`, content: line, isPlaceholder: false }))
}
const normalizeColumns = (columns: number): number => {
  if (!Number.isFinite(columns)) {
    return 0
  }
  return Math.max(0, Math.floor(columns))
}
export const MultilineTextInput: React.FC<MultilineTextInputProps> = ({
  value,
  onChange,
  onSubmit,
  placeholder,
  focus = false,
  isDisabled = false,
  isPasteActive = false,
  tokenLabel,
  onDebugKeyEvent,
  gutter,
  width,
  backgroundColor,
}) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const [cursor, setCursor] = useState<number>(value.length)
  const internalUpdateRef = useRef(false)
  const state: MultilineTextBufferState = useMemo(
    () => ({ value, cursor: clampCursor(cursor, value) }),
    [cursor, value],
  )
  const applyNextState = (nextState: MultilineTextBufferState): void => {
    internalUpdateRef.current = true
    setCursor(nextState.cursor)
    onChange(nextState.value)
  }
  useEffect(() => {
    if (internalUpdateRef.current) {
      internalUpdateRef.current = false
      return
    }
    setCursor(value.length)
  }, [value])
  useInput(
    (input, key) => {
      if (!focus || isDisabled || isPasteActive) {
        return
      }
      if (onDebugKeyEvent) {
        onDebugKeyEvent({ input, key })
      }
      const isCtrlJ = key.ctrl && input.toLowerCase() === 'j'
      const isAltEnter =
        key.meta && (key.return || input === '\r' || input === '\n' || input === '')
      const isEscapedAltEnter = input === '\u001b\r' || input === '\u001b\n'
      if (isCtrlJ || isAltEnter || isEscapedAltEnter) {
        applyNextState(insertText(state, '\n'))
        return
      }
      if (key.return) {
        onSubmit(value)
        return
      }
      if (isBackspaceKey(input, key)) {
        applyNextState(backspace(state))
        return
      }
      if (key.delete) {
        applyNextState(deleteForward(state))
        return
      }
      if (key.leftArrow) {
        setCursor(moveCursorLeft(state).cursor)
        return
      }
      if (key.rightArrow) {
        setCursor(moveCursorRight(state).cursor)
        return
      }
      if (!input) {
        return
      }
      if (key.ctrl || key.meta) {
        return
      }
      applyNextState(insertText(state, input))
    },
    { isActive: focus && !isDisabled },
  )
  const resolvedTokenLabel = useMemo<TokenLabelLookup>(
    () => tokenLabel ?? (() => null),
    [tokenLabel],
  )
  const hardLines = useMemo(
    () => toHardLines(value, placeholder, resolvedTokenLabel),
    [placeholder, resolvedTokenLabel, value],
  )
  const tokenizedCursor = useMemo(
    () => getTokenizedCursorCoordinates(value, cursor, resolvedTokenLabel),
    [cursor, resolvedTokenLabel, value],
  )
  const gutterSpacer = gutter?.spacer ?? 0
  const safeSpacer = Number.isFinite(gutterSpacer) ? Math.max(0, Math.floor(gutterSpacer)) : 0
  const gutterColumns = gutter ? gutter.glyph.length + safeSpacer : 0
  const totalColumns = useMemo(() => {
    if (typeof width === 'number') {
      return normalizeColumns(width)
    }
    return normalizeColumns(stdout?.columns ?? 80)
  }, [stdout?.columns, width])
  const wrapped = useMemo<WrappedLayout>(() => {
    const lines: RenderLine[] = []
    let cursorRow = 0
    let cursorColumn = 0
    for (let hardIndex = 0; hardIndex < hardLines.length; hardIndex += 1) {
      const hardLine = hardLines[hardIndex]
      if (!hardLine) {
        continue
      }
      const hardLineStart = lines.length
      const isFirstHardLine = hardIndex === 0
      const firstPrefixColumns = isFirstHardLine ? PROMPT.length : PROMPT_SPACER.length
      const firstWrapWidth = Math.max(1, totalColumns - gutterColumns - firstPrefixColumns)
      const restWrapWidth = Math.max(1, totalColumns - gutterColumns - PROMPT_SPACER.length)
      const wrappedHardLine = softWrapLine(hardLine.content, {
        first: firstWrapWidth,
        rest: restWrapWidth,
      })
      const isCursorHardLine = hardIndex === tokenizedCursor.row
      const cursorOffset = isCursorHardLine
        ? getSoftWrappedCursorOffset(wrappedHardLine, tokenizedCursor.column)
        : null
      if (isCursorHardLine && cursorOffset) {
        cursorRow = hardLineStart + cursorOffset.rowOffset
        cursorColumn = cursorOffset.column
      }
      for (
        let segmentIndex = 0;
        segmentIndex < wrappedHardLine.segments.length;
        segmentIndex += 1
      ) {
        const segment = wrappedHardLine.segments[segmentIndex]
        if (segment === undefined) {
          continue
        }
        lines.push({
          id: `${hardLine.id}-seg-${segmentIndex}`,
          content: segment,
          isPlaceholder: hardLine.isPlaceholder,
        })
      }
      if (isCursorHardLine && cursorOffset?.needsTrailingEmptyLine) {
        lines.push({
          id: `${hardLine.id}-seg-${wrappedHardLine.segments.length}`,
          content: '',
          isPlaceholder: hardLine.isPlaceholder,
        })
      }
    }
    if (lines.length === 0) {
      return {
        lines: [{ id: 'empty', content: '', isPlaceholder: true }],
        cursorRow: 0,
        cursorColumn: 0,
      }
    }
    const safeCursorRow = Math.max(0, Math.min(cursorRow, lines.length - 1))
    const cursorLine = lines[safeCursorRow]
    const safeCursorColumn = cursorLine
      ? Math.max(0, Math.min(cursorColumn, cursorLine.content.length))
      : 0
    return {
      lines,
      cursorRow: safeCursorRow,
      cursorColumn: safeCursorColumn,
    }
  }, [gutterColumns, hardLines, tokenizedCursor.column, tokenizedCursor.row, totalColumns])
  const backgroundProps = inkBackgroundColorProps(backgroundColor)
  return (
    <Box flexDirection="column" height={wrapped.lines.length}>
      {wrapped.lines.map((line, lineIndex) => {
        const isCursorLine = lineIndex === wrapped.cursorRow
        const safeColumn = isCursorLine ? Math.min(wrapped.cursorColumn, line.content.length) : 0
        const before = isCursorLine ? line.content.slice(0, safeColumn) : line.content
        const cursorCharacter = isCursorLine
          ? safeColumn < line.content.length
            ? line.content.charAt(safeColumn)
            : ' '
          : ''
        const after =
          isCursorLine && safeColumn < line.content.length ? line.content.slice(safeColumn + 1) : ''
        const prefix = lineIndex === 0 ? PROMPT : PROMPT_SPACER
        const lineColorProps = line.isPlaceholder ? inkColorProps(theme.mutedText) : {}
        const spacerText = safeSpacer > 0 ? ' '.repeat(safeSpacer) : ''
        const renderedColumns = isCursorLine
          ? before.length + cursorCharacter.length + after.length
          : before.length
        const usedColumns = gutterColumns + prefix.length + renderedColumns
        const fillerColumns =
          typeof width === 'number' && width > usedColumns ? width - usedColumns : 0
        const filler = fillerColumns > 0 ? ' '.repeat(fillerColumns) : ''
        return (
          <Box key={line.id}>
            {gutter ? (
              <>
                <Text {...backgroundProps} {...inkColorProps(gutter.color)}>
                  {gutter.glyph}
                </Text>
                {spacerText ? <Text {...backgroundProps}>{spacerText}</Text> : null}
              </>
            ) : null}
            <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
              {prefix}
            </Text>
            {isCursorLine ? (
              <>
                <Text {...backgroundProps} {...lineColorProps}>
                  {before}
                </Text>
                <Text inverse {...backgroundProps} {...lineColorProps}>
                  {cursorCharacter}
                </Text>
                <Text {...backgroundProps} {...lineColorProps}>
                  {after}
                </Text>
              </>
            ) : (
              <Text {...backgroundProps} {...lineColorProps}>
                {before}
              </Text>
            )}
            {filler ? <Text {...backgroundProps}>{filler}</Text> : null}
          </Box>
        )
      })}
    </Box>
  )
}
</file>

<file path="src/tui/components/core/ScrollableOutput.tsx">
import { memo, useMemo } from 'react'
import { Box, Text } from 'ink'
import type { HistoryEntry } from '../../types'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps, type InkColorValue } from '../../theme/theme-types'
import {
  DEFAULT_MARKDOWN_STATE,
  resolveMarkdownSlotColor,
  tokenizeMarkdownLine,
} from '../../markdown/markdown-highlight'
const padRight = (value: string, width: number | undefined): string => {
  if (typeof width !== 'number' || width <= 0) {
    return value
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type ScrollableOutputProps = {
  lines: readonly HistoryEntry[]
  visibleRows: number
  scrollOffset: number
  contentWidth?: number | undefined
  backgroundColor?: InkColorValue
}
export const ScrollableOutput = memo(
  ({ lines, visibleRows, scrollOffset, contentWidth, backgroundColor }: ScrollableOutputProps) => {
    const { theme } = useTheme()
    const startIndex = Math.max(0, Math.min(scrollOffset, Math.max(0, lines.length - visibleRows)))
    const endIndex = Math.min(lines.length, startIndex + visibleRows)
    const visibleLines = useMemo(
      () => lines.slice(startIndex, endIndex),
      [lines, startIndex, endIndex],
    )
    const backgroundProps = inkBackgroundColorProps(backgroundColor)
    const padded = useMemo(() => {
      const next: Array<HistoryEntry | null> = [...visibleLines]
      while (next.length < visibleRows) {
        next.push(null)
      }
      return next
    }, [visibleLines, visibleRows])
    const markdownStartState = useMemo(() => {
      let state = DEFAULT_MARKDOWN_STATE
      for (let i = 0; i < startIndex; i += 1) {
        const entry = lines[i]
        if (!entry || entry.format !== 'markdown') {
          state = DEFAULT_MARKDOWN_STATE
          continue
        }
        if (/^\s*```/.test(entry.content)) {
          state = { inCodeBlock: !state.inCodeBlock }
        }
      }
      return state
    }, [lines, startIndex])
    const decorated = useMemo(() => {
      let state = markdownStartState
      return padded.map((entry) => {
        if (!entry) {
          state = DEFAULT_MARKDOWN_STATE
          return { entry, content: '', spans: null }
        }
        const content =
          typeof contentWidth === 'number' && contentWidth > 0
            ? entry.content.slice(0, contentWidth)
            : entry.content
        if (entry.format === 'markdown') {
          const tokenized = tokenizeMarkdownLine(content, state)
          state = tokenized.nextState
          return { entry, content, spans: tokenized.spans }
        }
        state = DEFAULT_MARKDOWN_STATE
        return { entry, content, spans: null }
      })
    }, [contentWidth, markdownStartState, padded])
    return (
      <Box flexDirection="column" height={visibleRows} overflow="hidden">
        {decorated.map((row, index) => {
          if (!row.entry) {
            return (
              <Text key={`blank-${startIndex + index}`} {...backgroundProps}>
                {padRight('', contentWidth)}
              </Text>
            )
          }
          const key = `${row.entry.id}-${startIndex + index}`
          if (row.entry.format === 'markdown' && row.spans) {
            const paddingLength =
              typeof contentWidth === 'number' && contentWidth > 0
                ? Math.max(0, contentWidth - row.content.length)
                : 0
            return (
              <Text key={key} {...backgroundProps}>
                {row.spans.map((span, spanIndex) => (
                  <Text
                    key={`${key}-span-${spanIndex}`}
                    {...inkColorProps(resolveMarkdownSlotColor(theme, span.slot))}
                    {...(span.bold ? { bold: true } : {})}
                    {...(span.italic ? { italic: true } : {})}
                    {...(span.underline ? { underline: true } : {})}
                  >
                    {span.text}
                  </Text>
                ))}
                {paddingLength > 0 ? ' '.repeat(paddingLength) : null}
              </Text>
            )
          }
          const color =
            row.entry.kind === 'user'
              ? theme.accent
              : row.entry.kind === 'progress'
                ? theme.warning
                : theme.text
          return (
            <Text key={key} {...backgroundProps} {...inkColorProps(color)}>
              {padRight(row.entry.content, contentWidth)}
            </Text>
          )
        })}
      </Box>
    )
  },
)
ScrollableOutput.displayName = 'ScrollableOutput'
</file>

<file path="src/tui/components/core/soft-wrap.ts">
export type SoftWrapWidths = {
  readonly first: number
  readonly rest: number
}
export type SoftWrappedLine = {
  readonly segments: readonly string[]
  readonly segmentStarts: readonly number[]
  readonly segmentWidths: readonly number[]
}
export type SoftWrapCursorOffset = {
  readonly rowOffset: number
  readonly column: number
  readonly needsTrailingEmptyLine: boolean
}
const normalizeWrapWidth = (width: number): number => {
  if (!Number.isFinite(width)) {
    return 1
  }
  return Math.max(1, Math.floor(width))
}
export const softWrapLine = (displayLine: string, widths: SoftWrapWidths): SoftWrappedLine => {
  const firstWidth = normalizeWrapWidth(widths.first)
  const restWidth = normalizeWrapWidth(widths.rest)
  const segments: string[] = []
  const segmentStarts: number[] = []
  const segmentWidths: number[] = []
  if (!displayLine) {
    segments.push('')
    segmentStarts.push(0)
    segmentWidths.push(firstWidth)
    return { segments, segmentStarts, segmentWidths }
  }
  const whitespacePattern = /\s/
  let offset = 0
  let segmentIndex = 0
  while (offset < displayLine.length) {
    const segmentWidth = segmentIndex === 0 ? firstWidth : restWidth
    const window = displayLine.slice(offset, offset + segmentWidth)
    let breakIndex = window.length
    // Prefer breaking after the last whitespace that fits in the window.
    // Keep the whitespace on the previous segment to avoid trimming and to
    // preserve cursor-to-display mappings.
    const hasMoreContent = offset + window.length < displayLine.length
    if (hasMoreContent && window.length === segmentWidth) {
      for (let index = window.length - 1; index >= 0; index -= 1) {
        const character = window.charAt(index)
        if (whitespacePattern.test(character)) {
          breakIndex = index + 1
          break
        }
      }
    }
    const next = window.slice(0, breakIndex)
    segments.push(next)
    segmentStarts.push(offset)
    segmentWidths.push(segmentWidth)
    offset += next.length
    segmentIndex += 1
  }
  return { segments, segmentStarts, segmentWidths }
}
export const getSoftWrappedCursorOffset = (
  wrapped: SoftWrappedLine,
  displayColumn: number,
): SoftWrapCursorOffset => {
  const safeColumn = Math.max(0, Math.floor(displayColumn))
  const totalLength = wrapped.segments.reduce((sum, segment) => sum + segment.length, 0)
  const clampedColumn = Math.min(safeColumn, totalLength)
  if (totalLength === 0) {
    return { rowOffset: 0, column: 0, needsTrailingEmptyLine: false }
  }
  // Cursor is inside the rendered characters.
  if (clampedColumn < totalLength) {
    for (let index = 0; index < wrapped.segments.length; index += 1) {
      const segment = wrapped.segments[index]
      const segmentStart = wrapped.segmentStarts[index]
      if (segment === undefined || segmentStart === undefined) {
        continue
      }
      const segmentEnd = segmentStart + segment.length
      if (clampedColumn < segmentEnd) {
        return {
          rowOffset: index,
          column: clampedColumn - segmentStart,
          needsTrailingEmptyLine: false,
        }
      }
    }
    // Fallback to last segment if something unexpected happens.
    const lastIndex = Math.max(0, wrapped.segments.length - 1)
    const lastStart = wrapped.segmentStarts[lastIndex] ?? 0
    return {
      rowOffset: lastIndex,
      column: Math.max(0, clampedColumn - lastStart),
      needsTrailingEmptyLine: false,
    }
  }
  // Cursor is at end-of-line: place it after the last character.
  const lastIndex = Math.max(0, wrapped.segments.length - 1)
  const lastSegment = wrapped.segments[lastIndex] ?? ''
  const lastWidth = wrapped.segmentWidths[lastIndex] ?? 1
  if (lastSegment.length >= lastWidth) {
    // The last segment has no remaining cells to render the cursor.
    return { rowOffset: wrapped.segments.length, column: 0, needsTrailingEmptyLine: true }
  }
  return { rowOffset: lastIndex, column: lastSegment.length, needsTrailingEmptyLine: false }
}
</file>

<file path="src/tui/components/core/status-indicators-layout.ts">
import { TOGGLE_LABELS } from '../../config'
import type { ModelProvider } from '../../../model-providers'
export type IndicatorStyle = 'primary' | 'muted' | 'success' | 'warning' | 'danger'
export type IndicatorSegment = {
  id: string
  label: string
  value: string
  style: IndicatorStyle
  raw: string
}
export type IndicatorLine = {
  segments: readonly IndicatorSegment[]
}
const PROVIDER_KEYS: ReadonlySet<ModelProvider> = new Set(['openai', 'gemini', 'other'])
const normalizeChipBody = (raw: string): string | null => {
  const trimmed = raw.trim()
  if (!trimmed.startsWith('[') || !trimmed.endsWith(']')) {
    return null
  }
  return trimmed.slice(1, -1)
}
type ParsedChip = {
  key: string
  value: string
  raw: string
}
const parseIndicatorChip = (raw: string): ParsedChip | null => {
  const body = normalizeChipBody(raw)
  if (!body) {
    return null
  }
  const separatorIndex = body.indexOf(':')
  if (separatorIndex === -1) {
    return { key: 'model', value: body, raw }
  }
  const key = body.slice(0, separatorIndex).trim()
  const value = body.slice(separatorIndex + 1).trim()
  if (!key) {
    return null
  }
  return { key, value, raw }
}
const toTitleCase = (value: string): string => {
  if (!value) {
    return value
  }
  return `${value[0]?.toUpperCase() ?? ''}${value.slice(1)}`
}
const resolveToggleLabel = (key: string): string | null => {
  if (key === 'smart') {
    return 'Smart'
  }
  const label = (TOGGLE_LABELS as Record<string, string>)[key]
  return label ?? null
}
const resolveSegmentLabel = (chip: ParsedChip): string => {
  if (chip.key === 'model') {
    return 'Model'
  }
  if (chip.key === 'polish') {
    return 'Polish'
  }
  if (chip.key === 'target') {
    return 'Target'
  }
  if (chip.key === 'instr') {
    return 'Meta'
  }
  const toggleLabel = resolveToggleLabel(chip.key)
  if (toggleLabel) {
    return toggleLabel
  }
  if (chip.key === 'openai') {
    return 'OpenAI'
  }
  if (chip.key === 'urls') {
    return 'URLs'
  }
  if (PROVIDER_KEYS.has(chip.key as ModelProvider)) {
    return toTitleCase(chip.key)
  }
  return toTitleCase(chip.key)
}
const resolveSegmentStyle = (chip: ParsedChip): IndicatorStyle => {
  if (
    chip.key === 'status' ||
    chip.key === 'model' ||
    chip.key === 'polish' ||
    chip.key === 'target'
  ) {
    return 'primary'
  }
  if (chip.key === 'tokens') {
    return 'primary'
  }
  if (PROVIDER_KEYS.has(chip.key as ModelProvider)) {
    const suffix = chip.value.toLowerCase()
    if (suffix === 'ok') {
      return 'success'
    }
    if (suffix === 'missing-key' || suffix === 'missing') {
      return 'warning'
    }
    if (suffix === 'error') {
      return 'danger'
    }
    return 'muted'
  }
  if (chip.key === 'files' || chip.key === 'urls') {
    const count = Number.parseInt(chip.value, 10)
    return Number.isFinite(count) && count > 0 ? 'primary' : 'muted'
  }
  const toggleLabel = resolveToggleLabel(chip.key)
  if (toggleLabel) {
    return chip.value.toLowerCase() === 'on' ? 'primary' : 'muted'
  }
  if (chip.key === 'intent') {
    return chip.value.toLowerCase() === 'file' ? 'primary' : 'muted'
  }
  if (chip.key === 'tests') {
    return chip.value.toLowerCase() === 'running' ? 'warning' : 'muted'
  }
  return 'muted'
}
const resolveSortWeight = (segment: IndicatorSegment): number => {
  switch (segment.label) {
    case 'Status':
      return 0
    case 'Model':
      return 1
    case 'Polish':
      return 1
    case 'Target':
      return 1
    case 'OpenAI':
    case 'Gemini':
    case 'Other':
      return 2
    case 'Tokens':
      return 3
    case 'Intent':
      return 4
    case 'File':
      return 5
    case 'Meta':
      return 6
    case 'Root':
      return 7
    case 'Files':
      return 8
    case 'URLs':
      return 9
    default:
      return 10
  }
}
export const resolveIndicatorSegments = (chips: readonly string[]): IndicatorSegment[] => {
  const segments: IndicatorSegment[] = []
  for (const raw of chips) {
    const parsed = parseIndicatorChip(raw)
    if (!parsed) {
      continue
    }
    const label = resolveSegmentLabel(parsed)
    segments.push({
      id: `${parsed.key}-${segments.length}`,
      label,
      value: parsed.value,
      style: resolveSegmentStyle(parsed),
      raw,
    })
  }
  return segments.sort((left, right) => {
    const weightDiff = resolveSortWeight(left) - resolveSortWeight(right)
    return weightDiff !== 0 ? weightDiff : left.label.localeCompare(right.label)
  })
}
export const formatIndicatorSegmentPlain = (segment: IndicatorSegment): string =>
  `${segment.label}: ${segment.value}`
export const formatIndicatorLines = (params: {
  chips: readonly string[]
  maxWidth: number
}): IndicatorLine[] => {
  const maxWidth = Math.max(16, Math.floor(params.maxWidth))
  const segments = resolveIndicatorSegments(params.chips)
  const lines: IndicatorSegment[][] = []
  let current: IndicatorSegment[] = []
  let currentLength = 0
  const separator = ' · '
  const pushLine = (): void => {
    if (current.length === 0) {
      return
    }
    lines.push(current)
    current = []
    currentLength = 0
  }
  for (const segment of segments) {
    const token = formatIndicatorSegmentPlain(segment)
    const tokenLength = token.length
    if (current.length === 0) {
      current = [segment]
      currentLength = tokenLength
      continue
    }
    const nextLength = currentLength + separator.length + tokenLength
    if (nextLength <= maxWidth) {
      current.push(segment)
      currentLength = nextLength
      continue
    }
    pushLine()
    current = [segment]
    currentLength = tokenLength
  }
  pushLine()
  return lines.map((segmentsForLine) => ({ segments: segmentsForLine }))
}
</file>

<file path="src/tui/components/core/Toast.tsx">
import { Box, Text, useStdout } from 'ink'
import type { ToastKind } from '../../notifier'
import { TOAST_HEIGHT } from '../../toast-constants'
import { useTheme } from '../../theme/theme-provider'
import {
  inkBackgroundColorProps,
  inkBorderColorProps,
  inkColorProps,
} from '../../theme/theme-types'
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type ToastProps = {
  message: string
  kind: ToastKind
}
type ToastChromeTone = 'default' | 'warning' | 'error'
type ToastChrome = {
  borderTone: ToastChromeTone
  titleTone: ToastChromeTone
  title: string
}
const toastChrome = (kind: ToastKind): ToastChrome => {
  switch (kind) {
    case 'info':
      return { borderTone: 'default', titleTone: 'default', title: 'Notice' }
    case 'progress':
      return { borderTone: 'warning', titleTone: 'warning', title: 'Working' }
    case 'warning':
      return { borderTone: 'warning', titleTone: 'warning', title: 'Warning' }
    case 'error':
      return { borderTone: 'error', titleTone: 'error', title: 'Error' }
    default: {
      const exhaustive: never = kind
      return exhaustive
    }
  }
}
export { TOAST_HEIGHT }
export const Toast = ({ message, kind }: ToastProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const chrome = toastChrome(kind)
  // Ink doesn't paint "empty" cells when rendering overlapping/absolute layers.
  // To keep the toast opaque, we explicitly pad each content line to the
  // available inner width so it prints background-colored spaces.
  const terminalColumns = stdout?.columns ?? 80
  const toastWidth = terminalColumns
  const borderColumns = 2
  const paddingColumns = 2
  const contentWidth = Math.max(0, toastWidth - borderColumns - paddingColumns)
  const borderColor =
    chrome.borderTone === 'warning'
      ? theme.warning
      : chrome.borderTone === 'error'
        ? theme.error
        : theme.border
  const titleColor =
    chrome.titleTone === 'warning'
      ? theme.warning
      : chrome.titleTone === 'error'
        ? theme.error
        : theme.mutedText
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  return (
    <Box
      flexDirection="column"
      borderStyle="round"
      paddingX={1}
      paddingY={0}
      height={TOAST_HEIGHT}
      width={toastWidth}
      overflow="hidden"
      {...inkBorderColorProps(borderColor)}
      {...backgroundProps}
    >
      <Text {...backgroundProps} {...inkColorProps(titleColor)}>
        {padRight(chrome.title, contentWidth)}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.text)}>
        {padRight(message, contentWidth)}
      </Text>
    </Box>
  )
}
</file>

<file path="src/tui/components/core/ToastOverlay.tsx">
import { Box } from 'ink'
import React, { memo, useEffect } from 'react'
import {
  TOAST_ANIMATION_TICK_MS,
  TOAST_HEIGHT,
  TOAST_HORIZONTAL_INSET_COLUMNS,
  TOAST_TOP_OFFSET_ROWS,
} from '../../toast-constants'
import type { ToastId, ToastItem } from '../../notifier'
import { useToastContext } from '../../notifier'
import { Toast } from './Toast'
const useAnimatedInt = (targetValue: number, initialValue: number) => {
  const [value, setValue] = React.useState(initialValue)
  useEffect(() => {
    if (value === targetValue) {
      return
    }
    const timer = setTimeout(() => {
      setValue((prev) => {
        if (prev === targetValue) {
          return prev
        }
        const direction = prev < targetValue ? 1 : -1
        return prev + direction
      })
    }, TOAST_ANIMATION_TICK_MS)
    return () => {
      clearTimeout(timer)
    }
  }, [targetValue, value])
  return {
    value,
    isComplete: value === targetValue,
  }
}
type ToastOverlayItemProps = {
  toast: ToastItem
  onExitComplete: (id: ToastId) => void
}
const ToastOverlayItem = ({ toast, onExitComplete }: ToastOverlayItemProps) => {
  const targetHeight = toast.isExiting ? 0 : TOAST_HEIGHT
  const { value: height, isComplete } = useAnimatedInt(targetHeight, 0)
  useEffect(() => {
    if (!toast.isExiting) {
      return
    }
    if (!isComplete) {
      return
    }
    onExitComplete(toast.id)
  }, [isComplete, onExitComplete, toast.id, toast.isExiting])
  if (height === 0 && toast.isExiting) {
    return null
  }
  return (
    <Box height={height} overflow="hidden">
      <Toast message={toast.message} kind={toast.kind} />
    </Box>
  )
}
export const ToastOverlay = memo(() => {
  const { toasts, removeToast } = useToastContext()
  return (
    <Box
      position="absolute"
      width="100%"
      height="100%"
      flexDirection="column"
      justifyContent="flex-start"
      alignItems="flex-start"
      paddingX={TOAST_HORIZONTAL_INSET_COLUMNS}
      paddingTop={TOAST_TOP_OFFSET_ROWS}
    >
      {toasts.map((toast) => (
        <ToastOverlayItem key={toast.id} toast={toast} onExitComplete={removeToast} />
      ))}
    </Box>
  )
})
ToastOverlay.displayName = 'ToastOverlay'
</file>

<file path="src/tui/components/popups/BudgetsPopup.tsx">
import { Box, Text, useStdout } from 'ink'
import type { ContextOverflowStrategy } from '../../../config'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
const STRATEGY_OPTIONS = [
  '',
  'fail',
  'drop-smart',
  'drop-url',
  'drop-largest',
  'drop-oldest',
] as const satisfies ReadonlyArray<ContextOverflowStrategy | ''>
const formatStrategy = (strategy: ContextOverflowStrategy | ''): string =>
  strategy ? strategy : '(unset)'
export type BudgetsPopupProps = {
  selectionIndex: number
  maxContextTokensDraft: string
  maxInputTokensDraft: string
  contextOverflowStrategyDraft: ContextOverflowStrategy | ''
  errorMessage: string | null
  onMaxContextTokensChange: (next: string) => void
  onMaxInputTokensChange: (next: string) => void
  onSubmit: () => void
}
export const BudgetsPopup = ({
  selectionIndex,
  maxContextTokensDraft,
  maxInputTokensDraft,
  contextOverflowStrategyDraft,
  errorMessage,
  onMaxContextTokensChange,
  onMaxInputTokensChange,
  onSubmit,
}: BudgetsPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 44, 80)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const popupHeight = 16 + (errorMessage ? 1 : 0)
  const activeRowProps = {
    ...inkColorProps(theme.selectionText),
    ...inkBackgroundColorProps(theme.selectionBackground),
  }
  const inactiveRowProps = { ...backgroundProps, ...inkColorProps(theme.text) }
  const normalizedSelection = clamp(selectionIndex, 0, 2)
  const budgetsEnabled = Boolean(maxContextTokensDraft.trim() || maxInputTokensDraft.trim())
  const effectiveStrategy = budgetsEnabled
    ? contextOverflowStrategyDraft || 'fail'
    : contextOverflowStrategyDraft
  const effectiveLine = budgetsEnabled
    ? `Effective: input=${maxInputTokensDraft.trim() || 'unset'} · context=${maxContextTokensDraft.trim() || 'unset'} · overflow=${formatStrategy(effectiveStrategy)}`
    : `Effective: budgets disabled · overflow=${formatStrategy(effectiveStrategy)}`
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Token Budgets', contentWidth)}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('↑/↓ select · Enter apply · ←/→ change strategy · Esc close', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Box flexDirection="row">
          <Text {...(normalizedSelection === 0 ? activeRowProps : inactiveRowProps)}>
            {padRight('Max context tokens:', 18)}
          </Text>
          <SingleLineTextInput
            value={maxContextTokensDraft}
            onChange={onMaxContextTokensChange}
            onSubmit={() => onSubmit()}
            placeholder="unset"
            focus={normalizedSelection === 0}
            width={Math.max(1, contentWidth - 18)}
            backgroundColor={theme.popupBackground}
          />
        </Box>
        <Box flexDirection="row">
          <Text {...(normalizedSelection === 1 ? activeRowProps : inactiveRowProps)}>
            {padRight('Max input tokens:', 18)}
          </Text>
          <SingleLineTextInput
            value={maxInputTokensDraft}
            onChange={onMaxInputTokensChange}
            onSubmit={() => onSubmit()}
            placeholder="unset"
            focus={normalizedSelection === 1}
            width={Math.max(1, contentWidth - 18)}
            backgroundColor={theme.popupBackground}
          />
        </Box>
        <Box flexDirection="row">
          <Text {...(normalizedSelection === 2 ? activeRowProps : inactiveRowProps)}>
            {padRight('Overflow strategy:', 18)}
          </Text>
          <Text {...(normalizedSelection === 2 ? activeRowProps : inactiveRowProps)}>
            {padRight(formatStrategy(contextOverflowStrategyDraft), contentWidth - 18)}
          </Text>
        </Box>
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight(effectiveLine, contentWidth)}
      </Text>
      {errorMessage ? (
        <Text {...backgroundProps} {...inkColorProps(theme.error)}>
          {padRight(errorMessage, contentWidth)}
        </Text>
      ) : null}
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Tip: clearing both token fields disables budgets.', contentWidth)}
      </Text>
    </PopupSheet>
  )
}
export const BUDGET_STRATEGY_OPTIONS = STRATEGY_OPTIONS
</file>

<file path="src/tui/components/popups/ExportPopup.tsx">
import { Box, Text, useStdout } from 'ink'
import type { ExportHistoryItem } from '../../types'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, Math.max(0, width - 1)).concat('…') : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
const joinItemLine = (item: ExportHistoryItem): string => `${item.title} · ${item.detail}`
export type ExportPopupProps = {
  selectionIndex: number
  format: 'json' | 'yaml'
  outPathDraft: string
  historyItems: ExportHistoryItem[]
  historySelectionIndex: number
  historyErrorMessage: string | null
  onOutPathChange: (next: string) => void
  onSubmit: () => void
}
export const ExportPopup = ({
  selectionIndex,
  format,
  outPathDraft,
  historyItems,
  historySelectionIndex,
  historyErrorMessage,
  onOutPathChange,
  onSubmit,
}: ExportPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 54, 92)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const activeRowProps = {
    ...inkColorProps(theme.selectionText),
    ...inkBackgroundColorProps(theme.selectionBackground),
  }
  const inactiveRowProps = { ...backgroundProps, ...inkColorProps(theme.text) }
  const normalizedSelection = clamp(selectionIndex, 0, 2)
  const visibleRows = 7
  const historyStartIndex = clamp(
    historySelectionIndex - Math.floor(visibleRows / 2),
    0,
    Math.max(0, historyItems.length - visibleRows),
  )
  const historySlice = historyItems.slice(historyStartIndex, historyStartIndex + visibleRows)
  return (
    <PopupSheet
      width={popupWidth}
      height={18}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Export History Payload', contentWidth)}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('↑/↓ select · ←/→ change · Enter export · Esc close', contentWidth)}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Tip: use an absolute path, or relative to cwd.', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...(normalizedSelection === 0 ? activeRowProps : inactiveRowProps)}>
          {padRight(`Format: ${format}`, contentWidth)}
        </Text>
        <Box flexDirection="row">
          <Text {...(normalizedSelection === 1 ? activeRowProps : inactiveRowProps)}>
            {padRight('Out:', 6)}
          </Text>
          <SingleLineTextInput
            value={outPathDraft}
            onChange={onOutPathChange}
            onSubmit={() => onSubmit()}
            placeholder={`prompt-export.${format}`}
            focus={normalizedSelection === 1}
            width={Math.max(1, contentWidth - 6)}
            backgroundColor={theme.popupBackground}
          />
        </Box>
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        {historyItems.length === 0 ? (
          <Text
            {...backgroundProps}
            {...inkColorProps(historyErrorMessage ? theme.error : theme.mutedText)}
          >
            {padRight(historyErrorMessage ?? 'Loading history…', contentWidth)}
          </Text>
        ) : (
          historySlice.map((item, offset) => {
            const absoluteIndex = historyStartIndex + offset
            const isSelected = absoluteIndex === historySelectionIndex
            const rowProps =
              normalizedSelection === 2 && isSelected
                ? activeRowProps
                : { ...backgroundProps, ...inkColorProps(theme.text) }
            return (
              <Text key={item.selector} {...rowProps}>
                {padRight(`${isSelected ? '›' : ' '} ${joinItemLine(item)}`, contentWidth)}
              </Text>
            )
          })
        )}
      </Box>
    </PopupSheet>
  )
}
</file>

<file path="src/tui/components/popups/IntentFilePopup.tsx">
import { useMemo, type ComponentProps } from 'react'
import { Box, Text, useStdout } from 'ink'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { clampSelectionIndex, resolveWindowedValues } from './list-windowing'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type IntentFilePopupProps = {
  draft: string
  suggestions: readonly string[]
  suggestedSelectionIndex: number
  suggestedFocused: boolean
  maxHeight?: number
  onDraftChange: (value: string) => void
  onSubmitDraft: (value: string) => void
}
export const IntentFilePopup = ({
  draft,
  suggestions,
  suggestedSelectionIndex,
  suggestedFocused,
  maxHeight,
  onDraftChange,
  onSubmitDraft,
}: IntentFilePopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const popupHeight = Math.max(9, Math.floor(maxHeight ?? 9))
  const suggestionRows = useMemo(() => {
    const paddingRows = 2 * POPUP_PADDING_Y
    const contentRows = Math.max(1, popupHeight - paddingRows)
    const fixedRows = 3
    return Math.max(0, contentRows - fixedRows)
  }, [popupHeight])
  const hasSuggestions = suggestions.length > 0
  const safeSuggestedSelection = clampSelectionIndex(suggestions.length, suggestedSelectionIndex)
  const effectiveSuggestedFocused = hasSuggestions && suggestedFocused
  const visibleSuggestions = useMemo(
    () => resolveWindowedValues(suggestions, safeSuggestedSelection, suggestionRows, { lead: 1 }),
    [safeSuggestedSelection, suggestions, suggestionRows],
  )
  const focusedSelectionProps = {
    ...inkColorProps(theme.selectionText),
    ...inkBackgroundColorProps(theme.selectionBackground),
  }
  const unfocusedSelectionProps = {
    ...inkColorProps(theme.chipText),
    ...inkBackgroundColorProps(theme.chipBackground),
  }
  const suggestionLines = useMemo(() => {
    const lines: Array<{ key: string; label: string; props: ComponentProps<typeof Text> }> = []
    if (!hasSuggestions) {
      lines.push({
        key: 'empty',
        label: '(type to search)',
        props: { ...backgroundProps, ...inkColorProps(theme.mutedText) },
      })
    } else {
      if (visibleSuggestions.showBefore) {
        lines.push({
          key: 'before',
          label: '… earlier …',
          props: { ...backgroundProps, ...inkColorProps(theme.mutedText) },
        })
      }
      visibleSuggestions.values.forEach((value, index) => {
        const actualIndex = visibleSuggestions.start + index
        const isSelected = actualIndex === safeSuggestedSelection
        const textProps = isSelected
          ? effectiveSuggestedFocused
            ? focusedSelectionProps
            : unfocusedSelectionProps
          : { ...backgroundProps, ...inkColorProps(theme.text) }
        lines.push({ key: `${value}-${actualIndex}`, label: value, props: textProps })
      })
      if (visibleSuggestions.showAfter) {
        lines.push({
          key: 'after',
          label: '… later …',
          props: { ...backgroundProps, ...inkColorProps(theme.mutedText) },
        })
      }
    }
    while (lines.length < suggestionRows) {
      lines.push({ key: `pad-${lines.length}`, label: '', props: backgroundProps })
    }
    return lines
  }, [
    backgroundProps,
    effectiveSuggestedFocused,
    focusedSelectionProps,
    hasSuggestions,
    safeSuggestedSelection,
    suggestionRows,
    theme.mutedText,
    theme.text,
    unfocusedSelectionProps,
    visibleSuggestions.showAfter,
    visibleSuggestions.showBefore,
    visibleSuggestions.start,
    visibleSuggestions.values,
  ])
  const pathLabel = 'Path: '
  const inputWidth = Math.max(1, contentWidth - pathLabel.length)
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Intent File', contentWidth)}
      </Text>
      <Box flexDirection="row">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {pathLabel}
        </Text>
        <SingleLineTextInput
          value={draft}
          onChange={onDraftChange}
          onSubmit={() => onSubmitDraft(draft)}
          placeholder="prompts/intent.md"
          focus={!effectiveSuggestedFocused}
          width={inputWidth}
          backgroundColor={theme.popupBackground}
        />
      </Box>
      {suggestionRows > 0 ? (
        <Box flexDirection="column" height={suggestionRows} flexShrink={0} overflow="hidden">
          {suggestionLines.map((line) => (
            <Text key={line.key} {...line.props}>
              {padRight(line.label, contentWidth)}
            </Text>
          ))}
        </Box>
      ) : null}
      <Box flexShrink={0}>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Tab suggestions · ↑/↓ select · Enter apply · Esc close', contentWidth)}
        </Text>
      </Box>
    </PopupSheet>
  )
}
</file>

<file path="src/tui/components/popups/list-popup-layout.ts">
export const DEFAULT_MAX_VISIBLE_LIST_ITEMS = 6
export const DEFAULT_MAX_VISIBLE_SUGGESTIONS = 4
export type ListPopupHeights = {
  selectedRows: number
  suggestionRows: number
}
type ResolveListPopupHeightsOptions = {
  maxHeight: number | undefined
  hasSuggestions: boolean
  instructionRows?: number
}
export const resolveListPopupHeights = ({
  maxHeight,
  hasSuggestions,
  instructionRows = 1,
}: ResolveListPopupHeightsOptions): ListPopupHeights => {
  if (!hasSuggestions) {
    return { selectedRows: DEFAULT_MAX_VISIBLE_LIST_ITEMS, suggestionRows: 0 }
  }
  const fallbackHeight = 16
  const resolvedHeight = maxHeight ?? fallbackHeight
  const paddingRows = 4
  const contentHeight = Math.max(1, resolvedHeight - paddingRows)
  const safeInstructionRows = Math.max(1, Math.floor(instructionRows))
  const fixedRows = 4 + safeInstructionRows
  const availableRows = Math.max(contentHeight - fixedRows, 1)
  const selectedMin = Math.min(3, availableRows)
  const suggestionRows = Math.min(
    DEFAULT_MAX_VISIBLE_SUGGESTIONS,
    Math.max(0, availableRows - selectedMin),
  )
  const selectedRows = Math.max(1, availableRows - suggestionRows)
  return { selectedRows, suggestionRows }
}
</file>

<file path="src/tui/components/popups/list-popup-model.ts">
import { DEFAULT_MAX_VISIBLE_LIST_ITEMS, resolveListPopupHeights } from './list-popup-layout'
import { resolveCursorWindow } from './list-window'
import { clampSelectionIndex, resolveWindowedValues } from './list-windowing'
export type ListPopupLayout = 'input-first' | 'selected-first'
export type ListPopupRowTone = 'text' | 'muted'
export type ListPopupRowSelection = 'none' | 'focused' | 'unfocused'
export type ListPopupRowModel = {
  key: string
  label: string
  tone: ListPopupRowTone
  selection: ListPopupRowSelection
}
export type ListPopupSectionId = 'selected' | 'suggestions'
export type ListPopupSectionModel = {
  id: ListPopupSectionId
  header: string
  rows: readonly ListPopupRowModel[]
  fixedRowCount?: number
}
export type ListPopupInputModel =
  | {
      variant: 'inline'
      label: string
      focus: boolean
    }
  | {
      variant: 'titled'
      title: string
      focus: boolean
    }
export type ListPopupInstructionsModel = {
  normalizedLines: readonly string[]
  renderLines: readonly string[]
  rowCount: number
}
export type ListPopupBlockModel =
  | {
      type: 'spacer'
      key: string
    }
  | {
      type: 'input'
      input: ListPopupInputModel
    }
  | {
      type: 'section'
      section: ListPopupSectionModel
    }
  | {
      type: 'instructions'
      lines: readonly string[]
    }
export type ListPopupModel = {
  hasSuggestions: boolean
  safeSuggestedSelection: number
  effectiveSuggestedFocused: boolean
  effectiveSelectedFocused: boolean
  shouldHighlightSelectedAsFocused: boolean
  input: ListPopupInputModel
  selectedSection: ListPopupSectionModel
  suggestionsSection?: ListPopupSectionModel
  instructions: ListPopupInstructionsModel
  blocks: readonly ListPopupBlockModel[]
}
export type BuildListPopupModelOptions = {
  items: readonly string[]
  selectedIndex: number
  emptyLabel: string
  instructions: string
  layout: ListPopupLayout
  popupHeight: number
  suggestedItems?: readonly string[]
  suggestedSelectionIndex?: number
  suggestedFocused?: boolean
  selectedFocused?: boolean
}
export const parseListPopupInstructions = (instructions: string): ListPopupInstructionsModel => {
  const normalized = instructions.replaceAll('\\n', '\n')
  const normalizedLines = normalized
    .split('\n')
    .map((line) => line.trim())
    .filter((line) => line.length > 0)
  return {
    normalizedLines,
    renderLines: normalizedLines.length > 0 ? normalizedLines : [instructions],
    rowCount: Math.max(1, normalizedLines.length),
  }
}
const buildSelectedRowsFixedHeight = ({
  items,
  selectedIndex,
  emptyLabel,
  maxRows,
  shouldHighlightSelectedAsFocused,
}: {
  items: readonly string[]
  selectedIndex: number
  emptyLabel: string
  maxRows: number
  shouldHighlightSelectedAsFocused: boolean
}): readonly ListPopupRowModel[] => {
  const rows: Array<ListPopupRowModel> = []
  if (items.length === 0) {
    rows.push({ key: 'empty', label: emptyLabel, tone: 'muted', selection: 'none' })
  } else {
    const window = resolveWindowedValues(items, selectedIndex, maxRows)
    if (window.showBefore) {
      rows.push({
        key: 'before',
        label: '… earlier entries …',
        tone: 'muted',
        selection: 'none',
      })
    }
    window.values.forEach((value, index) => {
      const actualIndex = window.start + index
      const isSelected = actualIndex === selectedIndex
      rows.push({
        key: `${value}-${actualIndex}`,
        label: `${actualIndex + 1}. ${value}`,
        tone: 'text',
        selection: isSelected
          ? shouldHighlightSelectedAsFocused
            ? 'focused'
            : 'unfocused'
          : 'none',
      })
    })
    if (window.showAfter) {
      rows.push({
        key: 'after',
        label: '… later entries …',
        tone: 'muted',
        selection: 'none',
      })
    }
  }
  while (rows.length < maxRows) {
    rows.push({ key: `pad-${rows.length}`, label: '', tone: 'text', selection: 'none' })
  }
  return rows
}
const buildSelectedRowsFreeHeight = ({
  items,
  selectedIndex,
  emptyLabel,
  shouldHighlightSelectedAsFocused,
}: {
  items: readonly string[]
  selectedIndex: number
  emptyLabel: string
  shouldHighlightSelectedAsFocused: boolean
}): readonly ListPopupRowModel[] => {
  const rows: Array<ListPopupRowModel> = []
  if (items.length === 0) {
    rows.push({ key: 'empty', label: emptyLabel, tone: 'muted', selection: 'none' })
    return rows
  }
  const range = resolveCursorWindow(items.length, selectedIndex, DEFAULT_MAX_VISIBLE_LIST_ITEMS)
  const start = range.startIndex
  const end = range.endIndexExclusive
  if (start > 0) {
    rows.push({ key: 'before', label: '… earlier entries …', tone: 'muted', selection: 'none' })
  }
  items.slice(start, end).forEach((value, index) => {
    const actualIndex = start + index
    const isSelected = actualIndex === selectedIndex
    rows.push({
      key: `${value}-${actualIndex}`,
      label: `${actualIndex + 1}. ${value}`,
      tone: 'text',
      selection: isSelected ? (shouldHighlightSelectedAsFocused ? 'focused' : 'unfocused') : 'none',
    })
  })
  if (end < items.length) {
    rows.push({ key: 'after', label: '… later entries …', tone: 'muted', selection: 'none' })
  }
  return rows
}
const buildSuggestionRows = ({
  suggestedItems,
  safeSuggestedSelection,
  maxRows,
  effectiveSuggestedFocused,
}: {
  suggestedItems: readonly string[]
  safeSuggestedSelection: number
  maxRows: number
  effectiveSuggestedFocused: boolean
}): readonly ListPopupRowModel[] => {
  const rows: Array<ListPopupRowModel> = []
  if (maxRows <= 0) {
    return rows
  }
  const window = resolveWindowedValues(suggestedItems, safeSuggestedSelection, maxRows, { lead: 1 })
  if (window.showBefore) {
    rows.push({
      key: 'before',
      label: '… earlier suggestions …',
      tone: 'muted',
      selection: 'none',
    })
  }
  window.values.forEach((value, index) => {
    const actualIndex = window.start + index
    const isSelected = actualIndex === safeSuggestedSelection
    rows.push({
      key: `${value}-${actualIndex}`,
      label: value,
      tone: 'text',
      selection: isSelected ? (effectiveSuggestedFocused ? 'focused' : 'unfocused') : 'none',
    })
  })
  if (window.showAfter) {
    rows.push({
      key: 'after',
      label: '… later suggestions …',
      tone: 'muted',
      selection: 'none',
    })
  }
  while (rows.length < maxRows) {
    rows.push({ key: `pad-${rows.length}`, label: '', tone: 'text', selection: 'none' })
  }
  return rows
}
export const buildListPopupModel = (options: BuildListPopupModelOptions): ListPopupModel => {
  const {
    items,
    selectedIndex,
    emptyLabel,
    instructions,
    suggestedItems,
    suggestedSelectionIndex,
    suggestedFocused,
    selectedFocused,
    layout,
    popupHeight,
  } = options
  const safeSuggestedItems = suggestedItems ?? []
  const hasSuggestions = safeSuggestedItems.length > 0
  const safeSuggestedSelection = clampSelectionIndex(
    safeSuggestedItems.length,
    suggestedSelectionIndex ?? 0,
  )
  const effectiveSuggestedFocused = Boolean(hasSuggestions && suggestedFocused)
  const effectiveSelectedFocused = Boolean(selectedFocused)
  const shouldHighlightSelectedAsFocused = selectedFocused ?? true
  const instructionsModel = parseListPopupInstructions(instructions)
  const heights = resolveListPopupHeights({
    maxHeight: popupHeight,
    hasSuggestions,
    instructionRows: instructionsModel.rowCount,
  })
  const selectedRows = hasSuggestions
    ? buildSelectedRowsFixedHeight({
        items,
        selectedIndex,
        emptyLabel,
        maxRows: heights.selectedRows,
        shouldHighlightSelectedAsFocused,
      })
    : buildSelectedRowsFreeHeight({
        items,
        selectedIndex,
        emptyLabel,
        shouldHighlightSelectedAsFocused,
      })
  const selectedSection: ListPopupSectionModel = {
    id: 'selected',
    header: 'Selected',
    rows: selectedRows,
    ...(hasSuggestions ? { fixedRowCount: heights.selectedRows } : {}),
  }
  const suggestionsSection: ListPopupSectionModel | undefined =
    hasSuggestions && heights.suggestionRows > 0
      ? {
          id: 'suggestions',
          header: 'Suggestions',
          rows: buildSuggestionRows({
            suggestedItems: safeSuggestedItems,
            safeSuggestedSelection,
            maxRows: heights.suggestionRows,
            effectiveSuggestedFocused,
          }),
          fixedRowCount: heights.suggestionRows,
        }
      : undefined
  const input: ListPopupInputModel = hasSuggestions
    ? {
        variant: 'inline',
        label: 'Add:',
        focus: !effectiveSuggestedFocused && !effectiveSelectedFocused,
      }
    : {
        variant: 'titled',
        title: 'Add new',
        focus: !effectiveSelectedFocused,
      }
  const blocks: Array<ListPopupBlockModel> = []
  if (!hasSuggestions) {
    blocks.push({ type: 'spacer', key: 'after-title' })
  }
  const pushInputAndSelected = (): void => {
    if (layout === 'selected-first') {
      blocks.push({ type: 'section', section: selectedSection })
      if (!hasSuggestions) {
        blocks.push({ type: 'spacer', key: 'between-selected-and-input' })
      }
      blocks.push({ type: 'input', input })
      return
    }
    blocks.push({ type: 'input', input })
    if (!hasSuggestions) {
      blocks.push({ type: 'spacer', key: 'between-input-and-selected' })
    }
    blocks.push({ type: 'section', section: selectedSection })
  }
  pushInputAndSelected()
  if (suggestionsSection) {
    blocks.push({ type: 'section', section: suggestionsSection })
  }
  if (!hasSuggestions && instructionsModel.rowCount <= 1) {
    blocks.push({ type: 'spacer', key: 'before-instructions' })
  }
  blocks.push({ type: 'instructions', lines: instructionsModel.renderLines })
  return {
    hasSuggestions,
    safeSuggestedSelection,
    effectiveSuggestedFocused,
    effectiveSelectedFocused,
    shouldHighlightSelectedAsFocused,
    input,
    selectedSection,
    ...(suggestionsSection ? { suggestionsSection } : {}),
    instructions: instructionsModel,
    blocks,
  }
}
</file>

<file path="src/tui/components/popups/list-windowing.ts">
import { resolveWindowedList } from './list-window'
export type WindowedValues<T> = {
  start: number
  end: number
  values: readonly T[]
  showBefore: boolean
  showAfter: boolean
}
const clamp = (value: number, min: number, max: number): number => {
  if (value < min) {
    return min
  }
  if (value > max) {
    return max
  }
  return value
}
export const clampSelectionIndex = (itemCount: number, selectedIndex: number): number => {
  if (itemCount <= 0) {
    return 0
  }
  return clamp(selectedIndex, 0, itemCount - 1)
}
export type ResolveWindowedValuesOptions = {
  lead?: number
}
export const resolveWindowedValues = <T>(
  items: readonly T[],
  selectedIndex: number,
  maxVisibleRows: number,
  options: ResolveWindowedValuesOptions = {},
): WindowedValues<T> => {
  if (items.length === 0 || maxVisibleRows <= 0) {
    return {
      start: 0,
      end: 0,
      values: [],
      showBefore: false,
      showAfter: false,
    }
  }
  const window = resolveWindowedList({
    itemCount: items.length,
    selectedIndex,
    maxVisibleRows,
    ...(options.lead === undefined ? {} : { lead: options.lead }),
  })
  return {
    start: window.start,
    end: window.end,
    values: items.slice(window.start, window.end),
    showBefore: window.showBefore,
    showAfter: window.showAfter,
  }
}
export type WindowBounds = {
  start: number
  end: number
}
export const ensureLeadingHeaderVisible = <Row extends { type: string }>(
  rows: readonly Row[],
  bounds: WindowBounds,
  maxRows: number,
  headerType: Row['type'],
  itemType: Row['type'],
): WindowBounds => {
  const { start, end } = bounds
  if (start <= 0 || end - start >= maxRows) {
    return bounds
  }
  const first = rows[start]
  const previous = rows[start - 1]
  if (first?.type === itemType && previous?.type === headerType) {
    const nextStart = start - 1
    const nextEnd = Math.min(rows.length, nextStart + maxRows)
    return { start: nextStart, end: nextEnd }
  }
  return bounds
}
</file>

<file path="src/tui/components/popups/ResumePopup.tsx">
import { Box, Text, useStdout } from 'ink'
import type { ResumeHistoryItem, ResumeMode, ResumeSourceKind } from '../../types'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, Math.max(0, width - 1)).concat('…') : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
const formatMode = (mode: ResumeMode): string => (mode === 'strict' ? 'strict' : 'best-effort')
const formatSource = (source: ResumeSourceKind): string => (source === 'file' ? 'file' : 'history')
const joinItemLine = (item: ResumeHistoryItem): string => `${item.title} · ${item.detail}`
export type ResumePopupProps = {
  selectionIndex: number
  sourceKind: ResumeSourceKind
  mode: ResumeMode
  historyItems: ResumeHistoryItem[]
  historySelectionIndex: number
  historyErrorMessage: string | null
  payloadPathDraft: string
  suggestedItems: string[]
  suggestedSelectionIndex: number
  suggestedFocused: boolean
  onPayloadPathChange: (next: string) => void
  onSubmit: () => void
}
export const ResumePopup = ({
  selectionIndex,
  sourceKind,
  mode,
  historyItems,
  historySelectionIndex,
  historyErrorMessage,
  payloadPathDraft,
  suggestedItems,
  suggestedSelectionIndex,
  suggestedFocused,
  onPayloadPathChange,
  onSubmit,
}: ResumePopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 54, 92)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const activeRowProps = {
    ...inkColorProps(theme.selectionText),
    ...inkBackgroundColorProps(theme.selectionBackground),
  }
  const inactiveRowProps = { ...backgroundProps, ...inkColorProps(theme.text) }
  const normalizedSelection = clamp(selectionIndex, 0, 2)
  const visibleRows = 7
  const historyStartIndex = clamp(
    historySelectionIndex - Math.floor(visibleRows / 2),
    0,
    Math.max(0, historyItems.length - visibleRows),
  )
  const historySlice = historyItems.slice(historyStartIndex, historyStartIndex + visibleRows)
  return (
    <PopupSheet
      width={popupWidth}
      height={18}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Resume Generation', contentWidth)}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight(
          '↑/↓ select · ←/→ change · Enter resume · Tab suggestions · Esc close',
          contentWidth,
        )}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Note: resumes file-path context only; URL/smart treated missing.', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...(normalizedSelection === 0 ? activeRowProps : inactiveRowProps)}>
          {padRight(`Source: ${formatSource(sourceKind)}`, contentWidth)}
        </Text>
        <Text {...(normalizedSelection === 1 ? activeRowProps : inactiveRowProps)}>
          {padRight(`Mode: ${formatMode(mode)}`, contentWidth)}
        </Text>
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      {sourceKind === 'history' ? (
        <Box flexDirection="column">
          {historyItems.length === 0 ? (
            <Text
              {...backgroundProps}
              {...inkColorProps(historyErrorMessage ? theme.error : theme.mutedText)}
            >
              {padRight(historyErrorMessage ?? 'Loading history…', contentWidth)}
            </Text>
          ) : (
            historySlice.map((item, offset) => {
              const absoluteIndex = historyStartIndex + offset
              const isSelected = absoluteIndex === historySelectionIndex
              const rowProps =
                normalizedSelection === 2 && isSelected
                  ? activeRowProps
                  : { ...backgroundProps, ...inkColorProps(theme.text) }
              return (
                <Text key={item.selector} {...rowProps}>
                  {padRight(`${isSelected ? '›' : ' '} ${joinItemLine(item)}`, contentWidth)}
                </Text>
              )
            })
          )}
        </Box>
      ) : (
        <Box flexDirection="column">
          <Box flexDirection="row">
            <Text
              {...(normalizedSelection === 2 && !suggestedFocused
                ? activeRowProps
                : inactiveRowProps)}
            >
              {padRight('Payload:', 9)}
            </Text>
            <SingleLineTextInput
              value={payloadPathDraft}
              onChange={onPayloadPathChange}
              onSubmit={() => onSubmit()}
              placeholder="path/to/payload.json"
              focus={normalizedSelection === 2 && !suggestedFocused}
              width={Math.max(1, contentWidth - 9)}
              backgroundColor={theme.popupBackground}
            />
          </Box>
          {suggestedFocused && suggestedItems.length > 0 ? (
            <Box flexDirection="column" marginTop={1}>
              {suggestedItems.slice(0, 6).map((suggestion, index) => {
                const isSelected = index === suggestedSelectionIndex
                const rowProps = isSelected ? activeRowProps : inactiveRowProps
                return (
                  <Text key={suggestion} {...rowProps}>
                    {padRight(`${isSelected ? '›' : ' '} ${suggestion}`, contentWidth)}
                  </Text>
                )
              })}
            </Box>
          ) : null}
        </Box>
      )}
    </PopupSheet>
  )
}
</file>

<file path="src/tui/components/popups/SmartPopup.tsx">
import { useMemo, type ComponentProps } from 'react'
import { Box, Text, useStdout } from 'ink'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { clampSelectionIndex, resolveWindowedValues } from './list-windowing'
import { PopupSheet } from './PopupSheet'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type SmartPopupProps = {
  savedRoot: string | null
  draft: string
  suggestedItems: readonly string[]
  suggestedSelectionIndex: number
  suggestedFocused: boolean
  maxHeight?: number
  onDraftChange: (value: string) => void
  onSubmitRoot: (value: string) => void
}
export const SmartPopup = ({
  savedRoot,
  draft,
  suggestedItems,
  suggestedSelectionIndex,
  suggestedFocused,
  maxHeight,
  onDraftChange,
  onSubmitRoot,
}: SmartPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const hasSuggestions = suggestedItems.length > 0
  const safeSuggestedSelection = clampSelectionIndex(suggestedItems.length, suggestedSelectionIndex)
  const effectiveSuggestedFocused = hasSuggestions && suggestedFocused
  const popupHeight = Math.max(9, Math.floor(maxHeight ?? 9))
  const suggestionRows = useMemo(() => {
    const paddingRows = 2 * POPUP_PADDING_Y
    const contentRows = Math.max(1, popupHeight - paddingRows)
    const fixedRows = 5
    return Math.max(0, contentRows - fixedRows)
  }, [popupHeight])
  const visibleSuggestions = useMemo(
    () =>
      resolveWindowedValues(suggestedItems, safeSuggestedSelection, suggestionRows, { lead: 1 }),
    [safeSuggestedSelection, suggestedItems, suggestionRows],
  )
  const savedLabel = savedRoot ? savedRoot : '(none)'
  const focusedSelectionProps = {
    ...inkColorProps(theme.selectionText),
    ...inkBackgroundColorProps(theme.selectionBackground),
  }
  const unfocusedSelectionProps = {
    ...inkColorProps(theme.chipText),
    ...inkBackgroundColorProps(theme.chipBackground),
  }
  const suggestionLines = useMemo(() => {
    const lines: Array<{ key: string; label: string; props: ComponentProps<typeof Text> }> = []
    if (!hasSuggestions) {
      lines.push({
        key: 'empty',
        label: '(type to filter)',
        props: { ...backgroundProps, ...inkColorProps(theme.mutedText) },
      })
    } else {
      if (visibleSuggestions.showBefore) {
        lines.push({
          key: 'before',
          label: '… earlier …',
          props: { ...backgroundProps, ...inkColorProps(theme.mutedText) },
        })
      }
      visibleSuggestions.values.forEach((value, index) => {
        const actualIndex = visibleSuggestions.start + index
        const isSelected = actualIndex === safeSuggestedSelection
        const textProps = isSelected
          ? effectiveSuggestedFocused
            ? focusedSelectionProps
            : unfocusedSelectionProps
          : { ...backgroundProps, ...inkColorProps(theme.text) }
        lines.push({ key: `${value}-${actualIndex}`, label: value, props: textProps })
      })
      if (visibleSuggestions.showAfter) {
        lines.push({
          key: 'after',
          label: '… later …',
          props: { ...backgroundProps, ...inkColorProps(theme.mutedText) },
        })
      }
    }
    while (lines.length < suggestionRows) {
      lines.push({ key: `pad-${lines.length}`, label: '', props: backgroundProps })
    }
    return lines
  }, [
    backgroundProps,
    effectiveSuggestedFocused,
    focusedSelectionProps,
    hasSuggestions,
    safeSuggestedSelection,
    suggestionRows,
    theme.mutedText,
    theme.text,
    unfocusedSelectionProps,
    visibleSuggestions.showAfter,
    visibleSuggestions.showBefore,
    visibleSuggestions.start,
    visibleSuggestions.values,
  ])
  const rootLabel = 'Root: '
  const inputWidth = Math.max(1, contentWidth - rootLabel.length)
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Smart Context Root', contentWidth)}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.text)}>
        {padRight('Enter to save · Tab suggestions · Esc close', contentWidth)}
      </Text>
      <Box flexDirection="row">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {rootLabel}
        </Text>
        <SingleLineTextInput
          value={draft}
          onChange={onDraftChange}
          onSubmit={onSubmitRoot}
          placeholder="relative/dir"
          focus={!effectiveSuggestedFocused}
          width={inputWidth}
          backgroundColor={theme.popupBackground}
        />
      </Box>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight(`Saved root: ${savedLabel}`, contentWidth)}
      </Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Suggestions', contentWidth)}
      </Text>
      {suggestionRows > 0 ? (
        <Box flexDirection="column" height={suggestionRows} flexShrink={0} overflow="hidden">
          {suggestionLines.map((line) => (
            <Text key={line.key} {...line.props}>
              {padRight(line.label, contentWidth)}
            </Text>
          ))}
        </Box>
      ) : null}
    </PopupSheet>
  )
}
</file>

<file path="src/tui/components/popups/ThemePickerPopup.tsx">
import { useMemo } from 'react'
import { Box, Text, useStdout } from 'ink'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { resolveWindowedValues } from './list-windowing'
import { PopupSheet } from './PopupSheet'
export type ThemePickerPopupProps = {
  selectionIndex: number
  initialThemeName: string
  maxHeight?: number
}
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const resolveListRows = (popupHeight: number, hasError: boolean): number => {
  const paddingRows = 2 * POPUP_PADDING_Y
  const contentHeight = Math.max(1, popupHeight - paddingRows)
  const fixedRows = 4 + (hasError ? 1 : 0)
  return Math.max(1, contentHeight - fixedRows)
}
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  if (trimmed.length === width) {
    return trimmed
  }
  return `${trimmed}${' '.repeat(width - trimmed.length)}`
}
export const ThemePickerPopup = ({
  selectionIndex,
  initialThemeName,
  maxHeight,
}: ThemePickerPopupProps) => {
  const { theme, themes, activeThemeName, error } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  // Keep the popup reasonably sized and deterministic.
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(10, popupWidth - paddingColumns)
  const fallbackHeight = 16
  const popupHeight = Math.max(10, Math.floor(maxHeight ?? fallbackHeight))
  const listRows = useMemo(() => resolveListRows(popupHeight, Boolean(error)), [error, popupHeight])
  const names = useMemo(() => themes.map((descriptor) => descriptor.name), [themes])
  const labelsByName = useMemo(() => {
    const entries = themes.map((descriptor) => [descriptor.name, descriptor.label] as const)
    return new Map(entries)
  }, [themes])
  const initialLabel = labelsByName.get(initialThemeName) ?? initialThemeName
  const clampedSelection = Math.min(selectionIndex, Math.max(names.length - 1, 0))
  const window = useMemo(
    () => resolveWindowedValues(names, clampedSelection, listRows),
    [clampedSelection, listRows, names],
  )
  const selectedTextProps = {
    ...inkColorProps(theme.selectionText),
    ...inkBackgroundColorProps(theme.selectionBackground),
  }
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const renderFill = (width: number): string => (width > 0 ? ' '.repeat(width) : '')
  const headerLeft = 'Theme'
  const headerRight = 'esc'
  const headerGap = Math.max(0, contentWidth - headerLeft.length - headerRight.length)
  const currentLabel = `Current: ${initialLabel}`
  const listLines = useMemo((): Array<{ key: string; label: string; isSelected: boolean }> => {
    const lines: Array<{ key: string; label: string; isSelected: boolean }> = []
    if (names.length === 0) {
      lines.push({
        key: 'empty',
        label: padRight('No themes loaded.', contentWidth),
        isSelected: false,
      })
      while (lines.length < listRows) {
        lines.push({
          key: `pad-${lines.length}`,
          label: padRight('', contentWidth),
          isSelected: false,
        })
      }
      return lines
    }
    if (window.showBefore) {
      lines.push({ key: 'before', label: '… earlier …', isSelected: false })
    }
    window.values.forEach((name, offset) => {
      const label = labelsByName.get(name) ?? name
      const isActive = name === activeThemeName
      const line = `${isActive ? '●' : ' '} ${label}`
      const isSelected = window.start + offset === clampedSelection
      lines.push({ key: name, label: line, isSelected })
    })
    if (window.showAfter) {
      lines.push({ key: 'after', label: '… later …', isSelected: false })
    }
    while (lines.length < listRows) {
      lines.push({ key: `pad-${lines.length}`, label: '', isSelected: false })
    }
    return lines.map((line) => ({ ...line, label: padRight(line.label, contentWidth) }))
  }, [activeThemeName, clampedSelection, contentWidth, labelsByName, listRows, names, window])
  const footer = '↑/↓ preview · Enter confirm · Esc cancel'
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Box flexDirection="row">
        <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
          {headerLeft}
        </Text>
        <Text {...backgroundProps}>{renderFill(headerGap)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {headerRight}
        </Text>
      </Box>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight(currentLabel, contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        {listLines.map((line) => (
          <Text
            key={line.key}
            {...(line.isSelected
              ? selectedTextProps
              : { ...backgroundProps, ...inkColorProps(theme.text) })}
          >
            {line.label}
          </Text>
        ))}
      </Box>
      {error ? (
        <Text {...backgroundProps} {...inkColorProps(theme.error)}>
          {padRight(error.message, contentWidth)}
        </Text>
      ) : null}
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight(footer, contentWidth)}
      </Text>
    </PopupSheet>
  )
}
</file>

<file path="src/tui/components/popups/TokenUsagePopup.tsx">
import { Box, Text, useStdout } from 'ink'
import type { TokenUsageBreakdown, TokenUsageRun } from '../../token-usage-store'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
const formatNumber = (value: number): string => value.toLocaleString('en-US')
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
const formatUsd = (value: number | null): string => {
  if (value === null) {
    return 'n/a'
  }
  if (value === 0) {
    return '$0.00'
  }
  if (value < 0.01) {
    return `$${value.toFixed(4)}`
  }
  return `$${value.toFixed(2)}`
}
const padCell = (value: string, width: number, align: 'left' | 'right'): string => {
  if (value.length >= width) {
    return value
  }
  const padding = ' '.repeat(width - value.length)
  return align === 'right' ? `${padding}${value}` : `${value}${padding}`
}
type Row = {
  label: string
  tokens: number
}
const renderTable = (rows: readonly Row[]): string[] => {
  const labelWidth = Math.max(12, ...rows.map((row) => row.label.length))
  const tokenWidth = Math.max(8, ...rows.map((row) => formatNumber(row.tokens).length))
  return rows.map((row) => {
    const label = padCell(row.label, labelWidth, 'left')
    const tokens = padCell(formatNumber(row.tokens), tokenWidth, 'right')
    return `${label}  ${tokens}`
  })
}
export type TokenUsagePopupProps = {
  run: TokenUsageRun | null
  breakdown: TokenUsageBreakdown | null
  budgets: import('../../budget-settings').BudgetSettings
  latestContextOverflow: import('../../generation-pipeline-reducer').ContextOverflowDetails | null
}
export const TokenUsagePopup = ({
  run,
  breakdown,
  budgets,
  latestContextOverflow,
}: TokenUsagePopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const terminalRows = stdout?.rows ?? 24
  const popupHeight = clamp(terminalRows - 6, 18, 34)
  const budgetsEnabled = budgets.maxContextTokens !== null || budgets.maxInputTokens !== null
  const effectiveOverflow = budgets.contextOverflowStrategy ?? (budgetsEnabled ? 'fail' : null)
  const overflowSummary = latestContextOverflow
    ? `Last overflow: ${latestContextOverflow.strategy} · dropped ${latestContextOverflow.droppedPaths.length}`
    : 'Last overflow: none'
  const overflowPreview = latestContextOverflow
    ? (() => {
        const previewLimit = 5
        const preview = latestContextOverflow.droppedPaths
          .slice(0, previewLimit)
          .map((entry) => entry.path)
        const remaining = latestContextOverflow.droppedPaths.length - preview.length
        const suffix = remaining > 0 ? ` …(+${remaining} more)` : ''
        return preview.length > 0 ? `Dropped: ${preview.join(', ')}${suffix}` : 'Dropped: (none)'
      })()
    : null
  if (!run || !breakdown) {
    return (
      <PopupSheet
        width={popupWidth}
        height={popupHeight}
        paddingX={POPUP_PADDING_X}
        paddingY={POPUP_PADDING_Y}
        background={theme.popupBackground}
      >
        <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
          {padRight('Token Usage', contentWidth)}
        </Text>
        <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('No token usage recorded yet. Run generation first.', contentWidth)}
        </Text>
        <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight(
            `Budgets: input=${budgets.maxInputTokens ?? 'unset'} · context=${budgets.maxContextTokens ?? 'unset'} · overflow=${effectiveOverflow ?? 'unset'}`,
            contentWidth,
          )}
        </Text>
        <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight(overflowSummary, contentWidth)}
        </Text>
        {overflowPreview ? (
          <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
            {padRight(overflowPreview, contentWidth)}
          </Text>
        ) : null}
        <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Esc to close', contentWidth)}
        </Text>
      </PopupSheet>
    )
  }
  const inputRows = renderTable([
    { label: 'Intent', tokens: breakdown.input.intent },
    { label: 'Files', tokens: breakdown.input.files },
    { label: 'System', tokens: breakdown.input.system },
    { label: 'Input total', tokens: breakdown.input.total },
  ])
  const outputRows = renderTable([
    { label: 'Reasoning', tokens: breakdown.output.reasoning },
    { label: 'Final prompt', tokens: breakdown.output.prompt },
    { label: 'Output total', tokens: breakdown.output.total },
  ])
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight('Token Usage', contentWidth)}
      </Text>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.text)}>
          {padRight(`Model: ${run.model}`, contentWidth)}
        </Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight(`Started: ${run.startedAt}`, contentWidth)}
        </Text>
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Input', contentWidth)}
        </Text>
        {inputRows.map((line) => (
          <Text key={`input-${line}`} {...backgroundProps} {...inkColorProps(theme.text)}>
            {padRight(line, contentWidth)}
          </Text>
        ))}
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Output', contentWidth)}
        </Text>
        {outputRows.map((line) => (
          <Text key={`output-${line}`} {...backgroundProps} {...inkColorProps(theme.text)}>
            {padRight(line, contentWidth)}
          </Text>
        ))}
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Totals', contentWidth)}
        </Text>
        <Text {...backgroundProps} {...inkColorProps(theme.text)}>
          {padRight(`Total tokens ${formatNumber(breakdown.totals.tokens)}`, contentWidth)}
        </Text>
        <Text {...backgroundProps} {...inkColorProps(theme.text)}>
          {padRight(`Estimated cost ${formatUsd(breakdown.totals.estimatedCostUsd)}`, contentWidth)}
        </Text>
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Budgets', contentWidth)}
        </Text>
        <Text {...backgroundProps} {...inkColorProps(theme.text)}>
          {padRight(
            `input=${budgets.maxInputTokens ?? 'unset'} · context=${budgets.maxContextTokens ?? 'unset'} · overflow=${effectiveOverflow ?? 'unset'}`,
            contentWidth,
          )}
        </Text>
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight('Context Overflow', contentWidth)}
        </Text>
        <Text {...backgroundProps} {...inkColorProps(theme.text)}>
          {padRight(overflowSummary, contentWidth)}
        </Text>
        {overflowPreview ? (
          <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
            {padRight(overflowPreview, contentWidth)}
          </Text>
        ) : null}
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Esc to close', contentWidth)}
      </Text>
    </PopupSheet>
  )
}
</file>

<file path="src/tui/hooks/popup-manager/scan-orchestrator.ts">
import type { PopupAction, PopupScanKind } from '../../popup-reducer'
import type { HistoryEntry } from '../../types'
type ScanIdRef = {
  current: number
}
type PopupDispatch = (action: PopupAction) => void
type PushHistory = (content: string, kind?: HistoryEntry['kind']) => void
export type RunSuggestionScanOptions = {
  kind: PopupScanKind
  open: (scanId: number) => PopupAction
  scan: () => Promise<string[]>
}
export type PopupScanOrchestrator = {
  runSuggestionScan: (options: RunSuggestionScanOptions) => void
}
export const createPopupScanOrchestrator = ({
  scanIdRef,
  dispatch,
  pushHistory,
}: {
  scanIdRef: ScanIdRef
  dispatch: PopupDispatch
  pushHistory: PushHistory
}): PopupScanOrchestrator => {
  const nextScanId = (): number => {
    scanIdRef.current += 1
    return scanIdRef.current
  }
  const runSuggestionScan = ({ kind, open, scan }: RunSuggestionScanOptions): void => {
    const scanId = nextScanId()
    dispatch(open(scanId))
    const run = async (): Promise<void> => {
      try {
        const suggestions = await scan()
        dispatch({
          type: 'scan-suggestions-success',
          kind,
          scanId,
          suggestions,
        })
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown workspace scan error.'
        pushHistory(`[${kind}] Failed to scan workspace: ${message}`, 'system')
      }
    }
    void run()
  }
  return { runSuggestionScan }
}
</file>

<file path="src/tui/hooks/popup-scans.ts">
import path from 'node:path'
import {
  discoverDirectorySuggestions,
  discoverFileSuggestions,
  discoverIntentFileSuggestions,
  type DiscoverDirectorySuggestionsOptions,
  type DiscoverFileSuggestionsOptions,
  type DiscoverIntentFileSuggestionsOptions,
} from '../file-suggestions'
const IMAGE_EXTENSIONS = new Set(['.png', '.jpg', '.jpeg', '.webp', '.gif'])
const VIDEO_EXTENSIONS = new Set(['.mp4', '.mov', '.m4v', '.webm', '.avi', '.mpeg', '.mpg', '.gif'])
const PDF_EXTENSIONS = new Set(['.pdf'])
export const scanFileSuggestions = async (
  options: DiscoverFileSuggestionsOptions = {},
): Promise<string[]> => discoverFileSuggestions(options)
export const scanImageSuggestions = async (
  options: DiscoverFileSuggestionsOptions = {},
): Promise<string[]> => {
  const suggestions = await discoverFileSuggestions(options)
  return suggestions.filter((candidate) => {
    const ext = path.extname(candidate).toLowerCase()
    return IMAGE_EXTENSIONS.has(ext)
  })
}
export const scanVideoSuggestions = async (
  options: DiscoverFileSuggestionsOptions = {},
): Promise<string[]> => {
  const suggestions = await discoverFileSuggestions(options)
  return suggestions.filter((candidate) => {
    const ext = path.extname(candidate).toLowerCase()
    return VIDEO_EXTENSIONS.has(ext)
  })
}
export const scanPdfSuggestions = async (
  options: DiscoverFileSuggestionsOptions = {},
): Promise<string[]> => {
  const suggestions = await discoverFileSuggestions(options)
  return suggestions.filter((candidate) => {
    const ext = path.extname(candidate).toLowerCase()
    return PDF_EXTENSIONS.has(ext)
  })
}
export const scanSmartSuggestions = async (
  options: DiscoverDirectorySuggestionsOptions = {},
): Promise<string[]> => discoverDirectorySuggestions(options)
export const scanIntentSuggestions = async (
  options: DiscoverIntentFileSuggestionsOptions = {},
): Promise<string[]> => discoverIntentFileSuggestions(options)
</file>

<file path="src/tui/hooks/series-artifacts-io.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import type { SeriesResponse } from '../../prompt-generator-service'
import { buildSeriesOutputDirName, sanitizeForPathSegment } from '../../utils/series-path'
export type PrepareSeriesOutputDirResult = {
  seriesDir: string
  canWriteFiles: boolean
  errorMessage?: string
}
export const prepareSeriesOutputDir = async (
  intent: string,
  cwd: string = process.cwd(),
): Promise<PrepareSeriesOutputDirResult> => {
  const seriesDir = path.join(
    path.resolve(cwd, 'generated', 'series'),
    buildSeriesOutputDirName(intent),
  )
  try {
    await fs.mkdir(seriesDir, { recursive: true })
    return { seriesDir, canWriteFiles: true }
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown filesystem error.'
    return { seriesDir, canWriteFiles: false, errorMessage: message }
  }
}
export type WriteSeriesArtifactsResult = {
  writtenCount: number
  errors: Array<{ fileName: string; message: string }>
}
export const writeSeriesArtifacts = async (
  seriesDir: string,
  series: SeriesResponse,
): Promise<WriteSeriesArtifactsResult> => {
  const tasks: Array<{ fileName: string; content: string }> = []
  tasks.push({ fileName: '00-overview.md', content: series.overviewPrompt })
  series.atomicPrompts.forEach((step, index) => {
    const stepNumber = index + 1
    const stepPrefix = stepNumber.toString().padStart(2, '0')
    const titleSlug = sanitizeForPathSegment(step.title, 'step', 60)
    tasks.push({ fileName: `${stepPrefix}-${titleSlug}.md`, content: step.content })
  })
  const results = await Promise.allSettled(
    tasks.map(async (task) => {
      await fs.writeFile(path.join(seriesDir, task.fileName), task.content, 'utf8')
      return task.fileName
    }),
  )
  const errors: Array<{ fileName: string; message: string }> = []
  let writtenCount = 0
  results.forEach((result, index) => {
    const fileName = tasks[index]?.fileName ?? 'unknown'
    if (result.status === 'fulfilled') {
      writtenCount += 1
      return
    }
    const message = result.reason instanceof Error ? result.reason.message : String(result.reason)
    errors.push({ fileName, message })
  })
  return { writtenCount, errors }
}
</file>

<file path="src/tui/hooks/useCommandHistory.ts">
import { useCallback, useEffect, useMemo, useRef, useState } from 'react'
import { useLatestRef } from './useLatestRef'
import type { HistoryEntry } from '../types'
export type UseCommandHistoryOptions = {
  initialEntries: HistoryEntry[]
  visibleRows: number
}
export const useCommandHistory = ({
  initialEntries,
  visibleRows,
}: UseCommandHistoryOptions): {
  history: HistoryEntry[]
  pushHistory: (
    content: string,
    kind?: HistoryEntry['kind'],
    format?: HistoryEntry['format'],
  ) => void
  resetHistory: () => void
  clearHistory: () => void
  scroll: {
    offset: number
    scrollTo: (next: number) => void
    scrollBy: (delta: number) => void
  }
} => {
  const [history, setHistory] = useState<HistoryEntry[]>(() => [...initialEntries])
  const initialEntriesSnapshot = useMemo(() => [...initialEntries], [initialEntries])
  const initialEntriesRef = useLatestRef<HistoryEntry[]>(initialEntriesSnapshot)
  const historyIdRef = useRef(initialEntries.length)
  const [scrollOffset, setScrollOffset] = useState(0)
  const [isPinnedToBottom, setIsPinnedToBottom] = useState(true)
  const pushHistory = useCallback(
    (content: string, kind: HistoryEntry['kind'] = 'system', format?: HistoryEntry['format']) => {
      setHistory((prev) => [
        ...prev,
        {
          id: `entry-${historyIdRef.current++}`,
          content,
          kind,
          ...(format ? { format } : {}),
        },
      ])
      setIsPinnedToBottom(true)
    },
    [],
  )
  useEffect(() => {
    setScrollOffset((prev) => {
      const nextMax = Math.max(0, history.length - visibleRows)
      if (isPinnedToBottom) {
        return nextMax
      }
      return Math.min(prev, nextMax)
    })
  }, [history, visibleRows, isPinnedToBottom])
  const scrollTo = useCallback(
    (next: number) => {
      const nextMax = Math.max(0, history.length - visibleRows)
      const clamped = Math.max(0, Math.min(next, nextMax))
      setScrollOffset(clamped)
      setIsPinnedToBottom(clamped >= nextMax)
    },
    [history.length, visibleRows],
  )
  const scrollBy = useCallback(
    (delta: number) => {
      scrollTo(scrollOffset + delta)
    },
    [scrollOffset, scrollTo],
  )
  const resetHistory = useCallback(() => {
    const seed = [...initialEntriesRef.current]
    historyIdRef.current = seed.length
    setHistory(seed)
    setScrollOffset(Math.max(0, seed.length - visibleRows))
    setIsPinnedToBottom(true)
  }, [initialEntriesRef, visibleRows])
  const clearHistory = useCallback(() => {
    historyIdRef.current = 0
    setHistory([])
    setScrollOffset(0)
    setIsPinnedToBottom(true)
  }, [])
  return {
    history,
    pushHistory,
    resetHistory,
    clearHistory,
    scroll: {
      offset: scrollOffset,
      scrollTo,
      scrollBy,
    },
  }
}
</file>

<file path="src/tui/markdown/markdown-highlight.ts">
import type {
  InkColorValue,
  MarkdownThemeSlot,
  RequiredThemeSlot,
  ResolvedTheme,
} from '../theme/theme-types'
/**
 * Minimal markdown token styling for the history pane.
 *
 * This is intentionally *not* a full markdown renderer; it’s a lightweight
 * tokenizer that applies semantic theme slots to common markdown constructs so
 * generated prompts are easier to review in the TUI.
 */
export type MarkdownSpan = {
  text: string
  slot: MarkdownThemeSlot
  bold?: boolean
  italic?: boolean
  underline?: boolean
}
export type MarkdownLineTokenization = {
  spans: MarkdownSpan[]
  nextState: MarkdownTokenizationState
}
export type MarkdownTokenizationState = {
  inCodeBlock: boolean
}
export const DEFAULT_MARKDOWN_STATE: MarkdownTokenizationState = { inCodeBlock: false }
const SLOT_FALLBACKS: Record<MarkdownThemeSlot, RequiredThemeSlot> = {
  markdownText: 'text',
  markdownHeading: 'accent',
  markdownLink: 'accent',
  markdownLinkText: 'accent',
  markdownCode: 'warning',
  markdownBlockQuote: 'mutedText',
  markdownEmph: 'text',
  markdownStrong: 'text',
  markdownHorizontalRule: 'border',
  markdownListItem: 'mutedText',
  markdownListEnumeration: 'mutedText',
  markdownImage: 'accent',
  markdownImageText: 'text',
  markdownCodeBlock: 'mutedText',
}
export const resolveMarkdownSlotColor = (
  theme: ResolvedTheme,
  slot: MarkdownThemeSlot,
): InkColorValue => {
  const raw = theme[slot]
  if (raw !== undefined || Object.prototype.hasOwnProperty.call(theme, slot)) {
    return raw
  }
  return theme[SLOT_FALLBACKS[slot]]
}
const HR_RE = /^(?:\*\s*\*\s*\*\s*|-{3,}\s*|_{3,}\s*)$/
const HEADING_RE = /^(\s{0,3})(#{1,6})\s+(.*)$/
const BLOCKQUOTE_RE = /^(\s{0,3})>\s?(.*)$/
const BULLET_RE = /^(\s*)([-+*])\s+(.*)$/
const ENUM_RE = /^(\s*)(\d+)([.)])\s+(.*)$/
const startsFence = (line: string): boolean => /^\s*```/.test(line)
const pushSpan = (spans: MarkdownSpan[], span: MarkdownSpan) => {
  if (!span.text) {
    return
  }
  const last = spans[spans.length - 1]
  if (
    last &&
    last.slot === span.slot &&
    last.bold === span.bold &&
    last.italic === span.italic &&
    last.underline === span.underline
  ) {
    last.text += span.text
    return
  }
  spans.push(span)
}
const tokenizeInline = (text: string, baseSlot: MarkdownThemeSlot): MarkdownSpan[] => {
  const spans: MarkdownSpan[] = []
  let index = 0
  const flushPlain = (endExclusive: number) => {
    if (endExclusive <= index) {
      return
    }
    pushSpan(spans, { text: text.slice(index, endExclusive), slot: baseSlot })
    index = endExclusive
  }
  while (index < text.length) {
    const char = text[index]
    if (char === '`') {
      const end = text.indexOf('`', index + 1)
      if (end > index + 1) {
        flushPlain(index)
        pushSpan(spans, { text: text.slice(index, end + 1), slot: 'markdownCode' })
        index = end + 1
        continue
      }
    }
    if (text.startsWith('![', index)) {
      const closeBracket = text.indexOf(']', index + 2)
      const openParen = closeBracket >= 0 ? text.indexOf('(', closeBracket + 1) : -1
      const closeParen = openParen >= 0 ? text.indexOf(')', openParen + 1) : -1
      if (closeBracket >= 0 && openParen === closeBracket + 1 && closeParen > openParen) {
        flushPlain(index)
        pushSpan(spans, { text: '![', slot: 'markdownImage' })
        pushSpan(spans, {
          text: text.slice(index + 2, closeBracket),
          slot: 'markdownImageText',
        })
        pushSpan(spans, {
          text: text.slice(closeBracket, closeParen + 1),
          slot: 'markdownImage',
        })
        index = closeParen + 1
        continue
      }
    }
    if (char === '[') {
      const closeBracket = text.indexOf(']', index + 1)
      const openParen = closeBracket >= 0 ? text.indexOf('(', closeBracket + 1) : -1
      const closeParen = openParen >= 0 ? text.indexOf(')', openParen + 1) : -1
      if (closeBracket >= 0 && openParen === closeBracket + 1 && closeParen > openParen) {
        flushPlain(index)
        pushSpan(spans, { text: '[', slot: 'markdownLink' })
        pushSpan(spans, {
          text: text.slice(index + 1, closeBracket),
          slot: 'markdownLinkText',
          underline: true,
        })
        pushSpan(spans, {
          text: text.slice(closeBracket, closeParen + 1),
          slot: 'markdownLink',
        })
        index = closeParen + 1
        continue
      }
    }
    if (text.startsWith('**', index)) {
      const end = text.indexOf('**', index + 2)
      if (end > index + 2) {
        flushPlain(index)
        pushSpan(spans, { text: text.slice(index, end + 2), slot: 'markdownStrong', bold: true })
        index = end + 2
        continue
      }
    }
    if (char === '*') {
      const end = text.indexOf('*', index + 1)
      if (end > index + 1) {
        flushPlain(index)
        pushSpan(spans, { text: text.slice(index, end + 1), slot: 'markdownEmph', italic: true })
        index = end + 1
        continue
      }
    }
    const nextSpecialCandidates = [
      text.indexOf('`', index + 1),
      text.indexOf('![', index + 1),
      text.indexOf('[', index + 1),
      text.indexOf('*', index + 1),
    ].filter((value) => value >= 0)
    const nextSpecial = nextSpecialCandidates.length > 0 ? Math.min(...nextSpecialCandidates) : -1
    if (nextSpecial === -1) {
      flushPlain(text.length)
      break
    }
    flushPlain(nextSpecial)
  }
  return spans
}
const withLineStyle = (
  spans: MarkdownSpan[],
  style: Pick<MarkdownSpan, 'bold' | 'italic' | 'underline'>,
): MarkdownSpan[] => {
  if (!style.bold && !style.italic && !style.underline) {
    return spans
  }
  return spans.map((span) => ({
    ...span,
    ...(style.bold ? { bold: true } : {}),
    ...(style.italic ? { italic: true } : {}),
    ...(style.underline ? { underline: true } : {}),
  }))
}
export const tokenizeMarkdownLine = (
  line: string,
  state: MarkdownTokenizationState,
): MarkdownLineTokenization => {
  const trimmed = line.trimStart()
  if (startsFence(trimmed)) {
    return {
      spans: [{ text: line, slot: 'markdownCodeBlock' }],
      nextState: { inCodeBlock: !state.inCodeBlock },
    }
  }
  if (state.inCodeBlock) {
    return {
      spans: [{ text: line, slot: 'markdownCodeBlock' }],
      nextState: state,
    }
  }
  if (HR_RE.test(trimmed)) {
    return {
      spans: [{ text: line, slot: 'markdownHorizontalRule' }],
      nextState: state,
    }
  }
  const heading = HEADING_RE.exec(line)
  if (heading) {
    const indent = heading[1] ?? ''
    const hashes = heading[2] ?? ''
    const rest = heading[3] ?? ''
    const spans: MarkdownSpan[] = []
    if (indent) {
      pushSpan(spans, { text: indent, slot: 'markdownHeading' })
    }
    pushSpan(spans, { text: `${hashes} `, slot: 'markdownHeading' })
    tokenizeInline(rest, 'markdownHeading').forEach((span) => {
      pushSpan(spans, span)
    })
    return {
      spans: withLineStyle(spans, { bold: true }),
      nextState: state,
    }
  }
  const quote = BLOCKQUOTE_RE.exec(line)
  if (quote) {
    const indent = quote[1] ?? ''
    const rest = quote[2] ?? ''
    const spans: MarkdownSpan[] = []
    if (indent) {
      pushSpan(spans, { text: indent, slot: 'markdownBlockQuote' })
    }
    pushSpan(spans, { text: '> ', slot: 'markdownBlockQuote' })
    tokenizeInline(rest, 'markdownBlockQuote').forEach((span) => pushSpan(spans, span))
    return { spans, nextState: state }
  }
  const enumeration = ENUM_RE.exec(line)
  if (enumeration) {
    const indent = enumeration[1] ?? ''
    const number = enumeration[2] ?? ''
    const punctuation = enumeration[3] ?? '.'
    const rest = enumeration[4] ?? ''
    const spans: MarkdownSpan[] = []
    if (indent) {
      pushSpan(spans, { text: indent, slot: 'markdownText' })
    }
    pushSpan(spans, { text: `${number}${punctuation} `, slot: 'markdownListEnumeration' })
    tokenizeInline(rest, 'markdownText').forEach((span) => pushSpan(spans, span))
    return { spans, nextState: state }
  }
  const bullet = BULLET_RE.exec(line)
  if (bullet) {
    const indent = bullet[1] ?? ''
    const marker = bullet[2] ?? '-'
    const rest = bullet[3] ?? ''
    const spans: MarkdownSpan[] = []
    if (indent) {
      pushSpan(spans, { text: indent, slot: 'markdownText' })
    }
    pushSpan(spans, { text: `${marker} `, slot: 'markdownListItem' })
    tokenizeInline(rest, 'markdownText').forEach((span) => pushSpan(spans, span))
    return { spans, nextState: state }
  }
  return {
    spans: tokenizeInline(line, 'markdownText'),
    nextState: state,
  }
}
export const tokenizeMarkdownLines = (lines: readonly string[]): MarkdownSpan[][] => {
  let state = DEFAULT_MARKDOWN_STATE
  return lines.map((line) => {
    const tokenized = tokenizeMarkdownLine(line, state)
    state = tokenized.nextState
    return tokenized.spans
  })
}
</file>

<file path="src/tui/screens/command/hooks/context-popup-glue/popup-state-mutations.ts">
import { parseAbsolutePathFromInput } from '../../../../drag-drop-path'
import type { PopupState } from '../../../../types'
export const clampSelectionIndex = (selectionIndex: number, itemsLength: number): number =>
  Math.min(selectionIndex, Math.max(itemsLength - 1, 0))
type DeriveSuggestionsViewModelOptions = {
  draft: string
  suggestedItems: string[]
  suggestedSelectionIndex: number
  suggestedFocused: boolean
  filterSuggestions: (query: string, suggestions: string[]) => string[]
}
export type SuggestionsViewModel = {
  suggestions: string[]
  selectionIndex: number
  focused: boolean
  shouldDefocus: boolean
}
export const deriveSuggestionsViewModel = ({
  draft,
  suggestedItems,
  suggestedSelectionIndex,
  suggestedFocused,
  filterSuggestions,
}: DeriveSuggestionsViewModelOptions): SuggestionsViewModel => {
  if (suggestedItems.length === 0) {
    return {
      suggestions: [],
      selectionIndex: 0,
      focused: false,
      shouldDefocus: suggestedFocused,
    }
  }
  if (!draft.trim()) {
    return {
      suggestions: [],
      selectionIndex: 0,
      focused: false,
      shouldDefocus: suggestedFocused,
    }
  }
  const suggestions = filterSuggestions(draft, suggestedItems)
  const selectionIndex = clampSelectionIndex(suggestedSelectionIndex, suggestions.length)
  return {
    suggestions,
    selectionIndex,
    focused: suggestedFocused && suggestions.length > 0,
    shouldDefocus: suggestedFocused && suggestions.length === 0,
  }
}
type SuggestionFocusState = {
  suggestedFocused: boolean
  suggestedSelectionIndex: number
}
export const defocusSuggestionsIfEmpty = <T extends SuggestionFocusState>(
  prev: T,
  suggestionsLength: number,
): T => {
  if (!prev.suggestedFocused) {
    return prev
  }
  if (suggestionsLength > 0) {
    return prev
  }
  return { ...prev, suggestedFocused: false, suggestedSelectionIndex: 0 }
}
type SuggestedInputPopupState = {
  draft: string
  suggestedSelectionIndex: number
  suggestedFocused: boolean
}
export const updateSuggestedInputDraft = <T extends SuggestedInputPopupState>(
  prev: T,
  nextDraft: string,
): T => ({ ...prev, draft: nextDraft, suggestedSelectionIndex: 0, suggestedFocused: false })
type SuggestedSelectedListPopupState = {
  draft: string
  selectionIndex: number
  selectedFocused: boolean
  suggestedSelectionIndex: number
  suggestedFocused: boolean
}
export const updateSuggestedSelectedListDraft = <T extends SuggestedSelectedListPopupState>(
  prev: T,
  nextDraft: string,
): T => ({
  ...prev,
  draft: nextDraft,
  selectedFocused: false,
  suggestedSelectionIndex: 0,
  suggestedFocused: false,
})
export const resetSuggestedSelectedListAfterAdd = <T extends SuggestedSelectedListPopupState>(
  prev: T,
  nextSelectionIndex: number,
): T => ({
  ...prev,
  draft: '',
  selectionIndex: nextSelectionIndex,
  selectedFocused: false,
  suggestedFocused: false,
  suggestedSelectionIndex: 0,
})
export const getAutoAddAbsolutePathCandidate = (
  draft: string,
  isFilePath: (candidate: string) => boolean,
): string | null => {
  const candidate = parseAbsolutePathFromInput(draft)
  if (!candidate) {
    return null
  }
  return isFilePath(candidate) ? candidate : null
}
export const guardPopupUpdater = <T extends NonNullable<PopupState>['type']>(
  popupType: T,
  updater: (prev: Extract<NonNullable<PopupState>, { type: T }>) => PopupState,
): ((prev: PopupState) => PopupState) => {
  return (prev) => {
    if (!prev || prev.type !== popupType) {
      return prev
    }
    return updater(prev as Extract<NonNullable<PopupState>, { type: T }>)
  }
}
</file>

<file path="src/tui/screens/command/hooks/context-popup-glue/useFilePopupGlue.ts">
import { useCallback, useEffect, useMemo } from 'react'
import { stripTerminalPasteArtifacts } from '../../../../components/core/bracketed-paste'
import { filterFileSuggestions } from '../../../../file-suggestions'
import type { PopupState } from '../../../../types'
import {
  defocusSuggestionsIfEmpty,
  deriveSuggestionsViewModel,
  getAutoAddAbsolutePathCandidate,
  guardPopupUpdater,
  resetSuggestedSelectedListAfterAdd,
  updateSuggestedSelectedListDraft,
} from './popup-state-mutations'
const EMPTY_SUGGESTIONS: string[] = []
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
export type UseFilePopupGlueOptions = {
  popupState: PopupState
  files: string[]
  setPopupState: SetPopupState
  pushHistory: (content: string, kind?: 'system' | 'user' | 'progress') => void
  addFile: (value: string) => void
  removeFile: (index: number) => void
  consumeSuppressedTextInputChange: () => boolean
  isFilePath: (candidate: string) => boolean
}
export type UseFilePopupGlueResult = {
  filePopupSuggestions: string[]
  filePopupSuggestionSelectionIndex: number
  filePopupSuggestionsFocused: boolean
  onFilePopupDraftChange: (next: string) => void
  onAddFile: (value: string) => void
  onRemoveFile: (index: number) => void
}
export const useFilePopupGlue = ({
  popupState,
  files,
  setPopupState,
  pushHistory,
  addFile,
  removeFile,
  consumeSuppressedTextInputChange,
  isFilePath,
}: UseFilePopupGlueOptions): UseFilePopupGlueResult => {
  const addFileToContext = useCallback(
    (value: string): void => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      if (files.includes(trimmed)) {
        pushHistory(`Context file already added: ${trimmed}`)
        return
      }
      addFile(trimmed)
      pushHistory(`Context file added: ${trimmed}`)
    },
    [addFile, files, pushHistory],
  )
  const onAddFile = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      addFileToContext(trimmed)
      setPopupState(
        guardPopupUpdater('file', (prev) =>
          resetSuggestedSelectedListAfterAdd(prev, Math.max(files.length, 0)),
        ),
      )
    },
    [addFileToContext, files.length, setPopupState],
  )
  useEffect(() => {
    if (popupState?.type !== 'file') {
      return
    }
    const candidate = getAutoAddAbsolutePathCandidate(popupState.draft, isFilePath)
    if (!candidate) {
      return
    }
    onAddFile(candidate)
  }, [isFilePath, onAddFile, popupState])
  const onRemoveFile = useCallback(
    (index: number) => {
      if (index < 0 || index >= files.length) {
        return
      }
      const target = files[index]
      removeFile(index)
      pushHistory(`Context file removed: ${target}`)
    },
    [files, pushHistory, removeFile],
  )
  const filePopupDraft = popupState?.type === 'file' ? popupState.draft : ''
  const filePopupSuggestedItems =
    popupState?.type === 'file' ? popupState.suggestedItems : EMPTY_SUGGESTIONS
  const filePopupSuggestedFocused =
    popupState?.type === 'file' ? popupState.suggestedFocused : false
  const filePopupSuggestedSelectionIndex =
    popupState?.type === 'file' ? popupState.suggestedSelectionIndex : 0
  const suggestionsModel = useMemo(
    () =>
      deriveSuggestionsViewModel({
        draft: filePopupDraft,
        suggestedItems: filePopupSuggestedItems,
        suggestedSelectionIndex: filePopupSuggestedSelectionIndex,
        suggestedFocused: filePopupSuggestedFocused,
        filterSuggestions: (query, suggestions) =>
          filterFileSuggestions({ suggestions, query, exclude: files }),
      }),
    [
      filePopupDraft,
      filePopupSuggestedFocused,
      filePopupSuggestedItems,
      filePopupSuggestedSelectionIndex,
      files,
    ],
  )
  useEffect(() => {
    if (popupState?.type !== 'file') {
      return
    }
    if (!suggestionsModel.shouldDefocus) {
      return
    }
    setPopupState(
      guardPopupUpdater('file', (prev) =>
        defocusSuggestionsIfEmpty(prev, suggestionsModel.suggestions.length),
      ),
    )
  }, [
    popupState?.type,
    setPopupState,
    suggestionsModel.shouldDefocus,
    suggestionsModel.suggestions.length,
  ])
  const onFilePopupDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      const sanitized = stripTerminalPasteArtifacts(next)
      setPopupState(
        guardPopupUpdater('file', (prev) => updateSuggestedSelectedListDraft(prev, sanitized)),
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  return {
    filePopupSuggestions: suggestionsModel.suggestions,
    filePopupSuggestionSelectionIndex: suggestionsModel.selectionIndex,
    filePopupSuggestionsFocused: suggestionsModel.focused,
    onFilePopupDraftChange,
    onAddFile,
    onRemoveFile,
  }
}
</file>

<file path="src/tui/screens/command/hooks/context-popup-glue/useImagePopupGlue.ts">
import { useCallback, useEffect, useMemo } from 'react'
import { stripTerminalPasteArtifacts } from '../../../../components/core/bracketed-paste'
import { filterFileSuggestions } from '../../../../file-suggestions'
import type { PopupState } from '../../../../types'
import {
  defocusSuggestionsIfEmpty,
  deriveSuggestionsViewModel,
  getAutoAddAbsolutePathCandidate,
  guardPopupUpdater,
  resetSuggestedSelectedListAfterAdd,
  updateSuggestedSelectedListDraft,
} from './popup-state-mutations'
const EMPTY_SUGGESTIONS: string[] = []
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
export type UseImagePopupGlueOptions = {
  popupState: PopupState
  images: string[]
  setPopupState: SetPopupState
  pushHistory: (content: string, kind?: 'system' | 'user' | 'progress') => void
  addImage: (value: string) => void
  removeImage: (index: number) => void
  consumeSuppressedTextInputChange: () => boolean
  isFilePath: (candidate: string) => boolean
}
export type UseImagePopupGlueResult = {
  imagePopupSuggestions: string[]
  imagePopupSuggestionSelectionIndex: number
  imagePopupSuggestionsFocused: boolean
  onImagePopupDraftChange: (next: string) => void
  onAddImage: (value: string) => void
  onRemoveImage: (index: number) => void
}
export const useImagePopupGlue = ({
  popupState,
  images,
  setPopupState,
  pushHistory,
  addImage,
  removeImage,
  consumeSuppressedTextInputChange,
  isFilePath,
}: UseImagePopupGlueOptions): UseImagePopupGlueResult => {
  const addImageToContext = useCallback(
    (value: string): void => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      if (images.includes(trimmed)) {
        pushHistory(`[image] Already attached: ${trimmed}`)
        return
      }
      addImage(trimmed)
      pushHistory(`[image] Attached: ${trimmed}`)
    },
    [addImage, images, pushHistory],
  )
  const onAddImage = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      addImageToContext(trimmed)
      setPopupState(
        guardPopupUpdater('image', (prev) =>
          resetSuggestedSelectedListAfterAdd(prev, Math.max(images.length, 0)),
        ),
      )
    },
    [addImageToContext, images.length, setPopupState],
  )
  useEffect(() => {
    if (popupState?.type !== 'image') {
      return
    }
    const candidate = getAutoAddAbsolutePathCandidate(popupState.draft, isFilePath)
    if (!candidate) {
      return
    }
    onAddImage(candidate)
  }, [isFilePath, onAddImage, popupState])
  const onRemoveImage = useCallback(
    (index: number) => {
      if (index < 0 || index >= images.length) {
        return
      }
      const target = images[index]
      removeImage(index)
      pushHistory(`[image] Removed: ${target}`)
    },
    [images, pushHistory, removeImage],
  )
  const imagePopupDraft = popupState?.type === 'image' ? popupState.draft : ''
  const imagePopupSuggestedItems =
    popupState?.type === 'image' ? popupState.suggestedItems : EMPTY_SUGGESTIONS
  const imagePopupSuggestedFocused =
    popupState?.type === 'image' ? popupState.suggestedFocused : false
  const imagePopupSuggestedSelectionIndex =
    popupState?.type === 'image' ? popupState.suggestedSelectionIndex : 0
  const suggestionsModel = useMemo(
    () =>
      deriveSuggestionsViewModel({
        draft: imagePopupDraft,
        suggestedItems: imagePopupSuggestedItems,
        suggestedSelectionIndex: imagePopupSuggestedSelectionIndex,
        suggestedFocused: imagePopupSuggestedFocused,
        filterSuggestions: (query, suggestions) =>
          filterFileSuggestions({ suggestions, query, exclude: images }),
      }),
    [
      imagePopupDraft,
      imagePopupSuggestedFocused,
      imagePopupSuggestedItems,
      imagePopupSuggestedSelectionIndex,
      images,
    ],
  )
  useEffect(() => {
    if (popupState?.type !== 'image') {
      return
    }
    if (!suggestionsModel.shouldDefocus) {
      return
    }
    setPopupState(
      guardPopupUpdater('image', (prev) =>
        defocusSuggestionsIfEmpty(prev, suggestionsModel.suggestions.length),
      ),
    )
  }, [
    popupState?.type,
    setPopupState,
    suggestionsModel.shouldDefocus,
    suggestionsModel.suggestions.length,
  ])
  const onImagePopupDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      const sanitized = stripTerminalPasteArtifacts(next)
      setPopupState(
        guardPopupUpdater('image', (prev) => updateSuggestedSelectedListDraft(prev, sanitized)),
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  return {
    imagePopupSuggestions: suggestionsModel.suggestions,
    imagePopupSuggestionSelectionIndex: suggestionsModel.selectionIndex,
    imagePopupSuggestionsFocused: suggestionsModel.focused,
    onImagePopupDraftChange,
    onAddImage,
    onRemoveImage,
  }
}
</file>

<file path="src/tui/screens/command/hooks/context-popup-glue/usePdfPopupGlue.ts">
import { useCallback, useEffect, useMemo } from 'react'
import { stripTerminalPasteArtifacts } from '../../../../components/core/bracketed-paste'
import { filterFileSuggestions } from '../../../../file-suggestions'
import type { PopupState } from '../../../../types'
import {
  defocusSuggestionsIfEmpty,
  deriveSuggestionsViewModel,
  getAutoAddAbsolutePathCandidate,
  guardPopupUpdater,
  resetSuggestedSelectedListAfterAdd,
  updateSuggestedSelectedListDraft,
} from './popup-state-mutations'
const EMPTY_SUGGESTIONS: string[] = []
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
export type UsePdfPopupGlueOptions = {
  popupState: PopupState
  pdfs: string[]
  setPopupState: SetPopupState
  pushHistory: (content: string, kind?: 'system' | 'user' | 'progress') => void
  addPdf: (value: string) => void
  removePdf: (index: number) => void
  consumeSuppressedTextInputChange: () => boolean
  isFilePath: (candidate: string) => boolean
}
export type UsePdfPopupGlueResult = {
  pdfPopupSuggestions: string[]
  pdfPopupSuggestionSelectionIndex: number
  pdfPopupSuggestionsFocused: boolean
  onPdfPopupDraftChange: (next: string) => void
  onAddPdf: (value: string) => void
  onRemovePdf: (index: number) => void
}
export const usePdfPopupGlue = ({
  popupState,
  pdfs,
  setPopupState,
  pushHistory,
  addPdf,
  removePdf,
  consumeSuppressedTextInputChange,
  isFilePath,
}: UsePdfPopupGlueOptions): UsePdfPopupGlueResult => {
  const addPdfToContext = useCallback(
    (value: string): void => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      if (pdfs.includes(trimmed)) {
        pushHistory(`[pdf] Already attached: ${trimmed}`)
        return
      }
      addPdf(trimmed)
      pushHistory(`[pdf] Attached: ${trimmed}`)
    },
    [addPdf, pdfs, pushHistory],
  )
  const onAddPdf = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      addPdfToContext(trimmed)
      setPopupState(
        guardPopupUpdater('pdf', (prev) =>
          resetSuggestedSelectedListAfterAdd(prev, Math.max(pdfs.length, 0)),
        ),
      )
    },
    [addPdfToContext, pdfs.length, setPopupState],
  )
  useEffect(() => {
    if (popupState?.type !== 'pdf') {
      return
    }
    const candidate = getAutoAddAbsolutePathCandidate(popupState.draft, isFilePath)
    if (!candidate) {
      return
    }
    onAddPdf(candidate)
  }, [isFilePath, onAddPdf, popupState])
  const onRemovePdf = useCallback(
    (index: number) => {
      if (index < 0 || index >= pdfs.length) {
        return
      }
      const target = pdfs[index]
      removePdf(index)
      pushHistory(`[pdf] Removed: ${target}`)
    },
    [pdfs, pushHistory, removePdf],
  )
  const pdfPopupDraft = popupState?.type === 'pdf' ? popupState.draft : ''
  const pdfPopupSuggestedItems =
    popupState?.type === 'pdf' ? popupState.suggestedItems : EMPTY_SUGGESTIONS
  const pdfPopupSuggestedFocused = popupState?.type === 'pdf' ? popupState.suggestedFocused : false
  const pdfPopupSuggestedSelectionIndex =
    popupState?.type === 'pdf' ? popupState.suggestedSelectionIndex : 0
  const suggestionsModel = useMemo(
    () =>
      deriveSuggestionsViewModel({
        draft: pdfPopupDraft,
        suggestedItems: pdfPopupSuggestedItems,
        suggestedSelectionIndex: pdfPopupSuggestedSelectionIndex,
        suggestedFocused: pdfPopupSuggestedFocused,
        filterSuggestions: (query, suggestions) =>
          filterFileSuggestions({ suggestions, query, exclude: pdfs }),
      }),
    [
      pdfPopupDraft,
      pdfPopupSuggestedFocused,
      pdfPopupSuggestedItems,
      pdfPopupSuggestedSelectionIndex,
      pdfs,
    ],
  )
  useEffect(() => {
    if (popupState?.type !== 'pdf') {
      return
    }
    if (!suggestionsModel.shouldDefocus) {
      return
    }
    setPopupState(
      guardPopupUpdater('pdf', (prev) =>
        defocusSuggestionsIfEmpty(prev, suggestionsModel.suggestions.length),
      ),
    )
  }, [
    popupState?.type,
    setPopupState,
    suggestionsModel.shouldDefocus,
    suggestionsModel.suggestions.length,
  ])
  const onPdfPopupDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      const sanitized = stripTerminalPasteArtifacts(next)
      setPopupState(
        guardPopupUpdater('pdf', (prev) => updateSuggestedSelectedListDraft(prev, sanitized)),
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  return {
    pdfPopupSuggestions: suggestionsModel.suggestions,
    pdfPopupSuggestionSelectionIndex: suggestionsModel.selectionIndex,
    pdfPopupSuggestionsFocused: suggestionsModel.focused,
    onPdfPopupDraftChange,
    onAddPdf,
    onRemovePdf,
  }
}
</file>

<file path="src/tui/screens/command/hooks/context-popup-glue/useSmartPopupGlue.ts">
import { useCallback, useEffect, useMemo } from 'react'
import { stripTerminalPasteArtifacts } from '../../../../components/core/bracketed-paste'
import { filterDirectorySuggestions } from '../../../../file-suggestions'
import type { PopupState } from '../../../../types'
import {
  defocusSuggestionsIfEmpty,
  deriveSuggestionsViewModel,
  guardPopupUpdater,
  updateSuggestedInputDraft,
} from './popup-state-mutations'
const EMPTY_SUGGESTIONS: string[] = []
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
export type UseSmartPopupGlueOptions = {
  popupState: PopupState
  smartContextEnabled: boolean
  smartContextRoot: string | null
  setPopupState: SetPopupState
  notify: (message: string) => void
  toggleSmartContext: () => void
  setSmartRoot: (value: string) => void
  consumeSuppressedTextInputChange: () => boolean
}
export type UseSmartPopupGlueResult = {
  smartPopupSuggestions: string[]
  smartPopupSuggestionSelectionIndex: number
  smartPopupSuggestionsFocused: boolean
  onSmartPopupDraftChange: (next: string) => void
  onSmartToggle: (nextEnabled: boolean) => void
  onSmartRootSubmit: (value: string) => void
}
export const useSmartPopupGlue = ({
  popupState,
  smartContextEnabled,
  smartContextRoot,
  setPopupState,
  notify,
  toggleSmartContext,
  setSmartRoot,
  consumeSuppressedTextInputChange,
}: UseSmartPopupGlueOptions): UseSmartPopupGlueResult => {
  const onSmartToggle = useCallback(
    (nextEnabled: boolean) => {
      if (smartContextEnabled === nextEnabled) {
        return
      }
      const shouldClearRoot = !nextEnabled && Boolean(smartContextRoot)
      if (shouldClearRoot) {
        setSmartRoot('')
        setPopupState((prev) =>
          prev?.type === 'smart' && prev.draft === smartContextRoot
            ? updateSuggestedInputDraft(prev, '')
            : prev,
        )
      }
      toggleSmartContext()
      notify(
        nextEnabled
          ? 'Smart context enabled'
          : shouldClearRoot
            ? 'Smart context disabled; root cleared'
            : 'Smart context disabled',
      )
    },
    [
      notify,
      setPopupState,
      setSmartRoot,
      smartContextEnabled,
      smartContextRoot,
      toggleSmartContext,
    ],
  )
  const onSmartRootSubmit = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      const shouldEnable = Boolean(trimmed) && !smartContextEnabled
      setSmartRoot(trimmed)
      if (shouldEnable) {
        toggleSmartContext()
      }
      notify(
        trimmed
          ? shouldEnable
            ? `Smart context enabled; root set to ${trimmed}`
            : `Smart context root set to ${trimmed}`
          : 'Smart context root cleared',
      )
      if (trimmed) {
        setPopupState((prev) => (prev?.type === 'smart' ? null : prev))
        return
      }
      setPopupState(guardPopupUpdater('smart', (prev) => updateSuggestedInputDraft(prev, trimmed)))
    },
    [notify, setPopupState, setSmartRoot, smartContextEnabled, toggleSmartContext],
  )
  const smartPopupDraft = popupState?.type === 'smart' ? popupState.draft : ''
  const smartPopupSuggestedItems =
    popupState?.type === 'smart' ? popupState.suggestedItems : EMPTY_SUGGESTIONS
  const smartPopupSuggestedFocused =
    popupState?.type === 'smart' ? popupState.suggestedFocused : false
  const smartPopupSuggestedSelectionIndex =
    popupState?.type === 'smart' ? popupState.suggestedSelectionIndex : 0
  const suggestionsModel = useMemo(
    () =>
      deriveSuggestionsViewModel({
        draft: smartPopupDraft,
        suggestedItems: smartPopupSuggestedItems,
        suggestedSelectionIndex: smartPopupSuggestedSelectionIndex,
        suggestedFocused: smartPopupSuggestedFocused,
        filterSuggestions: (query, suggestions) => {
          const excluded = smartContextRoot ? [smartContextRoot] : []
          return filterDirectorySuggestions({ suggestions, query, exclude: excluded })
        },
      }),
    [
      smartContextRoot,
      smartPopupDraft,
      smartPopupSuggestedFocused,
      smartPopupSuggestedItems,
      smartPopupSuggestedSelectionIndex,
    ],
  )
  useEffect(() => {
    if (popupState?.type !== 'smart') {
      return
    }
    if (!suggestionsModel.shouldDefocus) {
      return
    }
    setPopupState(
      guardPopupUpdater('smart', (prev) =>
        defocusSuggestionsIfEmpty(prev, suggestionsModel.suggestions.length),
      ),
    )
  }, [
    popupState?.type,
    setPopupState,
    suggestionsModel.shouldDefocus,
    suggestionsModel.suggestions.length,
  ])
  const onSmartPopupDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      const sanitized = stripTerminalPasteArtifacts(next)
      setPopupState(
        guardPopupUpdater('smart', (prev) => updateSuggestedInputDraft(prev, sanitized)),
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  return {
    smartPopupSuggestions: suggestionsModel.suggestions,
    smartPopupSuggestionSelectionIndex: suggestionsModel.selectionIndex,
    smartPopupSuggestionsFocused: suggestionsModel.focused,
    onSmartPopupDraftChange,
    onSmartToggle,
    onSmartRootSubmit,
  }
}
</file>

<file path="src/tui/screens/command/hooks/context-popup-glue/useUrlPopupGlue.ts">
import { useCallback } from 'react'
import { parseUrlArgs, validateHttpUrlCandidate } from '../../utils/url-args'
import type { PopupState } from '../../../../types'
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
export type UseUrlPopupGlueOptions = {
  popupState: PopupState
  urls: string[]
  setPopupState: SetPopupState
  pushHistory: (content: string, kind?: 'system' | 'user' | 'progress') => void
  addUrl: (value: string) => void
  removeUrl: (index: number) => void
  updateUrl: (index: number, value: string) => void
  consumeSuppressedTextInputChange: () => boolean
}
export type UseUrlPopupGlueResult = {
  onUrlPopupDraftChange: (next: string) => void
  onAddUrl: (value: string) => void
  onRemoveUrl: (index: number) => void
}
export const useUrlPopupGlue = ({
  popupState,
  urls,
  setPopupState,
  pushHistory,
  addUrl,
  removeUrl,
  updateUrl,
  consumeSuppressedTextInputChange,
}: UseUrlPopupGlueOptions): UseUrlPopupGlueResult => {
  const onAddUrl = useCallback(
    (value: string) => {
      const currentPopup = popupState?.type === 'url' ? popupState : null
      const editingIndex = currentPopup?.editingIndex ?? null
      if (editingIndex !== null) {
        if (editingIndex < 0 || editingIndex >= urls.length) {
          setPopupState((prev) =>
            prev?.type === 'url'
              ? { ...prev, draft: '', editingIndex: null, selectedFocused: false }
              : prev,
          )
          return
        }
        const trimmed = value.trim()
        if (!trimmed) {
          pushHistory('Warning: URL cannot be empty.', 'system')
          return
        }
        const validation = validateHttpUrlCandidate(trimmed)
        if (!validation.ok) {
          pushHistory(`Warning: ${validation.message}`, 'system')
          return
        }
        const duplicateIndex = urls.findIndex(
          (existing, idx) => idx !== editingIndex && existing === trimmed,
        )
        if (duplicateIndex !== -1) {
          pushHistory(`Context URL already added: ${trimmed}`, 'system')
          return
        }
        const previous = urls[editingIndex]
        if (!previous) {
          return
        }
        if (previous === trimmed) {
          setPopupState((prev) =>
            prev?.type === 'url'
              ? { ...prev, draft: '', editingIndex: null, selectedFocused: false }
              : prev,
          )
          return
        }
        updateUrl(editingIndex, trimmed)
        pushHistory(`Context URL updated: ${previous} → ${trimmed}`)
        setPopupState((prev) =>
          prev?.type === 'url'
            ? { ...prev, draft: '', editingIndex: null, selectedFocused: false }
            : prev,
        )
        return
      }
      const candidates = parseUrlArgs(value)
      if (candidates.length === 0) {
        return
      }
      const seen = new Set<string>()
      const baseIndex = urls.length
      let addedCount = 0
      for (const candidate of candidates) {
        if (seen.has(candidate)) {
          continue
        }
        seen.add(candidate)
        const validation = validateHttpUrlCandidate(candidate)
        if (!validation.ok) {
          pushHistory(`Warning: ${validation.message}`, 'system')
          continue
        }
        if (urls.includes(candidate)) {
          pushHistory(`Context URL already added: ${candidate}`, 'system')
          continue
        }
        addUrl(candidate)
        addedCount += 1
        pushHistory(`Context URL added: ${candidate}`)
      }
      if (addedCount === 0) {
        return
      }
      setPopupState((prev) =>
        prev?.type === 'url'
          ? {
              ...prev,
              draft: '',
              selectionIndex: Math.max(baseIndex + addedCount - 1, 0),
              selectedFocused: false,
              editingIndex: null,
            }
          : prev,
      )
    },
    [addUrl, popupState, pushHistory, setPopupState, updateUrl, urls],
  )
  const onRemoveUrl = useCallback(
    (index: number) => {
      if (index < 0 || index >= urls.length) {
        return
      }
      const target = urls[index]
      if (!target) {
        return
      }
      removeUrl(index)
      pushHistory(`Context URL removed: ${target}`)
      setPopupState((prev) => {
        if (prev?.type !== 'url') {
          return prev
        }
        const nextMaxIndex = Math.max(urls.length - 2, 0)
        const selectionIndexBefore = prev.selectionIndex
        const nextSelectionIndexUnclamped =
          selectionIndexBefore > index
            ? selectionIndexBefore - 1
            : selectionIndexBefore === index
              ? Math.min(index, nextMaxIndex)
              : selectionIndexBefore
        const nextEditingIndex =
          prev.editingIndex === null
            ? null
            : index === prev.editingIndex
              ? null
              : index < prev.editingIndex
                ? prev.editingIndex - 1
                : prev.editingIndex
        const nextDraft = index === prev.editingIndex ? '' : prev.draft
        return {
          ...prev,
          selectionIndex: Math.min(nextSelectionIndexUnclamped, nextMaxIndex),
          selectedFocused: urls.length > 1 ? prev.selectedFocused : false,
          editingIndex: nextEditingIndex,
          draft: nextDraft,
        }
      })
    },
    [pushHistory, removeUrl, setPopupState, urls],
  )
  const onUrlPopupDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) => (prev?.type === 'url' ? { ...prev, draft: next } : prev))
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  return {
    onUrlPopupDraftChange,
    onAddUrl,
    onRemoveUrl,
  }
}
</file>

<file path="src/tui/screens/command/hooks/context-popup-glue/useVideoPopupGlue.ts">
import { useCallback, useEffect, useMemo } from 'react'
import { stripTerminalPasteArtifacts } from '../../../../components/core/bracketed-paste'
import { filterFileSuggestions } from '../../../../file-suggestions'
import type { PopupState } from '../../../../types'
import {
  defocusSuggestionsIfEmpty,
  deriveSuggestionsViewModel,
  getAutoAddAbsolutePathCandidate,
  guardPopupUpdater,
  resetSuggestedSelectedListAfterAdd,
  updateSuggestedSelectedListDraft,
} from './popup-state-mutations'
const EMPTY_SUGGESTIONS: string[] = []
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
export type UseVideoPopupGlueOptions = {
  popupState: PopupState
  videos: string[]
  setPopupState: SetPopupState
  pushHistory: (content: string, kind?: 'system' | 'user' | 'progress') => void
  addVideo: (value: string) => void
  removeVideo: (index: number) => void
  consumeSuppressedTextInputChange: () => boolean
  isFilePath: (candidate: string) => boolean
}
export type UseVideoPopupGlueResult = {
  videoPopupSuggestions: string[]
  videoPopupSuggestionSelectionIndex: number
  videoPopupSuggestionsFocused: boolean
  onVideoPopupDraftChange: (next: string) => void
  onAddVideo: (value: string) => void
  onRemoveVideo: (index: number) => void
}
export const useVideoPopupGlue = ({
  popupState,
  videos,
  setPopupState,
  pushHistory,
  addVideo,
  removeVideo,
  consumeSuppressedTextInputChange,
  isFilePath,
}: UseVideoPopupGlueOptions): UseVideoPopupGlueResult => {
  const addVideoToContext = useCallback(
    (value: string): void => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      if (videos.includes(trimmed)) {
        pushHistory(`[video] Already attached: ${trimmed}`)
        return
      }
      addVideo(trimmed)
      pushHistory(`[video] Attached: ${trimmed}`)
    },
    [addVideo, pushHistory, videos],
  )
  const onAddVideo = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      addVideoToContext(trimmed)
      setPopupState(
        guardPopupUpdater('video', (prev) =>
          resetSuggestedSelectedListAfterAdd(prev, Math.max(videos.length, 0)),
        ),
      )
    },
    [addVideoToContext, setPopupState, videos.length],
  )
  useEffect(() => {
    if (popupState?.type !== 'video') {
      return
    }
    const candidate = getAutoAddAbsolutePathCandidate(popupState.draft, isFilePath)
    if (!candidate) {
      return
    }
    onAddVideo(candidate)
  }, [isFilePath, onAddVideo, popupState])
  const onRemoveVideo = useCallback(
    (index: number) => {
      if (index < 0 || index >= videos.length) {
        return
      }
      const target = videos[index]
      removeVideo(index)
      pushHistory(`[video] Removed: ${target}`)
    },
    [pushHistory, removeVideo, videos],
  )
  const videoPopupDraft = popupState?.type === 'video' ? popupState.draft : ''
  const videoPopupSuggestedItems =
    popupState?.type === 'video' ? popupState.suggestedItems : EMPTY_SUGGESTIONS
  const videoPopupSuggestedFocused =
    popupState?.type === 'video' ? popupState.suggestedFocused : false
  const videoPopupSuggestedSelectionIndex =
    popupState?.type === 'video' ? popupState.suggestedSelectionIndex : 0
  const suggestionsModel = useMemo(
    () =>
      deriveSuggestionsViewModel({
        draft: videoPopupDraft,
        suggestedItems: videoPopupSuggestedItems,
        suggestedSelectionIndex: videoPopupSuggestedSelectionIndex,
        suggestedFocused: videoPopupSuggestedFocused,
        filterSuggestions: (query, suggestions) =>
          filterFileSuggestions({ suggestions, query, exclude: videos }),
      }),
    [
      videoPopupDraft,
      videoPopupSuggestedFocused,
      videoPopupSuggestedItems,
      videoPopupSuggestedSelectionIndex,
      videos,
    ],
  )
  useEffect(() => {
    if (popupState?.type !== 'video') {
      return
    }
    if (!suggestionsModel.shouldDefocus) {
      return
    }
    setPopupState(
      guardPopupUpdater('video', (prev) =>
        defocusSuggestionsIfEmpty(prev, suggestionsModel.suggestions.length),
      ),
    )
  }, [
    popupState?.type,
    setPopupState,
    suggestionsModel.shouldDefocus,
    suggestionsModel.suggestions.length,
  ])
  const onVideoPopupDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      const sanitized = stripTerminalPasteArtifacts(next)
      setPopupState(
        guardPopupUpdater('video', (prev) => updateSuggestedSelectedListDraft(prev, sanitized)),
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  return {
    videoPopupSuggestions: suggestionsModel.suggestions,
    videoPopupSuggestionSelectionIndex: suggestionsModel.selectionIndex,
    videoPopupSuggestionsFocused: suggestionsModel.focused,
    onVideoPopupDraftChange,
    onAddVideo,
    onRemoveVideo,
  }
}
</file>

<file path="src/tui/screens/command/hooks/popup-shortcuts/budgets-popup-shortcuts.ts">
import type { Key } from 'ink'
import type { ContextOverflowStrategy } from '../../../../../config'
import type { PopupState } from '../../../../types'
import { wrapIndex } from './selection'
import { guardPopupUpdater } from './types'
const FIELD_COUNT = 3
const STRATEGY_OPTIONS = [
  '',
  'fail',
  'drop-smart',
  'drop-url',
  'drop-largest',
  'drop-oldest',
] as const satisfies ReadonlyArray<ContextOverflowStrategy | ''>
export type HandleBudgetsPopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'budgets' }>
  key: Key
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  onBudgetsSubmit: () => void
}
const cycleStrategy = (
  current: ContextOverflowStrategy | '',
  delta: number,
): ContextOverflowStrategy | '' => {
  const index = STRATEGY_OPTIONS.indexOf(current)
  const safeIndex = index >= 0 ? index : 0
  return STRATEGY_OPTIONS[wrapIndex(safeIndex + delta, STRATEGY_OPTIONS.length)] ?? ''
}
export const handleBudgetsPopupShortcuts = ({
  popupState,
  key,
  setPopupState,
  closePopup,
  onBudgetsSubmit,
}: HandleBudgetsPopupShortcutsOptions): void => {
  if (key.escape) {
    closePopup()
    return
  }
  if (key.upArrow) {
    setPopupState(
      guardPopupUpdater('budgets', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex - 1, FIELD_COUNT),
      })),
    )
    return
  }
  if (key.downArrow) {
    setPopupState(
      guardPopupUpdater('budgets', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex + 1, FIELD_COUNT),
      })),
    )
    return
  }
  if (popupState.selectionIndex === 2 && (key.leftArrow || key.rightArrow)) {
    const delta = key.rightArrow ? 1 : -1
    setPopupState(
      guardPopupUpdater('budgets', (prev) => ({
        ...prev,
        contextOverflowStrategyDraft: cycleStrategy(prev.contextOverflowStrategyDraft, delta),
        errorMessage: null,
      })),
    )
    return
  }
  if (popupState.selectionIndex === 2 && key.return) {
    onBudgetsSubmit()
  }
}
</file>

<file path="src/tui/screens/command/hooks/popup-shortcuts/escape-only-popup-shortcuts.ts">
import type { Key } from 'ink'
export const handleEscapeOnlyPopupShortcuts = (key: Key, closePopup: () => void): void => {
  if (key.escape) {
    closePopup()
  }
}
</file>

<file path="src/tui/screens/command/hooks/popup-shortcuts/export-popup-shortcuts.ts">
import path from 'node:path'
import type { Key } from 'ink'
import type { PopupState } from '../../../../types'
import { guardPopupUpdater } from './types'
export type HandleExportPopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'export' }>
  key: Key
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  onExportSubmit: () => void
}
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const replaceExtension = (filePath: string, nextExt: string): string => {
  const ext = path.extname(filePath)
  if (!ext) {
    return filePath
  }
  const lowered = ext.toLowerCase()
  if (lowered === '.json' || lowered === '.yaml' || lowered === '.yml') {
    return filePath.slice(0, Math.max(0, filePath.length - ext.length)).concat(nextExt)
  }
  return filePath
}
export const handleExportPopupShortcuts = ({
  popupState,
  key,
  setPopupState,
  closePopup,
  onExportSubmit,
}: HandleExportPopupShortcutsOptions): void => {
  if (key.escape) {
    closePopup()
    return
  }
  const historyCount = popupState.historyItems.length
  if (popupState.selectionIndex === 2) {
    if (key.upArrow && historyCount > 0) {
      if (popupState.historySelectionIndex === 0) {
        setPopupState(guardPopupUpdater('export', (prev) => ({ ...prev, selectionIndex: 1 })))
        return
      }
      setPopupState(
        guardPopupUpdater('export', (prev) => ({
          ...prev,
          historySelectionIndex: clamp(prev.historySelectionIndex - 1, 0, historyCount - 1),
        })),
      )
      return
    }
    if (key.downArrow && historyCount > 0) {
      setPopupState(
        guardPopupUpdater('export', (prev) => ({
          ...prev,
          historySelectionIndex: clamp(prev.historySelectionIndex + 1, 0, historyCount - 1),
        })),
      )
      return
    }
    if (key.return) {
      onExportSubmit()
      return
    }
  }
  if (key.upArrow) {
    setPopupState(
      guardPopupUpdater('export', (prev) => ({
        ...prev,
        selectionIndex: clamp(prev.selectionIndex - 1, 0, 2),
      })),
    )
    return
  }
  if (key.downArrow) {
    setPopupState(
      guardPopupUpdater('export', (prev) => ({
        ...prev,
        selectionIndex: clamp(prev.selectionIndex + 1, 0, 2),
      })),
    )
    return
  }
  if (popupState.selectionIndex === 0 && (key.leftArrow || key.rightArrow)) {
    setPopupState(
      guardPopupUpdater('export', (prev) => {
        const format = prev.format === 'json' ? 'yaml' : 'json'
        const nextExt = format === 'json' ? '.json' : '.yaml'
        return {
          ...prev,
          format,
          outPathDraft: replaceExtension(prev.outPathDraft, nextExt),
        }
      }),
    )
    return
  }
  if (key.return && popupState.selectionIndex < 2) {
    setPopupState(
      guardPopupUpdater('export', (prev) => ({
        ...prev,
        selectionIndex: clamp(prev.selectionIndex + 1, 0, 2),
      })),
    )
  }
}
</file>

<file path="src/tui/screens/command/hooks/popup-shortcuts/history-popup-shortcuts.ts">
import type { Key } from 'ink'
import type { PopupState } from '../../../../types'
import { guardPopupUpdater } from './types'
export type HandleHistoryPopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'history' }>
  key: Key
  itemCount: number
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
}
export const handleHistoryPopupShortcuts = ({
  key,
  itemCount,
  setPopupState,
  closePopup,
}: HandleHistoryPopupShortcutsOptions): void => {
  if (key.upArrow && itemCount > 0) {
    setPopupState(
      guardPopupUpdater('history', (prev) => ({
        ...prev,
        selectionIndex: Math.max(prev.selectionIndex - 1, 0),
      })),
    )
    return
  }
  if (key.downArrow && itemCount > 0) {
    setPopupState(
      guardPopupUpdater('history', (prev) => ({
        ...prev,
        selectionIndex: Math.min(prev.selectionIndex + 1, itemCount - 1),
      })),
    )
    return
  }
  if (key.escape) {
    closePopup()
  }
}
</file>

<file path="src/tui/screens/command/hooks/popup-shortcuts/intent-popup-shortcuts.ts">
import type { Key } from 'ink'
import type { PopupState } from '../../../../types'
import { getSuggestedInputPopupEffect } from './suggested-input'
export type HandleIntentPopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'intent' }>
  key: Key
  suggestions: readonly string[]
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  onIntentFileSubmit: (value: string) => void
}
export const handleIntentPopupShortcuts = ({
  popupState,
  key,
  suggestions,
  setPopupState,
  closePopup,
  onIntentFileSubmit,
}: HandleIntentPopupShortcutsOptions): void => {
  const effect = getSuggestedInputPopupEffect({
    popupType: 'intent',
    popupState,
    suggestions,
    key,
  })
  switch (effect.type) {
    case 'close':
      closePopup()
      return
    case 'set':
      setPopupState(effect.updater)
      return
    case 'selectSuggestion':
      onIntentFileSubmit(effect.value)
      return
    case 'remove':
    case 'none':
      return
    default: {
      const exhaustive: never = effect
      return exhaustive
    }
  }
}
</file>

<file path="src/tui/screens/command/hooks/popup-shortcuts/model-popup-shortcuts.ts">
import type { Key } from 'ink'
import { isBackspaceKey } from '../../../../components/core/text-input-keys'
import type { ModelOption, PopupState } from '../../../../types'
import { clampIndex, wrapIndex } from './selection'
import { guardPopupUpdater } from './types'
export type HandleModelPopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'model' }>
  input: string
  key: Key
  options: readonly ModelOption[]
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  onModelPopupSubmit: (option: ModelOption | null | undefined) => void
}
export const handleModelPopupShortcuts = ({
  popupState,
  input,
  key,
  options,
  setPopupState,
  closePopup,
  onModelPopupSubmit,
}: HandleModelPopupShortcutsOptions): void => {
  const modelSelectionIndex = clampIndex(popupState.selectionIndex, options.length)
  if (key.upArrow && options.length > 0) {
    setPopupState(
      guardPopupUpdater('model', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex - 1, options.length),
      })),
    )
    return
  }
  if (key.downArrow && options.length > 0) {
    setPopupState(
      guardPopupUpdater('model', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex + 1, options.length),
      })),
    )
    return
  }
  const draftIsEmpty = popupState.query.trim().length === 0
  if (
    popupState.kind === 'polish' &&
    (key.delete || (draftIsEmpty && isBackspaceKey(input, key)))
  ) {
    onModelPopupSubmit(null)
    return
  }
  if (key.escape) {
    closePopup()
    return
  }
  if (key.return) {
    onModelPopupSubmit(options[modelSelectionIndex])
  }
}
</file>

<file path="src/tui/screens/command/hooks/popup-shortcuts/reasoning-popup-shortcuts.ts">
import type { Key } from 'ink'
import type { PopupState } from '../../../../types'
import { clamp } from './selection'
import { guardPopupUpdater } from './types'
export type HandleReasoningPopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'reasoning' }>
  key: Key
  lineCount: number
  visibleRows: number
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
}
export const handleReasoningPopupShortcuts = ({
  key,
  lineCount,
  visibleRows,
  setPopupState,
  closePopup,
}: HandleReasoningPopupShortcutsOptions): void => {
  const maxOffset = Math.max(0, lineCount - visibleRows)
  if (key.upArrow) {
    setPopupState(
      guardPopupUpdater('reasoning', (prev) => ({
        ...prev,
        scrollOffset: clamp(prev.scrollOffset - 1, 0, maxOffset),
      })),
    )
    return
  }
  if (key.downArrow) {
    setPopupState(
      guardPopupUpdater('reasoning', (prev) => ({
        ...prev,
        scrollOffset: clamp(prev.scrollOffset + 1, 0, maxOffset),
      })),
    )
    return
  }
  if (key.pageUp) {
    setPopupState(
      guardPopupUpdater('reasoning', (prev) => ({
        ...prev,
        scrollOffset: clamp(prev.scrollOffset - visibleRows, 0, maxOffset),
      })),
    )
    return
  }
  if (key.pageDown) {
    setPopupState(
      guardPopupUpdater('reasoning', (prev) => ({
        ...prev,
        scrollOffset: clamp(prev.scrollOffset + visibleRows, 0, maxOffset),
      })),
    )
    return
  }
  if (key.escape) {
    closePopup()
  }
}
</file>

<file path="src/tui/screens/command/hooks/popup-shortcuts/resume-popup-shortcuts.ts">
import type { Key } from 'ink'
import type { PopupState } from '../../../../types'
import { guardPopupUpdater } from './types'
export type HandleResumePopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'resume' }>
  key: Key
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  onResumeSubmit: () => void
}
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
export const handleResumePopupShortcuts = ({
  popupState,
  key,
  setPopupState,
  closePopup,
  onResumeSubmit,
}: HandleResumePopupShortcutsOptions): void => {
  if (key.escape) {
    closePopup()
    return
  }
  const historyCount = popupState.historyItems.length
  const suggestionCount = popupState.suggestedItems.length
  if (
    key.tab &&
    popupState.selectionIndex === 2 &&
    popupState.sourceKind === 'file' &&
    suggestionCount
  ) {
    setPopupState(
      guardPopupUpdater('resume', (prev) => ({
        ...prev,
        suggestedFocused: !prev.suggestedFocused,
        suggestedSelectionIndex: 0,
      })),
    )
    return
  }
  if (popupState.selectionIndex === 2) {
    if (popupState.sourceKind === 'history') {
      if (key.upArrow && historyCount > 0) {
        if (popupState.historySelectionIndex === 0) {
          setPopupState(guardPopupUpdater('resume', (prev) => ({ ...prev, selectionIndex: 1 })))
          return
        }
        setPopupState(
          guardPopupUpdater('resume', (prev) => ({
            ...prev,
            historySelectionIndex: clamp(prev.historySelectionIndex - 1, 0, historyCount - 1),
          })),
        )
        return
      }
      if (key.downArrow && historyCount > 0) {
        setPopupState(
          guardPopupUpdater('resume', (prev) => ({
            ...prev,
            historySelectionIndex: clamp(prev.historySelectionIndex + 1, 0, historyCount - 1),
          })),
        )
        return
      }
      if (key.return) {
        onResumeSubmit()
        return
      }
    } else {
      if (popupState.suggestedFocused && suggestionCount > 0) {
        if (key.upArrow) {
          if (popupState.suggestedSelectionIndex === 0) {
            setPopupState(
              guardPopupUpdater('resume', (prev) => ({
                ...prev,
                suggestedFocused: false,
                suggestedSelectionIndex: 0,
              })),
            )
            return
          }
          setPopupState(
            guardPopupUpdater('resume', (prev) => ({
              ...prev,
              suggestedSelectionIndex: clamp(
                prev.suggestedSelectionIndex - 1,
                0,
                suggestionCount - 1,
              ),
            })),
          )
          return
        }
        if (key.downArrow) {
          setPopupState(
            guardPopupUpdater('resume', (prev) => ({
              ...prev,
              suggestedSelectionIndex: clamp(
                prev.suggestedSelectionIndex + 1,
                0,
                suggestionCount - 1,
              ),
            })),
          )
          return
        }
        if (key.return) {
          const selected = popupState.suggestedItems[popupState.suggestedSelectionIndex]
          if (!selected) {
            return
          }
          setPopupState(
            guardPopupUpdater('resume', (prev) => ({
              ...prev,
              payloadPathDraft: selected,
              suggestedFocused: false,
              suggestedSelectionIndex: 0,
            })),
          )
          return
        }
      }
      if (!popupState.suggestedFocused && key.downArrow && suggestionCount > 0) {
        setPopupState(
          guardPopupUpdater('resume', (prev) => ({
            ...prev,
            suggestedFocused: true,
            suggestedSelectionIndex: 0,
          })),
        )
        return
      }
      if (!popupState.suggestedFocused && key.upArrow) {
        setPopupState(guardPopupUpdater('resume', (prev) => ({ ...prev, selectionIndex: 1 })))
        return
      }
      if (!popupState.suggestedFocused && key.return) {
        onResumeSubmit()
        return
      }
    }
  }
  if (key.upArrow) {
    setPopupState(
      guardPopupUpdater('resume', (prev) => ({
        ...prev,
        selectionIndex: clamp(prev.selectionIndex - 1, 0, 2),
      })),
    )
    return
  }
  if (key.downArrow) {
    setPopupState(
      guardPopupUpdater('resume', (prev) => ({
        ...prev,
        selectionIndex: clamp(prev.selectionIndex + 1, 0, 2),
      })),
    )
    return
  }
  if (popupState.selectionIndex === 0 && (key.leftArrow || key.rightArrow)) {
    setPopupState(
      guardPopupUpdater('resume', (prev) => ({
        ...prev,
        sourceKind: prev.sourceKind === 'history' ? 'file' : 'history',
        suggestedFocused: false,
        suggestedSelectionIndex: 0,
      })),
    )
    return
  }
  if (popupState.selectionIndex === 1 && (key.leftArrow || key.rightArrow)) {
    setPopupState(
      guardPopupUpdater('resume', (prev) => ({
        ...prev,
        mode: prev.mode === 'strict' ? 'best-effort' : 'strict',
      })),
    )
    return
  }
  if (key.return && popupState.selectionIndex < 2) {
    setPopupState(guardPopupUpdater('resume', (prev) => ({ ...prev, selectionIndex: 2 })))
  }
}
</file>

<file path="src/tui/screens/command/hooks/popup-shortcuts/selection.ts">
export const clamp = (value: number, min: number, max: number): number => {
  return Math.max(min, Math.min(value, max))
}
export const clampIndex = (index: number, length: number): number => {
  if (length <= 0) {
    return 0
  }
  return clamp(index, 0, length - 1)
}
export const wrapIndex = (index: number, length: number): number => {
  if (length <= 0) {
    return 0
  }
  return ((index % length) + length) % length
}
</file>

<file path="src/tui/screens/command/hooks/popup-shortcuts/smart-popup-shortcuts.ts">
import type { Key } from 'ink'
import { isBackspaceKey } from '../../../../components/core/text-input-keys'
import type { PopupState } from '../../../../types'
import { guardPopupUpdater, type PopupShortcutEffect } from './types'
import { getSuggestedInputPopupEffect } from './suggested-input'
export type HandleSmartPopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'smart' }>
  input: string
  key: Key
  suggestions: readonly string[]
  smartContextRoot: string | null
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  onSmartRootSubmit: (value: string) => void
}
const applySmartPopupEffect = (
  effect: PopupShortcutEffect,
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void,
  closePopup: () => void,
): void => {
  if (effect.type === 'close') {
    closePopup()
    return
  }
  if (effect.type === 'set') {
    setPopupState(effect.updater)
  }
}
export const handleSmartPopupShortcuts = ({
  popupState,
  input,
  key,
  suggestions,
  smartContextRoot,
  setPopupState,
  closePopup,
  onSmartRootSubmit,
}: HandleSmartPopupShortcutsOptions): void => {
  const hasSuggestions = suggestions.length > 0
  if (popupState.suggestedFocused && hasSuggestions) {
    const effect = getSuggestedInputPopupEffect({
      popupType: 'smart',
      popupState,
      suggestions,
      key,
    })
    if (effect.type === 'selectSuggestion') {
      setPopupState(
        guardPopupUpdater('smart', (prev) => ({
          ...prev,
          draft: effect.value,
          suggestedFocused: false,
        })),
      )
      return
    }
    applySmartPopupEffect(effect, setPopupState, closePopup)
    return
  }
  const effect = getSuggestedInputPopupEffect({
    popupType: 'smart',
    popupState,
    suggestions,
    key,
  })
  if (effect.type === 'selectSuggestion') {
    setPopupState(
      guardPopupUpdater('smart', (prev) => ({
        ...prev,
        draft: effect.value,
        suggestedFocused: false,
      })),
    )
    return
  }
  applySmartPopupEffect(effect, setPopupState, closePopup)
  if (effect.type !== 'none') {
    return
  }
  const draftIsEmpty = popupState.draft.trim().length === 0
  if ((key.delete || (draftIsEmpty && isBackspaceKey(input, key))) && smartContextRoot) {
    onSmartRootSubmit('')
  }
}
</file>

<file path="src/tui/screens/command/hooks/popup-shortcuts/suggested-input.ts">
import type { PopupState } from '../../../../types'
import { clampIndex } from './selection'
import {
  NO_EFFECT,
  guardPopupUpdater,
  type PopupKey,
  type PopupShortcutEffect,
  type PopupStateFor,
  type PopupType,
} from './types'
export type SuggestedInputPopupType = Extract<PopupType, 'smart' | 'intent'>
export type SuggestedInputPopupState = PopupStateFor<'smart'> | PopupStateFor<'intent'>
export type GetSuggestedInputPopupEffectOptions = {
  popupType: SuggestedInputPopupType
  popupState: SuggestedInputPopupState
  suggestions: readonly string[]
  key: PopupKey
}
const guardSuggestedInputUpdater = (
  popupType: SuggestedInputPopupType,
  updater: (prev: SuggestedInputPopupState) => SuggestedInputPopupState,
): ((prev: PopupState) => PopupState) => {
  return guardPopupUpdater(
    popupType,
    updater as (prev: PopupStateFor<typeof popupType>) => PopupStateFor<typeof popupType>,
  )
}
export const getSuggestedInputPopupEffect = ({
  popupType,
  popupState,
  suggestions,
  key,
}: GetSuggestedInputPopupEffectOptions): PopupShortcutEffect => {
  const hasSuggestions = suggestions.length > 0
  const maxSuggestedIndex = Math.max(suggestions.length - 1, 0)
  if (key.escape) {
    return { type: 'close' }
  }
  if (popupState.suggestedFocused && hasSuggestions) {
    if (key.tab) {
      return {
        type: 'set',
        updater: guardSuggestedInputUpdater(popupType, (prev) => ({
          ...prev,
          suggestedFocused: false,
        })),
      }
    }
    if (key.upArrow) {
      return {
        type: 'set',
        updater: guardSuggestedInputUpdater(popupType, (prev) => {
          const effectiveIndex = clampIndex(prev.suggestedSelectionIndex, suggestions.length)
          if (effectiveIndex === 0) {
            return { ...prev, suggestedFocused: false }
          }
          return {
            ...prev,
            suggestedSelectionIndex: Math.max(prev.suggestedSelectionIndex - 1, 0),
          }
        }),
      }
    }
    if (key.downArrow) {
      return {
        type: 'set',
        updater: guardSuggestedInputUpdater(popupType, (prev) => ({
          ...prev,
          suggestedSelectionIndex: Math.min(prev.suggestedSelectionIndex + 1, maxSuggestedIndex),
        })),
      }
    }
    if (key.return) {
      const effectiveIndex = clampIndex(popupState.suggestedSelectionIndex, suggestions.length)
      const selection = suggestions[effectiveIndex]
      return selection ? { type: 'selectSuggestion', value: selection } : NO_EFFECT
    }
    return NO_EFFECT
  }
  if (key.tab && !key.shift && hasSuggestions) {
    return {
      type: 'set',
      updater: guardSuggestedInputUpdater(popupType, (prev) => ({
        ...prev,
        suggestedFocused: true,
        suggestedSelectionIndex: 0,
      })),
    }
  }
  if (key.downArrow && hasSuggestions) {
    return {
      type: 'set',
      updater: guardSuggestedInputUpdater(popupType, (prev) => ({
        ...prev,
        suggestedFocused: true,
        suggestedSelectionIndex: 0,
      })),
    }
  }
  return NO_EFFECT
}
</file>

<file path="src/tui/screens/command/hooks/popup-shortcuts/suggested-selected-list-popup-shortcuts.ts">
import type { Key } from 'ink'
import { isBackspaceKey } from '../../../../components/core/text-input-keys'
import type { PopupState } from '../../../../types'
import {
  applySuggestedSelectedListEffect,
  getSuggestedSelectedListPopupEffect,
  type SuggestedSelectedListPopupState,
  type SuggestedSelectedListPopupType,
} from './suggested-selected-list'
export type HandleSuggestedSelectedListPopupShortcutsOptions = {
  popupType: SuggestedSelectedListPopupType
  popupState: SuggestedSelectedListPopupState
  input: string
  key: Key
  itemsLength: number
  suggestions: readonly string[]
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  onRemove: (index: number) => void
  onSelectSuggestion: (value: string) => void
}
export const handleSuggestedSelectedListPopupShortcuts = ({
  popupType,
  popupState,
  input,
  key,
  itemsLength,
  suggestions,
  setPopupState,
  closePopup,
  onRemove,
  onSelectSuggestion,
}: HandleSuggestedSelectedListPopupShortcutsOptions): void => {
  const effect = getSuggestedSelectedListPopupEffect({
    popupType,
    popupState,
    itemsLength,
    suggestions,
    key,
    isBackspace: isBackspaceKey(input, key),
  })
  applySuggestedSelectedListEffect(effect, setPopupState, closePopup, onRemove, onSelectSuggestion)
}
</file>

<file path="src/tui/screens/command/hooks/popup-shortcuts/theme-popup-shortcuts.ts">
import type { Key } from 'ink'
import type { PopupState } from '../../../../types'
import { wrapIndex } from './selection'
import { guardPopupUpdater } from './types'
export type HandleThemePopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'theme' }>
  key: Key
  themeCount: number
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  onThemeConfirm: () => void
  onThemeCancel: () => void
}
export const handleThemePopupShortcuts = ({
  key,
  themeCount,
  setPopupState,
  onThemeConfirm,
  onThemeCancel,
}: HandleThemePopupShortcutsOptions): void => {
  if (key.upArrow && themeCount > 0) {
    setPopupState(
      guardPopupUpdater('theme', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex - 1, themeCount),
      })),
    )
    return
  }
  if (key.downArrow && themeCount > 0) {
    setPopupState(
      guardPopupUpdater('theme', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex + 1, themeCount),
      })),
    )
    return
  }
  if (key.escape) {
    onThemeCancel()
    return
  }
  if (key.return) {
    onThemeConfirm()
  }
}
export type HandleThemeModePopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'themeMode' }>
  key: Key
  optionCount: number
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  onThemeModeConfirm: () => void
  onThemeModeCancel: () => void
}
export const handleThemeModePopupShortcuts = ({
  key,
  optionCount,
  setPopupState,
  onThemeModeConfirm,
  onThemeModeCancel,
}: HandleThemeModePopupShortcutsOptions): void => {
  if ((key.leftArrow || key.upArrow) && optionCount > 0) {
    setPopupState(
      guardPopupUpdater('themeMode', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex - 1, optionCount),
      })),
    )
    return
  }
  if ((key.rightArrow || key.downArrow) && optionCount > 0) {
    setPopupState(
      guardPopupUpdater('themeMode', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex + 1, optionCount),
      })),
    )
    return
  }
  if (key.escape) {
    onThemeModeCancel()
    return
  }
  if (key.return) {
    onThemeModeConfirm()
  }
}
</file>

<file path="src/tui/screens/command/hooks/popup-shortcuts/toggle-popup-shortcuts.ts">
import type { Key } from 'ink'
import type { PopupState, ToggleField } from '../../../../types'
import { wrapIndex } from './selection'
import { guardPopupUpdater } from './types'
const TOGGLE_OPTION_COUNT = 2
export type HandleTogglePopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'toggle' }>
  key: Key
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  applyToggleSelection: (field: ToggleField, value: boolean) => void
}
export const handleTogglePopupShortcuts = ({
  popupState,
  key,
  setPopupState,
  closePopup,
  applyToggleSelection,
}: HandleTogglePopupShortcutsOptions): void => {
  if (key.leftArrow || key.upArrow) {
    setPopupState(
      guardPopupUpdater('toggle', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex - 1, TOGGLE_OPTION_COUNT),
      })),
    )
    return
  }
  if (key.rightArrow || key.downArrow) {
    setPopupState(
      guardPopupUpdater('toggle', (prev) => ({
        ...prev,
        selectionIndex: wrapIndex(prev.selectionIndex + 1, TOGGLE_OPTION_COUNT),
      })),
    )
    return
  }
  if (key.escape) {
    closePopup()
    return
  }
  if (key.return) {
    applyToggleSelection(popupState.field, popupState.selectionIndex === 0)
  }
}
</file>

<file path="src/tui/screens/command/hooks/popup-shortcuts/types.ts">
import type { PopupState } from '../../../../types'
export type NonNullPopupState = Exclude<PopupState, null>
export type PopupType = NonNullPopupState['type']
export type PopupStateFor<T extends PopupType> = Extract<NonNullPopupState, { type: T }>
export type PopupKey = {
  upArrow?: boolean
  downArrow?: boolean
  leftArrow?: boolean
  rightArrow?: boolean
  tab?: boolean
  shift?: boolean
  return?: boolean
  escape?: boolean
  delete?: boolean
  pageUp?: boolean
  pageDown?: boolean
  ctrl?: boolean
}
export type PopupShortcutEffect =
  | { type: 'none' }
  | { type: 'close' }
  | { type: 'set'; updater: (prev: PopupState) => PopupState }
  | { type: 'remove'; index: number }
  | { type: 'selectSuggestion'; value: string }
export const NO_EFFECT: PopupShortcutEffect = { type: 'none' }
export const guardPopupUpdater = <T extends PopupType>(
  popupType: T,
  updater: (prev: PopupStateFor<T>) => PopupStateFor<T>,
): ((prev: PopupState) => PopupState) => {
  return (prev) => {
    if (!prev || prev.type !== popupType) {
      return prev
    }
    return updater(prev as PopupStateFor<T>)
  }
}
</file>

<file path="src/tui/screens/command/hooks/popup-shortcuts/url-popup-shortcuts.ts">
import type { Key } from 'ink'
import { isBackspaceKey } from '../../../../components/core/text-input-keys'
import type { PopupState } from '../../../../types'
import { isControlKey } from '../../utils/control-key'
import { clampIndex } from './selection'
import { guardPopupUpdater } from './types'
export type HandleUrlPopupShortcutsOptions = {
  popupState: Extract<Exclude<PopupState, null>, { type: 'url' }>
  input: string
  key: Key
  urls: readonly string[]
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  closePopup: () => void
  onRemoveUrl: (index: number) => void
}
export const handleUrlPopupShortcuts = ({
  popupState,
  input,
  key,
  urls,
  setPopupState,
  closePopup,
  onRemoveUrl,
}: HandleUrlPopupShortcutsOptions): void => {
  const draftIsEmpty = popupState.draft.trim().length === 0
  if (popupState.editingIndex !== null) {
    if (key.escape) {
      setPopupState(
        guardPopupUpdater('url', (prev) => ({
          ...prev,
          draft: '',
          editingIndex: null,
          selectedFocused: false,
        })),
      )
      return
    }
    if (draftIsEmpty && (key.delete || isBackspaceKey(input, key))) {
      if (urls.length > 0) {
        onRemoveUrl(popupState.selectionIndex)
      }
      return
    }
    return
  }
  if (key.escape) {
    closePopup()
    return
  }
  if (!popupState.selectedFocused && (key.upArrow || key.downArrow) && urls.length > 0) {
    setPopupState(
      guardPopupUpdater('url', (prev) => ({
        ...prev,
        selectedFocused: true,
        selectionIndex: clampIndex(prev.selectionIndex, urls.length),
      })),
    )
    return
  }
  if (popupState.selectedFocused) {
    if (key.upArrow) {
      setPopupState(
        guardPopupUpdater('url', (prev) => {
          if (prev.selectionIndex === 0) {
            return { ...prev, selectedFocused: false }
          }
          return { ...prev, selectionIndex: Math.max(prev.selectionIndex - 1, 0) }
        }),
      )
      return
    }
    if (key.downArrow) {
      if (popupState.selectionIndex >= urls.length - 1) {
        return
      }
      setPopupState(
        guardPopupUpdater('url', (prev) => ({
          ...prev,
          selectionIndex: Math.min(prev.selectionIndex + 1, urls.length - 1),
        })),
      )
      return
    }
    if (key.delete || isBackspaceKey(input, key)) {
      if (urls.length > 0) {
        onRemoveUrl(popupState.selectionIndex)
      }
      return
    }
    if (isControlKey(input, key, 'e')) {
      return
    }
    if (input.toLowerCase() === 'e' && urls.length > 0) {
      const selected = urls[popupState.selectionIndex]
      if (!selected) {
        return
      }
      setPopupState(
        guardPopupUpdater('url', (prev) => ({
          ...prev,
          draft: selected,
          editingIndex: prev.selectionIndex,
          selectedFocused: false,
        })),
      )
      return
    }
    return
  }
  if (draftIsEmpty && isBackspaceKey(input, key) && urls.length > 0) {
    onRemoveUrl(popupState.selectionIndex)
  }
}
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenHistoryAndTests.ts">
import type { MutableRefObject } from 'react'
import { useMemo, useRef } from 'react'
import { usePersistentCommandHistory } from '../../../hooks/usePersistentCommandHistory'
import { useStableCallback } from '../../../hooks/useStableCallback'
import type { HistoryEntry } from '../../../types'
import { usePromptTestRunner } from './usePromptTestRunner'
const DEFAULT_TEST_FILE = 'prompt-tests.yaml'
type PushHistory = (
  content: string,
  kind?: HistoryEntry['kind'],
  format?: HistoryEntry['format'],
) => void
export type UseCommandScreenHistoryAndTestsResult = {
  pushHistoryRef: MutableRefObject<PushHistory>
  pushHistoryProxy: PushHistory
  clearHistoryRef: MutableRefObject<() => void>
  clearHistoryProxy: () => void
  scrollToRef: MutableRefObject<(row: number) => void>
  scrollToProxy: (row: number) => void
  closeTestPopupRef: MutableRefObject<() => void>
  closeTestPopupProxy: () => void
  commandHistoryValues: string[]
  addCommandHistoryEntry: (value: string) => void
  isTestCommandRunning: boolean
  lastTestFile: string | null
  runTestsFromCommandProxy: (value: string) => void
  onTestPopupSubmit: (value: string) => void
}
export const useCommandScreenHistoryAndTests = (): UseCommandScreenHistoryAndTestsResult => {
  const pushHistoryRef = useRef<PushHistory>((_content, _kind) => {
    throw new Error('pushHistoryRef.current has not been initialized yet.')
  })
  const pushHistoryProxy: PushHistory = useStableCallback(
    (content: string, kind: HistoryEntry['kind'] = 'system', format?: HistoryEntry['format']) => {
      pushHistoryRef.current(content, kind, format)
    },
  )
  const clearHistoryRef = useRef<() => void>(() => {
    throw new Error('clearHistoryRef.current has not been initialized yet.')
  })
  const clearHistoryProxy = useStableCallback(() => {
    clearHistoryRef.current()
  })
  const scrollToRef = useRef<(row: number) => void>(() => {
    throw new Error('scrollToRef.current has not been initialized yet.')
  })
  const scrollToProxy = useStableCallback((row: number) => {
    scrollToRef.current(row)
  })
  const closeTestPopupRef = useRef<() => void>(() => {
    throw new Error('closeTestPopupRef.current has not been initialized yet.')
  })
  const closeTestPopupProxy = useStableCallback(() => {
    closeTestPopupRef.current()
  })
  const { entries: commandHistoryEntries, addEntry: addCommandHistoryEntry } =
    usePersistentCommandHistory({
      onError: (message) => {
        pushHistoryProxy(`[history] ${message}`, 'system')
      },
    })
  const commandHistoryValues = useMemo(
    () => commandHistoryEntries.map((entry) => entry.value),
    [commandHistoryEntries],
  )
  const { isTestCommandRunning, lastTestFile, runTestsFromCommand, onTestPopupSubmit } =
    usePromptTestRunner({
      defaultTestFile: DEFAULT_TEST_FILE,
      pushHistory: pushHistoryProxy,
      clearHistory: clearHistoryProxy,
      closeTestPopup: closeTestPopupProxy,
      addCommandHistoryEntry,
    })
  return {
    pushHistoryRef,
    pushHistoryProxy,
    clearHistoryRef,
    clearHistoryProxy,
    scrollToRef,
    scrollToProxy,
    closeTestPopupRef,
    closeTestPopupProxy,
    commandHistoryValues,
    addCommandHistoryEntry,
    isTestCommandRunning,
    lastTestFile,
    runTestsFromCommandProxy: runTestsFromCommand,
    onTestPopupSubmit,
  }
}
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenShell.ts">
import type { WriteStream } from 'node:tty'
import { useEffect } from 'react'
import { COMMAND_DESCRIPTORS, POPUP_HEIGHTS } from '../../../config'
import { useCommandHistory } from '../../../hooks/useCommandHistory'
import type { HistoryEntry, PopupState } from '../../../types'
import { useCommandMenuManager } from './useCommandMenuManager'
import { useCommandScreenLayout } from './useCommandScreenLayout'
import { useDroppedFileDetection } from './useDroppedFileDetection'
import { useHistoryScrollKeys } from './useHistoryScrollKeys'
import { usePopupSelectionClamp } from './usePopupSelectionClamp'
import { useSessionCommands } from './useSessionCommands'
import { useTerminalEffects } from './useTerminalEffects'
const APP_STATIC_ROWS = 7
const COMMAND_SCREEN_OVERHEAD_ROWS = 3
const COMMAND_MENU_HEIGHT = COMMAND_DESCRIPTORS.length + 2
const EMPTY_HISTORY: HistoryEntry[] = []
type SetPopupState = import('react').Dispatch<import('react').SetStateAction<PopupState>>
type UseCommandScreenShellOptions = {
  stdout: WriteStream | undefined
  setTerminalSize: (rows: number, columns: number) => void
  interactiveTransportPath?: string | undefined
  // screen state
  terminalRows: number
  inputValue: string
  debugKeyLine: string | null
  debugKeysEnabled: boolean
  helpOpen: boolean
  reservedRows: number
  popupState: PopupState
  isPopupOpen: boolean
  setPopupState: SetPopupState
  commandMenuSignal?: number | undefined
  commandSelectionIndex: number
  setCommandSelectionIndex: (next: number | ((prev: number) => number)) => void
  // generation state
  isGenerating: boolean
  awaitingInteractiveMode:
    | import('../../../generation-pipeline-reducer').InteractiveAwaitingMode
    | null
  // context
  files: string[]
  urls: string[]
  lastGeneratedPrompt: string | null
  resetContext: () => void
  // mutable refs
  lastUserIntentRef: import('react').MutableRefObject<string | null>
  lastTypedIntentRef: import('react').MutableRefObject<string>
  // setters
  setInputValue: (value: string | ((prev: string) => string)) => void
  setIntentFilePath: (value: string) => void
  setMetaInstructions: (value: string) => void
  scrollToRef: import('react').MutableRefObject<(row: number) => void>
  clearHistoryRef: import('react').MutableRefObject<() => void>
  pushHistoryRef: import('react').MutableRefObject<
    (content: string, kind?: HistoryEntry['kind']) => void
  >
  scrollToProxy: (row: number) => void
}
export type UseCommandScreenShellResult = {
  // menu
  isCommandMode: boolean
  commandMenuArgsRaw: string
  visibleCommands: readonly import('../../../types').CommandDescriptor[]
  isCommandMenuActive: boolean
  menuHeight: number
  selectedCommand: import('../../../types').CommandDescriptor | undefined
  // layout
  overlayHeight: number
  inputBarHint: string | undefined
  inputBarDebugLine: string | undefined
  isAwaitingTransportInput: boolean
  historyRows: number
  // history output
  history: HistoryEntry[]
  scrollOffset: number
  scrollBy: (delta: number) => void
  // history
  scrollTo: (row: number) => void
  clearHistory: () => void
  // session
  handleNewCommand: (argsRaw: string) => void
  handleReuseCommand: () => void
}
export const useCommandScreenShell = ({
  stdout,
  setTerminalSize,
  interactiveTransportPath,
  terminalRows,
  inputValue,
  debugKeyLine,
  debugKeysEnabled,
  helpOpen,
  reservedRows,
  popupState,
  isPopupOpen,
  setPopupState,
  commandMenuSignal,
  commandSelectionIndex,
  setCommandSelectionIndex,
  isGenerating,
  awaitingInteractiveMode,
  files,
  urls,
  lastGeneratedPrompt,
  resetContext,
  lastUserIntentRef,
  lastTypedIntentRef,
  setInputValue,
  setIntentFilePath,
  setMetaInstructions,
  scrollToRef,
  clearHistoryRef,
  pushHistoryRef,
  scrollToProxy,
}: UseCommandScreenShellOptions): UseCommandScreenShellResult => {
  const { droppedFilePath, existsSync } = useDroppedFileDetection(inputValue)
  const {
    isCommandMode,
    commandMenuArgsRaw,
    visibleCommands,
    isCommandMenuActive,
    menuHeight,
    selectedCommand,
  } = useCommandMenuManager({
    inputValue,
    existsSync,
    popupState,
    helpOpen,
    ...(commandMenuSignal !== undefined ? { commandMenuSignal } : {}),
    commands: COMMAND_DESCRIPTORS,
    commandMenuHeight: COMMAND_MENU_HEIGHT,
    commandSelectionIndex,
    setCommandSelectionIndex,
    setInputValue,
    setPopupState,
    scrollTo: scrollToProxy,
  })
  const { overlayHeight, inputBarHint, inputBarDebugLine, isAwaitingTransportInput, historyRows } =
    useCommandScreenLayout({
      terminalRows,
      reservedRows,
      helpOpen,
      isPopupOpen,
      popupState,
      menuHeight,
      popupHeights: POPUP_HEIGHTS,
      inputValue,
      droppedFilePath,
      debugKeysEnabled,
      debugKeyLine,
      interactiveTransportPath,
      isGenerating,
      awaitingInteractiveMode,
      isCommandMenuActive,
      appStaticRows: APP_STATIC_ROWS,
      commandScreenOverheadRows: COMMAND_SCREEN_OVERHEAD_ROWS,
    })
  const {
    history,
    resetHistory: resetOutputHistory,
    clearHistory,
    pushHistory: pushOutputHistory,
    scroll,
  } = useCommandHistory({
    initialEntries: EMPTY_HISTORY,
    visibleRows: historyRows,
  })
  // Keep upstream refs in sync.
  useEffect(() => {
    pushHistoryRef.current = pushOutputHistory
    clearHistoryRef.current = clearHistory
    scrollToRef.current = scroll.scrollTo
  }, [
    clearHistory,
    pushHistoryRef,
    pushOutputHistory,
    scroll.scrollTo,
    scrollToRef,
    clearHistoryRef,
  ])
  useTerminalEffects({
    stdout,
    setTerminalSize,
    interactiveTransportPath,
    history,
    pushHistory: pushOutputHistory,
  })
  usePopupSelectionClamp({
    setPopupState,
    filesLength: files.length,
    urlsLength: urls.length,
  })
  useHistoryScrollKeys({
    isCommandMenuActive,
    isPopupOpen,
    helpOpen,
    historyRows,
    scrollBy: scroll.scrollBy,
  })
  const { handleNewCommand, handleReuseCommand } = useSessionCommands({
    isGenerating,
    lastGeneratedPrompt,
    resetContext,
    resetHistory: resetOutputHistory,
    scrollTo: scroll.scrollTo,
    setInputValue,
    setPopupState,
    setIntentFilePath,
    setMetaInstructions,
    lastUserIntentRef,
    lastTypedIntentRef,
    pushHistory: pushOutputHistory,
  })
  return {
    isCommandMode,
    commandMenuArgsRaw,
    visibleCommands,
    isCommandMenuActive,
    menuHeight,
    selectedCommand,
    overlayHeight,
    inputBarHint,
    inputBarDebugLine,
    isAwaitingTransportInput,
    historyRows,
    history,
    scrollOffset: scroll.offset,
    scrollBy: scroll.scrollBy,
    scrollTo: scroll.scrollTo,
    clearHistory,
    handleNewCommand,
    handleReuseCommand,
  }
}
</file>

<file path="src/tui/screens/command/hooks/useDroppedFileDetection.ts">
import fs from 'node:fs'
import { useEffect, useMemo, useState } from 'react'
import { parseAbsolutePathFromInput } from '../../../drag-drop-path'
type StatFn = (candidate: string) => Promise<fs.Stats>
export type UseDroppedFileDetectionFs = {
  stat: StatFn
}
export type UseDroppedFileDetectionResult = {
  droppedFilePath: string | null
  existsSync: (candidate: string) => boolean
  isFilePath: (candidate: string) => boolean
}
type PathProbe = {
  exists: boolean
  isFile: boolean
}
const getErrnoCode = (error: unknown): string | null => {
  if (!(error instanceof Error)) {
    return null
  }
  const maybeWithCode = error as unknown as { code?: unknown }
  return typeof maybeWithCode.code === 'string' ? maybeWithCode.code : null
}
const DEFAULT_FS_IMPL: UseDroppedFileDetectionFs = {
  stat: (candidate: string) => fs.promises.stat(candidate),
}
export const useDroppedFileDetection = (
  inputValue: string,
  fsImpl: UseDroppedFileDetectionFs = DEFAULT_FS_IMPL,
): UseDroppedFileDetectionResult => {
  const [cacheVersion, setCacheVersion] = useState(0)
  const cache = useMemo(() => new Map<string, PathProbe>(), [])
  const inFlight = useMemo(() => new Set<string>(), [])
  const candidate = useMemo(() => parseAbsolutePathFromInput(inputValue), [inputValue])
  useEffect(() => {
    if (!candidate) {
      return
    }
    if (cache.has(candidate) || inFlight.has(candidate)) {
      return
    }
    inFlight.add(candidate)
    let cancelled = false
    void (async () => {
      try {
        const stats = await fsImpl.stat(candidate)
        if (cancelled) {
          return
        }
        cache.set(candidate, { exists: true, isFile: stats.isFile() })
      } catch (error) {
        if (cancelled) {
          return
        }
        const code = getErrnoCode(error)
        if (code === 'ENOENT' || code === 'ENOTDIR') {
          cache.set(candidate, { exists: false, isFile: false })
        } else {
          cache.set(candidate, { exists: false, isFile: false })
        }
      } finally {
        if (!cancelled) {
          inFlight.delete(candidate)
          setCacheVersion((prev) => prev + 1)
        }
      }
    })()
    return () => {
      cancelled = true
      inFlight.delete(candidate)
    }
  }, [candidate, cache, fsImpl, inFlight])
  const droppedFilePath = useMemo(() => {
    if (!candidate) {
      return null
    }
    const entry = cache.get(candidate)
    return entry?.exists && entry.isFile ? candidate : null
  }, [cacheVersion, cache, candidate])
  const existsSync = useMemo(() => {
    return (path: string): boolean => cache.get(path)?.exists ?? false
  }, [cacheVersion, cache])
  const isFilePath = useMemo(() => {
    return (path: string): boolean => {
      const entry = cache.get(path)
      return entry?.exists === true && entry.isFile
    }
  }, [cacheVersion, cache])
  return {
    droppedFilePath,
    existsSync,
    isFilePath,
  }
}
</file>

<file path="src/tui/screens/command/hooks/useDroppedFilePath.ts">
import { useDroppedFileDetection } from './useDroppedFileDetection'
export type UseDroppedFilePathResult = string | null
export const useDroppedFilePath = (inputValue: string): UseDroppedFilePathResult => {
  const { droppedFilePath } = useDroppedFileDetection(inputValue)
  return droppedFilePath
}
</file>

<file path="src/tui/screens/command/hooks/useIntentPopupGlue.ts">
import { useCallback, useEffect, useMemo } from 'react'
import { stripTerminalPasteArtifacts } from '../../../components/core/bracketed-paste'
import { filterIntentFileSuggestions } from '../../../file-suggestions'
import type { PopupState } from '../../../types'
const EMPTY_SUGGESTIONS: string[] = []
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
export type UseIntentPopupGlueOptions = {
  popupState: PopupState
  setPopupState: SetPopupState
}
export type UseIntentPopupGlueResult = {
  intentPopupSuggestions: string[]
  intentPopupSuggestionSelectionIndex: number
  intentPopupSuggestionsFocused: boolean
  onIntentPopupDraftChange: (next: string) => void
}
export const useIntentPopupGlue = ({
  popupState,
  setPopupState,
}: UseIntentPopupGlueOptions): UseIntentPopupGlueResult => {
  const intentPopupDraft = popupState?.type === 'intent' ? popupState.draft : ''
  const intentPopupSuggestedItems =
    popupState?.type === 'intent' ? popupState.suggestedItems : EMPTY_SUGGESTIONS
  const intentPopupSuggestedFocused =
    popupState?.type === 'intent' ? popupState.suggestedFocused : false
  const intentPopupSuggestedSelectionIndex =
    popupState?.type === 'intent' ? popupState.suggestedSelectionIndex : 0
  const intentPopupSuggestions = useMemo(() => {
    if (!intentPopupSuggestedItems.length) {
      return []
    }
    if (!intentPopupDraft.trim()) {
      return []
    }
    return filterIntentFileSuggestions({
      suggestions: intentPopupSuggestedItems,
      query: intentPopupDraft,
      limit: 200,
    })
  }, [intentPopupDraft, intentPopupSuggestedItems])
  const intentPopupSuggestionSelectionIndex = Math.min(
    intentPopupSuggestedSelectionIndex,
    Math.max(intentPopupSuggestions.length - 1, 0),
  )
  const intentPopupSuggestionsFocused =
    intentPopupSuggestedFocused && intentPopupSuggestions.length > 0
  useEffect(() => {
    if (popupState?.type !== 'intent') {
      return
    }
    if (!intentPopupSuggestedFocused) {
      return
    }
    if (intentPopupSuggestions.length > 0) {
      return
    }
    setPopupState((prev) =>
      prev?.type === 'intent'
        ? { ...prev, suggestedFocused: false, suggestedSelectionIndex: 0 }
        : prev,
    )
  }, [intentPopupSuggestedFocused, intentPopupSuggestions.length, popupState?.type, setPopupState])
  const onIntentPopupDraftChange = useCallback(
    (next: string) => {
      const sanitized = stripTerminalPasteArtifacts(next)
      setPopupState((prev) =>
        prev?.type === 'intent'
          ? {
              ...prev,
              draft: sanitized,
              suggestedSelectionIndex: 0,
              suggestedFocused: false,
            }
          : prev,
      )
    },
    [setPopupState],
  )
  return {
    intentPopupSuggestions,
    intentPopupSuggestionSelectionIndex,
    intentPopupSuggestionsFocused,
    onIntentPopupDraftChange,
  }
}
</file>

<file path="src/tui/screens/command/hooks/usePopupSelectionClamp.ts">
import { useEffect } from 'react'
import type { PopupState } from '../../../types'
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
export type UsePopupSelectionClampOptions = {
  setPopupState: SetPopupState
  filesLength: number
  urlsLength: number
}
export const usePopupSelectionClamp = ({
  setPopupState,
  filesLength,
  urlsLength,
}: UsePopupSelectionClampOptions): void => {
  useEffect(() => {
    setPopupState((prev) => {
      if (!prev) {
        return prev
      }
      if (prev.type === 'file') {
        const maxIndex = Math.max(filesLength - 1, 0)
        const nextIndex = Math.min(prev.selectionIndex, maxIndex)
        return prev.selectionIndex === nextIndex ? prev : { ...prev, selectionIndex: nextIndex }
      }
      if (prev.type === 'url') {
        const maxIndex = Math.max(urlsLength - 1, 0)
        const nextIndex = Math.min(prev.selectionIndex, maxIndex)
        const shouldResetFocus =
          urlsLength === 0 && (prev.selectedFocused || prev.editingIndex !== null)
        if (!shouldResetFocus && prev.selectionIndex === nextIndex) {
          return prev
        }
        return {
          ...prev,
          selectionIndex: nextIndex,
          selectedFocused: urlsLength === 0 ? false : prev.selectedFocused,
          editingIndex: urlsLength === 0 ? null : prev.editingIndex,
          draft: urlsLength === 0 && prev.editingIndex !== null ? '' : prev.draft,
        }
      }
      return prev
    })
  }, [filesLength, setPopupState, urlsLength])
}
</file>

<file path="src/tui/screens/command/hooks/usePromptTestRunner.ts">
import path from 'node:path'
import { useState } from 'react'
import { useStableCallback } from '../../../hooks/useStableCallback'
import { runPromptTestSuite, type PromptTestRunReporter } from '../../../../test-command'
import type { HistoryEntry } from '../../../types'
type PushHistory = (
  content: string,
  kind?: HistoryEntry['kind'],
  format?: HistoryEntry['format'],
) => void
type ClearHistory = () => void
type CloseTestPopup = () => void
type AddCommandHistoryEntry = (value: string) => void
export type UsePromptTestRunnerOptions = {
  defaultTestFile: string
  pushHistory: PushHistory
  clearHistory: ClearHistory
  closeTestPopup: CloseTestPopup
  addCommandHistoryEntry: AddCommandHistoryEntry
}
export type UsePromptTestRunnerResult = {
  isTestCommandRunning: boolean
  lastTestFile: string | null
  runTestsFromCommand: (value: string) => void
  onTestPopupSubmit: (value: string) => void
}
export const usePromptTestRunner = ({
  defaultTestFile,
  pushHistory,
  clearHistory,
  closeTestPopup,
  addCommandHistoryEntry,
}: UsePromptTestRunnerOptions): UsePromptTestRunnerResult => {
  const [isTestCommandRunning, setIsTestCommandRunning] = useState(false)
  const [lastTestFile, setLastTestFile] = useState<string | null>(null)
  const runTestsFromCommand = useStableCallback((value: string) => {
    void (async () => {
      const normalized = value.trim()
      const targetFile = normalized || lastTestFile || defaultTestFile
      if (!targetFile) {
        pushHistory('No test file specified. Use /test <file>.', 'system')
        return
      }
      if (isTestCommandRunning) {
        pushHistory('Test run already in progress. Please wait.', 'system')
        return
      }
      const resolvedPath = path.resolve(process.cwd(), targetFile)
      clearHistory()
      setIsTestCommandRunning(true)
      setLastTestFile(targetFile)
      closeTestPopup()
      pushHistory(`[tests] Running ${resolvedPath}`, 'progress')
      try {
        const reporter: PromptTestRunReporter = {
          onSuiteLoaded: (suite, loadedPath) => {
            pushHistory(
              `[tests] Loaded ${suite.tests.length} test(s) from ${loadedPath}`,
              'progress',
            )
          },
          onTestStart: (ordinal, test) => {
            pushHistory(`[tests] (${ordinal}) ${test.name}`, 'progress')
          },
          onTestComplete: (_ordinal, result) => {
            const status = result.pass ? 'PASS' : 'FAIL'
            const reason = result.reason ? ` · ${result.reason}` : ''
            pushHistory(
              `[tests] ${status} ${result.name}${reason}`,
              result.pass ? 'system' : 'progress',
            )
          },
          onComplete: (results) => {
            const passed = results.filter((result) => result.pass).length
            const failed = results.length - passed
            const kind: HistoryEntry['kind'] = failed > 0 ? 'progress' : 'system'
            pushHistory(`[tests] Summary · passed ${passed} · failed ${failed}`, kind)
          },
        }
        await runPromptTestSuite(resolvedPath, { reporter })
        pushHistory('[tests] Complete.', 'progress')
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown test execution error.'
        pushHistory(`[tests] Failed: ${message}`, 'progress')
      } finally {
        setIsTestCommandRunning(false)
      }
    })()
  })
  const onTestPopupSubmit = useStableCallback((value: string) => {
    const trimmed = value.trim()
    addCommandHistoryEntry(`/test${trimmed ? ` ${trimmed}` : ''}`)
    runTestsFromCommand(value)
  })
  return {
    isTestCommandRunning,
    lastTestFile,
    runTestsFromCommand,
    onTestPopupSubmit,
  }
}
</file>

<file path="src/tui/screens/command/hooks/useTerminalEffects.ts">
import type { WriteStream } from 'node:tty'
import { useEffect } from 'react'
import type { HistoryEntry } from '../../../types'
type PushHistory = (
  content: string,
  kind?: HistoryEntry['kind'],
  format?: HistoryEntry['format'],
) => void
type SetTerminalSize = (rows: number, columns: number) => void
export type UseTerminalEffectsOptions = {
  stdout: WriteStream | undefined
  setTerminalSize: SetTerminalSize
  interactiveTransportPath?: string | undefined
  history: HistoryEntry[]
  pushHistory: PushHistory
}
export const useTerminalEffects = ({
  stdout,
  setTerminalSize,
  interactiveTransportPath,
  history,
  pushHistory,
}: UseTerminalEffectsOptions): void => {
  useEffect(() => {
    if (!stdout) {
      return undefined
    }
    stdout.write('\x1bc')
    stdout.write('\x1b[?2004h')
    return () => {
      stdout.write('\x1b[?2004l')
    }
  }, [stdout])
  useEffect(() => {
    if (!interactiveTransportPath) {
      return
    }
    const transportLine = `Interactive transport listening on ${interactiveTransportPath}`
    if (history.some((entry) => entry.content === transportLine)) {
      return
    }
    pushHistory(transportLine, 'system')
  }, [history, interactiveTransportPath, pushHistory])
  useEffect(() => {
    if (!stdout) {
      return undefined
    }
    const handleResize = (): void => {
      setTerminalSize(stdout.rows, stdout.columns)
    }
    stdout.on('resize', handleResize)
    return () => {
      stdout.off('resize', handleResize)
    }
  }, [setTerminalSize, stdout])
}
</file>

<file path="src/tui/screens/command/utils/submit-plan.ts">
import { resolveIntentSource } from '../../../intent-source'
import type { IntentSourceSelection } from '../../../intent-source'
import type { CommandDescriptor } from '../../../types'
export type SubmitPlanHistoryKind = 'system' | 'user' | 'progress'
export type SubmitPlanAction =
  | { type: 'set-input'; value: string }
  | { type: 'push-history'; kind: SubmitPlanHistoryKind; content: string }
  | { type: 'add-command-history'; value: string }
  | { type: 'set-last-user-intent'; value: string }
  | { type: 'submit-refinement'; value: string }
  | {
      type: 'run-generation'
      intent?: string
      intentFile?: string
      resume?:
        | { kind: 'history'; selector: string; mode: import('../../../types').ResumeMode }
        | { kind: 'file'; payloadPath: string; mode: import('../../../types').ResumeMode }
    }
  | { type: 'run-new'; argsRaw: string }
  | { type: 'run-reuse' }
  | { type: 'run-command'; commandId: CommandDescriptor['id']; argsRaw: string }
export type ResolveSubmitPlanInput = {
  expandedValue: string
  isAwaitingRefinement: boolean
  popupOpen: boolean
  isCommandMenuActive: boolean
  selectedCommandId: CommandDescriptor['id'] | null
  commandMenuArgsRaw: string
  isCommandMode: boolean
  intentFilePath: string
  isGenerating: boolean
}
const buildCommandHistoryEntry = (commandId: CommandDescriptor['id'], argsRaw: string): string => {
  const trimmedArgs = argsRaw.trim()
  return `/${commandId}${trimmedArgs ? ` ${trimmedArgs}` : ''}`
}
export const resolveSubmitPlan = (input: ResolveSubmitPlanInput): SubmitPlanAction[] => {
  const actions: SubmitPlanAction[] = []
  if (input.isAwaitingRefinement) {
    actions.push({ type: 'submit-refinement', value: input.expandedValue })
    actions.push({ type: 'set-input', value: '' })
    return actions
  }
  if (input.popupOpen) {
    return actions
  }
  if (input.isCommandMenuActive) {
    if (input.selectedCommandId) {
      actions.push({
        type: 'add-command-history',
        value: buildCommandHistoryEntry(input.selectedCommandId, input.commandMenuArgsRaw),
      })
      if (input.selectedCommandId === 'new') {
        actions.push({ type: 'run-new', argsRaw: input.commandMenuArgsRaw })
      } else if (input.selectedCommandId === 'reuse') {
        actions.push({ type: 'run-reuse' })
      } else {
        actions.push({
          type: 'run-command',
          commandId: input.selectedCommandId,
          argsRaw: input.commandMenuArgsRaw,
        })
      }
    }
    actions.push({ type: 'set-input', value: '' })
    return actions
  }
  if (input.isCommandMode) {
    actions.push({ type: 'set-input', value: '' })
    return actions
  }
  const trimmed = input.expandedValue.trim()
  const intentSource: IntentSourceSelection = resolveIntentSource(trimmed, input.intentFilePath)
  if (intentSource.kind === 'empty') {
    actions.push({ type: 'set-input', value: '' })
    return actions
  }
  if (input.isGenerating) {
    actions.push({
      type: 'push-history',
      kind: 'system',
      content: 'Generation already running. Please wait.',
    })
    actions.push({ type: 'set-input', value: '' })
    return actions
  }
  if (intentSource.kind === 'file') {
    actions.push({
      type: 'push-history',
      kind: 'user',
      content: `> [intent file] ${intentSource.intentFile}`,
    })
    if (trimmed.length > 0) {
      actions.push({
        type: 'push-history',
        kind: 'system',
        content: 'Typed intent ignored because an intent file is active.',
      })
    }
    actions.push({ type: 'set-input', value: '' })
    actions.push({ type: 'run-generation', intentFile: intentSource.intentFile })
    return actions
  }
  actions.push({ type: 'add-command-history', value: intentSource.intent })
  actions.push({ type: 'push-history', kind: 'user', content: `> ${intentSource.intent}` })
  actions.push({ type: 'set-last-user-intent', value: intentSource.intent })
  actions.push({ type: 'set-input', value: '' })
  actions.push({ type: 'run-generation', intent: intentSource.intent })
  return actions
}
</file>

<file path="src/tui/screens/command/utils/url-args.ts">
export const parseUrlArgs = (raw: string): string[] => {
  return raw
    .split(/[\s,]+/g)
    .map((value) => value.trim())
    .filter((value) => value.length > 0)
}
export type UrlValidationResult =
  | { ok: true }
  | {
      ok: false
      message: string
    }
export const validateHttpUrlCandidate = (candidate: string): UrlValidationResult => {
  const trimmed = candidate.trim()
  if (!trimmed) {
    return { ok: false, message: 'URL is empty.' }
  }
  let parsed: URL
  try {
    parsed = new URL(trimmed)
  } catch {
    return { ok: false, message: `Invalid URL: ${trimmed}` }
  }
  if (!/^https?:$/.test(parsed.protocol)) {
    return {
      ok: false,
      message: `Unsupported URL protocol for ${trimmed}. Only HTTP(S) URLs are allowed.`,
    }
  }
  return { ok: true }
}
</file>

<file path="src/tui/screens/command/command-screen-reducer.ts">
/*
 * Command screen reducer (pure state transitions).
 *
 * This is the “screen model” for the main Ink TUI screen.
 *
 * Reducers in plain terms:
 * - Instead of calling many `setState(...)` functions from all over the file,
 *   we send small “actions” describing what happened.
 * - The reducer is a pure function that turns (previousState + action) into
 *   nextState.
 *
 * Why this helps:
 * - Keeps state transitions explicit and testable.
 * - Lets us update multiple related fields in one render.
 * - Reduces accidental bugs where different `setState` calls race.
 *
 * This file is intentionally pure: no React/Ink imports.
 */
// Lightweight replacement for React's SetStateAction.
export type SetStateAction<State> = State | ((prev: State) => State)
export type CommandScreenState = {
  terminalRows: number
  terminalColumns: number
  inputValue: string
  isPasteActive: boolean
  commandSelectionIndex: number
  debugKeyLine: string | null
  // Command screen UI options (single source of truth).
  intentFilePath: string
  copyEnabled: boolean
  chatGptEnabled: boolean
  jsonOutputEnabled: boolean
}
export type CommandScreenAction =
  | { type: 'set-terminal-size'; rows: number; columns: number }
  | { type: 'set-input'; next: SetStateAction<string> }
  | { type: 'set-paste-active'; isPasteActive: boolean }
  | { type: 'set-command-selection'; next: SetStateAction<number> }
  | { type: 'set-debug-line'; line: string | null }
  | { type: 'set-intent-file-path'; next: SetStateAction<string> }
  | { type: 'set-copy-enabled'; next: SetStateAction<boolean> }
  | { type: 'set-chatgpt-enabled'; next: SetStateAction<boolean> }
  | { type: 'set-json-output-enabled'; next: SetStateAction<boolean> }
export const createInitialCommandScreenState = (options: {
  terminalRows: number
  terminalColumns: number
}): CommandScreenState => ({
  terminalRows: options.terminalRows,
  terminalColumns: options.terminalColumns,
  inputValue: '',
  isPasteActive: false,
  commandSelectionIndex: 0,
  debugKeyLine: null,
  intentFilePath: '',
  copyEnabled: true,
  chatGptEnabled: false,
  jsonOutputEnabled: false,
})
export const commandScreenReducer = (
  state: CommandScreenState,
  action: CommandScreenAction,
): CommandScreenState => {
  switch (action.type) {
    case 'set-terminal-size':
      // Terminal resize events can be noisy; avoid needless rerenders.
      if (state.terminalRows === action.rows && state.terminalColumns === action.columns) {
        return state
      }
      return { ...state, terminalRows: action.rows, terminalColumns: action.columns }
    case 'set-input': {
      const nextValue =
        typeof action.next === 'function' ? action.next(state.inputValue) : action.next
      return nextValue === state.inputValue ? state : { ...state, inputValue: nextValue }
    }
    case 'set-paste-active':
      return action.isPasteActive === state.isPasteActive
        ? state
        : { ...state, isPasteActive: action.isPasteActive }
    case 'set-command-selection': {
      const nextIndex =
        typeof action.next === 'function' ? action.next(state.commandSelectionIndex) : action.next
      return nextIndex === state.commandSelectionIndex
        ? state
        : { ...state, commandSelectionIndex: nextIndex }
    }
    case 'set-debug-line':
      return action.line === state.debugKeyLine ? state : { ...state, debugKeyLine: action.line }
    case 'set-intent-file-path': {
      const nextValue =
        typeof action.next === 'function' ? action.next(state.intentFilePath) : action.next
      return nextValue === state.intentFilePath ? state : { ...state, intentFilePath: nextValue }
    }
    case 'set-copy-enabled': {
      const nextValue =
        typeof action.next === 'function' ? action.next(state.copyEnabled) : action.next
      return nextValue === state.copyEnabled ? state : { ...state, copyEnabled: nextValue }
    }
    case 'set-chatgpt-enabled': {
      const nextValue =
        typeof action.next === 'function' ? action.next(state.chatGptEnabled) : action.next
      return nextValue === state.chatGptEnabled ? state : { ...state, chatGptEnabled: nextValue }
    }
    case 'set-json-output-enabled': {
      const nextValue =
        typeof action.next === 'function' ? action.next(state.jsonOutputEnabled) : action.next
      return nextValue === state.jsonOutputEnabled
        ? state
        : { ...state, jsonOutputEnabled: nextValue }
    }
    default:
      return state
  }
}
</file>

<file path="src/tui/screens/command/useCommandScreen.ts">
/*
 * useCommandScreen
 *
 * This hook owns the CommandScreen "screen model" state.
 *
 * Incremental refactor note:
 * - The legacy `CommandScreen.tsx` had a lot of scattered `useState` calls.
 * - We’re starting by moving the highest-churn UI state into a reducer.
 * - The rest of the orchestration logic can migrate here over time.
 */
import { useCallback, useMemo, useReducer } from 'react'
import { useStdout } from 'ink'
import {
  commandScreenReducer,
  createInitialCommandScreenState,
  type CommandScreenState,
  type SetStateAction,
} from './command-screen-reducer'
export type UseCommandScreenResult = {
  state: CommandScreenState
  setTerminalSize: (rows: number, columns: number) => void
  setInputValue: (next: SetStateAction<string>) => void
  setPasteActive: (isPasteActive: boolean) => void
  setCommandSelectionIndex: (next: SetStateAction<number>) => void
  setDebugKeyLine: (line: string | null) => void
  setIntentFilePath: (next: SetStateAction<string>) => void
  setCopyEnabled: (next: SetStateAction<boolean>) => void
  setChatGptEnabled: (next: SetStateAction<boolean>) => void
  setJsonOutputEnabled: (next: SetStateAction<boolean>) => void
}
export const useCommandScreen = (): UseCommandScreenResult => {
  const { stdout } = useStdout()
  const initialState = useMemo(
    () =>
      createInitialCommandScreenState({
        terminalRows: stdout?.rows ?? 24,
        terminalColumns: stdout?.columns ?? 80,
      }),
    [stdout?.columns, stdout?.rows],
  )
  const [state, dispatch] = useReducer(commandScreenReducer, initialState)
  const setTerminalSize = useCallback((rows: number, columns: number) => {
    dispatch({ type: 'set-terminal-size', rows, columns })
  }, [])
  const setInputValue = useCallback((next: SetStateAction<string>) => {
    dispatch({ type: 'set-input', next })
  }, [])
  const setPasteActive = useCallback((isPasteActive: boolean) => {
    dispatch({ type: 'set-paste-active', isPasteActive })
  }, [])
  const setCommandSelectionIndex = useCallback((next: SetStateAction<number>) => {
    dispatch({ type: 'set-command-selection', next })
  }, [])
  const setDebugKeyLine = useCallback((line: string | null) => {
    dispatch({ type: 'set-debug-line', line })
  }, [])
  const setIntentFilePath = useCallback((next: SetStateAction<string>) => {
    dispatch({ type: 'set-intent-file-path', next })
  }, [])
  const setCopyEnabled = useCallback((next: SetStateAction<boolean>) => {
    dispatch({ type: 'set-copy-enabled', next })
  }, [])
  const setChatGptEnabled = useCallback((next: SetStateAction<boolean>) => {
    dispatch({ type: 'set-chatgpt-enabled', next })
  }, [])
  const setJsonOutputEnabled = useCallback((next: SetStateAction<boolean>) => {
    dispatch({ type: 'set-json-output-enabled', next })
  }, [])
  return {
    state,
    setTerminalSize,
    setInputValue,
    setPasteActive,
    setCommandSelectionIndex,
    setDebugKeyLine,
    setIntentFilePath,
    setCopyEnabled,
    setChatGptEnabled,
    setJsonOutputEnabled,
  }
}
</file>

<file path="src/tui/theme/theme-adapter.ts">
const isRecord = (value: unknown): value is Record<string, unknown> =>
  typeof value === 'object' && value !== null
export const adaptOpencodeThemeJson = (value: unknown): unknown | null => {
  if (!isRecord(value)) {
    return null
  }
  const themeRaw = value.theme
  if (!isRecord(themeRaw)) {
    return null
  }
  const looksLikeOpencode =
    'textMuted' in themeRaw ||
    'backgroundPanel' in themeRaw ||
    'backgroundElement' in themeRaw ||
    'primary' in themeRaw
  if (!looksLikeOpencode) {
    return null
  }
  const theme: Record<string, unknown> = { ...themeRaw }
  if (theme.mutedText === undefined && themeRaw.textMuted !== undefined) {
    theme.mutedText = 'textMuted'
  }
  if (theme.panelBackground === undefined && themeRaw.backgroundPanel !== undefined) {
    theme.panelBackground = 'backgroundPanel'
  }
  if (theme.popupBackground === undefined && theme.panelBackground !== undefined) {
    theme.popupBackground = 'panelBackground'
  }
  if (theme.accent === undefined && themeRaw.primary !== undefined) {
    theme.accent = 'primary'
  }
  if (theme.accentText === undefined && themeRaw.background !== undefined) {
    theme.accentText = 'background'
  }
  if (theme.selectionBackground === undefined) {
    if (themeRaw.backgroundElement !== undefined) {
      theme.selectionBackground = 'backgroundElement'
    } else if (themeRaw.backgroundPanel !== undefined) {
      theme.selectionBackground = 'backgroundPanel'
    }
  }
  if (theme.selectionText === undefined && themeRaw.text !== undefined) {
    theme.selectionText = 'text'
  }
  if (theme.chipBackground === undefined) {
    if (themeRaw.backgroundElement !== undefined) {
      theme.chipBackground = 'backgroundElement'
    } else if (themeRaw.backgroundPanel !== undefined) {
      theme.chipBackground = 'backgroundPanel'
    }
  }
  if (theme.chipText === undefined && themeRaw.text !== undefined) {
    theme.chipText = 'text'
  }
  if (theme.chipMutedText === undefined) {
    if (themeRaw.textMuted !== undefined) {
      theme.chipMutedText = 'textMuted'
    } else if (themeRaw.text !== undefined) {
      theme.chipMutedText = 'text'
    }
  }
  const adapted: Record<string, unknown> = { theme }
  if ('defs' in value) {
    adapted.defs = value.defs
  }
  return adapted
}
</file>

<file path="src/tui/theme/theme-discovery.ts">
import fs from 'node:fs/promises'
import type { Dirent } from 'node:fs'
import path from 'node:path'
export type ThemeDiscoveryError = {
  kind: 'read'
  filePath: string
  message: string
}
const pathExists = async (targetPath: string): Promise<boolean> => {
  try {
    await fs.stat(targetPath)
    return true
  } catch {
    return false
  }
}
export const defaultGlobalThemesDir = (homedir: string): string =>
  path.join(homedir, '.config', 'prompt-maker-cli', 'themes')
export const listThemeJsonFiles = async (
  themesDir: string,
): Promise<{ files: string[]; errors: ThemeDiscoveryError[] }> => {
  const errors: ThemeDiscoveryError[] = []
  if (!(await pathExists(themesDir))) {
    return { files: [], errors }
  }
  let entries: Dirent[]
  try {
    entries = await fs.readdir(themesDir, { withFileTypes: true })
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown directory read error.'
    errors.push({ kind: 'read', filePath: themesDir, message })
    return { files: [], errors }
  }
  const files = entries
    .filter((entry) => entry.isFile() && entry.name.endsWith('.json'))
    .map((entry) => path.join(themesDir, entry.name))
    .sort((a, b) => path.basename(a).localeCompare(path.basename(b)))
  return { files, errors }
}
export const discoverProjectThemesDirs = async (
  cwd: string,
  stopAt?: string | undefined,
): Promise<string[]> => {
  const discovered: string[] = []
  const resolvedStopAt = stopAt ? path.resolve(stopAt) : null
  let current: string | null = path.resolve(cwd)
  while (current !== null) {
    const candidate = path.join(current, '.prompt-maker-cli', 'themes')
    if (await pathExists(candidate)) {
      discovered.push(candidate)
    }
    if (resolvedStopAt && current === resolvedStopAt) {
      current = null
      continue
    }
    const parent = path.dirname(current)
    if (parent === current) {
      current = null
      continue
    }
    current = parent
  }
  return discovered
}
</file>

<file path="src/tui/theme/theme-loader.ts">
import os from 'node:os'
import path from 'node:path'
import { adaptOpencodeThemeJson } from './theme-adapter'
import {
  defaultGlobalThemesDir,
  discoverProjectThemesDirs,
  listThemeJsonFiles,
} from './theme-discovery'
import { readJsonFile } from './theme-parse'
import { listThemes, type ThemeRegistryEntry } from './theme-registry'
import type { ThemeJson } from './theme-types'
import { validateThemeJson } from './theme-validate'
export type ThemeSource = 'builtin' | 'global' | 'project'
export type ThemeDescriptor = {
  name: string
  label: string
  source: ThemeSource
  theme: ThemeJson
  filePath?: string
}
export type ThemeLoadErrorKind = 'read' | 'parse' | 'validate'
export type ThemeLoadError = {
  kind: ThemeLoadErrorKind
  filePath: string
  message: string
}
export type LoadThemesOptions = {
  cwd?: string
  stopAt?: string | undefined
  homedir?: string
  globalThemesDir?: string
}
type ThemeCandidate = {
  name: string
  theme: ThemeJson
}
const loadThemeCandidateFromFile = async (
  filePath: string,
): Promise<{ ok: true; candidate: ThemeCandidate } | { ok: false; error: ThemeLoadError }> => {
  const name = path.basename(filePath, '.json')
  let parsed: unknown
  try {
    parsed = await readJsonFile(filePath)
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown theme file read error.'
    const kind: ThemeLoadErrorKind = error instanceof SyntaxError ? 'parse' : 'read'
    return { ok: false, error: { kind, filePath, message } }
  }
  const validated = validateThemeJson(parsed)
  if (validated.ok) {
    return { ok: true, candidate: { name, theme: validated.theme } }
  }
  const adapted = adaptOpencodeThemeJson(parsed)
  if (adapted) {
    const adaptedValidated = validateThemeJson(adapted)
    if (adaptedValidated.ok) {
      return { ok: true, candidate: { name, theme: adaptedValidated.theme } }
    }
    return {
      ok: false,
      error: {
        kind: 'validate',
        filePath,
        message: `Theme JSON invalid (after adapting opencode schema): ${adaptedValidated.message}`,
      },
    }
  }
  return { ok: false, error: { kind: 'validate', filePath, message: validated.message } }
}
const toBuiltinDescriptor = (entry: ThemeRegistryEntry): ThemeDescriptor => ({
  name: entry.name,
  label: entry.label,
  source: 'builtin',
  theme: entry.theme,
})
const toCustomDescriptor = (
  candidate: ThemeCandidate,
  source: Exclude<ThemeSource, 'builtin'>,
  filePath: string,
  existing?: ThemeDescriptor,
): ThemeDescriptor => ({
  name: candidate.name,
  label: existing?.label ?? candidate.name,
  source,
  theme: candidate.theme,
  filePath,
})
export const loadThemes = async (
  options: LoadThemesOptions = {},
): Promise<{ themes: ThemeDescriptor[]; errors: ThemeLoadError[] }> => {
  const cwd = options.cwd ?? process.cwd()
  const homedir = options.homedir ?? os.homedir()
  const globalThemesDir = options.globalThemesDir ?? defaultGlobalThemesDir(homedir)
  const builtins = listThemes()
  const order: string[] = builtins.map((theme) => theme.name)
  const extras = new Set<string>()
  const themesByName = new Map<string, ThemeDescriptor>()
  for (const entry of builtins) {
    themesByName.set(entry.name, toBuiltinDescriptor(entry))
  }
  const errors: ThemeLoadError[] = []
  const applyThemeFiles = async (
    source: Exclude<ThemeSource, 'builtin'>,
    themesDir: string,
  ): Promise<void> => {
    const listing = await listThemeJsonFiles(themesDir)
    errors.push(...listing.errors)
    for (const filePath of listing.files) {
      const loaded = await loadThemeCandidateFromFile(filePath)
      if (!loaded.ok) {
        errors.push(loaded.error)
        continue
      }
      const existing = themesByName.get(loaded.candidate.name)
      themesByName.set(
        loaded.candidate.name,
        toCustomDescriptor(loaded.candidate, source, filePath, existing),
      )
      if (!order.includes(loaded.candidate.name)) {
        extras.add(loaded.candidate.name)
      }
    }
  }
  await applyThemeFiles('global', globalThemesDir)
  const projectDirs = await discoverProjectThemesDirs(cwd, options.stopAt)
  const orderedProjectDirs = [...projectDirs].reverse()
  for (const dir of orderedProjectDirs) {
    await applyThemeFiles('project', dir)
  }
  const extrasSorted = Array.from(extras).sort((a, b) => a.localeCompare(b))
  const finalNames = [...order, ...extrasSorted]
  return {
    themes: finalNames
      .map((name) => themesByName.get(name))
      .filter((theme): theme is ThemeDescriptor => Boolean(theme)),
    errors,
  }
}
</file>

<file path="src/tui/theme/theme-parse.ts">
import fs from 'node:fs/promises'
export const readJsonFile = async (filePath: string): Promise<unknown> => {
  const raw = await fs.readFile(filePath, 'utf8')
  return JSON.parse(raw) as unknown
}
</file>

<file path="src/tui/theme/theme-resolver.ts">
import {
  MARKDOWN_THEME_SLOTS,
  REQUIRED_THEME_SLOTS,
  type InkColorValue,
  type RequiredThemeSlot,
  type ResolvedTheme,
  type ThemeAppearanceMode,
  type ThemeColorValue,
  type ThemeJson,
  type ThemeSlot,
} from './theme-types'
export type ResolveColorContext = {
  mode: ThemeAppearanceMode
  defs: Record<string, ThemeColorValue>
  theme: Record<string, ThemeColorValue>
}
type ResolutionState = {
  stack: string[]
  cache: Map<string, InkColorValue>
}
const HEX_6_RE = /^#[0-9a-fA-F]{6}$/
const HEX_8_RE = /^#[0-9a-fA-F]{8}$/
const ANSI_16_HEX: readonly string[] = [
  '#000000',
  '#800000',
  '#008000',
  '#808000',
  '#000080',
  '#800080',
  '#008080',
  '#c0c0c0',
  '#808080',
  '#ff0000',
  '#00ff00',
  '#ffff00',
  '#0000ff',
  '#ff00ff',
  '#00ffff',
  '#ffffff',
]
const ANSI_CUBE_LEVELS: readonly number[] = [0, 95, 135, 175, 215, 255]
const toHexByte = (value: number): string => value.toString(16).padStart(2, '0')
const rgbToHex = (r: number, g: number, b: number): string =>
  `#${toHexByte(r)}${toHexByte(g)}${toHexByte(b)}`
export const ansiToHex = (code: number): string => {
  if (!Number.isInteger(code) || code < 0 || code > 255) {
    throw new Error(`ANSI color must be an integer 0..255 (received: ${String(code)})`)
  }
  if (code < 16) {
    const value = ANSI_16_HEX[code]
    if (!value) {
      throw new Error(`Internal ANSI mapping missing for code ${code}`)
    }
    return value
  }
  if (code < 232) {
    const index = code - 16
    const rIndex = Math.floor(index / 36)
    const gIndex = Math.floor((index % 36) / 6)
    const bIndex = index % 6
    const r = ANSI_CUBE_LEVELS[rIndex]
    const g = ANSI_CUBE_LEVELS[gIndex]
    const b = ANSI_CUBE_LEVELS[bIndex]
    if (r === undefined || g === undefined || b === undefined) {
      throw new Error(`Internal ANSI color cube mapping failed for code ${code}`)
    }
    return rgbToHex(r, g, b)
  }
  const gray = 8 + 10 * (code - 232)
  return rgbToHex(gray, gray, gray)
}
const normalizeSpecial = (value: string): InkColorValue | null => {
  const lowered = value.toLowerCase()
  if (lowered === 'none' || lowered === 'transparent') {
    return undefined
  }
  return null
}
const normalizeHex = (value: string): InkColorValue | null => {
  if (HEX_6_RE.test(value)) {
    return value.toLowerCase()
  }
  if (HEX_8_RE.test(value)) {
    const rgb = value.slice(0, 7).toLowerCase()
    const alpha = value.slice(7, 9).toLowerCase()
    return alpha === '00' ? undefined : rgb
  }
  return null
}
const isVariant = (
  value: ThemeColorValue,
): value is { dark: ThemeColorValue; light: ThemeColorValue } =>
  typeof value === 'object' && value !== null && 'dark' in value && 'light' in value
const resolveNamed = (
  key: string,
  raw: ThemeColorValue,
  ctx: ResolveColorContext,
  state: ResolutionState,
) => {
  const cached = state.cache.get(key)
  if (cached !== undefined || state.cache.has(key)) {
    return cached
  }
  const cycleStart = state.stack.indexOf(key)
  if (cycleStart >= 0) {
    const cyclePath = [...state.stack.slice(cycleStart), key].join(' -> ')
    throw new Error(`Theme reference cycle detected: ${cyclePath}`)
  }
  state.stack.push(key)
  const resolved = resolveColorInternal(raw, ctx, state)
  state.stack.pop()
  state.cache.set(key, resolved)
  return resolved
}
const resolveReference = (
  name: string,
  ctx: ResolveColorContext,
  state: ResolutionState,
): InkColorValue => {
  const def = ctx.defs[name]
  if (def !== undefined) {
    return resolveNamed(`defs.${name}`, def, ctx, state)
  }
  const themeValue = ctx.theme[name]
  if (themeValue !== undefined) {
    return resolveNamed(`theme.${name}`, themeValue, ctx, state)
  }
  throw new Error(`Unknown theme color reference: ${name}`)
}
const resolveColorInternal = (
  value: ThemeColorValue,
  ctx: ResolveColorContext,
  state: ResolutionState,
): InkColorValue => {
  if (typeof value === 'number') {
    return ansiToHex(value)
  }
  if (typeof value === 'string') {
    const special = normalizeSpecial(value)
    if (special !== null) {
      return special
    }
    const hex = normalizeHex(value)
    if (hex !== null) {
      return hex
    }
    return resolveReference(value, ctx, state)
  }
  if (isVariant(value)) {
    const selected = value[ctx.mode]
    return resolveColorInternal(selected, ctx, state)
  }
  const exhaustive: never = value
  return exhaustive
}
export const resolveColor = (value: ThemeColorValue, ctx: ResolveColorContext): InkColorValue => {
  return resolveColorInternal(value, ctx, { stack: [], cache: new Map() })
}
const resolveRequiredSlot = (
  slot: RequiredThemeSlot,
  ctx: ResolveColorContext,
  state: ResolutionState,
): InkColorValue => {
  const raw = ctx.theme[slot]
  if (raw === undefined) {
    throw new Error(`Theme is missing required slot: ${slot}`)
  }
  return resolveNamed(`theme.${slot}`, raw, ctx, state)
}
export const resolveTheme = (themeJson: ThemeJson, mode: ThemeAppearanceMode): ResolvedTheme => {
  const missing = REQUIRED_THEME_SLOTS.filter((slot) => themeJson.theme[slot] === undefined)
  if (missing.length > 0) {
    throw new Error(`Theme is missing required slots: ${missing.join(', ')}`)
  }
  const ctx: ResolveColorContext = {
    mode,
    defs: themeJson.defs ?? {},
    theme: themeJson.theme,
  }
  const state: ResolutionState = { stack: [], cache: new Map() }
  const resolved: Partial<ResolvedTheme> = {}
  for (const slot of REQUIRED_THEME_SLOTS) {
    resolved[slot] = resolveRequiredSlot(slot, ctx, state)
  }
  for (const slot of MARKDOWN_THEME_SLOTS) {
    const raw = ctx.theme[slot]
    if (raw !== undefined) {
      resolved[slot] = resolveNamed(`theme.${slot}`, raw, ctx, state)
    }
  }
  return resolved as ResolvedTheme
}
</file>

<file path="src/tui/theme/theme-types.ts">
// Theme types for the Ink TUI.
//
// This module intentionally has *no* imports from `ink` (or any components).
// It defines the data model only; resolution/loading happens elsewhere.
/**
 * User-facing mode preference.
 *
 * We use `system` (not `auto`) to mirror the upstream OpenCode TUI concept:
 * "system" resolves to the terminal's current background/appearance.
 */
export type ThemeMode = 'light' | 'dark' | 'system'
/**
 * Ink-compatible color value.
 *
 * Ink accepts named colors (e.g. `cyanBright`) and can also accept 0–255 ANSI codes.
 * We keep this loose for now; later we can narrow based on the resolver we add.
 */
export type InkColorValue = string | number | undefined
export const asInkColor = (value: InkColorValue): string | undefined =>
  value === undefined ? undefined : (value as unknown as string)
export const inkColorProps = (value: InkColorValue): { color?: string } => {
  const resolved = asInkColor(value)
  return resolved === undefined ? {} : { color: resolved }
}
export const inkBackgroundColorProps = (value: InkColorValue): { backgroundColor?: string } => {
  const resolved = asInkColor(value)
  return resolved === undefined ? {} : { backgroundColor: resolved }
}
export const inkBorderColorProps = (value: InkColorValue): { borderColor?: string } => {
  const resolved = asInkColor(value)
  return resolved === undefined ? {} : { borderColor: resolved }
}
export type RequiredThemeSlot =
  | 'background'
  | 'text'
  | 'mutedText'
  | 'border'
  | 'accent'
  | 'accentText'
  | 'warning'
  | 'error'
  | 'success'
  | 'panelBackground'
  | 'popupBackground'
  | 'selectionBackground'
  | 'selectionText'
  | 'chipBackground'
  | 'chipText'
  | 'chipMutedText'
export type MarkdownThemeSlot =
  | 'markdownText'
  | 'markdownHeading'
  | 'markdownLink'
  | 'markdownLinkText'
  | 'markdownCode'
  | 'markdownBlockQuote'
  | 'markdownEmph'
  | 'markdownStrong'
  | 'markdownHorizontalRule'
  | 'markdownListItem'
  | 'markdownListEnumeration'
  | 'markdownImage'
  | 'markdownImageText'
  | 'markdownCodeBlock'
export type ThemeSlot = RequiredThemeSlot | MarkdownThemeSlot
export const REQUIRED_THEME_SLOTS: readonly RequiredThemeSlot[] = [
  'background',
  'text',
  'mutedText',
  'border',
  'accent',
  'accentText',
  'warning',
  'error',
  'success',
  'panelBackground',
  'popupBackground',
  'selectionBackground',
  'selectionText',
  'chipBackground',
  'chipText',
  'chipMutedText',
]
export const MARKDOWN_THEME_SLOTS: readonly MarkdownThemeSlot[] = [
  'markdownText',
  'markdownHeading',
  'markdownLink',
  'markdownLinkText',
  'markdownCode',
  'markdownBlockQuote',
  'markdownEmph',
  'markdownStrong',
  'markdownHorizontalRule',
  'markdownListItem',
  'markdownListEnumeration',
  'markdownImage',
  'markdownImageText',
  'markdownCodeBlock',
]
export type ResolvedTheme = Record<RequiredThemeSlot, InkColorValue> &
  Partial<Record<MarkdownThemeSlot, InkColorValue>>
// Concrete appearance mode after resolving `system`.
export type ThemeAppearanceMode = Exclude<ThemeMode, 'system'>
// Theme JSON model (structure only): modeled after OpenCode's TUI themes.
export type ThemeColorVariant = {
  dark: ThemeColorValue
  light: ThemeColorValue
}
/**
 * A single color value in theme JSON.
 *
 * Mirrors OpenCode behavior:
 * - hex strings ("#RRGGBB")
 * - reference strings (defs key or another theme slot key)
 * - ANSI code numbers (0–255)
 * - variants `{ dark, light }`
 * - special strings like "none" / "transparent" (resolver handles meaning)
 */
export type ThemeColorValue = string | number | ThemeColorVariant
export type ThemeJson<TSlots extends string = ThemeSlot> = {
  defs?: Record<string, ThemeColorValue>
  theme: Partial<Record<TSlots, ThemeColorValue>> & Record<string, ThemeColorValue>
}
</file>

<file path="src/tui/app-container-keymap.ts">
import type { Key } from 'ink'
export type AppContainerView = 'generate' | 'tests'
export type AppContainerKeyAction =
  | { type: 'none' }
  | { type: 'exit' }
  | { type: 'toggle-help'; nextIsHelpOpen: boolean }
  | { type: 'open-command-palette' }
  | { type: 'switch-to-tests' }
  | { type: 'switch-to-generate-and-open-command-palette' }
const toControlCharacter = (letter: string): string | null => {
  if (!letter) {
    return null
  }
  const normalized = letter.toLowerCase()
  const code = normalized.charCodeAt(0)
  if (code < 97 || code > 122) {
    return null
  }
  return String.fromCharCode(code - 96)
}
const matchesControlKey = (input: string, key: Key, target: string): boolean => {
  if (!target || !input) {
    return false
  }
  if (key.ctrl && input.toLowerCase() === target.toLowerCase()) {
    return true
  }
  const controlChar = toControlCharacter(target)
  return controlChar ? input === controlChar : false
}
export type ResolveAppContainerKeyActionOptions = {
  input: string
  key: Key
  view: AppContainerView
  isPopupOpen: boolean
  isHelpOpen: boolean
}
export const resolveAppContainerKeyAction = ({
  input,
  key,
  view,
  isPopupOpen,
  isHelpOpen,
}: ResolveAppContainerKeyActionOptions): AppContainerKeyAction => {
  if (isHelpOpen) {
    if (key.escape) {
      return { type: 'toggle-help', nextIsHelpOpen: false }
    }
    return { type: 'none' }
  }
  if (matchesControlKey(input, key, 'c')) {
    return { type: 'exit' }
  }
  if (key.escape) {
    return { type: 'none' }
  }
  if (view === 'generate' && isPopupOpen) {
    if (matchesControlKey(input, key, 'g') || matchesControlKey(input, key, 't')) {
      return { type: 'none' }
    }
  }
  if (matchesControlKey(input, key, 'g')) {
    return view === 'generate'
      ? { type: 'open-command-palette' }
      : { type: 'switch-to-generate-and-open-command-palette' }
  }
  if (matchesControlKey(input, key, 't')) {
    return { type: 'switch-to-tests' }
  }
  return { type: 'none' }
}
</file>

<file path="src/tui/AppContainer.tsx">
import React, { useCallback, useEffect, useMemo, useRef, useState } from 'react'
import { Box, Text, useApp, useInput, useStdout } from 'ink'
import cliCursor from 'cli-cursor'
import { CommandScreen, type CommandScreenHandle } from './CommandScreen'
import { TestRunnerScreen, type TestRunnerScreenHandle } from './TestRunnerScreen'
import { ContextProvider } from './context'
import { ToastProvider, useNotifier } from './notifier'
import { ThemeProvider, useTheme } from './theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from './theme/theme-types'
import { HelpOverlay } from './components/core/HelpOverlay'
import { ToastOverlay } from './components/core/ToastOverlay'
import { COMMAND_DESCRIPTORS } from './config'
import { createHelpSections, estimateHelpOverlayHeight } from './help-config'
import { resolveAppContainerKeyAction } from './app-container-keymap'
export type AppContainerProps = {
  interactiveTransport?: string | undefined
}
const AppContainerInner: React.FC<AppContainerProps> = ({ interactiveTransport }) => {
  const { theme } = useTheme()
  const { exit } = useApp()
  const { stdout } = useStdout()
  const [view, setView] = useState<'generate' | 'tests'>('generate')
  const [isPopupOpen, setIsPopupOpen] = useState(false)
  const [isHelpOpen, setIsHelpOpen] = useState(false)
  const [pendingCommandMenu, setPendingCommandMenu] = useState(false)
  const [commandMenuSignal, setCommandMenuSignal] = useState(0)
  const { notify } = useNotifier({ autoDismissMs: 2200 })
  const commandScreenRef = useRef<CommandScreenHandle | null>(null)
  const testRunnerRef = useRef<TestRunnerScreenHandle | null>(null)
  useEffect(() => {
    if (!process.stdout.isTTY) {
      return
    }
    cliCursor.hide()
    return () => {
      cliCursor.show()
    }
  }, [])
  useEffect(() => {
    if (view !== 'generate') {
      setIsPopupOpen(false)
    }
  }, [view])
  useEffect(() => {
    if (view === 'generate' && pendingCommandMenu) {
      commandScreenRef.current?.suppressNextInput()
      setCommandMenuSignal((prev) => prev + 1)
      setPendingCommandMenu(false)
      return
    }
    if (view !== 'generate' && pendingCommandMenu) {
      setPendingCommandMenu(false)
    }
  }, [pendingCommandMenu, view])
  const openHelp = useCallback(() => {
    if (isHelpOpen) {
      return
    }
    if (view === 'generate') {
      commandScreenRef.current?.suppressNextInput()
    } else {
      testRunnerRef.current?.suppressNextInput()
    }
    setIsHelpOpen(true)
  }, [isHelpOpen, view])
  useInput((input, key) => {
    const action = resolveAppContainerKeyAction({
      input,
      key,
      view,
      isPopupOpen,
      isHelpOpen,
    })
    if (action.type === 'none') {
      return
    }
    if (action.type === 'toggle-help') {
      if (!isHelpOpen && action.nextIsHelpOpen) {
        if (view === 'generate') {
          commandScreenRef.current?.suppressNextInput()
        } else {
          testRunnerRef.current?.suppressNextInput()
        }
      }
      setIsHelpOpen(action.nextIsHelpOpen)
      return
    }
    if (action.type === 'exit') {
      if (view === 'generate') {
        commandScreenRef.current?.suppressNextInput()
      }
      exit()
      return
    }
    if (action.type === 'open-command-palette') {
      commandScreenRef.current?.suppressNextInput()
      setCommandMenuSignal((prev) => prev + 1)
      return
    }
    if (action.type === 'switch-to-generate-and-open-command-palette') {
      setPendingCommandMenu(true)
      setView('generate')
      return
    }
    if (action.type === 'switch-to-tests') {
      if (view === 'generate') {
        commandScreenRef.current?.suppressNextInput()
      }
      setView('tests')
    }
  })
  const terminalRows = stdout?.rows ?? 24
  const helpMaxHeight = Math.max(10, terminalRows - 6)
  const helpSections = useMemo(
    () => createHelpSections({ commandDescriptors: COMMAND_DESCRIPTORS }),
    [],
  )
  const helpIdealHeight = useMemo(() => estimateHelpOverlayHeight(helpSections), [helpSections])
  const helpOverlayHeight = Math.min(helpIdealHeight, helpMaxHeight)
  const reservedRows = isHelpOpen ? helpOverlayHeight + 1 : 0
  return (
    <ContextProvider>
      <Box
        flexDirection="column"
        paddingX={2}
        paddingY={1}
        height="100%"
        width="100%"
        {...inkBackgroundColorProps(theme.background)}
      >
        <Text {...inkColorProps(theme.accent)}>Prompt Maker · Command Palette Preview</Text>
        <Text {...inkColorProps(theme.mutedText)}>
          Ctrl+G → Command Palette · Ctrl+T → Test Runner · Type /help for help · Ctrl+C or /exit to
          exit.
        </Text>
        <Box flexDirection="column" flexGrow={1} marginTop={1}>
          {view === 'generate' ? (
            <>
              <Text {...inkColorProps(theme.mutedText)}>
                Type intents freely or prefix with /command. Use arrow keys to browse history.
              </Text>
              {interactiveTransport ? (
                <Text {...inkColorProps(theme.mutedText)}>
                  Interactive transport listening on {interactiveTransport}. Remote refinements will
                  appear in history.
                </Text>
              ) : null}
              <Box flexDirection="column" flexGrow={1} marginTop={1}>
                <CommandScreen
                  ref={commandScreenRef}
                  interactiveTransportPath={interactiveTransport}
                  onPopupVisibilityChange={setIsPopupOpen}
                  commandMenuSignal={commandMenuSignal}
                  helpOpen={isHelpOpen}
                  reservedRows={reservedRows}
                  onOpenHelp={openHelp}
                  notify={notify}
                />
              </Box>
            </>
          ) : (
            <>
              <Text {...inkColorProps(theme.mutedText)}>
                Enter a test file and press Enter to run suites.
              </Text>
              <TestRunnerScreen ref={testRunnerRef} helpOpen={isHelpOpen} />
            </>
          )}
        </Box>
        {isHelpOpen ? (
          <Box marginTop={1}>
            <HelpOverlay activeView={view} maxHeight={helpMaxHeight} />
          </Box>
        ) : null}
        <ToastOverlay />
      </Box>
    </ContextProvider>
  )
}
export const AppContainer: React.FC<AppContainerProps> = ({ interactiveTransport }) => (
  <ThemeProvider>
    <ToastProvider>
      <AppContainerInner interactiveTransport={interactiveTransport} />
    </ToastProvider>
  </ThemeProvider>
)
</file>

<file path="src/tui/budget-settings.ts">
import type { ContextOverflowStrategy } from '../config'
export type BudgetSettings = {
  maxContextTokens: number | null
  maxInputTokens: number | null
  contextOverflowStrategy: ContextOverflowStrategy | null
}
export type BudgetSettingsDraft = {
  maxContextTokensDraft: string
  maxInputTokensDraft: string
  contextOverflowStrategyDraft: ContextOverflowStrategy | ''
}
export type BudgetSettingsParseResult =
  | { ok: true; settings: BudgetSettings }
  | { ok: false; errorMessage: string }
const parseOptionalPositiveInteger = (
  raw: string,
  label: string,
): { ok: true; value: number | null } | { ok: false; errorMessage: string } => {
  const trimmed = raw.trim()
  if (!trimmed) {
    return { ok: true, value: null }
  }
  if (!/^[0-9]+$/.test(trimmed)) {
    return { ok: false, errorMessage: `${label} must be a positive integer.` }
  }
  const parsed = Number(trimmed)
  if (!Number.isFinite(parsed) || !Number.isInteger(parsed) || parsed <= 0) {
    return { ok: false, errorMessage: `${label} must be a positive integer.` }
  }
  return { ok: true, value: parsed }
}
export const parseBudgetSettingsDraft = (draft: BudgetSettingsDraft): BudgetSettingsParseResult => {
  const maxContextResult = parseOptionalPositiveInteger(
    draft.maxContextTokensDraft,
    'Max context tokens',
  )
  if (!maxContextResult.ok) {
    return maxContextResult
  }
  const maxInputResult = parseOptionalPositiveInteger(draft.maxInputTokensDraft, 'Max input tokens')
  if (!maxInputResult.ok) {
    return maxInputResult
  }
  const budgetsEnabled = maxContextResult.value !== null || maxInputResult.value !== null
  const effectiveStrategy: ContextOverflowStrategy | null = draft.contextOverflowStrategyDraft
    ? draft.contextOverflowStrategyDraft
    : budgetsEnabled
      ? 'fail'
      : null
  return {
    ok: true,
    settings: {
      maxContextTokens: maxContextResult.value,
      maxInputTokens: maxInputResult.value,
      contextOverflowStrategy: effectiveStrategy,
    },
  }
}
</file>

<file path="src/tui/DEVELOPER_NOTE.md">
# prompt-maker-cli TUI Developer Note

This document explains the _current_ Ink TUI architecture in `src/tui/**` after the refactor series.

Goals of this architecture:

- Keep UI behavior stable while making state flow explicit.
- Reduce avoidable re-renders by stabilizing props/callbacks.
- Make complex state transitions unit-testable (pure reducers).

## Directory structure

### Entry points (thin re-exports)

These files stay intentionally small so “big screens” don’t live in the root `tui/` folder:

- `src/tui/CommandScreen.tsx`
- `src/tui/TestRunnerScreen.tsx`

### Screens

Each screen owns orchestration + rendering, and delegates state transitions to a reducer.

- Command screen
  - `src/tui/screens/command/CommandScreen.tsx`
  - `src/tui/screens/command/useCommandScreen.ts`
  - `src/tui/screens/command/command-screen-reducer.ts`
  - `src/tui/screens/command/components/*`

- Test runner
  - `src/tui/screens/test-runner/TestRunnerScreen.tsx`
  - `src/tui/screens/test-runner/useTestRunnerScreen.ts`
  - `src/tui/screens/test-runner/test-runner-reducer.ts`
  - `src/tui/screens/test-runner/components/*`

### Cross-screen hooks and reducers

These modules are shared “feature hooks” used by screens:

- Popup state machine
  - Pure reducer: `src/tui/popup-reducer.ts`
  - Hook + effects: `src/tui/hooks/usePopupManager.ts`

- Generation pipeline
  - Pure reducer: `src/tui/generation-pipeline-reducer.ts`
  - Hook + effects: `src/tui/hooks/useGenerationPipeline.ts`

### Core UI components

Reusable Ink components live in:

- `src/tui/components/core/*`
- `src/tui/components/popups/*`

Rule of thumb:

- **Core components** should be mostly presentational: props in, render out.
- Side effects (timers, subscriptions, async work) belong in hooks/screens.

## Input routing invariants

Input routing is one of the easiest places for TUIs to regress.

The intended priority order is:

1. **Help overlay**: when help is open, most screen input should be suppressed.
2. **Popup input**: when any popup is open, it should “own” the keyboard.
3. **Screen input**: otherwise, the active screen handles keys.
4. **AppContainer global keys**: keys like exit remain truly global.

Implementation notes:

- `usePopupManager` exposes `popupState` and popup actions; screens should use `popupState !== null` to decide whether screen-level inputs should be active.
- Avoid “fallthrough” key handling where the same key can be interpreted by both a popup and the screen.

## Reducers: responsibilities and patterns

Why reducers are used here (plain terms):

- Instead of calling many different `setState(...)` functions from many callbacks, we send a single “action” describing what happened.
- The reducer is a pure function that returns the next state.

Benefits:

- Easier to reason about state transitions.
- Easier to test (no Ink/TTY required).
- Often fewer renders because multiple fields can change in one dispatch.

Patterns used in this codebase:

- Pure reducers live in `*-reducer.ts` files with no React/Ink imports.
- Hooks (e.g. `usePopupManager`, `useGenerationPipeline`) wrap reducers and are responsible for effects.

### Stale closure (important React/Ink concept)

A “stale closure” happens when a callback captures old values.

Example:

- If a stable `handleStreamEvent` callback closes over an old terminal width, it would keep wrapping output at the wrong width after resize.

Fix pattern used here:

- Keep callbacks stable, but read changing values from refs (e.g. `terminalColumnsRef.current`).

## Performance notes

### Windowing and log/history rendering

- Main history rendering is windowed using `ScrollableOutput` which slices to the visible rows.
- List/windowing primitives live in `src/tui/components/popups/list-window.ts`.
- Test runner logs are capped by `useLogBuffer` (default 20 entries) in `src/tui/useLogBuffer.ts`.

### Where re-renders come from

Typical rerender causes in this TUI:

- Passing newly-created arrays/objects as props (e.g. `statusChips`, filtered lists).
- Recreating callbacks each render.
- Doing heavy formatting work in render paths.

Mitigations used:

- Memoize derived arrays when it actually prevents work.
- Use reducer dispatches to group related state updates.
- Use refs to avoid stale closures when callbacks must stay stable.

### Before vs after (high-level)

Before:

- `CommandScreen.tsx` and popup/generation logic mixed together in large, hard-to-test modules.
- More state “fan out” across many `useState` hooks.
- Async popup suggestion scans relied on implicit “prev.type === …” guards.

After:

- Popups and pipeline have explicit reducers and tests.
- Screens are organized under `screens/*` with presentational subcomponents.
- Async suggestion scans are guarded with scan IDs to avoid stale updates.

Why it matters:

- Less accidental rerender churn during typing.
- Fewer subtle regressions when splitting UI or moving logic.
- Faster onboarding: new contributors can locate “state vs effects vs rendering” more easily.

## How to add a new popup safely

1. Add a new popup union member in `src/tui/types.ts`.
2. Add explicit transitions in `src/tui/popup-reducer.ts`.
3. Add the render branch in `src/tui/screens/command/components/PopupArea.tsx`.
4. Add a reducer unit test in `src/__tests__/tui/popup-reducer.test.ts`.

Keep behavior stable:

- Prefer “open-\*” actions for initial state.
- If the popup loads async suggestions, ensure scan results are only applied when popup type + scan id still match.

## How to add a new screen safely

1. Create `src/tui/screens/<name>/`.
2. Add a pure reducer `*-reducer.ts`.
3. Add a screen hook `use<Name>Screen.ts` to expose a view-model API.
4. Keep the root-level entry file (`src/tui/<Name>Screen.tsx`) as a small re-export.

## Manual verification checklist

These checks are intentionally “human-visible” and catch most regressions quickly:

- Responsiveness while typing quickly in the command input.
- Large history:
  - generate a long output and scroll; ensure no lag.
- Large model list:
  - open model popup and type queries quickly.
- Popup churn:
  - open/close popups repeatedly; ensure no delayed updates after switching.
- Test runner:
  - tab between fields, run tests, confirm statuses + logs update.
</file>

<file path="src/tui/generation-pipeline-reducer.ts">
/*
 * Generation pipeline reducer (pure state transitions).
 *
 * This reducer manages UI-facing generation state for the Ink TUI:
 * - whether we are generating
 * - current status message
 * - whether we're waiting for interactive input/refinement
 * - latest token telemetry
 *
 * Why a reducer?
 * - It’s common to update multiple related fields at once (e.g. set generating +
 *   clear interactive waiting + update the status message).
 * - A single reducer action lets us do that in one render.
 *
 * This file is intentionally pure:
 * - no React/Ink imports
 * - easy to unit test
 */
import type { StreamEventInput } from '../generate/types'
import type { GeneratePipelineResult } from '../generate-command'
export type InteractiveAwaitingMode = 'transport' | 'tty'
export type GenerationTelemetry = GeneratePipelineResult['telemetry']
export type ResumeLoadedDetails = Extract<StreamEventInput, { event: 'resume.loaded' }>
export type ContextOverflowDetails = Extract<StreamEventInput, { event: 'context.overflow' }>
export type GenerationPipelineState = {
  isGenerating: boolean
  statusMessage: string
  isAwaitingRefinement: boolean
  awaitingInteractiveMode: InteractiveAwaitingMode | null
  latestTelemetry: GenerationTelemetry | null
  latestResumeLoaded: ResumeLoadedDetails | null
  latestContextOverflow: ContextOverflowDetails | null
}
export type GenerationPipelineAction =
  | { type: 'generation-start'; statusMessage: string }
  | { type: 'generation-stop'; statusMessage?: string }
  | { type: 'set-status'; statusMessage: string }
  | {
      type: 'set-awaiting-interactive'
      awaitingInteractiveMode: InteractiveAwaitingMode | null
      statusMessage?: string
    }
  | { type: 'set-awaiting-refinement'; isAwaitingRefinement: boolean }
  | { type: 'set-telemetry'; telemetry: GenerationTelemetry | null }
  | { type: 'set-resume-loaded'; details: ResumeLoadedDetails | null }
  | { type: 'set-context-overflow'; details: ContextOverflowDetails | null }
export const INITIAL_GENERATION_PIPELINE_STATE: GenerationPipelineState = {
  isGenerating: false,
  statusMessage: 'Idle',
  isAwaitingRefinement: false,
  awaitingInteractiveMode: null,
  latestTelemetry: null,
  latestResumeLoaded: null,
  latestContextOverflow: null,
}
export const generationPipelineReducer = (
  state: GenerationPipelineState,
  action: GenerationPipelineAction,
): GenerationPipelineState => {
  switch (action.type) {
    case 'generation-start':
      return {
        ...state,
        isGenerating: true,
        statusMessage: action.statusMessage,
        isAwaitingRefinement: false,
        awaitingInteractiveMode: null,
        latestTelemetry: null,
        latestResumeLoaded: null,
        latestContextOverflow: null,
      }
    case 'generation-stop':
      return {
        ...state,
        isGenerating: false,
        // Preserve the existing status message unless explicitly overridden.
        statusMessage: action.statusMessage ?? state.statusMessage,
        isAwaitingRefinement: false,
        awaitingInteractiveMode: null,
      }
    case 'set-status':
      return { ...state, statusMessage: action.statusMessage }
    case 'set-awaiting-interactive':
      return {
        ...state,
        awaitingInteractiveMode: action.awaitingInteractiveMode,
        ...(action.statusMessage ? { statusMessage: action.statusMessage } : {}),
      }
    case 'set-awaiting-refinement':
      return { ...state, isAwaitingRefinement: action.isAwaitingRefinement }
    case 'set-telemetry':
      return { ...state, latestTelemetry: action.telemetry }
    case 'set-resume-loaded':
      return { ...state, latestResumeLoaded: action.details }
    case 'set-context-overflow':
      return { ...state, latestContextOverflow: action.details }
    default:
      return state
  }
}
</file>

<file path="src/tui/index.tsx">
import { render } from 'ink'
import { AppContainer } from './AppContainer'
type TuiOptions = {
  interactiveTransport?: string
}
const parseTuiArgs = (argv: string[]): TuiOptions => {
  const options: TuiOptions = {}
  for (let i = 0; i < argv.length; i += 1) {
    const token = argv[i]
    if (!token) {
      continue
    }
    if (token === '--interactive-transport') {
      const next = argv[i + 1]
      if (next && !next.startsWith('-')) {
        options.interactiveTransport = next
        i += 1
      }
      continue
    }
    if (token.startsWith('--interactive-transport=')) {
      options.interactiveTransport = token.split('=').slice(1).join('=')
    }
  }
  return options
}
const EXIT_CLEAR_SEQUENCE = '\u001b[0m\u001b[2J\u001b[H'
const clearTerminalOnExit = (): void => {
  if (process.stdout.isTTY) {
    process.stdout.write(EXIT_CLEAR_SEQUENCE)
  }
}
export const runTuiCommand = async (argv: string[]): Promise<void> => {
  const options = parseTuiArgs(argv)
  const { waitUntilExit } = render(
    <AppContainer interactiveTransport={options.interactiveTransport} />,
  )
  await waitUntilExit()
  clearTerminalOnExit()
}
</file>

<file path="src/tui/model-options.ts">
import { loadCliConfig, resolveGeminiCredentials, resolveOpenAiCredentials } from '../config'
import type { ModelDefinition } from '../model-providers'
import { inferProviderFromModelId } from '../model-providers'
import { getAvailableModels, getVideoCapableGeminiModels } from '../utils/model-manager'
import type { ModelOption } from './types'
const DEFAULT_MODEL_FALLBACK = 'gpt-4o-mini'
const BUILT_IN_MODEL_DEFINITIONS: ModelDefinition[] = [
  {
    id: 'gpt-4o-mini',
    label: 'GPT-4o Mini',
    provider: 'openai',
    description: 'Fast, general-purpose GPT-4o variant.',
    capabilities: ['fast', 'economical', 'general-purpose'],
    notes: 'Great default for day-to-day prompts.',
    default: true,
  },
  {
    id: 'gpt-4o',
    label: 'GPT-4o',
    provider: 'openai',
    description: 'Flagship GPT-4 omni model.',
    capabilities: ['multimodal', 'high-quality'],
    notes: 'Use when quality matters more than speed.',
  },
  {
    id: 'gpt-4.1',
    label: 'GPT-4.1',
    provider: 'openai',
    description: 'Reasoning-optimized GPT-4 family model.',
    capabilities: ['reasoning', 'multimodal'],
    notes: 'Best for planning and structured tasks.',
  },
  {
    id: 'gpt-5.2',
    label: 'GPT-5.2',
    provider: 'openai',
    description: 'Next-gen GPT model with advanced reasoning.',
    capabilities: ['reasoning', 'multimodal', 'long-context'],
    notes: 'Use when you need top-tier quality and depth.',
  },
  {
    id: 'gemini-1.5-pro',
    label: 'Gemini 1.5 Pro',
    provider: 'gemini',
    description: 'Google Gemini multimodal model with long context.',
    capabilities: ['multimodal', 'long-context'],
    notes: 'Required for video uploads or giant contexts.',
  },
  {
    id: 'gemini-1.5-flash',
    label: 'Gemini 1.5 Flash',
    provider: 'gemini',
    description: 'Lower-latency Gemini model.',
    capabilities: ['fast', 'multimodal'],
    notes: 'Use for chatty flows that still need multimodal.',
  },
  {
    id: 'gemini-3-pro-preview',
    label: 'Gemini 3 Pro (Preview)',
    provider: 'gemini',
    description: 'Latest Gemini 3 preview model.',
    capabilities: ['multimodal', 'reasoning', 'preview'],
    notes: 'Preview tier—expect rapid changes.',
  },
]
const cloneOption = (option: ModelOption): ModelOption => ({
  ...option,
  capabilities: [...option.capabilities],
})
const normalizeModelDefinition = (
  definition: ModelDefinition,
  source: ModelOption['source'],
): ModelOption => {
  const provider = definition.provider ?? inferProviderFromModelId(definition.id)
  const label = definition.label?.trim() || definition.id
  const description = definition.description?.trim() || `${label} (${provider})`
  const capabilities = definition.capabilities?.map((cap) => cap.trim()).filter(Boolean) ?? []
  const normalized: ModelOption = {
    id: definition.id,
    label,
    provider,
    description,
    capabilities,
    source,
  }
  if (definition.default) {
    normalized.default = true
  }
  if (definition.notes?.trim()) {
    normalized.notes = definition.notes.trim()
  }
  return normalized
}
const BUILT_IN_MODEL_OPTIONS = BUILT_IN_MODEL_DEFINITIONS.map((definition) =>
  normalizeModelDefinition(definition, 'builtin'),
)
let cachedModelOptions: ModelOption[] | null = null
let cachedWarning: string | null = null
let cachedModelOptionsNoDiscovery: ModelOption[] | null = null
let cachedWarningNoDiscovery: string | null = null
const mergeModelOptions = (base: ModelOption[], overrides: ModelOption[]): ModelOption[] => {
  const merged = new Map<string, ModelOption>()
  base.forEach((option) => merged.set(option.id, option))
  overrides.forEach((option) => merged.set(option.id, option))
  return Array.from(merged.values())
}
export const getBuiltInModelOptions = (): ModelOption[] => BUILT_IN_MODEL_OPTIONS.map(cloneOption)
export type LoadModelOptionsResult = {
  options: ModelOption[]
  warning?: string
}
export type LoadModelOptionsOptions = {
  includeDiscovered?: boolean
}
export const loadModelOptions = async (
  options: LoadModelOptionsOptions = {},
): Promise<LoadModelOptionsResult> => {
  const includeDiscovered = options.includeDiscovered !== false
  const cached = includeDiscovered ? cachedModelOptions : cachedModelOptionsNoDiscovery
  const warning = includeDiscovered ? cachedWarning : cachedWarningNoDiscovery
  if (cached) {
    const warningResult = warning
      ? { options: cached.map(cloneOption), warning }
      : { options: cached.map(cloneOption) }
    return warningResult
  }
  try {
    const config = await loadCliConfig()
    const extraDefinitions = config?.promptGenerator?.models ?? []
    const normalizedExtras = extraDefinitions.map((definition) =>
      normalizeModelDefinition(definition, 'config'),
    )
    const reservedIds = new Set(
      [...BUILT_IN_MODEL_OPTIONS, ...normalizedExtras].map((option) => option.id),
    )
    let discoveredOptions: ModelOption[] = []
    let videoCapableGeminiIds: Set<string> | null = null
    if (includeDiscovered) {
      try {
        const [openAiCredentials, geminiCredentials] = await Promise.all([
          resolveOpenAiCredentials().catch(() => null),
          resolveGeminiCredentials().catch(() => null),
        ])
        if (geminiCredentials?.apiKey) {
          try {
            const videoIds = await getVideoCapableGeminiModels(geminiCredentials.apiKey, {
              ...(geminiCredentials.baseUrl ? { baseUrl: geminiCredentials.baseUrl } : {}),
            })
            videoCapableGeminiIds = new Set(videoIds)
          } catch {
            videoCapableGeminiIds = null
          }
        }
        const discovered = await getAvailableModels(
          openAiCredentials?.apiKey,
          geminiCredentials?.apiKey,
          {
            ...(openAiCredentials?.baseUrl ? { openAiBaseUrl: openAiCredentials.baseUrl } : {}),
            ...(geminiCredentials?.baseUrl ? { geminiBaseUrl: geminiCredentials.baseUrl } : {}),
            ...(process.env.OPENAI_ORG_ID?.trim()
              ? { openAiOrganizationId: process.env.OPENAI_ORG_ID.trim() }
              : {}),
            ...(process.env.OPENAI_PROJECT_ID?.trim()
              ? { openAiProjectId: process.env.OPENAI_PROJECT_ID.trim() }
              : {}),
          },
        )
        const discoveredIds = [...discovered.openai, ...discovered.gemini].filter(
          (modelId) => !reservedIds.has(modelId),
        )
        discoveredOptions = discoveredIds.map((modelId) =>
          normalizeModelDefinition({ id: modelId }, 'discovered'),
        )
      } catch {
        // Best-effort; dynamic discovery should never block model selection.
      }
    }
    const mergedBase = mergeModelOptions(BUILT_IN_MODEL_OPTIONS, [
      ...normalizedExtras,
      ...discoveredOptions,
    ])
    const merged =
      videoCapableGeminiIds && videoCapableGeminiIds.size > 0
        ? mergedBase.map((option) => {
            if (option.provider !== 'gemini') {
              return option
            }
            if (!videoCapableGeminiIds.has(option.id)) {
              return option
            }
            if (option.capabilities.includes('video')) {
              return option
            }
            return { ...option, capabilities: [...option.capabilities, 'video'] }
          })
        : mergedBase
    if (includeDiscovered) {
      cachedModelOptions = merged
      cachedWarning = null
    } else {
      cachedModelOptionsNoDiscovery = merged
      cachedWarningNoDiscovery = null
    }
    return { options: merged.map(cloneOption) }
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown CLI config error.'
    const fallbackOptions = BUILT_IN_MODEL_OPTIONS
    const fallbackWarning = `Failed to load CLI model entries: ${message}`
    if (includeDiscovered) {
      cachedModelOptions = fallbackOptions
      cachedWarning = fallbackWarning
    } else {
      cachedModelOptionsNoDiscovery = fallbackOptions
      cachedWarningNoDiscovery = fallbackWarning
    }
    return { options: fallbackOptions.map(cloneOption), warning: fallbackWarning }
  }
}
export const getPreferredModelId = (
  options: ModelOption[],
  requestedId?: string | null,
): string => {
  if (requestedId) {
    const requested = options.find((option) => option.id === requestedId)
    if (requested) {
      return requested.id
    }
  }
  const defaultOption = options.find((option) => option.default)
  if (defaultOption) {
    return defaultOption.id
  }
  return options[0]?.id ?? DEFAULT_MODEL_FALLBACK
}
export const DEFAULT_MODEL_ID = DEFAULT_MODEL_FALLBACK
</file>

<file path="src/tui/notifier.ts">
/**
 * TODO: the dedupe logic in showToast could be made more sophisticated.
 * It should not be deduping at all. Need to figure out a better way to handle
 * repeated messages that need to be updated (e.g. progress toasts).
 */
import React, {
  createContext,
  useCallback,
  useContext,
  useEffect,
  useMemo,
  useRef,
  useState,
} from 'react'
import { DEFAULT_MAX_TOASTS, TOAST_ANIMATION_TICK_MS, TOAST_HEIGHT } from './toast-constants'
import {
  addOrReuseToast,
  beginExitToast,
  createToastTrimmedMessage,
  getLatestActiveToast,
  getOldestActiveToastId,
  removeToast as removeToastFromList,
} from './toast-state'
import type { ToastId, ToastItem, ToastKind } from './toast-state'
export type { ToastId, ToastItem, ToastKind } from './toast-state'
export type NotifyOptions = {
  kind?: ToastKind
  autoDismissMs?: number
}
export type UseNotifierOptions = {
  autoDismissMs?: number
}
type ToastContextValue = {
  toasts: ToastItem[]
  showToast: (message: string, options?: NotifyOptions) => ToastId | null
  dismissToast: (id: ToastId) => void
  removeToast: (id: ToastId) => void
  dismissLatest: () => void
  maxToasts: number
  defaultAutoDismissMs: number
  exitAnimationMs: number
}
const DEFAULT_AUTO_DISMISS_MS = 2200
const ToastContext = createContext<ToastContextValue | null>(null)
export type ToastProviderProps = {
  children: React.ReactNode
  maxToasts?: number
  defaultAutoDismissMs?: number
  exitAnimationMs?: number
}
export const ToastProvider = ({
  children,
  maxToasts = DEFAULT_MAX_TOASTS,
  defaultAutoDismissMs = DEFAULT_AUTO_DISMISS_MS,
  exitAnimationMs = TOAST_HEIGHT * TOAST_ANIMATION_TICK_MS,
}: ToastProviderProps) => {
  const [toasts, setToasts] = useState<ToastItem[]>([])
  const nextToastIdRef = useRef(1)
  const dismissTimersRef = useRef(new Map<ToastId, ReturnType<typeof setTimeout>>())
  const removalTimersRef = useRef(new Map<ToastId, ReturnType<typeof setTimeout>>())
  const toastsRef = useRef<ToastItem[]>([])
  useEffect(() => {
    toastsRef.current = toasts
  }, [toasts])
  const clearDismissTimer = useCallback((id: ToastId): void => {
    const timer = dismissTimersRef.current.get(id)
    if (!timer) {
      return
    }
    clearTimeout(timer)
    dismissTimersRef.current.delete(id)
  }, [])
  const clearRemovalTimer = useCallback((id: ToastId): void => {
    const timer = removalTimersRef.current.get(id)
    if (!timer) {
      return
    }
    clearTimeout(timer)
    removalTimersRef.current.delete(id)
  }, [])
  const removeToast = useCallback(
    (id: ToastId): void => {
      clearDismissTimer(id)
      clearRemovalTimer(id)
      setToasts((prev) => removeToastFromList(prev, id))
    },
    [clearDismissTimer, clearRemovalTimer],
  )
  const beginExit = useCallback(
    (id: ToastId): void => {
      clearDismissTimer(id)
      setToasts((prev) => beginExitToast(prev, id))
      if (removalTimersRef.current.has(id)) {
        return
      }
      const timer = setTimeout(() => {
        removeToast(id)
      }, exitAnimationMs)
      removalTimersRef.current.set(id, timer)
    },
    [clearDismissTimer, exitAnimationMs, removeToast],
  )
  const dismissToast = useCallback(
    (id: ToastId): void => {
      beginExit(id)
    },
    [beginExit],
  )
  const dismissLatest = useCallback((): void => {
    const latestActive = getLatestActiveToast(toastsRef.current)
    if (!latestActive) {
      return
    }
    beginExit(latestActive.id)
  }, [beginExit])
  const showToast = useCallback(
    (message: string, options: NotifyOptions = {}): ToastId | null => {
      const trimmed = createToastTrimmedMessage(message)
      if (!trimmed) {
        return null
      }
      const kind = options.kind ?? 'info'
      const autoDismissMs = options.autoDismissMs ?? defaultAutoDismissMs
      const result = addOrReuseToast({
        toasts: toastsRef.current,
        message: trimmed,
        kind,
        autoDismissMs,
        now: Date.now(),
        nextToastId: nextToastIdRef.current,
      })
      nextToastIdRef.current = result.nextToastId
      const toastId = result.toastId
      if (result.action === 'added') {
        toastsRef.current = result.toasts
        setToasts(result.toasts)
      }
      clearDismissTimer(toastId)
      if (autoDismissMs !== null) {
        const timer = setTimeout(() => {
          dismissToast(toastId)
        }, autoDismissMs)
        dismissTimersRef.current.set(toastId, timer)
      }
      return toastId
    },
    [clearDismissTimer, defaultAutoDismissMs, dismissToast],
  )
  useEffect(() => {
    const activeToasts = toasts.filter((toast) => !toast.isExiting)
    if (activeToasts.length <= maxToasts) {
      return
    }
    const oldestActiveId = getOldestActiveToastId(toasts)
    if (oldestActiveId === null) {
      return
    }
    beginExit(oldestActiveId)
  }, [beginExit, maxToasts, toasts])
  useEffect(() => {
    const dismissTimers = dismissTimersRef.current
    const removalTimers = removalTimersRef.current
    return () => {
      for (const timer of dismissTimers.values()) {
        clearTimeout(timer)
      }
      dismissTimers.clear()
      for (const timer of removalTimers.values()) {
        clearTimeout(timer)
      }
      removalTimers.clear()
    }
  }, [])
  const value = useMemo<ToastContextValue>(
    () => ({
      toasts,
      showToast,
      dismissToast,
      removeToast,
      dismissLatest,
      maxToasts,
      defaultAutoDismissMs,
      exitAnimationMs,
    }),
    [
      defaultAutoDismissMs,
      dismissLatest,
      dismissToast,
      exitAnimationMs,
      maxToasts,
      removeToast,
      showToast,
      toasts,
    ],
  )
  return React.createElement(ToastContext.Provider, { value }, children)
}
export const useToastContext = (): ToastContextValue => {
  const value = useContext(ToastContext)
  if (!value) {
    throw new Error('useToastContext must be used within a ToastProvider')
  }
  return value
}
export const useNotifier = (options: UseNotifierOptions = {}) => {
  const { toasts, showToast, dismissToast, dismissLatest, defaultAutoDismissMs } = useToastContext()
  const defaultDismissMs = options.autoDismissMs ?? defaultAutoDismissMs
  const notify = useCallback(
    (message: string, notifyOptions: NotifyOptions = {}): void => {
      void showToast(message, {
        ...notifyOptions,
        autoDismissMs: notifyOptions.autoDismissMs ?? defaultDismissMs,
      })
    },
    [defaultDismissMs, showToast],
  )
  return {
    toasts,
    notify,
    showToast,
    dismiss: dismissLatest,
    dismissToast,
  }
}
</file>

<file path="src/tui/provider-status.ts">
import { resolveGeminiCredentials, resolveOpenAiCredentials } from '../config'
import { inferProviderFromModelId } from '../model-providers'
import type { ModelProvider } from '../model-providers'
import type { ProviderStatus } from './types'
const statusCache = new Map<ModelProvider, ProviderStatus>()
const inflightChecks = new Map<ModelProvider, Promise<ProviderStatus>>()
const buildStatus = (
  provider: ModelProvider,
  status: ProviderStatus['status'],
  message: string,
): ProviderStatus => ({ provider, status, message })
const resolveStatusInternal = async (provider: ModelProvider): Promise<ProviderStatus> => {
  try {
    if (provider === 'openai') {
      await resolveOpenAiCredentials()
      return buildStatus('openai', 'ok', 'Credentials available')
    }
    if (provider === 'gemini') {
      await resolveGeminiCredentials()
      return buildStatus('gemini', 'ok', 'Credentials available')
    }
    return buildStatus(provider, 'ok', 'Custom provider (not validated)')
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown provider error.'
    const status: ProviderStatus['status'] = /missing/i.test(message) ? 'missing' : 'error'
    return buildStatus(provider, status, message)
  }
}
export const checkProviderStatus = async (provider: ModelProvider): Promise<ProviderStatus> => {
  if (statusCache.has(provider)) {
    return statusCache.get(provider) as ProviderStatus
  }
  if (inflightChecks.has(provider)) {
    return inflightChecks.get(provider) as Promise<ProviderStatus>
  }
  const promise = resolveStatusInternal(provider).then((status) => {
    statusCache.set(provider, status)
    return status
  })
  inflightChecks.set(provider, promise)
  try {
    return await promise
  } finally {
    inflightChecks.delete(provider)
  }
}
export const checkModelProviderStatus = async (modelId: string): Promise<ProviderStatus> => {
  const provider = inferProviderFromModelId(modelId)
  return await checkProviderStatus(provider)
}
export const invalidateProviderStatus = (provider?: ModelProvider): void => {
  if (provider) {
    statusCache.delete(provider)
    inflightChecks.delete(provider)
    return
  }
  statusCache.clear()
  inflightChecks.clear()
}
</file>

<file path="src/tui/resume-history.ts">
import fs from 'node:fs/promises'
import { GENERATE_JSON_PAYLOAD_SCHEMA_VERSION, type GenerateJsonPayload } from '../generate/types'
import { resolveHistoryFilePath } from '../history-logger'
import type { ResumeHistoryItem } from './types'
type JsonRecord = Record<string, unknown>
const isRecord = (value: unknown): value is JsonRecord =>
  typeof value === 'object' && value !== null && !Array.isArray(value)
const isStringArray = (value: unknown): value is string[] =>
  Array.isArray(value) && value.every((entry) => typeof entry === 'string')
const isContextPaths = (value: unknown): value is GenerateJsonPayload['contextPaths'] =>
  Array.isArray(value) &&
  value.every(
    (entry) =>
      isRecord(entry) && typeof entry.path === 'string' && typeof entry.source === 'string',
  )
const isSupportedGeneratePayload = (value: unknown): value is GenerateJsonPayload => {
  if (!isRecord(value)) {
    return false
  }
  if (value.schemaVersion !== GENERATE_JSON_PAYLOAD_SCHEMA_VERSION) {
    return false
  }
  return (
    typeof value.intent === 'string' &&
    typeof value.model === 'string' &&
    typeof value.targetModel === 'string' &&
    typeof value.prompt === 'string' &&
    isStringArray(value.refinements) &&
    typeof value.iterations === 'number' &&
    Number.isFinite(value.iterations) &&
    typeof value.interactive === 'boolean' &&
    typeof value.timestamp === 'string' &&
    isContextPaths(value.contextPaths)
  )
}
const isMissingFileError = (error: unknown): error is { code: string } =>
  typeof error === 'object' &&
  error !== null &&
  'code' in error &&
  typeof (error as { code?: unknown }).code === 'string' &&
  (error as { code: string }).code === 'ENOENT'
const formatTimestamp = (raw: string): string => {
  const date = new Date(raw)
  if (!Number.isFinite(date.getTime())) {
    return raw
  }
  return date.toISOString().replace('T', ' ').replace(/\..*$/, '')
}
const summarizeIntent = (intent: string, limit: number): string => {
  const normalized = intent.replace(/\s+/g, ' ').trim()
  if (!normalized) {
    return '(empty intent)'
  }
  if (normalized.length <= limit) {
    return normalized
  }
  return `${normalized.slice(0, Math.max(0, limit - 1))}…`
}
export type ResumeHistoryLoadResult =
  | { ok: true; items: ResumeHistoryItem[] }
  | { ok: false; errorMessage: string }
export const loadResumeHistoryItems = async (options?: {
  limit?: number
}): Promise<ResumeHistoryLoadResult> => {
  const filePath = resolveHistoryFilePath()
  let raw: string
  try {
    raw = await fs.readFile(filePath, 'utf8')
  } catch (error) {
    if (isMissingFileError(error)) {
      return {
        ok: false,
        errorMessage: `No prompt history found at ${filePath}. Run a generation first to create it.`,
      }
    }
    const message = error instanceof Error ? error.message : 'Unknown file error.'
    return { ok: false, errorMessage: `Failed to read history file ${filePath}: ${message}` }
  }
  const lines = raw
    .split('\n')
    .map((line) => line.trim())
    .filter((line) => line.length > 0)
  if (lines.length === 0) {
    return { ok: false, errorMessage: `History file ${filePath} is empty.` }
  }
  const supported: GenerateJsonPayload[] = []
  const schemaVersions = new Set<string>()
  for (const line of lines) {
    try {
      const parsed = JSON.parse(line) as unknown
      if (isRecord(parsed) && typeof parsed.schemaVersion === 'string') {
        schemaVersions.add(parsed.schemaVersion)
      }
      if (isSupportedGeneratePayload(parsed)) {
        supported.push(parsed)
      }
    } catch {
      // ignore invalid json lines
    }
  }
  if (supported.length === 0) {
    const versions = Array.from(schemaVersions).sort().join(', ') || '(unknown)'
    return {
      ok: false,
      errorMessage:
        `No resumable history entries found. ` +
        `This version supports schemaVersion=${GENERATE_JSON_PAYLOAD_SCHEMA_VERSION}, but history contains: ${versions}. ` +
        'Upgrade/downgrade prompt-maker-cli to match, or generate a new prompt to create a compatible history entry.',
    }
  }
  const limit = options?.limit ?? 30
  const newestFirst = supported.slice(-limit).reverse()
  const items: ResumeHistoryItem[] = newestFirst.map((payload, index) => {
    const selector = index === 0 ? 'last' : `last:${index + 1}`
    const fileCount = payload.contextPaths.filter((entry) => entry.source === 'file').length
    return {
      selector,
      title: `${formatTimestamp(payload.timestamp)} · ${payload.model}`,
      detail: `files:${fileCount} · iters:${payload.iterations} · ${summarizeIntent(payload.intent, 60)}`,
    }
  })
  return { ok: true, items }
}
</file>

<file path="src/tui/toast-constants.ts">
export const TOAST_HEIGHT = 5
export const TOAST_ANIMATION_TICK_MS = 40
export const DEFAULT_MAX_TOASTS = 5
// How many terminal rows to offset the toast stack
// from the top edge of the screen.
export const TOAST_TOP_OFFSET_ROWS = 2
// Horizontal inset (in terminal columns) for toast overlay.
export const TOAST_HORIZONTAL_INSET_COLUMNS = 0
</file>

<file path="src/tui/toast-state.ts">
export type ToastKind = 'info' | 'progress' | 'warning' | 'error'
export type ToastId = number
export type ToastItem = {
  id: ToastId
  message: string
  kind: ToastKind
  createdAt: number
  autoDismissMs: number | null
  isExiting: boolean
}
export const createToastTrimmedMessage = (message: string): string | null => {
  const trimmed = message.trim()
  return trimmed ? trimmed : null
}
export const getLatestActiveToast = (toasts: ToastItem[]): ToastItem | null => {
  const latestActive = [...toasts].reverse().find((toast) => !toast.isExiting)
  return latestActive ?? null
}
export const getOldestActiveToastId = (toasts: ToastItem[]): ToastId | null => {
  const oldest = toasts.find((toast) => !toast.isExiting)
  return oldest ? oldest.id : null
}
export type AddOrReuseToastResult =
  | {
      action: 'added'
      toastId: ToastId
      toasts: ToastItem[]
      nextToastId: ToastId
    }
  | {
      action: 'reused'
      toastId: ToastId
      toasts: ToastItem[]
      nextToastId: ToastId
    }
export const addOrReuseToast = (input: {
  toasts: ToastItem[]
  message: string
  kind: ToastKind
  autoDismissMs: number | null
  now: number
  nextToastId: ToastId
}): AddOrReuseToastResult => {
  const latestActive = getLatestActiveToast(input.toasts)
  const shouldReuse =
    latestActive !== null &&
    latestActive.message === input.message &&
    latestActive.kind === input.kind
  if (shouldReuse) {
    return {
      action: 'reused',
      toastId: latestActive.id,
      toasts: input.toasts,
      nextToastId: input.nextToastId,
    }
  }
  const toast: ToastItem = {
    id: input.nextToastId,
    message: input.message,
    kind: input.kind,
    createdAt: input.now,
    autoDismissMs: input.autoDismissMs,
    isExiting: false,
  }
  return {
    action: 'added',
    toastId: toast.id,
    toasts: [...input.toasts, toast],
    nextToastId: toast.id + 1,
  }
}
export const beginExitToast = (toasts: ToastItem[], id: ToastId): ToastItem[] => {
  const toast = toasts.find((candidate) => candidate.id === id)
  if (!toast || toast.isExiting) {
    return toasts
  }
  return toasts.map((candidate) =>
    candidate.id === id ? { ...candidate, isExiting: true } : candidate,
  )
}
export const removeToast = (toasts: ToastItem[], id: ToastId): ToastItem[] =>
  toasts.filter((toast) => toast.id !== id)
</file>

<file path="src/utils/model-manager.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
export type ProviderModelLists = {
  openai: string[]
  gemini: string[]
}
export type ModelCache = {
  timestamp: number
  models: ProviderModelLists
}
export type GetAvailableModelsOptions = {
  cacheFilePath?: string
  now?: number
  fetchImpl?: typeof fetch
  openAiBaseUrl?: string
  geminiBaseUrl?: string
  openAiOrganizationId?: string
  openAiProjectId?: string
}
const ONE_DAY_MS = 24 * 60 * 60 * 1000
export const DEFAULT_MODEL_CACHE_FILE = path.join(
  os.homedir(),
  '.config',
  'prompt-maker-cli',
  'models-cache.json',
)
// Hardcoded fallbacks in case of network failure / first run offline.
export const FALLBACK_MODELS: ProviderModelLists = {
  openai: ['gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-3.5-turbo'],
  gemini: ['gemini-1.5-pro-latest', 'gemini-1.5-flash-latest'],
}
type OpenAiModelListResponse = {
  data: Array<{ id: string }>
}
type GeminiModelListResponse = {
  models: Array<{
    name: string
    supportedGenerationMethods?: string[]
    inputTokenLimit?: number
  }>
}
const normalizeModelId = (value: string): string => value.trim()
const normalizeModelList = (models: readonly string[]): string[] => {
  const unique = new Set(models.map(normalizeModelId).filter(Boolean))
  return Array.from(unique).sort((a, b) => a.localeCompare(b))
}
const isRecord = (value: unknown): value is Record<string, unknown> =>
  typeof value === 'object' && value !== null
const isOpenAiModelListResponse = (value: unknown): value is OpenAiModelListResponse => {
  if (!isRecord(value)) {
    return false
  }
  const data = value.data
  if (!Array.isArray(data)) {
    return false
  }
  return data.every((entry) => isRecord(entry) && typeof entry.id === 'string')
}
const isGeminiModelListResponse = (value: unknown): value is GeminiModelListResponse => {
  if (!isRecord(value)) {
    return false
  }
  const models = value.models
  if (!Array.isArray(models)) {
    return false
  }
  return models.every((entry) => {
    if (!isRecord(entry) || typeof entry.name !== 'string') {
      return false
    }
    if (
      entry.supportedGenerationMethods !== undefined &&
      (!Array.isArray(entry.supportedGenerationMethods) ||
        !entry.supportedGenerationMethods.every((method) => typeof method === 'string'))
    ) {
      return false
    }
    if (entry.inputTokenLimit !== undefined && typeof entry.inputTokenLimit !== 'number') {
      return false
    }
    return true
  })
}
const buildOpenAiModelsUrl = (baseUrl?: string): string => {
  const normalized = baseUrl?.trim().replace(/\/$/, '')
  if (!normalized) {
    return 'https://api.openai.com/v1/models'
  }
  if (normalized.endsWith('/v1')) {
    return `${normalized}/models`
  }
  return `${normalized}/v1/models`
}
const normalizeGeminiBaseUrl = (value?: string): string => {
  const normalized = value?.trim().replace(/\/$/, '')
  if (!normalized) {
    return 'https://generativelanguage.googleapis.com'
  }
  const suffixes = ['/v1beta/models', '/v1/models', '/v1beta', '/v1']
  return suffixes.reduce((current, suffix) => {
    return current.endsWith(suffix) ? current.slice(0, -suffix.length) : current
  }, normalized)
}
const buildGeminiModelsUrl = (baseUrl?: string, apiKey?: string): string => {
  const root = normalizeGeminiBaseUrl(baseUrl)
  const key = apiKey?.trim() || ''
  return `${root}/v1beta/models?key=${encodeURIComponent(key)}`
}
const fetchJson = async (
  fetchImpl: typeof fetch,
  url: string,
  init?: RequestInit,
): Promise<unknown> => {
  const response = await fetchImpl(url, init)
  if (!response.ok) {
    throw new Error(`Fetch failed (${response.status}): ${response.statusText}`)
  }
  return (await response.json()) as unknown
}
const fetchOpenAiModels = async (
  apiKey: string | null | undefined,
  options: {
    fetchImpl: typeof fetch
    baseUrl?: string
    organizationId?: string
    projectId?: string
  },
): Promise<string[]> => {
  const trimmedKey = apiKey?.trim()
  if (!trimmedKey) {
    return []
  }
  const url = buildOpenAiModelsUrl(options.baseUrl)
  const headers: Record<string, string> = { Authorization: `Bearer ${trimmedKey}` }
  if (options.organizationId?.trim()) {
    headers['OpenAI-Organization'] = options.organizationId.trim()
  }
  if (options.projectId?.trim()) {
    headers['OpenAI-Project'] = options.projectId.trim()
  }
  const data = await fetchJson(options.fetchImpl, url, {
    headers,
  })
  if (!isOpenAiModelListResponse(data)) {
    throw new Error('OpenAI models response had unexpected shape.')
  }
  return data.data
    .map((entry) => entry.id)
    .filter((id) => {
      const normalized = id.toLowerCase()
      return (
        normalized.startsWith('gpt') ||
        normalized.startsWith('o') ||
        normalized.startsWith('chatgpt')
      )
    })
}
export type GeminiModelDescriptor = {
  id: string
  supportedGenerationMethods: string[]
  inputTokenLimit?: number
}
export const MIN_VIDEO_INPUT_TOKEN_LIMIT = 128_000
const fetchGeminiModels = async (
  apiKey: string | null | undefined,
  options: {
    fetchImpl: typeof fetch
    baseUrl?: string
  },
): Promise<string[]> => {
  const trimmedKey = apiKey?.trim()
  if (!trimmedKey) {
    return []
  }
  const url = buildGeminiModelsUrl(options.baseUrl, trimmedKey)
  const data = await fetchJson(options.fetchImpl, url)
  if (!isGeminiModelListResponse(data)) {
    throw new Error('Gemini models response had unexpected shape.')
  }
  return data.models
    .filter((entry) => entry.supportedGenerationMethods?.includes('generateContent'))
    .map((entry) => entry.name.replace(/^models\//, ''))
}
const fetchGeminiModelDescriptors = async (
  apiKey: string | null | undefined,
  options: {
    fetchImpl: typeof fetch
    baseUrl?: string
  },
): Promise<GeminiModelDescriptor[]> => {
  const trimmedKey = apiKey?.trim()
  if (!trimmedKey) {
    return []
  }
  const url = buildGeminiModelsUrl(options.baseUrl, trimmedKey)
  const data = await fetchJson(options.fetchImpl, url)
  if (!isGeminiModelListResponse(data)) {
    throw new Error('Gemini models response had unexpected shape.')
  }
  return data.models
    .filter((entry) => entry.supportedGenerationMethods?.includes('generateContent'))
    .map((entry) => ({
      id: entry.name.replace(/^models\//, ''),
      supportedGenerationMethods: entry.supportedGenerationMethods ?? [],
      ...(typeof entry.inputTokenLimit === 'number'
        ? { inputTokenLimit: entry.inputTokenLimit }
        : {}),
    }))
}
const buildGeminiGenerateContentUrl = (
  baseUrl: string,
  apiKey: string,
  modelId: string,
): string => {
  const normalized = normalizeGeminiBaseUrl(baseUrl)
  return `${normalized}/v1beta/models/${encodeURIComponent(modelId)}:generateContent?key=${encodeURIComponent(apiKey)}`
}
type GeminiProbeOutcome =
  | { ok: true }
  | {
      ok: false
      reason: 'model-not-found' | 'filedata-unsupported' | 'unknown'
      details: string
    }
export const probeGeminiModelSupportsFileData = async (
  fetchImpl: typeof fetch,
  modelId: string,
  apiKey: string,
  baseUrl?: string,
): Promise<GeminiProbeOutcome> => {
  const trimmedKey = apiKey.trim()
  if (!trimmedKey) {
    return { ok: false, reason: 'unknown', details: 'Missing Gemini API key.' }
  }
  const url = buildGeminiGenerateContentUrl(
    baseUrl ?? 'https://generativelanguage.googleapis.com',
    trimmedKey,
    modelId,
  )
  // Important: we purposely use a bogus fileUri to test schema support without actually uploading.
  // If the model supports fileData, the request should fail for a different reason (e.g. bad file uri),
  // not "Unknown name fileData".
  const body = {
    contents: [
      {
        role: 'user',
        parts: [
          {
            fileData: {
              mimeType: 'video/mp4',
              // Intentionally invalid to avoid ambiguous 404s.
              fileUri: 'INVALID_FILE_URI',
            },
          },
          { text: 'Describe the video.' },
        ],
      },
    ],
    generationConfig: { temperature: 0.2 },
  }
  const response = await fetchImpl(url, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(body),
  })
  if (response.ok) {
    return { ok: true }
  }
  const details = await response.text()
  if (response.status === 404) {
    return { ok: false, reason: 'model-not-found', details }
  }
  if (response.status === 400 && details.includes('Unknown name') && details.includes('fileData')) {
    return { ok: false, reason: 'filedata-unsupported', details }
  }
  // If it fails for any other reason, we assume the schema was accepted.
  return { ok: true }
}
export const getVideoCapableGeminiModels = async (
  apiKey: string | null | undefined,
  options: {
    fetchImpl?: typeof fetch
    baseUrl?: string
  } = {},
): Promise<string[]> => {
  const fetchImpl = options.fetchImpl ?? globalThis.fetch
  if (!fetchImpl) {
    return []
  }
  const trimmedKey = apiKey?.trim()
  if (!trimmedKey) {
    return []
  }
  const descriptors = await fetchGeminiModelDescriptors(trimmedKey, {
    fetchImpl,
    ...(options.baseUrl ? { baseUrl: options.baseUrl } : {}),
  })
  const candidates = descriptors
    .filter((model) => {
      const normalizedId = model.id.toLowerCase()
      const isGemini = normalizedId.includes('gemini')
      const tokenLimit = model.inputTokenLimit ?? 0
      return isGemini && tokenLimit >= MIN_VIDEO_INPUT_TOKEN_LIMIT
    })
    .sort((a, b) => {
      const aLimit = a.inputTokenLimit ?? 0
      const bLimit = b.inputTokenLimit ?? 0
      if (bLimit !== aLimit) {
        return bLimit - aLimit
      }
      return a.id.localeCompare(b.id)
    })
  const supported: string[] = []
  for (const model of candidates) {
    const probe = await probeGeminiModelSupportsFileData(
      fetchImpl,
      model.id,
      trimmedKey,
      options.baseUrl,
    )
    if (probe.ok) {
      supported.push(model.id)
    }
  }
  return supported
}
const readCacheFile = async (cacheFilePath: string): Promise<ModelCache | null> => {
  try {
    const raw = await fs.readFile(cacheFilePath, 'utf8')
    const parsed = JSON.parse(raw) as unknown
    if (!isRecord(parsed)) {
      return null
    }
    const timestamp = parsed.timestamp
    const models = parsed.models
    if (typeof timestamp !== 'number' || !isRecord(models)) {
      return null
    }
    const openai = models.openai
    const gemini = models.gemini
    if (!Array.isArray(openai) || !Array.isArray(gemini)) {
      return null
    }
    if (
      !openai.every((id) => typeof id === 'string') ||
      !gemini.every((id) => typeof id === 'string')
    ) {
      return null
    }
    return {
      timestamp,
      models: {
        openai: normalizeModelList(openai),
        gemini: normalizeModelList(gemini),
      },
    }
  } catch {
    return null
  }
}
const writeCacheFile = async (
  cacheFilePath: string,
  models: ProviderModelLists,
  now: number,
): Promise<void> => {
  await fs.mkdir(path.dirname(cacheFilePath), { recursive: true })
  const payload: ModelCache = { timestamp: now, models }
  await fs.writeFile(cacheFilePath, JSON.stringify(payload, null, 2), 'utf8')
}
export const getAvailableModels = async (
  openAiKey?: string | null,
  geminiKey?: string | null,
  options: GetAvailableModelsOptions = {},
): Promise<ProviderModelLists> => {
  const cacheFilePath = options.cacheFilePath ?? DEFAULT_MODEL_CACHE_FILE
  const now = options.now ?? Date.now()
  const cached = await readCacheFile(cacheFilePath)
  if (cached) {
    const age = now - cached.timestamp
    if (age >= 0 && age < ONE_DAY_MS) {
      return cached.models
    }
    const canRefresh = Boolean(openAiKey?.trim() || geminiKey?.trim())
    if (!canRefresh) {
      return cached.models
    }
  } else {
    const canRefresh = Boolean(openAiKey?.trim() || geminiKey?.trim())
    if (!canRefresh) {
      return FALLBACK_MODELS
    }
  }
  const fetchImpl = options.fetchImpl ?? globalThis.fetch
  if (!fetchImpl) {
    return cached?.models ?? FALLBACK_MODELS
  }
  let openai: string[] = []
  let gemini: string[] = []
  try {
    const results = await Promise.allSettled([
      fetchOpenAiModels(openAiKey, {
        fetchImpl,
        ...(options.openAiBaseUrl ? { baseUrl: options.openAiBaseUrl } : {}),
        ...(options.openAiOrganizationId ? { organizationId: options.openAiOrganizationId } : {}),
        ...(options.openAiProjectId ? { projectId: options.openAiProjectId } : {}),
      }),
      fetchGeminiModels(geminiKey, {
        fetchImpl,
        ...(options.geminiBaseUrl ? { baseUrl: options.geminiBaseUrl } : {}),
      }),
    ])
    const openAiResult = results[0]
    if (openAiResult.status === 'fulfilled') {
      openai = openAiResult.value
    } else {
      console.error('Failed to fetch OpenAI models', openAiResult.reason)
    }
    const geminiResult = results[1]
    if (geminiResult.status === 'fulfilled') {
      gemini = geminiResult.value
    } else {
      console.error('Failed to fetch Gemini models', geminiResult.reason)
    }
  } catch (error) {
    console.error('Failed to refresh model cache', error)
  }
  const result: ProviderModelLists = {
    openai: normalizeModelList(openai.length > 0 ? openai : FALLBACK_MODELS.openai),
    gemini: normalizeModelList(gemini.length > 0 ? gemini : FALLBACK_MODELS.gemini),
  }
  try {
    await writeCacheFile(cacheFilePath, result, now)
  } catch (error) {
    console.error('Failed to write model cache', error)
  }
  return result
}
</file>

<file path="src/compose-command.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import yargs from 'yargs'
import type { ArgumentsCamelCase } from 'yargs'
type ComposeArgs = {
  recipe: string
  input: string
  help: boolean
}
export const runComposeCommand = async (argv: string[]): Promise<void> => {
  try {
    const { args, showHelp } = parseComposeArgs(argv)
    if (args.help) {
      showHelp()
      return
    }
    const recipePath = path.resolve(process.cwd(), args.recipe)
    const recipeText = await readRecipeFile(recipePath)
    const composed = composeDeterministicPrompt(recipeText, args.input)
    process.stdout.write(composed)
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown compose error.'
    console.error(message)
    process.exitCode = 1
  }
}
const HELP_FLAGS = new Set(['--help', '-h'])
const COMPOSE_HELP_TEXT = `Usage:\n  prompt-maker-cli compose --recipe <path> --input <text>\n\nOptions:\n  --recipe  Path to a recipe file (text for now)\n  --input   Input text to compose into the recipe\n  --help,-h Show help\n`
const parseComposeArgs = (argv: string[]): { args: ComposeArgs; showHelp: () => void } => {
  const { optionArgs, helpRequested } = stripHelpFlags(argv)
  if (helpRequested) {
    return {
      args: {
        recipe: '',
        input: '',
        help: true,
      },
      showHelp: () => {
        process.stdout.write(COMPOSE_HELP_TEXT)
      },
    }
  }
  const parser = yargs(optionArgs)
    .scriptName('prompt-maker-cli compose')
    .usage('Usage:\n  prompt-maker-cli compose --recipe <path> --input <text>')
    .option('recipe', {
      type: 'string',
      describe: 'Path to a recipe file (text for now)',
    })
    .option('input', {
      type: 'string',
      describe: 'Input text to compose into the recipe',
    })
    .check((argv) => {
      const recipe = argv.recipe
      if (typeof recipe !== 'string' || recipe.trim().length === 0) {
        throw new Error('--recipe is required.')
      }
      const input = argv.input
      if (typeof input !== 'string' || input.trim().length === 0) {
        throw new Error('--input is required.')
      }
      return true
    })
    .help('help')
    .alias('help', 'h')
    .exitProcess(false)
    .showHelpOnFail(false)
    .parserConfiguration({ 'halt-at-non-option': true })
    .strict(false)
    .fail((msg, err) => {
      throw err ?? new Error(msg ?? 'Invalid CLI arguments.')
    })
  const parsed = parser.parseSync() as ArgumentsCamelCase<{
    recipe?: string
    input?: string
    help?: boolean
  }>
  const recipe = parsed.recipe?.trim() ?? ''
  const input = parsed.input ?? ''
  return {
    args: {
      recipe,
      input,
      help: Boolean(parsed.help),
    },
    showHelp: () => {
      process.stdout.write(COMPOSE_HELP_TEXT)
    },
  }
}
const readRecipeFile = async (filePath: string): Promise<string> => {
  try {
    return await fs.readFile(filePath, 'utf8')
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown file error.'
    throw new Error(`Failed to read recipe file ${formatDisplayPath(filePath)}: ${message}`)
  }
}
export const composeDeterministicPrompt = (recipeText: string, input: string): string => {
  const normalizedRecipe = normalizeNewlines(recipeText).trimEnd()
  const normalizedInput = normalizeNewlines(input)
  return `${normalizedRecipe}\n---\n${normalizedInput}\n`
}
const normalizeNewlines = (value: string): string => value.replace(/\r\n/g, '\n')
const formatDisplayPath = (absolutePath: string): string => {
  const relative = path.relative(process.cwd(), absolutePath)
  return relative && !relative.startsWith('..') ? relative : absolutePath
}
const stripHelpFlags = (tokens: string[]): { optionArgs: string[]; helpRequested: boolean } => {
  const optionArgs: string[] = []
  let helpRequested = false
  let passthrough = false
  tokens.forEach((token) => {
    if (passthrough) {
      optionArgs.push(token)
      return
    }
    if (token === '--') {
      optionArgs.push(token)
      passthrough = true
      return
    }
    if (HELP_FLAGS.has(token)) {
      helpRequested = true
      return
    }
    optionArgs.push(token)
  })
  return { optionArgs, helpRequested }
}
</file>

<file path="src/history-logger.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
export const resolveHistoryFilePath = (): string => {
  const envHome = process.env.HOME?.trim()
  const homeDir = envHome && envHome.length > 0 ? envHome : os.homedir()
  return path.join(homeDir, '.config', 'prompt-maker-cli', 'history.jsonl')
}
export const appendToHistory = async (payload: object): Promise<void> => {
  const historyFile = resolveHistoryFilePath()
  try {
    await fs.mkdir(path.dirname(historyFile), { recursive: true })
    const line = `${JSON.stringify(payload)}\n`
    await fs.appendFile(historyFile, line, 'utf8')
  } catch (error) {
    console.warn('Failed to write history entry:', error)
  }
}
</file>

<file path="tests/mocks/yargs.ts">
import yargsParser from 'yargs-parser'
type PositionalOptions = {
  default?: unknown
}
type Builder = {
  positional: (name: string, opts: PositionalOptions) => Builder
}
type YargsApi = {
  scriptName: (...args: unknown[]) => YargsApi
  usage: (...args: unknown[]) => YargsApi
  option: (...args: unknown[]) => YargsApi
  alias: (...args: unknown[]) => YargsApi
  help: (...args: unknown[]) => YargsApi
  exitProcess: (...args: unknown[]) => YargsApi
  showHelpOnFail: (...args: unknown[]) => YargsApi
  parserConfiguration: (...args: unknown[]) => YargsApi
  strict: (...args: unknown[]) => YargsApi
  command: (_pattern: string, _desc: string, builder?: (cmd: Builder) => Builder) => YargsApi
  check: (handler: (argv: Record<string, unknown>) => boolean | void) => YargsApi
  fail: (handler: (msg?: string, err?: Error) => void) => YargsApi
  showHelp: () => void
  parseSync: () => Record<string, unknown>
}
const BOOLEAN_OPTIONS = [
  'interactive',
  'copy',
  'open-chatgpt',
  'polish',
  'json',
  'progress',
  'smart-context',
  'help',
]
const ARRAY_OPTIONS = ['context', 'image', 'video']
const NUMBER_OPTIONS = ['max-input-tokens', 'max-context-tokens']
const DEFAULTS: Record<string, unknown> = {
  context: [],
  image: [],
  video: [],
  progress: true,
}
const ALIASES: Record<string, string> = {
  c: 'context',
  f: 'intent-file',
  i: 'interactive',
  h: 'help',
}
const createYargs = (argv: string[]): YargsApi => {
  const positionalDefaults: Record<string, unknown> = {}
  const checkHandlers: Array<(argv: Record<string, unknown>) => boolean | void> = []
  let failHandler: ((msg?: string, err?: Error) => void) | undefined
  const api: YargsApi = {
    scriptName: () => api,
    usage: () => api,
    option: () => api,
    alias: () => api,
    help: () => api,
    exitProcess: () => api,
    showHelpOnFail: () => api,
    parserConfiguration: () => api,
    strict: () => api,
    check(handler: (argv: Record<string, unknown>) => boolean | void) {
      checkHandlers.push(handler)
      return api
    },
    command(_pattern: string, _desc: string, builder?: (cmd: Builder) => Builder) {
      if (builder) {
        builder({
          positional(name, opts) {
            if (opts && Object.prototype.hasOwnProperty.call(opts, 'default')) {
              positionalDefaults[name] = opts.default
            }
            return this
          },
        })
      }
      return api
    },
    fail(handler: (msg?: string, err?: Error) => void) {
      failHandler = handler
      return api
    },
    showHelp: () => undefined,
    parseSync() {
      try {
        const parsed = yargsParser(argv, {
          alias: ALIASES,
          array: ARRAY_OPTIONS,
          boolean: BOOLEAN_OPTIONS,
          number: NUMBER_OPTIONS,
          configuration: {
            'halt-at-non-option': true,
            'camel-case-expansion': true,
          },
          default: DEFAULTS,
        }) as Record<string, unknown>
        for (const [name, value] of Object.entries(positionalDefaults)) {
          if (parsed[name] === undefined || parsed[name] === '') {
            parsed[name] = value
          }
        }
        checkHandlers.forEach((handler) => {
          const result = handler(parsed)
          if (result === false) {
            throw new Error('Invalid CLI arguments.')
          }
        })
        return parsed
      } catch (error) {
        const err = error instanceof Error ? error : new Error(String(error))
        failHandler?.(err.message, err)
        throw err
      }
    },
  }
  return api
}
const yargs = (argv: string[]) => createYargs(argv)
export type ArgumentsCamelCase<T> = T
export default yargs
</file>

<file path="session-ses_4602.md">
# Integrating PDF attachments into Prompt attachments pipeline

**Session ID:** ses_4602ec1f1ffeUNtnac7LLUak22
**Created:** 1/8/2026, 4:54:08 PM
**Updated:** 1/9/2026, 7:18:39 AM

---

## Assistant

_Thinking:_



```
Tool: grep

**Input:**
```json
{
  "pattern": "useCommandScreenContextPopupBindings\\({",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Error:**
```
Error: ripgrep failed: rg: regex parse error:
    (?:useCommandScreenContextPopupBindings\({)
                                              ^
error: repetition quantifier expects a valid decimal

```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "oldString": "    images: options.context.images,\n    videos: options.context.videos,\n",
  "newString": "    images: options.context.images,\n    videos: options.context.videos,\n    pdfs: options.context.pdfs,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [251:27] Property 'pdfs' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [263:29] Property 'addPdf' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [264:32] Property 'removePdf' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [368:30] Property 'pdfs' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
</file_diagnostics>

```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "oldString": "    addVideo: options.context.addVideo,\n    removeVideo: options.context.removeVideo,\n",
  "newString": "    addVideo: options.context.addVideo,\n    removeVideo: options.context.removeVideo,\n    addPdf: options.context.addPdf,\n    removePdf: options.context.removePdf,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [251:27] Property 'pdfs' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [263:29] Property 'addPdf' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [264:32] Property 'removePdf' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [368:30] Property 'pdfs' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
</file_diagnostics>

```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "oldString": "    video: {\n      items: options.context.videos,\n      suggestions: context.videoPopupSuggestions,\n      onAdd: context.onAddVideo,\n      onRemove: context.onRemoveVideo,\n    },\n",
  "newString": "    video: {\n      items: options.context.videos,\n      suggestions: context.videoPopupSuggestions,\n      onAdd: context.onAddVideo,\n      onRemove: context.onRemoveVideo,\n    },\n\n    pdf: {\n      items: options.context.pdfs,\n      suggestions: context.pdfPopupSuggestions,\n      onAdd: context.onAddPdf,\n      onRemove: context.onRemovePdf,\n    },\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [251:27] Property 'pdfs' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [263:29] Property 'addPdf' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [264:32] Property 'removePdf' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
ERROR [368:30] Property 'pdfs' does not exist on type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_



```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupAndView.types.ts",
  "offset": 1,
  "limit": 90
}
```
**Output:**
```
<file>
00002| 
00003| import type { MutableRefObject } from 'react'
00004| 
00005| import type { NotifyOptions } from '../../../notifier'
00006| import type { HistoryEntry, ModelOption, ProviderStatusMap } from '../../../types'
00007| 
00008| export type PushHistory = (
00009|   content: string,
00010|   kind?: HistoryEntry['kind'],
00011|   format?: HistoryEntry['format'],
00012| ) => void
00013| 
00014| export type CommandContextOptions = {
00015|   interactiveTransportPath?: string | undefined
00016| 
00017|   notify: (message: string, options?: NotifyOptions) => void
00018|   stdout: WriteStream | undefined
00019| 
00020|   // context state
00021|   files: string[]
00022|   urls: string[]
00023|   images: string[]
00024|   videos: string[]
00025|   smartContextEnabled: boolean
00026|   smartContextRoot: string | null
00027|   metaInstructions: string
00028|   maxContextTokens: number | null
00029|   maxInputTokens: number | null
00030|   contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
00031|   lastReasoning: string | null
00032|   lastGeneratedPrompt: string | null
00033| 
00034|   // context dispatch
00035|   addFile: (value: string) => void
00036|   removeFile: (index: number) => void
00037|   addUrl: (value: string) => void
00038|   removeUrl: (index: number) => void
00039|   updateUrl: (index: number, value: string) => void
00040|   addImage: (value: string) => void
00041|   removeImage: (index: number) => void
00042|   addVideo: (value: string) => void
00043|   removeVideo: (index: number) => void
00044|   toggleSmartContext: () => void
00045|   setSmartRoot: (value: string) => void
00046|   setMetaInstructions: (value: string) => void
00047|   setBudgets: (value: {
00048|     maxContextTokens: number | null
00049|     maxInputTokens: number | null
00050|     contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
00051|   }) => void
00052|   resetContext: () => void
00053| }
00054| 
00055| export type CommandInputOptions = {
00056|   // screen state
00057|   terminalRows: number
00058|   terminalColumns: number
00059|   inputValue: string
00060|   isPasteActive: boolean
00061|   commandSelectionIndex: number
00062|   debugKeyLine: string | null
00063|   debugKeysEnabled: boolean
00064| 
00065|   setTerminalSize: (rows: number, columns: number) => void
00066|   setInputValue: (value: string | ((prev: string) => string)) => void
00067|   setPasteActive: (active: boolean) => void
00068|   setCommandSelectionIndex: (next: number | ((prev: number) => number)) => void
00069| 
00070|   // input local
00071|   intentFilePath: string
00072|   setIntentFilePath: (value: string) => void
00073|   copyEnabled: boolean
00074|   setCopyEnabled: (value: boolean) => void
00075|   chatGptEnabled: boolean
00076|   setChatGptEnabled: (value: boolean) => void
00077|   jsonOutputEnabled: boolean
00078|   setJsonOutputEnabled: (value: boolean) => void
00079| 
00080|   // refs
00081|   lastUserIntentRef: MutableRefObject<string | null>
00082|   lastTypedIntentRef: MutableRefObject<string>
00083| 
00084|   // suppression
00085|   consumeSuppressedTextInputChange: () => boolean
00086|   suppressNextInput: () => void
00087|   updateLastTypedIntent: (next: string) => void
00088| 
00089|   onDebugKeyEvent: (
00090|     event: import('../../../components/core/MultilineTextInput').DebugKeyEvent,
00091|   ) => void

(File has more lines. Use 'offset' parameter to read beyond line 91)
</file>
```
```

---

## Assistant

_Thinking:_



```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupAndView.types.ts",
  "oldString": "  images: string[]\n  videos: string[]\n",
  "newString": "  images: string[]\n  videos: string[]\n  pdfs: string[]\n"
}
```
```

---

## Assistant

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupAndView.types.ts",
  "oldString": "  addVideo: (value: string) => void\n  removeVideo: (index: number) => void\n",
  "newString": "  addVideo: (value: string) => void\n  removeVideo: (index: number) => void\n  addPdf: (value: string) => void\n  removePdf: (index: number) => void\n"
}
```
```

---

## Assistant

_Thinking:_



```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "    images: context.images,\n    videos: context.videos,\n    addImage: context.addImage,\n    addVideo: context.addVideo,\n",
  "newString": "    images: context.images,\n    videos: context.videos,\n    pdfs: context.pdfs,\n    addImage: context.addImage,\n    addVideo: context.addVideo,\n    addPdf: context.addPdf,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [42:5] Object literal may only specify known properties, and 'pdfs' does not exist in type 'UseCommandScreenPopupManagerOptions'.
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_

**Updating popup manager options**

I need to update the `UseCommandScreenPopupManagerOptions` type, which involves wrapping the `usePopupManager` to include PDFs and the `addPdf` function. First, I'll locate the `useCommandScreenPopupManager.ts` file and then focus on the options type. This task seems straightforward, but I want to ensure I capture all required details for a seamless integration. Let's dig into the code and see how this can be implemented effectively!

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupManager.ts",
  "offset": 1,
  "limit": 120
}
```
**Output:**
```
<file>
00002| import { useCallback, useEffect, useState } from 'react'
00003| 
00004| import { loadCliConfig } from '../../../../config'
00005| 
00006| import { usePopupManager } from '../../../hooks/usePopupManager'
00007| import type { NotifyOptions } from '../../../notifier'
00008| import { useTheme } from '../../../theme/theme-provider'
00009| import type {
00010|   HistoryEntry,
00011|   ModelOption,
00012|   PopupState,
00013|   ResumeMode,
00014|   ResumeSourceKind,
00015| } from '../../../types'
00016| 
00017| const DEFAULT_TEST_FILE = 'prompt-tests.yaml'
00018| 
00019| type PushHistory = (
00020|   content: string,
00021|   kind?: HistoryEntry['kind'],
00022|   format?: HistoryEntry['format'],
00023| ) => void
00024| 
00025| type UseCommandScreenPopupManagerOptions = {
00026|   currentModel: ModelOption['id']
00027|   currentTargetModel: ModelOption['id']
00028|   modelOptions: readonly ModelOption[]
00029|   smartContextEnabled: boolean
00030|   smartContextRoot: string | null
00031|   toggleSmartContext: () => void
00032|   setSmartRoot: (value: string) => void
00033|   urls: string[]
00034|   addUrl: (value: string) => void
00035|   images: string[]
00036|   videos: string[]
00037|   addImage: (value: string) => void
00038|   addVideo: (value: string) => void
00039|   lastTestFile: string | null
00040|   interactiveTransportPath?: string | undefined
00041|   isGenerating: boolean
00042|   lastUserIntentRef: import('react').MutableRefObject<string | null>
00043|   lastTypedIntentRef: import('react').MutableRefObject<string>
00044| 
00045|   pushHistoryProxy: PushHistory
00046|   notify: (message: string, options?: NotifyOptions) => void
00047|   setInputValue: (value: string | ((prev: string) => string)) => void
00048| 
00049|   runGeneration: (payload: {
00050|     intent?: string
00051|     intentFile?: string
00052|     resume?:
00053|       | { kind: 'history'; selector: string; mode: ResumeMode }
00054|       | { kind: 'file'; payloadPath: string; mode: ResumeMode }
00055|   }) => Promise<void>
00056|   runSeriesGeneration: (intent: string) => void
00057|   runTestsFromCommandProxy: (value: string) => void
00058| 
00059|   setCurrentModel: (value: ModelOption['id']) => void
00060|   setCurrentTargetModel: (value: ModelOption['id']) => void
00061|   setPolishModelId: (value: ModelOption['id'] | null) => void
00062|   setCopyEnabled: (value: boolean) => void
00063|   setChatGptEnabled: (value: boolean) => void
00064|   setJsonOutputEnabled: (value: boolean) => void
00065| 
00066|   intentFilePath: string
00067|   setIntentFilePath: (value: string) => void
00068| 
00069|   metaInstructions: string
00070|   setMetaInstructions: (value: string) => void
00071|   budgets: {
00072|     maxContextTokens: number | null
00073|     maxInputTokens: number | null
00074|     contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
00075|   }
00076|   setBudgets: (value: {
00077|     maxContextTokens: number | null
00078|     maxInputTokens: number | null
00079|     contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
00080|   }) => void
00081| 
00082|   polishModelId: ModelOption['id'] | null
00083|   copyEnabled: boolean
00084|   chatGptEnabled: boolean
00085|   jsonOutputEnabled: boolean
00086| }
00087| 
00088| export type UseCommandScreenPopupManagerResult = {
00089|   popupState: PopupState
00090|   setPopupState: import('react').Dispatch<import('react').SetStateAction<PopupState>>
00091|   actions: ReturnType<typeof usePopupManager>['actions']
00092|   isPopupOpen: boolean
00093| }
00094| 
00095| export const useCommandScreenPopupManager = ({
00096|   currentModel,
00097|   currentTargetModel,
00098|   modelOptions,
00099|   smartContextEnabled,
00100|   smartContextRoot,
00101|   toggleSmartContext,
00102|   setSmartRoot,
00103|   urls,
00104|   addUrl,
00105|   images,
00106|   videos,
00107|   addImage,
00108|   addVideo,
00109|   lastTestFile,
00110|   interactiveTransportPath,
00111|   isGenerating,
00112|   lastUserIntentRef,
00113|   lastTypedIntentRef,
00114|   pushHistoryProxy,
00115|   notify,
00116|   setInputValue,
00117|   runGeneration,
00118|   runSeriesGeneration,
00119|   runTestsFromCommandProxy,
00120|   setCurrentModel,
00121|   setCurrentTargetModel,

(File has more lines. Use 'offset' parameter to read beyond line 121)
</file>
```
```

---

## Assistant

_Thinking:_

I'm considering how to extend the options with PDFs and the addPdf function. I need to ensure that I integrate it properly with the use of PopupManager. It seems like I have to make some edits accordingly to get everything working. Alright, let's take it step by step to ensure I get it right! I want to make sure the integration is smooth and everything functions as intended.

```
Tool: grep

**Input:**
```json
{
  "pattern": "videos: string\\[\\]|addVideo",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupManager.ts"
}
```
**Output:**
```
Found 4 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupManager.ts:
  Line 36:   videos: string[]
  Line 39:   addVideo: (value: string) => void
  Line 110:   addVideo,
  Line 219:     addVideo,
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupManager.ts",
  "oldString": "  images: string[]\n  videos: string[]\n  addImage: (value: string) => void\n  addVideo: (value: string) => void\n",
  "newString": "  images: string[]\n  videos: string[]\n  pdfs: string[]\n  addImage: (value: string) => void\n  addVideo: (value: string) => void\n  addPdf: (value: string) => void\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupManager.ts",
  "oldString": "  images,\n  videos,\n  addImage,\n  addVideo,\n",
  "newString": "  images,\n  videos,\n  pdfs,\n  addImage,\n  addVideo,\n  addPdf,\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupManager.ts",
  "oldString": "    images,\n    videos,\n    addImage,\n    addVideo,\n",
  "newString": "    images,\n    videos,\n    pdfs,\n    addImage,\n    addVideo,\n    addPdf,\n"
}
```
```

---

## Assistant

_Thinking:_

I need to revisit the code to fix the earlier issue with the useCommandScreenBindings, which was failing due to the type. With the updated options type, it should compile successfully now. I also need to make sure to include PDFs and the addPdf function in the options that I'm passing to useCommandScreenPopupManager. Alright, time to reapply these updates to useCommandScreenBindings to get everything running smoothly!

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "    images: context.images,\n    videos: context.videos,\n    addImage: context.addImage,\n    addVideo: context.addVideo,\n",
  "newString": "    images: context.images,\n    videos: context.videos,\n    pdfs: context.pdfs,\n    addImage: context.addImage,\n    addVideo: context.addVideo,\n    addPdf: context.addPdf,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [47:5] An object literal cannot have multiple properties with the same name.
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_



```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "offset": 24,
  "limit": 35
}
```
**Output:**
```
<file>
00025| ): UseCommandScreenBindingsResult => {
00026|   const { context, input, popup, history, generation } = options
00027| 
00028|   const popupManager = useCommandScreenPopupManager({
00029|     currentModel: generation.currentModel,
00030|     polishModelId: generation.polishModelId,
00031|     currentTargetModel: generation.currentTargetModel,
00032| 
00033|     modelOptions: generation.modelOptions,
00034|     smartContextEnabled: context.smartContextEnabled,
00035|     smartContextRoot: context.smartContextRoot,
00036|     toggleSmartContext: context.toggleSmartContext,
00037|     setSmartRoot: context.setSmartRoot,
00038|     urls: context.urls,
00039|     addUrl: context.addUrl,
00040|     images: context.images,
00041|     videos: context.videos,
00042|     pdfs: context.pdfs,
00043|     addImage: context.addImage,
00044|     addVideo: context.addVideo,
00045|     addPdf: context.addPdf,
00046| 
00047|     addPdf: context.addPdf,
00048|     lastTestFile: history.lastTestFile,
00049|     ...(context.interactiveTransportPath
00050|       ? { interactiveTransportPath: context.interactiveTransportPath }
00051|       : {}),
00052|     isGenerating: generation.isGenerating,
00053|     lastUserIntentRef: input.lastUserIntentRef,
00054|     lastTypedIntentRef: input.lastTypedIntentRef,
00055|     pushHistoryProxy: history.pushHistoryProxy,
00056|     notify: context.notify,
00057|     setInputValue: input.setInputValue,
00058|     runGeneration: generation.runGeneration,
00059|     runSeriesGeneration: generation.runSeriesGeneration,

(File has more lines. Use 'offset' parameter to read beyond line 59)
</file>
```
```

---

## Assistant

_Thinking:_

**Reviewing code for duplicates**

I noticed there's a duplicate line involving addPdf. I need to remove this duplication to tidy up the code. It makes sense to go in and edit it, ensuring everything runs smoothly without unnecessary repetition. Cleaning this up will help maintain clarity in the code, making it easier to read and manage. Let’s get that sorted!

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "    addImage: context.addImage,\n    addVideo: context.addVideo,\n    addPdf: context.addPdf,\n\n    addPdf: context.addPdf,\n    lastTestFile: history.lastTestFile,\n",
  "newString": "    addImage: context.addImage,\n    addVideo: context.addVideo,\n    addPdf: context.addPdf,\n    lastTestFile: history.lastTestFile,\n"
}
```
```

---

## Assistant

_Thinking:_

**Updating screen bindings**

I need to update the rest of the `useCommandScreenBindings` function. It's essential to include the context for the popup bindings, specifically for handling PDFs with `addPdf` and `removePdf` features. I should focus around the context section, near line 192. It seems like an important patch, and I want to ensure everything integrates smoothly. I should probably double-check the related areas as well to avoid any issues. Let's patch this up!

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "      context: {\n        droppedFilePath,\n        files: context.files,\n        urls: context.urls,\n        images: context.images,\n        videos: context.videos,\n        smartContextEnabled: context.smartContextEnabled,\n",
  "newString": "      context: {\n        droppedFilePath,\n        files: context.files,\n        urls: context.urls,\n        images: context.images,\n        videos: context.videos,\n        pdfs: context.pdfs,\n        smartContextEnabled: context.smartContextEnabled,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [200:7] Object literal may only specify known properties, and 'pdfs' does not exist in type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 14 more ...; reasoningPopupHeight: number; }'.
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_

**Locating update types**

I need to update the types for options.context in the useCommandScreenPopupBindingsOptions. It seems like this is coming from the useCommandScreenPopupBindings.types file, but I'm thinking that useCommandScreenPopupBindings.ts also uses options defined in UseCommandScreenPopupBindingsOptions. I should probably take a look at the top of the file to confirm this. It feels a bit complicated, but I’m curious to see how it's structured!

```
Tool: grep

**Input:**
```json
{
  "pattern": "export type UseCommandScreenPopupBindingsOptions|context: \\{",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Output:**
```
Found 4 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts:
  Line 32: export type UseCommandScreenPopupBindingsOptions = {
  Line 100:   context: {
  Line 160:     context: {
  Line 465:       context: {
```
```

---

## Assistant

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "offset": 20,
  "limit": 130
}
```
**Output:**
```
<file>
00021| import {
00022|   useCommandScreenHistoryIntentPopupBindings,
00023|   type UseCommandScreenHistoryIntentPopupBindingsOptions,
00024| } from './useCommandScreenHistoryIntentPopupBindings'
00025| import {
00026|   useCommandScreenSubmitBindings,
00027|   type UseCommandScreenSubmitBindingsOptions,
00028| } from './useCommandScreenSubmitBindings'
00029| 
00030| type SetPopupState = import('react').Dispatch<import('react').SetStateAction<PopupState>>
00031| 
00032| export type UseCommandScreenPopupBindingsOptions = {
00033|   input: {
00034|     value: string
00035|     setValue: (value: string | ((prev: string) => string)) => void
00036|     setPasteActive: (active: boolean) => void
00037| 
00038|     consumeSuppressedTextInputChange: () => boolean
00039|     suppressNextInput: () => void
00040|     updateLastTypedIntent: (next: string) => void
00041| 
00042|     intentFilePath: string
00043|     lastUserIntentRef: import('react').MutableRefObject<string | null>
00044|   }
00045| 
00046|   popup: {
00047|     state: PopupState
00048|     setState: SetPopupState
00049|     isOpen: boolean
00050|     helpOpen: boolean
00051|     close: () => void
00052| 
00053|     actions: {
00054|       handleCommandSelection: (commandId: CommandDescriptor['id'], argsRaw?: string) => void
00055|       handleModelPopupSubmit: (option: ModelOption | null | undefined) => void
00056|       applyToggleSelection: (field: 'copy' | 'chatgpt' | 'json', value: boolean) => void
00057|       handleIntentFileSubmit: (value: string) => void
00058|       handleResumeSubmit: () => void
00059|       handleExportSubmit: () => void
00060|       handleSeriesIntentSubmit: (value: string) => void
00061|       handleBudgetsSubmit: () => void
00062|     }
00063|   }
00064| 
00065|   menu: {
00066|     isActive: boolean
00067|     selectedCommandId: CommandDescriptor['id'] | null
00068|     argsRaw: string
00069|     isCommandMode: boolean
00070| 
00071|     actions: {
00072|       handleNewCommand: (argsRaw: string) => void
00073|       handleReuseCommand: () => void
00074|     }
00075|   }
00076| 
00077|   generation: {
00078|     isGenerating: boolean
00079|     isAwaitingRefinement: boolean
00080|     submitRefinement: (value: string) => void
00081|     runGeneration: (payload: {
00082|       intent?: string
00083|       intentFile?: string
00084|       resume?:
00085|         | { kind: 'history'; selector: string; mode: import('../../../types').ResumeMode }
00086|         | { kind: 'file'; payloadPath: string; mode: import('../../../types').ResumeMode }
00087|     }) => Promise<void>
00088|   }
00089| 
00090|   history: {
00091|     pushHistory: (
00092|       content: string,
00093|       kind?: HistoryEntry['kind'],
00094|       format?: HistoryEntry['format'],
00095|     ) => void
00096|     addCommandHistoryEntry: (value: string) => void
00097|     commandHistoryValues: string[]
00098|   }
00099| 
00100|   context: {
00101|     droppedFilePath: string | null
00102| 
00103|     files: string[]
00104|     urls: string[]
00105|     images: string[]
00106|     videos: string[]
00107| 
00108|     smartContextEnabled: boolean
00109|     smartContextRoot: string | null
00110| 
00111|     addFile: (value: string) => void
00112|     removeFile: (index: number) => void
00113|     addUrl: (value: string) => void
00114|     removeUrl: (index: number) => void
00115|     updateUrl: (index: number, value: string) => void
00116|     addImage: (value: string) => void
00117|     removeImage: (index: number) => void
00118|     addVideo: (value: string) => void
00119|     removeVideo: (index: number) => void
00120|     toggleSmartContext: () => void
00121|     setSmartRoot: (value: string) => void
00122| 
00123|     notify: (message: string) => void
00124| 
00125|     modelOptions: ModelOption[]
00126| 
00127|     lastReasoning: string | null
00128|     terminalColumns: number
00129|     reasoningPopupHeight: number
00130|   }
00131| }
00132| 
00133| export type UseCommandScreenPopupBindingsResult = {
00134|   input: {
00135|     tokenLabel: (token: string) => string | null
00136|     onChange: (next: string) => void
00137|   }
00138|   submit: {
00139|     onSubmit: (value: string) => void
00140|     onSeriesSubmit: (value: string) => void
00141|   }
00142|   popup: {
00143|     model: {
00144|       options: ModelOption[]
00145|       recentCount: number
00146|       selection: number
00147|       onQueryChange: (next: string) => void
00148|     }
00149|     history: {
00150|       items: string[]

(File has more lines. Use 'offset' parameter to read beyond line 150)
</file>
```
```

---

## Assistant

_Thinking:_



```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "oldString": "    images: string[]\n    videos: string[]\n",
  "newString": "    images: string[]\n    videos: string[]\n    pdfs: string[]\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "oldString": "    addVideo: (value: string) => void\n    removeVideo: (index: number) => void\n",
  "newString": "    addVideo: (value: string) => void\n    removeVideo: (index: number) => void\n    addPdf: (value: string) => void\n    removePdf: (index: number) => void\n"
}
```
```

---

## Assistant

_Thinking:_

I need to revisit the useCommandScreenBindings to include PDFs in the context that I'm passing. It seems like that’s the next step and should help with compiling everything properly. I'll make sure to apply the earlier edit now to ensure everything flows together smoothly. It’s a little bit of a process, but I'm focused on getting this done right! Let’s see how this goes.

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "        images: context.images,\n        videos: context.videos,\n        smartContextEnabled: context.smartContextEnabled,\n",
  "newString": "        images: context.images,\n        videos: context.videos,\n        pdfs: context.pdfs,\n        smartContextEnabled: context.smartContextEnabled,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [67:3] Argument expression expected.
ERROR [67:4] ',' expected.
ERROR [75:1] Declaration or statement expected.
ERROR [25:4] A function whose declared type is neither 'undefined', 'void', nor 'any' must return a value.
ERROR [47:7] Object literal may only specify known properties, and 'addFile' does not exist in type 'UseCommandScreenPopupManagerOptions'.
ERROR [61:7] No value exists in scope for the shorthand property 'notify'. Either declare one or provide an initializer.
</file_diagnostics>

```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "        addVideo: context.addVideo,\n        removeVideo: context.removeVideo,\n",
  "newString": "        addVideo: context.addVideo,\n        removeVideo: context.removeVideo,\n        addPdf: context.addPdf,\n        removePdf: context.removePdf,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [67:3] Argument expression expected.
ERROR [67:4] ',' expected.
ERROR [75:1] Declaration or statement expected.
ERROR [25:4] A function whose declared type is neither 'undefined', 'void', nor 'any' must return a value.
ERROR [47:7] Object literal may only specify known properties, and 'addFile' does not exist in type 'UseCommandScreenPopupManagerOptions'.
ERROR [61:7] No value exists in scope for the shorthand property 'notify'. Either declare one or provide an initializer.
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_

**Investigating parse errors**

It looks like the `useCommandScreenBindings` file has some parse errors, particularly around line 67. I wonder if these issues stem from a previously misapplied patch. To really get to the bottom of this, I’ll need to check out that section of the file to pinpoint the problem. It could lead to fixing these errors in a timely manner, which is important to ensure everything runs smoothly!

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "offset": 40,
  "limit": 50
}
```
**Output:**
```
<file>
00041|         videos: context.videos,
00042|         pdfs: context.pdfs,
00043|         smartContextEnabled: context.smartContextEnabled,
00044| 
00045| 
00046|       smartContextRoot: context.smartContextRoot,
00047|       addFile: context.addFile,
00048|       removeFile: context.removeFile,
00049|       addUrl: context.addUrl,
00050|       removeUrl: context.removeUrl,
00051|       updateUrl: context.updateUrl,
00052|       addImage: context.addImage,
00053|       removeImage: context.removeImage,
00054|         addVideo: context.addVideo,
00055|         removeVideo: context.removeVideo,
00056|         addPdf: context.addPdf,
00057|         removePdf: context.removePdf,
00058| 
00059|       toggleSmartContext: context.toggleSmartContext,
00060|       setSmartRoot: context.setSmartRoot,
00061|       notify,
00062|       modelOptions: generation.modelOptions,
00063|       lastReasoning: context.lastReasoning,
00064|       terminalColumns: input.terminalColumns,
00065|       reasoningPopupHeight: POPUP_HEIGHTS.reasoning,
00066|     },
00067|   })
00068| 
00069|   return {
00070|     popupManager,
00071|     shell,
00072|     bindings,
00073|     enhancedStatusChips,
00074|   }
00075| }
00076| 

(End of file - total 76 lines)
</file>
```
```

---

## Assistant

_Thinking:_



```
Tool: bash

**Input:**
```json
{
  "command": "git checkout -- src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "description": "Restore useCommandScreenBindings.ts from HEAD"
}
```
```

---

## Assistant

_Thinking:_

**Reviewing command screen bindings**

I need to reapply modifications to the useCommandScreenBindings. I should carefully include options for add pdfs, addPdf, and removePdf in the popupManager. It seems important to also pass in the context to those popup bindings for proper functionality. First, I'll read the relevant section of the restored file that involves the popupManager call to ensure I understand how everything is connected. This will help me apply the changes correctly!

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "offset": 24,
  "limit": 70
}
```
**Output:**
```
<file>
00025| ): UseCommandScreenBindingsResult => {
00026|   const { context, input, popup, history, generation } = options
00027| 
00028|   const popupManager = useCommandScreenPopupManager({
00029|     currentModel: generation.currentModel,
00030|     polishModelId: generation.polishModelId,
00031|     currentTargetModel: generation.currentTargetModel,
00032| 
00033|     modelOptions: generation.modelOptions,
00034|     smartContextEnabled: context.smartContextEnabled,
00035|     smartContextRoot: context.smartContextRoot,
00036|     toggleSmartContext: context.toggleSmartContext,
00037|     setSmartRoot: context.setSmartRoot,
00038|     urls: context.urls,
00039|     addUrl: context.addUrl,
00040|     images: context.images,
00041|     videos: context.videos,
00042|     addImage: context.addImage,
00043|     addVideo: context.addVideo,
00044|     lastTestFile: history.lastTestFile,
00045|     ...(context.interactiveTransportPath
00046|       ? { interactiveTransportPath: context.interactiveTransportPath }
00047|       : {}),
00048|     isGenerating: generation.isGenerating,
00049|     lastUserIntentRef: input.lastUserIntentRef,
00050|     lastTypedIntentRef: input.lastTypedIntentRef,
00051|     pushHistoryProxy: history.pushHistoryProxy,
00052|     notify: context.notify,
00053|     setInputValue: input.setInputValue,
00054|     runGeneration: generation.runGeneration,
00055|     runSeriesGeneration: generation.runSeriesGeneration,
00056|     runTestsFromCommandProxy: history.runTestsFromCommandProxy,
00057|     setCurrentModel: generation.selectModel,
00058|     setCurrentTargetModel: generation.selectTargetModel,
00059|     setPolishModelId: generation.selectPolishModel,
00060|     setCopyEnabled: input.setCopyEnabled,
00061|     setChatGptEnabled: input.setChatGptEnabled,
00062|     setJsonOutputEnabled: input.setJsonOutputEnabled,
00063|     intentFilePath: input.intentFilePath,
00064|     setIntentFilePath: input.setIntentFilePath,
00065|     metaInstructions: context.metaInstructions,
00066|     setMetaInstructions: context.setMetaInstructions,
00067|     budgets: {
00068|       maxContextTokens: context.maxContextTokens,
00069|       maxInputTokens: context.maxInputTokens,
00070|       contextOverflowStrategy: context.contextOverflowStrategy,
00071|     },
00072|     setBudgets: context.setBudgets,
00073|     copyEnabled: input.copyEnabled,
00074| 
00075|     chatGptEnabled: input.chatGptEnabled,
00076|     jsonOutputEnabled: input.jsonOutputEnabled,
00077|   })
00078| 
00079|   history.closeTestPopupRef.current = () => {
00080|     popupManager.setPopupState((prev) => (prev?.type === 'test' ? null : prev))
00081|   }
00082| 
00083|   useCommandScreenPopupVisibility({
00084|     isPopupOpen: popupManager.isPopupOpen,
00085|     onPopupVisibilityChange: popup.onPopupVisibilityChange,
00086|   })
00087| 
00088|   const pushHistory: PushHistory = useStableCallback((content, kind) => {
00089|     history.pushHistoryRef.current(content, kind)
00090|   })
00091| 
00092|   const droppedFilePath = useDroppedFilePath(input.inputValue)
00093| 
00094|   const shell = useCommandScreenShell({

(File has more lines. Use 'offset' parameter to read beyond line 94)
</file>
```
```

---

## Assistant

_Thinking:_

**Updating popup manager**

I need to add pdfs to the popupManager options since we've updated the useCommandScreenPopupManager. I'll edit the lines surrounding images and videos accordingly. Later, when building the context object for bindings, I must include pdfs and the add/remove functionality. For now, I’ll update the popupManager call: after videos, I'll make sure to add context.pdfs, and after addVideo, I’ll add addPdf. Let's proceed with that!

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "    images: context.images,\n    videos: context.videos,\n    addImage: context.addImage,\n    addVideo: context.addVideo,\n",
  "newString": "    images: context.images,\n    videos: context.videos,\n    pdfs: context.pdfs,\n    addImage: context.addImage,\n    addVideo: context.addVideo,\n    addPdf: context.addPdf,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [194:5] Type '{ droppedFilePath: UseDroppedFilePathResult; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; ... 15 more ...; reasoningPopupHeight: 18; }' is missing the following properties from type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; pdfs: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 16 more ...; reasoningPopupHeight: number; }': pdfs, addPdf, removePdf
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_

**Fixing context bindings**

I’m noticing some issues, like later binding context objects being missing, which is causing compile errors, but it seems like the edits were applied. I’ll need to address this later. For now, I should patch the context object for the bindings. Let's open up that area so I can make the necessary edits. It's a bit intricate, but I’m determined to get it sorted out!

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "offset": 180,
  "limit": 50
}
```
**Output:**
```
<file>
00181|       },
00182|     },
00183|     generation: {
00184|       isGenerating: generation.isGenerating,
00185|       isAwaitingRefinement: generation.isAwaitingRefinement,
00186|       submitRefinement: generation.submitRefinement,
00187|       runGeneration: generation.runGeneration,
00188|     },
00189|     history: {
00190|       pushHistory,
00191|       addCommandHistoryEntry: history.addCommandHistoryEntry,
00192|       commandHistoryValues: history.commandHistoryValues,
00193|     },
00194|     context: {
00195|       droppedFilePath,
00196|       files: context.files,
00197|       urls: context.urls,
00198|       images: context.images,
00199|       videos: context.videos,
00200|       smartContextEnabled: context.smartContextEnabled,
00201|       smartContextRoot: context.smartContextRoot,
00202|       addFile: context.addFile,
00203|       removeFile: context.removeFile,
00204|       addUrl: context.addUrl,
00205|       removeUrl: context.removeUrl,
00206|       updateUrl: context.updateUrl,
00207|       addImage: context.addImage,
00208|       removeImage: context.removeImage,
00209|       addVideo: context.addVideo,
00210|       removeVideo: context.removeVideo,
00211|       toggleSmartContext: context.toggleSmartContext,
00212|       setSmartRoot: context.setSmartRoot,
00213|       notify,
00214|       modelOptions: generation.modelOptions,
00215|       lastReasoning: context.lastReasoning,
00216|       terminalColumns: input.terminalColumns,
00217|       reasoningPopupHeight: POPUP_HEIGHTS.reasoning,
00218|     },
00219|   })
00220| 
00221|   return {
00222|     popupManager,
00223|     shell,
00224|     bindings,
00225|     enhancedStatusChips,
00226|   }
00227| }
00228| 

(End of file - total 228 lines)
</file>
```
```

---

## Assistant

_Thinking:_



```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "      images: context.images,\n      videos: context.videos,\n      smartContextEnabled: context.smartContextEnabled,\n",
  "newString": "      images: context.images,\n      videos: context.videos,\n      pdfs: context.pdfs,\n      smartContextEnabled: context.smartContextEnabled,\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenBindings.ts",
  "oldString": "      addVideo: context.addVideo,\n      removeVideo: context.removeVideo,\n",
  "newString": "      addVideo: context.addVideo,\n      removeVideo: context.removeVideo,\n      addPdf: context.addPdf,\n      removePdf: context.removePdf,\n"
}
```
```

---

## Assistant

_Thinking:_



```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "offset": 230,
  "limit": 60
}
```
**Output:**
```
<file>
00231|     inputValue: options.input.value,
00232|     popupState: options.popup.state,
00233|     helpOpen: options.popup.helpOpen,
00234|     setInputValue: options.input.setValue,
00235|     setPasteActive: options.input.setPasteActive,
00236|     consumeSuppressedTextInputChange: options.input.consumeSuppressedTextInputChange,
00237|     suppressNextInput: options.input.suppressNextInput,
00238|     updateLastTypedIntent: options.input.updateLastTypedIntent,
00239|   } satisfies UseCommandScreenPasteBindingsOptions)
00240| 
00241|   const context = useCommandScreenContextPopupBindings({
00242|     inputValue: options.input.value,
00243|     popupState: options.popup.state,
00244|     helpOpen: options.popup.helpOpen,
00245|     isPopupOpen: options.popup.isOpen,
00246|     isCommandMode: options.menu.isCommandMode,
00247|     isCommandMenuActive: options.menu.isActive,
00248|     isGenerating: options.generation.isGenerating,
00249|     droppedFilePath: options.context.droppedFilePath,
00250|     files: options.context.files,
00251|     urls: options.context.urls,
00252|     images: options.context.images,
00253|     videos: options.context.videos,
00254|     pdfs: options.context.pdfs,
00255|     smartContextEnabled: options.context.smartContextEnabled,
00256|     smartContextRoot: options.context.smartContextRoot,
00257|     addFile: options.context.addFile,
00258|     removeFile: options.context.removeFile,
00259|     addUrl: options.context.addUrl,
00260|     removeUrl: options.context.removeUrl,
00261|     updateUrl: options.context.updateUrl,
00262|     addImage: options.context.addImage,
00263|     removeImage: options.context.removeImage,
00264|     addVideo: options.context.addVideo,
00265|     removeVideo: options.context.removeVideo,
00266|     addPdf: options.context.addPdf,
00267|     removePdf: options.context.removePdf,
00268|     toggleSmartContext: options.context.toggleSmartContext,
00269|     setSmartRoot: options.context.setSmartRoot,
00270|     setInputValue: options.input.setValue,
00271|     setPopupState: options.popup.setState,
00272|     suppressNextInput: options.input.suppressNextInput,
00273|     notify,
00274|     pushHistory,
00275|     addCommandHistoryEntry: options.history.addCommandHistoryEntry,
00276|     handleCommandSelection: options.popup.actions.handleCommandSelection,
00277|     consumeSuppressedTextInputChange: options.input.consumeSuppressedTextInputChange,
00278|   } satisfies UseCommandScreenContextPopupBindingsOptions)
00279| 
00280|   const historyAndIntent = useCommandScreenHistoryIntentPopupBindings({
00281|     popupState: options.popup.state,
00282|     setPopupState: options.popup.setState,
00283|     closePopup: options.popup.close,
00284|     setInputValue: options.input.setValue,
00285|     consumeSuppressedTextInputChange: options.input.consumeSuppressedTextInputChange,
00286|     suppressNextInput: options.input.suppressNextInput,
00287|     commandHistoryValues: options.history.commandHistoryValues,
00288|   } satisfies UseCommandScreenHistoryIntentPopupBindingsOptions)
00289| 
00290|   const { modelPopupOptions, modelPopupRecentCount, modelPopupSelection } = useModelPopupData({

(File has more lines. Use 'offset' parameter to read beyond line 290)
</file>
```
```

---

## Assistant

_Thinking:_

I'm checking if everything looks good with the recent addition of the PDF config into the usePopupKeyboardShortcuts call. I need to verify where exactly it's being passed. It's important to be thorough at this stage to ensure that the integration is seamless and functions as intended. Let’s double-check the relevant areas and confirm that there are no issues with the implementation. I'll take my time with this!

```
Tool: grep

**Input:**
```json
{
  "pattern": "pdf:\\s*\\{|video:\\s*\\{",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Output:**
```
Found 4 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts:
  Line 185:       video: {
  Line 363:     video: {
  Line 370:     pdf: {
  Line 490:         video: {
```
```

---

## Assistant

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "offset": 340,
  "limit": 60
}
```
**Output:**
```
<file>
00341|       onSubmit: options.popup.actions.handleBudgetsSubmit,
00342|     },
00343| 
00344|     file: {
00345|       items: options.context.files,
00346|       suggestions: context.filePopupSuggestions,
00347|       onAdd: context.onAddFile,
00348|       onRemove: context.onRemoveFile,
00349|     },
00350| 
00351|     url: {
00352|       items: options.context.urls,
00353|       onRemove: context.onRemoveUrl,
00354|     },
00355| 
00356|     image: {
00357|       items: options.context.images,
00358|       suggestions: context.imagePopupSuggestions,
00359|       onAdd: context.onAddImage,
00360|       onRemove: context.onRemoveImage,
00361|     },
00362| 
00363|     video: {
00364|       items: options.context.videos,
00365|       suggestions: context.videoPopupSuggestions,
00366|       onAdd: context.onAddVideo,
00367|       onRemove: context.onRemoveVideo,
00368|     },
00369| 
00370|     pdf: {
00371|       items: options.context.pdfs,
00372|       suggestions: context.pdfPopupSuggestions,
00373|       onAdd: context.onAddPdf,
00374|       onRemove: context.onRemovePdf,
00375|     },
00376| 
00377|     history: {
00378|       items: historyAndIntent.history.historyPopupItems,
00379|     },
00380| 
00381|     resume: {
00382|       onSubmit: options.popup.actions.handleResumeSubmit,
00383|     },
00384| 
00385|     export: {
00386|       onSubmit: options.popup.actions.handleExportSubmit,
00387|     },
00388| 
00389|     smart: {
00390|       suggestions: context.smartPopupSuggestions,
00391|       contextRoot: options.context.smartContextRoot,
00392|       onRootSubmit: context.onSmartRootSubmit,
00393|     },
00394| 
00395|     intent: {
00396|       suggestions: historyAndIntent.intent.intentPopupSuggestions,
00397|       onFileSubmit: options.popup.actions.handleIntentFileSubmit,
00398|     },
00399| 
00400|     reasoning: {

(File has more lines. Use 'offset' parameter to read beyond line 400)
</file>
```
```

---

## Assistant

_Thinking:_

**Updating PopupArea.tsx**

I’m working on updating PopupArea.tsx to render PDFs and accept props. First, I’ll add props fields similar to what’s used for images and videos. Then, I need to include a function to render the PDF popup. I also want to update the switch at the bottom to handle the 'pdf' case. I noticed there was an earlier diagnostic line at 507, so I’ll make sure to patch that as well. Let’s dig deeper around the video popup and the switch.

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/components/PopupArea.tsx",
  "offset": 240,
  "limit": 120
}
```
**Output:**
```
<file>
00241|   return <ListPopup {...viewModel} />
00242| }
00243| 
00244| const renderVideoPopup = (props: PopupAreaProps, popupState: PopupStateFor<'video'>) => {
00245|   const viewModel = {
00246|     title: 'Videos',
00247|     placeholder: 'path/to/video.mp4',
00248|     draft: popupState.draft,
00249|     items: props.videos,
00250|     selectedIndex: popupState.selectionIndex,
00251|     selectedFocused: popupState.selectedFocused,
00252|     layout: 'selected-first',
00253|     emptyLabel: 'No videos attached',
00254|     instructions:
00255|       "Enter add · ↑/↓ focus list · Del/Backspace remove · Tab suggestions · Esc close\nfzf: ^start $end 'exact",
00256|     suggestedItems: props.videoPopupSuggestions,
00257|     suggestedSelectionIndex: props.videoPopupSuggestionSelectionIndex,
00258|     suggestedFocused: props.videoPopupSuggestionsFocused,
00259|     maxHeight: props.overlayHeight,
00260|     onDraftChange: props.onVideoPopupDraftChange,
00261|     onSubmitDraft: props.onAddVideo,
00262|   } satisfies ComponentProps<typeof ListPopup>
00263| 
00264|   return <ListPopup {...viewModel} />
00265| }
00266| 
00267| const renderHistoryPopup = (props: PopupAreaProps, popupState: PopupStateFor<'history'>) => {
00268|   const viewModel = {
00269|     title: 'History',
00270|     placeholder: 'Type to filter…',
00271|     draft: popupState.draft,
00272|     items: props.historyPopupItems,
00273|     selectedIndex: popupState.selectionIndex,
00274|     selectedFocused: true,
00275|     emptyLabel: 'No history entries',
00276|     instructions: '↑/↓ select · Enter paste · Esc close',
00277|     onDraftChange: props.onHistoryPopupDraftChange,
00278|     onSubmitDraft: props.onHistoryPopupSubmit,
00279|   } satisfies ComponentProps<typeof ListPopup>
00280| 
00281|   return <ListPopup {...viewModel} />
00282| }
00283| 
00284| const renderResumePopup = (props: PopupAreaProps, popupState: PopupStateFor<'resume'>) => {
00285|   const viewModel = {
00286|     selectionIndex: popupState.selectionIndex,
00287|     sourceKind: popupState.sourceKind,
00288|     mode: popupState.mode,
00289|     historyItems: popupState.historyItems,
00290|     historySelectionIndex: popupState.historySelectionIndex,
00291|     historyErrorMessage: popupState.historyErrorMessage,
00292|     payloadPathDraft: popupState.payloadPathDraft,
00293|     suggestedItems: popupState.suggestedItems,
00294|     suggestedSelectionIndex: popupState.suggestedSelectionIndex,
00295|     suggestedFocused: popupState.suggestedFocused,
00296|     onPayloadPathChange: props.onResumePayloadPathDraftChange,
00297|     onSubmit: props.onResumeSubmit,
00298|   } satisfies ComponentProps<typeof ResumePopup>
00299| 
00300|   return <ResumePopup {...viewModel} />
00301| }
00302| 
00303| const renderExportPopup = (props: PopupAreaProps, popupState: PopupStateFor<'export'>) => {
00304|   const viewModel = {
00305|     selectionIndex: popupState.selectionIndex,
00306|     format: popupState.format,
00307|     outPathDraft: popupState.outPathDraft,
00308|     historyItems: popupState.historyItems,
00309|     historySelectionIndex: popupState.historySelectionIndex,
00310|     historyErrorMessage: popupState.historyErrorMessage,
00311|     onOutPathChange: props.onExportOutPathDraftChange,
00312|     onSubmit: props.onExportSubmit,
00313|   } satisfies ComponentProps<typeof ExportPopup>
00314| 
00315|   return <ExportPopup {...viewModel} />
00316| }
00317| 
00318| const renderIntentPopup = (props: PopupAreaProps, popupState: PopupStateFor<'intent'>) => {
00319|   const viewModel = {
00320|     draft: popupState.draft,
00321|     suggestions: props.intentPopupSuggestions,
00322|     suggestedSelectionIndex: props.intentPopupSuggestionSelectionIndex,
00323|     suggestedFocused: props.intentPopupSuggestionsFocused,
00324|     maxHeight: props.overlayHeight,
00325|     onDraftChange: props.onIntentPopupDraftChange,
00326|     onSubmitDraft: props.onIntentFileSubmit,
00327|   } satisfies ComponentProps<typeof IntentFilePopup>
00328| 
00329|   return <IntentFilePopup {...viewModel} />
00330| }
00331| 
00332| const renderSmartPopup = (props: PopupAreaProps, popupState: PopupStateFor<'smart'>) => {
00333|   const viewModel = {
00334|     savedRoot: props.smartContextRoot,
00335|     draft: popupState.draft,
00336|     suggestedItems: props.smartPopupSuggestions,
00337|     suggestedSelectionIndex: props.smartPopupSuggestionSelectionIndex,
00338|     suggestedFocused: props.smartPopupSuggestionsFocused,
00339|     maxHeight: props.overlayHeight,
00340|     onDraftChange: props.onSmartPopupDraftChange,
00341|     onSubmitRoot: props.onSmartRootSubmit,
00342|   } satisfies ComponentProps<typeof SmartPopup>
00343| 
00344|   return <SmartPopup {...viewModel} />
00345| }
00346| 
00347| const renderInstructionsPopup = (
00348|   props: PopupAreaProps,
00349|   popupState: PopupStateFor<'instructions'>,
00350| ) => {
00351|   const viewModel = {
00352|     draft: popupState.draft,
00353|     onDraftChange: props.onInstructionsDraftChange,
00354|     onSubmitDraft: props.onInstructionsSubmit,
00355|   } satisfies ComponentProps<typeof InstructionsPopup>
00356| 
00357|   return <InstructionsPopup {...viewModel} />
00358| }
00359| 
00360| const renderSeriesPopup = (props: PopupAreaProps, popupState: PopupStateFor<'series'>) => {

(File has more lines. Use 'offset' parameter to read beyond line 360)
</file>
```
```

---

## Assistant

_Thinking:_

**Updating popup structure**

I need to add a renderPdfPopup after the video popup. It seems important to define the props early on, especially for PDF items and any related suggestions or callbacks. I'll go ahead and edit the PopupAreaProps around line 60. That feels like a solid plan, and I’ll review the surrounding code to ensure everything aligns properly. Let's patch things up!

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/components/PopupArea.tsx",
  "offset": 50,
  "limit": 110
}
```
**Output:**
```
<file>
00051|   filePopupSuggestionsFocused: boolean
00052|   onFilePopupDraftChange: (next: string) => void
00053|   onAddFile: (value: string) => void
00054| 
00055|   // URL popup
00056|   urls: string[]
00057|   onUrlPopupDraftChange: (next: string) => void
00058|   onAddUrl: (value: string) => void
00059| 
00060|   // Image popup
00061|   images: string[]
00062|   imagePopupSuggestions: string[]
00063|   imagePopupSuggestionSelectionIndex: number
00064|   imagePopupSuggestionsFocused: boolean
00065|   onImagePopupDraftChange: (next: string) => void
00066|   onAddImage: (value: string) => void
00067| 
00068|   // Video popup
00069|   videos: string[]
00070|   videoPopupSuggestions: string[]
00071|   videoPopupSuggestionSelectionIndex: number
00072|   videoPopupSuggestionsFocused: boolean
00073|   onVideoPopupDraftChange: (next: string) => void
00074|   onAddVideo: (value: string) => void
00075| 
00076|   // History popup
00077|   historyPopupItems: string[]
00078|   onHistoryPopupDraftChange: (next: string) => void
00079|   onHistoryPopupSubmit: (value: string) => void
00080| 
00081|   // Resume popup
00082|   onResumePayloadPathDraftChange: (next: string) => void
00083|   onResumeSubmit: () => void
00084| 
00085|   // Export popup
00086|   onExportOutPathDraftChange: (next: string) => void
00087|   onExportSubmit: () => void
00088| 
00089|   // Intent popup
00090|   intentPopupSuggestions: string[]
00091|   intentPopupSuggestionSelectionIndex: number
00092|   intentPopupSuggestionsFocused: boolean
00093|   onIntentPopupDraftChange: (next: string) => void
00094|   onIntentFileSubmit: (value: string) => void
00095| 
00096|   // Instructions
00097|   onInstructionsDraftChange: (next: string) => void
00098|   onInstructionsSubmit: (value: string) => void
00099| 
00100|   // Series
00101|   isGenerating: boolean
00102|   onSeriesDraftChange: (next: string) => void
00103|   onSeriesSubmit: (value: string) => void
00104| 
00105|   // Test
00106|   isTestCommandRunning: boolean
00107|   onTestDraftChange: (next: string) => void
00108|   onTestSubmit: (value: string) => void
00109| 
00110|   // Tokens
00111|   tokenUsageRun: TokenUsageRun | null
00112|   tokenUsageBreakdown: TokenUsageBreakdown | null
00113|   maxContextTokens: number | null
00114|   maxInputTokens: number | null
00115|   contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
00116|   latestContextOverflow:
00117|     | import('../../../generation-pipeline-reducer').ContextOverflowDetails
00118|     | null
00119| 
00120|   // Budgets
00121|   onBudgetsMaxContextTokensDraftChange: (next: string) => void
00122|   onBudgetsMaxInputTokensDraftChange: (next: string) => void
00123|   onBudgetsSubmit: () => void
00124| 
00125|   // Settings
00126|   statusChips: string[]
00127| 
00128|   // Reasoning
00129|   reasoningPopupLines: HistoryEntry[]
00130|   reasoningPopupVisibleRows: number
00131| 
00132|   // Smart context
00133|   smartContextEnabled: boolean
00134|   smartContextRoot: string | null
00135|   smartPopupSuggestions: string[]
00136|   smartPopupSuggestionSelectionIndex: number
00137|   smartPopupSuggestionsFocused: boolean
00138|   onSmartPopupDraftChange: (next: string) => void
00139|   onSmartRootSubmit: (value: string) => void
00140| }
00141| 
00142| type NonNullPopupState = Exclude<PopupState, null>
00143| 
00144| type PopupStateFor<T extends NonNullPopupState['type']> = Extract<NonNullPopupState, { type: T }>
00145| 
00146| const renderModelPopup = (props: PopupAreaProps, popupState: PopupStateFor<'model'>) => {
00147|   const title =
00148|     popupState.kind === 'target'
00149|       ? 'Select target model'
00150|       : popupState.kind === 'polish'
00151|         ? 'Select polish model'
00152|         : 'Select model'
00153| 
00154|   const viewModel = {
00155|     title,
00156|     query: popupState.query,
00157|     options: props.modelPopupOptions,
00158|     selectedIndex: props.modelPopupSelection,
00159|     recentCount: props.modelPopupRecentCount,
00160|     maxHeight: props.overlayHeight,

(File has more lines. Use 'offset' parameter to read beyond line 160)
</file>
```
```

---

## Assistant

_Thinking:_



```
Tool: todowrite

**Input:**
```json
{
  "todos": [
    {
      "content": "Add PDF state to TUI context",
      "status": "completed",
      "priority": "high",
      "id": "1"
    },
    {
      "content": "Add PDF popup UI and commands",
      "status": "in_progress",
      "priority": "high",
      "id": "2"
    },
    {
      "content": "Wire TUI generation pipeline PDFs",
      "status": "pending",
      "priority": "high",
      "id": "3"
    },
    {
      "content": "Implement OpenAI PDF text extraction",
      "status": "pending",
      "priority": "high",
      "id": "4"
    },
    {
      "content": "Update tests for TUI and OpenAI",
      "status": "pending",
      "priority": "high",
      "id": "5"
    },
    {
      "content": "Run typecheck/tests/build",
      "status": "pending",
      "priority": "high",
      "id": "6"
    }
  ]
}
```
**Output:**
```
[
  {
    "content": "Add PDF state to TUI context",
    "status": "completed",
    "priority": "high",
    "id": "1"
  },
  {
    "content": "Add PDF popup UI and commands",
    "status": "in_progress",
    "priority": "high",
    "id": "2"
  },
  {
    "content": "Wire TUI generation pipeline PDFs",
    "status": "pending",
    "priority": "high",
    "id": "3"
  },
  {
    "content": "Implement OpenAI PDF text extraction",
    "status": "pending",
    "priority": "high",
    "id": "4"
  },
  {
    "content": "Update tests for TUI and OpenAI",
    "status": "pending",
    "priority": "high",
    "id": "5"
  },
  {
    "content": "Run typecheck/tests/build",
    "status": "pending",
    "priority": "high",
    "id": "6"
  }
]
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/components/PopupArea.tsx",
  "oldString": "  // Video popup\n  videos: string[]\n  videoPopupSuggestions: string[]\n  videoPopupSuggestionSelectionIndex: number\n  videoPopupSuggestionsFocused: boolean\n  onVideoPopupDraftChange: (next: string) => void\n  onAddVideo: (value: string) => void\n",
  "newString": "  // Video popup\n  videos: string[]\n  videoPopupSuggestions: string[]\n  videoPopupSuggestionSelectionIndex: number\n  videoPopupSuggestionsFocused: boolean\n  onVideoPopupDraftChange: (next: string) => void\n  onAddVideo: (value: string) => void\n\n  // PDF popup\n  pdfs: string[]\n  pdfPopupSuggestions: string[]\n  pdfPopupSuggestionSelectionIndex: number\n  pdfPopupSuggestionsFocused: boolean\n  onPdfPopupDraftChange: (next: string) => void\n  onAddPdf: (value: string) => void\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/components/PopupArea.tsx",
  "oldString": "const renderVideoPopup = (props: PopupAreaProps, popupState: PopupStateFor<'video'>) => {\n  const viewModel = {\n    title: 'Videos',\n    placeholder: 'path/to/video.mp4',\n    draft: popupState.draft,\n    items: props.videos,\n    selectedIndex: popupState.selectionIndex,\n    selectedFocused: popupState.selectedFocused,\n    layout: 'selected-first',\n    emptyLabel: 'No videos attached',\n    instructions:\n      \"Enter add · ↑/↓ focus list · Del/Backspace remove · Tab suggestions · Esc close\\nfzf: ^start $end 'exact\",\n    suggestedItems: props.videoPopupSuggestions,\n    suggestedSelectionIndex: props.videoPopupSuggestionSelectionIndex,\n    suggestedFocused: props.videoPopupSuggestionsFocused,\n    maxHeight: props.overlayHeight,\n    onDraftChange: props.onVideoPopupDraftChange,\n    onSubmitDraft: props.onAddVideo,\n  } satisfies ComponentProps<typeof ListPopup>\n\n  return <ListPopup {...viewModel} />\n}\n",
  "newString": "const renderVideoPopup = (props: PopupAreaProps, popupState: PopupStateFor<'video'>) => {\n  const viewModel = {\n    title: 'Videos',\n    placeholder: 'path/to/video.mp4',\n    draft: popupState.draft,\n    items: props.videos,\n    selectedIndex: popupState.selectionIndex,\n    selectedFocused: popupState.selectedFocused,\n    layout: 'selected-first',\n    emptyLabel: 'No videos attached',\n    instructions:\n      \"Enter add · ↑/↓ focus list · Del/Backspace remove · Tab suggestions · Esc close\\nfzf: ^start $end 'exact\",\n    suggestedItems: props.videoPopupSuggestions,\n    suggestedSelectionIndex: props.videoPopupSuggestionSelectionIndex,\n    suggestedFocused: props.videoPopupSuggestionsFocused,\n    maxHeight: props.overlayHeight,\n    onDraftChange: props.onVideoPopupDraftChange,\n    onSubmitDraft: props.onAddVideo,\n  } satisfies ComponentProps<typeof ListPopup>\n\n  return <ListPopup {...viewModel} />\n}\n\nconst renderPdfPopup = (props: PopupAreaProps, popupState: PopupStateFor<'pdf'>) => {\n  const viewModel = {\n    title: 'PDFs',\n    placeholder: 'path/to/document.pdf',\n    draft: popupState.draft,\n    items: props.pdfs,\n    selectedIndex: popupState.selectionIndex,\n    selectedFocused: popupState.selectedFocused,\n    layout: 'selected-first',\n    emptyLabel: 'No PDFs attached',\n    instructions:\n      \"Enter add · ↑/↓ focus list · Del/Backspace remove · Tab suggestions · Esc close\\nfzf: ^start $end 'exact\",\n    suggestedItems: props.pdfPopupSuggestions,\n    suggestedSelectionIndex: props.pdfPopupSuggestionSelectionIndex,\n    suggestedFocused: props.pdfPopupSuggestionsFocused,\n    maxHeight: props.overlayHeight,\n    onDraftChange: props.onPdfPopupDraftChange,\n    onSubmitDraft: props.onAddPdf,\n  } satisfies ComponentProps<typeof ListPopup>\n\n  return <ListPopup {...viewModel} />\n}\n"
}
```
```

```
Tool: grep

**Input:**
```json
{
  "pattern": "case 'video'\\:",
  "include": "src/tui/screens/command/components/PopupArea.tsx"
}
```
**Output:**
```
Found 1 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/components/PopupArea.tsx:
  Line 477:     case 'video':
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/components/PopupArea.tsx",
  "oldString": "    case 'video':\n      return renderVideoPopup(props, popupState)\n",
  "newString": "    case 'video':\n      return renderVideoPopup(props, popupState)\n\n    case 'pdf':\n      return renderPdfPopup(props, popupState)\n"
}
```
```

---

## Assistant

_Thinking:_



```
Tool: bash

**Input:**
```json
{
  "command": "npm run typecheck",
  "description": "Run TypeScript typecheck"
}
```
**Output:**
```

> @perceptron/prompt-maker-cli@0.1.0 typecheck
> tsc -p tsconfig.json --noEmit

src/__tests__/tui/command-screen-popup-bindings-contract.test.ts(239,7): error TS2739: Type '{ droppedFilePath: null; files: never[]; urls: never[]; images: never[]; videos: never[]; smartContextEnabled: false; smartContextRoot: null; addFile: Mock<any, any, any>; removeFile: Mock<any, any, any>; ... 13 more ...; reasoningPopupHeight: number; }' is missing the following properties from type '{ droppedFilePath: string | null; files: string[]; urls: string[]; images: string[]; videos: string[]; pdfs: string[]; smartContextEnabled: boolean; smartContextRoot: string | null; addFile: (value: string) => void; ... 16 more ...; reasoningPopupHeight: number; }': pdfs, addPdf, removePdf
src/__tests__/tui/context-popup-glue-contract.test.ts(31,11): error TS2739: Type '{ inputValue: string; popupState: null; helpOpen: false; isPopupOpen: false; isCommandMode: false; isCommandMenuActive: false; isGenerating: false; droppedFilePath: null; files: never[]; urls: never[]; ... 23 more ...; isFilePath: Mock<...>; }' is missing the following properties from type 'UseContextPopupGlueOptions': pdfs, addPdf, removePdf
src/__tests__/tui/popup-manager-command-mapping.test.ts(8,9): error TS2741: Property 'pdfs' is missing in type '{ copyEnabled: false; chatGptEnabled: false; jsonOutputEnabled: false; interactiveTransportPath: undefined; urls: never[]; images: never[]; videos: never[]; smartContextEnabled: false; smartContextRoot: null; }' but required in type 'CommandMappingContext'.
src/__tests__/usePopupManager.test.ts(48,9): error TS2739: Type '{ currentModel: string; currentTargetModel: string; modelOptions: ModelOption[]; activeThemeName: string; themeMode: "dark"; themes: { name: string; label: string; }[]; smartContextEnabled: false; ... 42 more ...; setExportDefaults: Mock<...>; }' is missing the following properties from type 'UsePopupManagerOptions': pdfs, addPdf
src/tui/screens/command/hooks/useCommandScreenController.ts(115,9): error TS2739: Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
src/tui/screens/command/hooks/useCommandScreenViewModel.ts(187,11): error TS2740: Type '{ reasoningPopupLines: HistoryEntry[]; reasoningPopupVisibleRows: number; statusChips: string[]; onBudgetsMaxContextTokensDraftChange: (next: string) => void; onBudgetsMaxInputTokensDraftChange: (next: string) => void; ... 63 more ...; overlayHeight: number; }' is missing the following properties from type 'PopupAreaProps': pdfs, pdfPopupSuggestions, pdfPopupSuggestionSelectionIndex, pdfPopupSuggestionsFocused, and 2 more.

```
```

---

## Assistant

_Thinking:_



```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/command-screen-popup-bindings-contract.test.ts",
  "offset": 220,
  "limit": 40
}
```
**Output:**
```
<file>
00221|         argsRaw: '',
00222|         isCommandMode: false,
00223|         actions: {
00224|           handleNewCommand: jest.fn(),
00225|           handleReuseCommand: jest.fn(),
00226|         },
00227|       },
00228|       generation: {
00229|         isGenerating: false,
00230|         isAwaitingRefinement: false,
00231|         submitRefinement: jest.fn(),
00232|         runGeneration: jest.fn(async () => undefined),
00233|       },
00234|       history: {
00235|         pushHistory: jest.fn(),
00236|         addCommandHistoryEntry: jest.fn(),
00237|         commandHistoryValues: [],
00238|       },
00239|       context: {
00240|         droppedFilePath: null,
00241|         files: [],
00242|         urls: [],
00243|         images: [],
00244|         videos: [],
00245|         smartContextEnabled: false,
00246|         smartContextRoot: null,
00247|         addFile: jest.fn(),
00248|         removeFile: jest.fn(),
00249|         addUrl: jest.fn(),
00250|         removeUrl: jest.fn(),
00251|         updateUrl: jest.fn(),
00252|         addImage: jest.fn(),
00253|         removeImage: jest.fn(),
00254|         addVideo: jest.fn(),
00255|         removeVideo: jest.fn(),
00256|         toggleSmartContext: jest.fn(),
00257|         setSmartRoot: jest.fn(),
00258|         notify: jest.fn(),
00259|         modelOptions: [],
00260|         lastReasoning: null,

(File has more lines. Use 'offset' parameter to read beyond line 260)
</file>
```
```

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/context-popup-glue-contract.test.ts",
  "offset": 1,
  "limit": 80
}
```
**Output:**
```
<file>
00002| import { JSDOM } from 'jsdom'
00003| 
00004| jest.mock('ink', () => ({
00005|   useInput: jest.fn(),
00006| }))
00007| 
00008| import {
00009|   useContextPopupGlue,
00010|   type UseContextPopupGlueOptions,
00011| } from '../../tui/screens/command/hooks/useContextPopupGlue'
00012| 
00013| const dom = new JSDOM('<!doctype html><html><body></body></html>')
00014| 
00015| type GlobalDom = { window: Window; document: Document }
00016| 
00017| beforeAll(() => {
00018|   const target = globalThis as unknown as GlobalDom
00019|   target.window = dom.window as unknown as Window
00020|   target.document = dom.window.document
00021| })
00022| 
00023| afterAll(() => {
00024|   const target = globalThis as unknown as Partial<GlobalDom>
00025|   delete target.window
00026|   delete target.document
00027| })
00028| 
00029| describe('useContextPopupGlue (shape contract)', () => {
00030|   it('returns stable keys and safe-to-call handlers', () => {
00031|     const options: UseContextPopupGlueOptions = {
00032|       inputValue: '',
00033|       popupState: null,
00034|       helpOpen: false,
00035|       isPopupOpen: false,
00036|       isCommandMode: false,
00037|       isCommandMenuActive: false,
00038|       isGenerating: false,
00039|       droppedFilePath: null,
00040|       files: [],
00041|       urls: [],
00042|       images: [],
00043|       videos: [],
00044|       smartContextEnabled: false,
00045|       smartContextRoot: null,
00046|       addFile: jest.fn(),
00047|       removeFile: jest.fn(),
00048|       addUrl: jest.fn(),
00049|       removeUrl: jest.fn(),
00050|       updateUrl: jest.fn(),
00051|       addImage: jest.fn(),
00052|       removeImage: jest.fn(),
00053|       addVideo: jest.fn(),
00054|       removeVideo: jest.fn(),
00055|       toggleSmartContext: jest.fn(),
00056|       setSmartRoot: jest.fn(),
00057|       setInputValue: jest.fn(),
00058|       setPopupState: jest.fn(),
00059|       suppressNextInput: jest.fn(),
00060|       notify: jest.fn(),
00061|       pushHistory: jest.fn(),
00062|       addCommandHistoryEntry: jest.fn(),
00063|       handleCommandSelection: jest.fn(),
00064|       consumeSuppressedTextInputChange: jest.fn(() => false),
00065|       isFilePath: jest.fn(() => true),
00066|     }
00067| 
00068|     const { result } = renderHook(() => useContextPopupGlue(options))
00069| 
00070|     const expectedKeys = [
00071|       'filePopupSuggestions',
00072|       'filePopupSuggestionSelectionIndex',
00073|       'filePopupSuggestionsFocused',
00074|       'onFilePopupDraftChange',
00075|       'onAddFile',
00076|       'onRemoveFile',
00077|       'onUrlPopupDraftChange',
00078|       'onAddUrl',
00079|       'onRemoveUrl',
00080|       'imagePopupSuggestions',
00081|       'imagePopupSuggestionSelectionIndex',

(File has more lines. Use 'offset' parameter to read beyond line 81)
</file>
```
```

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/popup-manager-command-mapping.test.ts",
  "offset": 1,
  "limit": 40
}
```
**Output:**
```
<file>
00002|   JSON_INTERACTIVE_ERROR,
00003|   mapPopupCommandSelection,
00004|   type CommandMappingContext,
00005| } from '../../tui/hooks/popup-manager/command-mapping'
00006| 
00007| describe('popup manager command mapping', () => {
00008|   const baseContext: CommandMappingContext = {
00009|     copyEnabled: false,
00010|     chatGptEnabled: false,
00011|     jsonOutputEnabled: false,
00012|     interactiveTransportPath: undefined,
00013|     urls: [],
00014|     images: [],
00015|     videos: [],
00016|     smartContextEnabled: false,
00017|     smartContextRoot: null,
00018|   }
00019| 
00020|   it('blocks /json when interactive transport is active', () => {
00021|     const result = mapPopupCommandSelection({
00022|       commandId: 'json',
00023|       argsRaw: undefined,
00024|       context: {
00025|         ...baseContext,
00026|         interactiveTransportPath: '/tmp/socket',
00027|       },
00028|     })
00029| 
00030|     expect(result).toEqual({
00031|       kind: 'steps',
00032|       steps: [
00033|         { type: 'push-history', message: JSON_INTERACTIVE_ERROR, kind: 'system' },
00034|         { type: 'set-input', value: '' },
00035|       ],
00036|     })
00037|   })
00038| 
00039|   it('maps /url args into add/warn steps with dedupe', () => {
00040|     const result = mapPopupCommandSelection({
00041|       commandId: 'url',

(File has more lines. Use 'offset' parameter to read beyond line 41)
</file>
```
```

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/usePopupManager.test.ts",
  "offset": 1,
  "limit": 80
}
```
**Output:**
```
<file>
00002| import { JSDOM } from 'jsdom'
00003| import type { MutableRefObject } from 'react'
00004| 
00005| import { resetRecentSessionModelsForTests } from '../tui/model-session'
00006| import { usePopupManager } from '../tui/hooks/usePopupManager'
00007| import type { UsePopupManagerOptions } from '../tui/hooks/usePopupManager'
00008| import type { ModelOption } from '../tui/types'
00009| 
00010| jest.mock('../tui/file-suggestions', () => ({
00011|   discoverDirectorySuggestions: jest.fn(),
00012|   discoverFileSuggestions: jest.fn(),
00013| }))
00014| 
00015| jest.mock('node:fs/promises', () => ({
00016|   readFile: jest.fn(),
00017| }))
00018| 
00019| const dom = new JSDOM('<!doctype html><html><body></body></html>')
00020| const globalScope = globalThis as typeof globalThis & {
00021|   window: Window & typeof globalThis
00022|   document: Document
00023|   navigator: Navigator
00024| }
00025| 
00026| globalScope.window = dom.window
00027| globalScope.document = dom.window.document
00028| globalScope.navigator = dom.window.navigator
00029| 
00030| beforeEach(() => {
00031|   resetRecentSessionModelsForTests()
00032| })
00033| 
00034| const defaultModelOptions: ModelOption[] = [
00035|   {
00036|     id: 'gpt-4o-mini',
00037|     label: 'gpt-4o-mini',
00038|     provider: 'openai',
00039|     description: 'test',
00040|     capabilities: [],
00041|     source: 'builtin',
00042|   },
00043| ]
00044| 
00045| const createOptions = (overrides: Partial<UsePopupManagerOptions> = {}): UsePopupManagerOptions => {
00046|   const baseRef: MutableRefObject<string | null> = { current: null }
00047| 
00048|   const defaults: UsePopupManagerOptions = {
00049|     currentModel: 'gpt-4o-mini',
00050|     currentTargetModel: 'gpt-4o-mini',
00051|     modelOptions: defaultModelOptions,
00052|     activeThemeName: 'pm-dark',
00053|     themeMode: 'dark',
00054|     themes: [
00055|       { name: 'pm-dark', label: 'Prompt Maker Dark' },
00056|       { name: 'pm-light', label: 'Prompt Maker Light' },
00057|     ],
00058|     smartContextEnabled: false,
00059|     smartContextRoot: null,
00060|     toggleSmartContext: jest.fn(),
00061|     setSmartRoot: jest.fn(),
00062|     urls: [],
00063|     addUrl: jest.fn(),
00064|     images: [],
00065|     videos: [],
00066|     addImage: jest.fn(),
00067|     addVideo: jest.fn(),
00068|     lastTestFile: null,
00069|     defaultTestFile: 'prompt.test.ts',
00070|     interactiveTransportPath: undefined,
00071|     isGenerating: false,
00072|     lastUserIntentRef: baseRef,
00073|     pushHistory: jest.fn(),
00074|     notify: jest.fn(),
00075|     setInputValue: jest.fn(),
00076|     runGeneration: jest.fn(async () => undefined),
00077|     runSeriesGeneration: jest.fn(),
00078|     runTestsFromCommand: jest.fn(),
00079|     exitApp: jest.fn(),
00080|     setCurrentModel: jest.fn(),
00081|     setCurrentTargetModel: jest.fn(),

(File has more lines. Use 'offset' parameter to read beyond line 81)
</file>
```
```

---

## Assistant

_Thinking:_

**Patching tests for PDF support**

I'm looking to make necessary changes for various tests. For the command-screen-popup-bindings-contract test, I’ll add pdfs and include mocks for addPdf/removePdf. I also need to update the context-popup-glue-contract test similarly, ensuring the expectedKeys list has the new pdf keys. For the popup-manager-command-mapping test, I'll add pdfs to the baseContext. Finally, in the usePopupManager test, I'll make sure createOptions includes pdfs and modify the type if needed. Let’s get these edits applied!

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/command-screen-popup-bindings-contract.test.ts",
  "oldString": "        images: [],\n        videos: [],\n",
  "newString": "        images: [],\n        videos: [],\n        pdfs: [],\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/command-screen-popup-bindings-contract.test.ts",
  "oldString": "        addVideo: jest.fn(),\n        removeVideo: jest.fn(),\n",
  "newString": "        addVideo: jest.fn(),\n        removeVideo: jest.fn(),\n        addPdf: jest.fn(),\n        removePdf: jest.fn(),\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/popup-manager-command-mapping.test.ts",
  "oldString": "    images: [],\n    videos: [],\n",
  "newString": "    images: [],\n    videos: [],\n    pdfs: [],\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/usePopupManager.test.ts",
  "oldString": "    images: [],\n    videos: [],\n    addImage: jest.fn(),\n    addVideo: jest.fn(),\n",
  "newString": "    images: [],\n    videos: [],\n    pdfs: [],\n    addImage: jest.fn(),\n    addVideo: jest.fn(),\n    addPdf: jest.fn(),\n"
}
```
```

---

## Assistant

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/context-popup-glue-contract.test.ts",
  "oldString": "      images: [],\n      videos: [],\n",
  "newString": "      images: [],\n      videos: [],\n      pdfs: [],\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [31:11] Type '{ inputValue: string; popupState: null; helpOpen: false; isPopupOpen: false; isCommandMode: false; isCommandMenuActive: false; isGenerating: false; droppedFilePath: null; files: never[]; urls: never[]; ... 24 more ...; isFilePath: Mock<...>; }' is missing the following properties from type 'UseContextPopupGlueOptions': addPdf, removePdf
</file_diagnostics>

```
```

---

## Assistant

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/context-popup-glue-contract.test.ts",
  "oldString": "      addVideo: jest.fn(),\n      removeVideo: jest.fn(),\n",
  "newString": "      addVideo: jest.fn(),\n      removeVideo: jest.fn(),\n      addPdf: jest.fn(),\n      removePdf: jest.fn(),\n"
}
```
```

---

## Assistant

_Thinking:_

**Updating expected keys**

I need to update the expectedKeys list to include the PDF keys. First, I’ll locate where this list exists in the file. I’ll search for `videoPopupSuggestions` and other relevant snippets. Once I find the section around expectedKeys, I can make the necessary edits to insert the PDF keys right after the video ones. It's a small but important detail to get right!

```
Tool: grep

**Input:**
```json
{
  "pattern": "'videoPopupSuggestions'|'onRemoveVideo'|'smartPopupSuggestions'",
  "include": "src/__tests__/tui/context-popup-glue-contract.test.ts"
}
```
**Output:**
```
Found 3 matches
/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/context-popup-glue-contract.test.ts:
  Line 89:       'videoPopupSuggestions',
  Line 94:       'onRemoveVideo',
  Line 95:       'smartPopupSuggestions',
```
```

---

## Assistant

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/context-popup-glue-contract.test.ts",
  "offset": 80,
  "limit": 30
}
```
**Output:**
```
<file>
00081|       'onAddUrl',
00082|       'onRemoveUrl',
00083|       'imagePopupSuggestions',
00084|       'imagePopupSuggestionSelectionIndex',
00085|       'imagePopupSuggestionsFocused',
00086|       'onImagePopupDraftChange',
00087|       'onAddImage',
00088|       'onRemoveImage',
00089|       'videoPopupSuggestions',
00090|       'videoPopupSuggestionSelectionIndex',
00091|       'videoPopupSuggestionsFocused',
00092|       'onVideoPopupDraftChange',
00093|       'onAddVideo',
00094|       'onRemoveVideo',
00095|       'smartPopupSuggestions',
00096|       'smartPopupSuggestionSelectionIndex',
00097|       'smartPopupSuggestionsFocused',
00098|       'onSmartPopupDraftChange',
00099|       'onSmartToggle',
00100|       'onSmartRootSubmit',
00101|     ].sort()
00102| 
00103|     expect(Object.keys(result.current).sort()).toEqual(expectedKeys)
00104| 
00105|     expect(() => result.current.onFilePopupDraftChange('x')).not.toThrow()
00106|     expect(() => result.current.onAddFile('/tmp/file.txt')).not.toThrow()
00107|     expect(() => result.current.onRemoveFile(0)).not.toThrow()
00108| 
00109|     expect(() => result.current.onUrlPopupDraftChange('https://example.com')).not.toThrow()
00110|     expect(() => result.current.onAddUrl('https://example.com')).not.toThrow()

(File has more lines. Use 'offset' parameter to read beyond line 110)
</file>
```
```

---

## Assistant

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/tui/context-popup-glue-contract.test.ts",
  "oldString": "      'onAddVideo',\n      'onRemoveVideo',\n      'smartPopupSuggestions',\n",
  "newString": "      'onAddVideo',\n      'onRemoveVideo',\n      'pdfPopupSuggestions',\n      'pdfPopupSuggestionSelectionIndex',\n      'pdfPopupSuggestionsFocused',\n      'onPdfPopupDraftChange',\n      'onAddPdf',\n      'onRemovePdf',\n      'smartPopupSuggestions',\n"
}
```
```

---

## Assistant

_Thinking:_



```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 80,
  "limit": 80
}
```
**Output:**
```
<file>
00081|     setLastReasoning,
00082|     setLastGeneratedPrompt,
00083|     resetContext,
00084|   } = useContextDispatch()
00085| 
00086|   const historyAndTests = useCommandScreenHistoryAndTests()
00087| 
00088|   const inputState = useCommandScreenInputState({
00089|     pushHistoryProxy: historyAndTests.pushHistoryProxy,
00090|   })
00091| 
00092|   const modelAndGeneration: UseCommandScreenModelGenerationResult = useCommandScreenModelGeneration(
00093|     {
00094|       pushHistoryProxy: historyAndTests.pushHistoryProxy,
00095|       notify,
00096|       files,
00097|       urls,
00098|       images,
00099|       videos,
00100|       smartContextEnabled,
00101|       smartContextRoot,
00102|       metaInstructions,
00103|       budgets: { maxContextTokens, maxInputTokens, contextOverflowStrategy },
00104|       ...(interactiveTransportPath ? { interactiveTransportPath } : {}),
00105|       terminalColumns: inputState.terminalColumns,
00106|       copyEnabled: inputState.copyEnabled,
00107|       chatGptEnabled: inputState.chatGptEnabled,
00108|       jsonOutputEnabled: inputState.jsonOutputEnabled,
00109|       isTestCommandRunning: historyAndTests.isTestCommandRunning,
00110|       setLastReasoning,
00111|       setLastGeneratedPrompt,
00112|     },
00113|   )
00114| 
00115|   const contextOptions: UseCommandScreenPopupAndViewOptions['context'] = useMemo(
00116|     () => ({
00117|       interactiveTransportPath,
00118|       notify,
00119|       stdout,
00120|       files,
00121|       urls,
00122|       images,
00123|       videos,
00124|       smartContextEnabled,
00125|       smartContextRoot,
00126|       metaInstructions,
00127|       maxContextTokens,
00128|       maxInputTokens,
00129|       contextOverflowStrategy,
00130|       lastReasoning,
00131|       lastGeneratedPrompt,
00132|       addFile,
00133|       removeFile,
00134|       addUrl,
00135|       removeUrl,
00136|       updateUrl,
00137|       addImage,
00138|       removeImage,
00139|       addVideo,
00140|       removeVideo,
00141|       toggleSmartContext,
00142|       setSmartRoot,
00143|       setMetaInstructions,
00144|       setBudgets,
00145|       resetContext,
00146|     }),
00147|     [
00148|       interactiveTransportPath,
00149|       notify,
00150|       stdout,
00151|       files,
00152|       urls,
00153|       images,
00154|       videos,
00155|       smartContextEnabled,
00156|       smartContextRoot,
00157|       metaInstructions,
00158|       maxContextTokens,
00159|       maxInputTokens,
00160|       contextOverflowStrategy,

(File has more lines. Use 'offset' parameter to read beyond line 160)
</file>
```
```

---

## Assistant

_Thinking:_



```
Tool: grep

**Input:**
```json
{
  "pattern": "const \\{[^}]*videos[^}]*\\} = useContextState",
  "include": "src/tui/screens/command/hooks/useCommandScreenController.ts"
}
```
**Output:**
```
No files found
```
```

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 20,
  "limit": 70
}
```
**Output:**
```
<file>
00021| export type UseCommandScreenControllerOptions = {
00022|   transport?: {
00023|     interactiveTransportPath?: string | undefined
00024|   }
00025|   popup: {
00026|     onPopupVisibilityChange?: (isOpen: boolean) => void
00027|     commandMenuSignal?: number
00028|     helpOpen: boolean
00029|     reservedRows: number
00030|   }
00031|   notify: (message: string, options?: NotifyOptions) => void
00032| }
00033| 
00034| export type UseCommandScreenControllerResult = {
00035|   view: UseCommandScreenPopupAndViewResult
00036|   actions: {
00037|     suppressNextInput: () => void
00038|   }
00039| }
00040| 
00041| export const useCommandScreenController = ({
00042|   transport,
00043|   popup,
00044|   notify,
00045| }: UseCommandScreenControllerOptions): UseCommandScreenControllerResult => {
00046|   const { stdout } = useStdout()
00047| 
00048|   const interactiveTransportPath = transport?.interactiveTransportPath
00049| 
00050|   const { onPopupVisibilityChange, commandMenuSignal, helpOpen, reservedRows } = popup
00051| 
00052|   const {
00053|     files,
00054|     urls,
00055|     images,
00056|     videos,
00057|     smartContextEnabled,
00058|     smartContextRoot,
00059|     metaInstructions,
00060|     maxContextTokens,
00061|     maxInputTokens,
00062|     contextOverflowStrategy,
00063|     lastReasoning,
00064|     lastGeneratedPrompt,
00065|   } = useContextState()
00066| 
00067|   const {
00068|     addFile,
00069|     removeFile,
00070|     addUrl,
00071|     removeUrl,
00072|     updateUrl,
00073|     addImage,
00074|     removeImage,
00075|     addVideo,
00076|     removeVideo,
00077|     toggleSmartContext,
00078|     setSmartRoot,
00079|     setMetaInstructions,
00080|     setBudgets,
00081|     setLastReasoning,
00082|     setLastGeneratedPrompt,
00083|     resetContext,
00084|   } = useContextDispatch()
00085| 
00086|   const historyAndTests = useCommandScreenHistoryAndTests()
00087| 
00088|   const inputState = useCommandScreenInputState({
00089|     pushHistoryProxy: historyAndTests.pushHistoryProxy,
00090|   })

(File has more lines. Use 'offset' parameter to read beyond line 90)
</file>
```
```

---

## Assistant

```
Tool: todowrite

**Input:**
```json
{
  "todos": [
    {
      "content": "Add PDF state to TUI context",
      "status": "completed",
      "priority": "high",
      "id": "1"
    },
    {
      "content": "Add PDF popup UI and commands",
      "status": "in_progress",
      "priority": "high",
      "id": "2"
    },
    {
      "content": "Wire TUI generation pipeline PDFs",
      "status": "pending",
      "priority": "high",
      "id": "3"
    },
    {
      "content": "Implement OpenAI PDF text extraction",
      "status": "pending",
      "priority": "high",
      "id": "4"
    },
    {
      "content": "Update tests for TUI and OpenAI",
      "status": "pending",
      "priority": "high",
      "id": "5"
    },
    {
      "content": "Run typecheck/tests/build",
      "status": "pending",
      "priority": "high",
      "id": "6"
    }
  ]
}
```
**Output:**
```
[
  {
    "content": "Add PDF state to TUI context",
    "status": "completed",
    "priority": "high",
    "id": "1"
  },
  {
    "content": "Add PDF popup UI and commands",
    "status": "in_progress",
    "priority": "high",
    "id": "2"
  },
  {
    "content": "Wire TUI generation pipeline PDFs",
    "status": "pending",
    "priority": "high",
    "id": "3"
  },
  {
    "content": "Implement OpenAI PDF text extraction",
    "status": "pending",
    "priority": "high",
    "id": "4"
  },
  {
    "content": "Update tests for TUI and OpenAI",
    "status": "pending",
    "priority": "high",
    "id": "5"
  },
  {
    "content": "Run typecheck/tests/build",
    "status": "pending",
    "priority": "high",
    "id": "6"
  }
]
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "    images,\n    videos,\n",
  "newString": "    images,\n    videos,\n    pdfs,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [79:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [80:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [82:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [83:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [122:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>

```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "    addVideo,\n    removeVideo,\n",
  "newString": "    addVideo,\n    removeVideo,\n    addPdf,\n    removePdf,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [79:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [80:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [82:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [83:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [122:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>

```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "       images,\n       videos,\n       smartContextEnabled,\n",
  "newString": "       images,\n       videos,\n       pdfs,\n       smartContextEnabled,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [79:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [80:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [82:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [83:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [122:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>

```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "       images,\n       videos,\n       smartContextEnabled,\n",
  "newString": "       images,\n       videos,\n       pdfs,\n       smartContextEnabled,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [79:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [80:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [82:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [83:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [122:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>

```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "       addVideo,\n       removeVideo,\n",
  "newString": "       addVideo,\n       removeVideo,\n       addPdf,\n       removePdf,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [79:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [80:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [82:5] Cannot redeclare block-scoped variable 'addPdf'.
ERROR [83:5] Cannot redeclare block-scoped variable 'removePdf'.
ERROR [122:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_

**Reviewing code duplicates**

I noticed that we accidentally included addPdf/removePdf twice in the dispatch destructuring within contextOptions and/or modelAndGeneration options. I think I need to review lines 70-85 after the edits to identify the duplicates. It’s a bit annoying when that happens, but it’s good to catch it early. I’ll make sure to double-check those areas and clean it up properly to avoid any issues down the line.

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 64,
  "limit": 30
}
```
**Output:**
```
<file>
00065|     lastReasoning,
00066|     lastGeneratedPrompt,
00067|   } = useContextState()
00068| 
00069|   const {
00070|     addFile,
00071|     removeFile,
00072|     addUrl,
00073|     removeUrl,
00074|     updateUrl,
00075|     addImage,
00076|     removeImage,
00077|     addVideo,
00078|     removeVideo,
00079|     addPdf,
00080|     removePdf,
00081| 
00082|     addPdf,
00083|     removePdf,
00084|     toggleSmartContext,
00085|     setSmartRoot,
00086|     setMetaInstructions,
00087|     setBudgets,
00088|     setLastReasoning,
00089|     setLastGeneratedPrompt,
00090|     resetContext,
00091|   } = useContextDispatch()
00092| 
00093|   const historyAndTests = useCommandScreenHistoryAndTests()
00094| 

(File has more lines. Use 'offset' parameter to read beyond line 94)
</file>
```
```

---

## Assistant

_Thinking:_



```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "    addPdf,\n    removePdf,\n\n    addPdf,\n    removePdf,\n    toggleSmartContext,\n",
  "newString": "    addPdf,\n    removePdf,\n    toggleSmartContext,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [119:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_

**Fixing context options**

I’m looking at the error regarding the contextOptions object missing pdfs and the addPdf/removePdf functions. It seems we attempted to address this, but maybe they weren’t inserted correctly or I placed them in the wrong location. I think it would be best to read the definition of contextOptions after line 115 to see where I might have gone wrong. It might help clarify how to properly include those functions!

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 112,
  "limit": 50
}
```
**Output:**
```
<file>
00113|       isTestCommandRunning: historyAndTests.isTestCommandRunning,
00114|       setLastReasoning,
00115|       setLastGeneratedPrompt,
00116|     },
00117|   )
00118| 
00119|   const contextOptions: UseCommandScreenPopupAndViewOptions['context'] = useMemo(
00120|     () => ({
00121|       interactiveTransportPath,
00122|       notify,
00123|       stdout,
00124|       files,
00125|       urls,
00126|       images,
00127|       videos,
00128|       smartContextEnabled,
00129|       smartContextRoot,
00130|       metaInstructions,
00131|       maxContextTokens,
00132|       maxInputTokens,
00133|       contextOverflowStrategy,
00134|       lastReasoning,
00135|       lastGeneratedPrompt,
00136|       addFile,
00137|       removeFile,
00138|       addUrl,
00139|       removeUrl,
00140|       updateUrl,
00141|       addImage,
00142|       removeImage,
00143|       addVideo,
00144|       removeVideo,
00145|       toggleSmartContext,
00146|       setSmartRoot,
00147|       setMetaInstructions,
00148|       setBudgets,
00149|       resetContext,
00150|     }),
00151|     [
00152|       interactiveTransportPath,
00153|       notify,
00154|       stdout,
00155|       files,
00156|       urls,
00157|       images,
00158|       videos,
00159|       smartContextEnabled,
00160|       smartContextRoot,
00161|       metaInstructions,
00162|       maxContextTokens,

(File has more lines. Use 'offset' parameter to read beyond line 162)
</file>
```
```

---

## Assistant

_Thinking:_

**Reviewing state structure**

I’m noticing that the PDFs are missing from the state destructure we did earlier. We added the useContextState, which was great, but it seems like we didn’t insert the PDFs properly in the state destructuring part. It might be that we overlooked this step. Let me check those earlier lines, like line 52 and see if I can find what went wrong. I want to make sure everything is accurately set up!

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 48,
  "limit": 20
}
```
**Output:**
```
<file>
00049| 
00050|   const { onPopupVisibilityChange, commandMenuSignal, helpOpen, reservedRows } = popup
00051| 
00052|   const {
00053|     files,
00054|     urls,
00055|     images,
00056|     videos,
00057|     pdfs,
00058|     smartContextEnabled,
00059| 
00060|     smartContextRoot,
00061|     metaInstructions,
00062|     maxContextTokens,
00063|     maxInputTokens,
00064|     contextOverflowStrategy,
00065|     lastReasoning,
00066|     lastGeneratedPrompt,
00067|   } = useContextState()
00068| 

(File has more lines. Use 'offset' parameter to read beyond line 68)
</file>
```
```

---

## Assistant

_Thinking:_

**Fixing context options**

I noticed there’s a blank line after `smartContextEnabled`. I need to make sure to add PDFs in the `contextOptions` object and in the dependency list. I also have to include the `addPdf` and `removePdf` functions, as well as the PDFs. This is crucial for everything to work seamlessly! Let's patch these details up and get everything sorted out. I want to make sure I've covered all my bases!

```
Tool: grep

**Input:**
```json
{
  "pattern": "images,\\n\\s*videos,\\n\\s*smartContextEnabled",
  "include": "src/tui/screens/command/hooks/useCommandScreenController.ts"
}
```
**Error:**
```
Error: ripgrep failed: rg: the literal "\n" is not allowed in a regex

Consider enabling multiline mode with the --multiline flag (or -U for short).
When multiline mode is enabled, new line characters can be matched.

```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>

```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      addVideo,\n      removeVideo,\n      toggleSmartContext,\n",
  "newString": "      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>

```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n      smartContextRoot,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n      smartContextRoot,\n"
}
```
**Error:**
```
Error: Found multiple matches for oldString. Provide more surrounding lines in oldString to identify the correct match.
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_

**Adjusting code edits**

I noticed some edits failed due to multiple matches, so I plan to make targeted edits with a clearer context. In my modelAndGeneration call on line 92, it seems like I may have mistakenly added PDFs earlier. The error is related to a contextOptions type mismatch that I need to focus on. I'll modify the contextOptions object on line 120 to include PDFs and the addPdf/removePdf functions after removeVideo, along with updating the dependencies array.

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 118,
  "limit": 40
}
```
**Output:**
```
<file>
00119| 
00120|   const contextOptions: UseCommandScreenPopupAndViewOptions['context'] = useMemo(
00121|     () => ({
00122|       interactiveTransportPath,
00123|       notify,
00124|       stdout,
00125|       files,
00126|       urls,
00127|       images,
00128|       videos,
00129|       smartContextEnabled,
00130|       smartContextRoot,
00131|       metaInstructions,
00132|       maxContextTokens,
00133|       maxInputTokens,
00134|       contextOverflowStrategy,
00135|       lastReasoning,
00136|       lastGeneratedPrompt,
00137|       addFile,
00138|       removeFile,
00139|       addUrl,
00140|       removeUrl,
00141|       updateUrl,
00142|       addImage,
00143|       removeImage,
00144|       addVideo,
00145|       removeVideo,
00146|       toggleSmartContext,
00147|       setSmartRoot,
00148|       setMetaInstructions,
00149|       setBudgets,
00150|       resetContext,
00151|     }),
00152|     [
00153|       interactiveTransportPath,
00154|       notify,
00155|       stdout,
00156|       files,
00157|       urls,
00158|       images,

(File has more lines. Use 'offset' parameter to read beyond line 158)
</file>
```
```

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 150,
  "limit": 30
}
```
**Output:**
```
<file>
00151|     }),
00152|     [
00153|       interactiveTransportPath,
00154|       notify,
00155|       stdout,
00156|       files,
00157|       urls,
00158|       images,
00159|       videos,
00160|       smartContextEnabled,
00161|       smartContextRoot,
00162|       metaInstructions,
00163|       maxContextTokens,
00164|       maxInputTokens,
00165|       contextOverflowStrategy,
00166|       lastReasoning,
00167|       lastGeneratedPrompt,
00168|       addFile,
00169|       removeFile,
00170|       addUrl,
00171|       removeUrl,
00172|       updateUrl,
00173|       addImage,
00174|       removeImage,
00175|       addVideo,
00176|       removeVideo,
00177|       toggleSmartContext,
00178|       setSmartRoot,
00179|       setMetaInstructions,
00180|       setBudgets,

(File has more lines. Use 'offset' parameter to read beyond line 180)
</file>
```
```

---

## Assistant

```
Tool: glob

**Input:**
```json
{
  "pattern": "noop",
  "path": "/Users/eroberts/Projects/prompt-maker/src"
}
```
**Output:**
```
No files found
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>

```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      addVideo,\n      removeVideo,\n      toggleSmartContext,\n",
  "newString": "      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 24 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': pdfs, addPdf, removePdf
</file_diagnostics>

```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n      smartContextRoot,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n      smartContextRoot,\n"
}
```
**Error:**
```
Error: Found multiple matches for oldString. Provide more surrounding lines in oldString to identify the correct match.
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n      smartContextRoot,\n      metaInstructions,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n      smartContextRoot,\n      metaInstructions,\n"
}
```
**Error:**
```
Error: Found multiple matches for oldString. Provide more surrounding lines in oldString to identify the correct match.
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n      smartContextRoot,\n      metaInstructions,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n      smartContextRoot,\n      metaInstructions,\n"
}
```
**Error:**
```
Error: Found multiple matches for oldString. Provide more surrounding lines in oldString to identify the correct match.
```
```

---

## Assistant

_Thinking:_



```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      stdout,\n      files,\n      urls,\n      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "      stdout,\n      files,\n      urls,\n      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
**Error:**
```
Error: Found multiple matches for oldString. Provide more surrounding lines in oldString to identify the correct match.
```
```

---

## Assistant

_Thinking:_



```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "    () => ({\n      interactiveTransportPath,\n      notify,\n      stdout,\n      files,\n      urls,\n      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "    () => ({\n      interactiveTransportPath,\n      notify,\n      stdout,\n      files,\n      urls,\n      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 25 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': addPdf, removePdf
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_

I'm noting that the PDFs are now included, which is great! However, I realize that the addPdf and removePdf features seem to be missing. I need to think about how to implement those properly, perhaps by patching the object in a similar way to what I did after removing the video. There’s a bit of organization needed here, but I feel like I’m on the right track. Let's ensure everything aligns well!

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      addVideo,\n      removeVideo,\n      toggleSmartContext,\n",
  "newString": "      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 25 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': addPdf, removePdf
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_

**Verifying patch application**

I’m thinking that the patch might not have applied correctly since there could be multiple matches in play. I definitely want to double-check this by reading the specific snippet to understand better what's going on. It's important to ensure I have the correct context before moving forward. I just need to make sure I’m thorough in this verification process to get things right!

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 118,
  "limit": 35
}
```
**Output:**
```
<file>
00119| 
00120|   const contextOptions: UseCommandScreenPopupAndViewOptions['context'] = useMemo(
00121|     () => ({
00122|       interactiveTransportPath,
00123|       notify,
00124|       stdout,
00125|       files,
00126|       urls,
00127|       images,
00128|       videos,
00129|       pdfs,
00130|       smartContextEnabled,
00131|       smartContextRoot,
00132|       metaInstructions,
00133|       maxContextTokens,
00134|       maxInputTokens,
00135|       contextOverflowStrategy,
00136|       lastReasoning,
00137|       lastGeneratedPrompt,
00138|       addFile,
00139|       removeFile,
00140|       addUrl,
00141|       removeUrl,
00142|       updateUrl,
00143|       addImage,
00144|       removeImage,
00145|       addVideo,
00146|       removeVideo,
00147|       toggleSmartContext,
00148|       setSmartRoot,
00149|       setMetaInstructions,
00150|       setBudgets,
00151|       resetContext,
00152|     }),
00153|     [

(File has more lines. Use 'offset' parameter to read beyond line 153)
</file>
```
```

---

## Assistant

_Thinking:_



```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      addVideo,\n      removeVideo,\n      toggleSmartContext,\n",
  "newString": "      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 25 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': addPdf, removePdf
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_

**Clarifying context**

It seems like there's an issue where a patch didn't apply, possibly due to multiple matches. I should provide additional context to make sure everything is unique. Maybe I need to include the preceding `addImage` and other relevant details to clarify what's going on. Ensuring that the information is complete will help identify any potential problems, so let's make sure I'm being thorough!

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      updateUrl,\n      addImage,\n      removeImage,\n      addVideo,\n      removeVideo,\n      toggleSmartContext,\n",
  "newString": "      updateUrl,\n      addImage,\n      removeImage,\n      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 25 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': addPdf, removePdf
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_

**Reviewing possible patches**

I’m noticing there’s still an error, but it looks like a patch might have been applied. I think it’s important to take a moment to re-read the snippet to uncover what might have gone wrong. Maybe I’ll find a clue that helps me determine what needs to be adjusted. There's a lot to think about here, and I definitely want to get this right! Let's see what I can find in that snippet.

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 138,
  "limit": 18
}
```
**Output:**
```
<file>
00139|       removeFile,
00140|       addUrl,
00141|       removeUrl,
00142|       updateUrl,
00143|       addImage,
00144|       removeImage,
00145|       addVideo,
00146|       removeVideo,
00147|       toggleSmartContext,
00148|       setSmartRoot,
00149|       setMetaInstructions,
00150|       setBudgets,
00151|       resetContext,
00152|     }),
00153|     [
00154|       interactiveTransportPath,
00155|       notify,
00156|       stdout,

(File has more lines. Use 'offset' parameter to read beyond line 156)
</file>
```
```

---

## Assistant

_Thinking:_



```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      removeVideo,\n      toggleSmartContext,\n",
  "newString": "      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [120:9] Type '{ interactiveTransportPath: string | undefined; notify: (message: string, options?: NotifyOptions | undefined) => void; stdout: WriteStream; files: string[]; ... 25 more ...; resetContext: () => void; }' is missing the following properties from type 'CommandContextOptions': addPdf, removePdf
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_



```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 140,
  "limit": 12
}
```
**Output:**
```
<file>
00141|       removeUrl,
00142|       updateUrl,
00143|       addImage,
00144|       removeImage,
00145|       addVideo,
00146|       removeVideo,
00147|       toggleSmartContext,
00148|       setSmartRoot,
00149|       setMetaInstructions,
00150|       setBudgets,
00151|       resetContext,
00152|     }),

(File has more lines. Use 'offset' parameter to read beyond line 152)
</file>
```
```

---

## Assistant

_Thinking:_



```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      addVideo,\n      removeVideo,\n      toggleSmartContext,\n      setSmartRoot,\n",
  "newString": "      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n      setSmartRoot,\n"
}
```
**Error:**
```
Error: Found multiple matches for oldString. Provide more surrounding lines in oldString to identify the correct match.
```
```

---

## Assistant

_Thinking:_

**Refining target strings**

I realize that the oldString shows up multiple times, including in the dependency array. To make sure I target the object section uniquely, I need to incorporate previous lines like `lastGeneratedPrompt`. By including `lastGeneratedPrompt` earlier, I can create a unique identification. It feels like a fun little puzzle to solve — I just want to make sure I'm handling it well! Let's patch it up to ensure clarity.

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      lastReasoning,\n      lastGeneratedPrompt,\n      addFile,\n      removeFile,\n      addUrl,\n      removeUrl,\n      updateUrl,\n      addImage,\n      removeImage,\n      addVideo,\n      removeVideo,\n      toggleSmartContext,\n      setSmartRoot,\n",
  "newString": "      lastReasoning,\n      lastGeneratedPrompt,\n      addFile,\n      removeFile,\n      addUrl,\n      removeUrl,\n      updateUrl,\n      addImage,\n      removeImage,\n      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n      setSmartRoot,\n"
}
```
**Error:**
```
Error: Found multiple matches for oldString. Provide more surrounding lines in oldString to identify the correct match.
```
```

---

## Assistant

_Thinking:_



```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "    () => ({\n      interactiveTransportPath,\n      notify,\n      stdout,\n      files,\n      urls,\n      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n      smartContextRoot,\n      metaInstructions,\n      maxContextTokens,\n      maxInputTokens,\n      contextOverflowStrategy,\n      lastReasoning,\n      lastGeneratedPrompt,\n      addFile,\n      removeFile,\n      addUrl,\n      removeUrl,\n      updateUrl,\n      addImage,\n      removeImage,\n      addVideo,\n      removeVideo,\n      toggleSmartContext,\n      setSmartRoot,\n      setMetaInstructions,\n      setBudgets,\n      resetContext,\n    }),\n",
  "newString": "    () => ({\n      interactiveTransportPath,\n      notify,\n      stdout,\n      files,\n      urls,\n      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n      smartContextRoot,\n      metaInstructions,\n      maxContextTokens,\n      maxInputTokens,\n      contextOverflowStrategy,\n      lastReasoning,\n      lastGeneratedPrompt,\n      addFile,\n      removeFile,\n      addUrl,\n      removeUrl,\n      updateUrl,\n      addImage,\n      removeImage,\n      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n      setSmartRoot,\n      setMetaInstructions,\n      setBudgets,\n      resetContext,\n    }),\n"
}
```
```

---

## Assistant

_Thinking:_



```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 150,
  "limit": 40
}
```
**Output:**
```
<file>
00151|       setMetaInstructions,
00152|       setBudgets,
00153|       resetContext,
00154|     }),
00155|     [
00156|       interactiveTransportPath,
00157|       notify,
00158|       stdout,
00159|       files,
00160|       urls,
00161|       images,
00162|       videos,
00163|       smartContextEnabled,
00164|       smartContextRoot,
00165|       metaInstructions,
00166|       maxContextTokens,
00167|       maxInputTokens,
00168|       contextOverflowStrategy,
00169|       lastReasoning,
00170|       lastGeneratedPrompt,
00171|       addFile,
00172|       removeFile,
00173|       addUrl,
00174|       removeUrl,
00175|       updateUrl,
00176|       addImage,
00177|       removeImage,
00178|       addVideo,
00179|       removeVideo,
00180|       toggleSmartContext,
00181|       setSmartRoot,
00182|       setMetaInstructions,
00183|       setBudgets,
00184|       resetContext,
00185|     ],
00186|   )
00187| 
00188|   const inputOptions: UseCommandScreenPopupAndViewOptions['input'] = useMemo(
00189|     () => ({
00190|       terminalRows: inputState.terminalRows,

(File has more lines. Use 'offset' parameter to read beyond line 190)
</file>
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      addVideo,\n      removeVideo,\n      toggleSmartContext,\n",
  "newString": "      addVideo,\n      removeVideo,\n      addPdf,\n      removePdf,\n      toggleSmartContext,\n"
}
```
```

---

## Assistant

_Thinking:_



```
Tool: grep

**Input:**
```json
{
  "pattern": "popup\\.context|popup: \\{|imagePopupSuggestions|videoPopupSuggestions",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupAndView.ts"
}
```
**Output:**
```
Found 23 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupAndView.ts:
  Line 31:     popup: {
  Line 47:         filePopupSuggestions: bindings.popup.context.file.suggestions,
  Line 48:         filePopupSuggestionSelectionIndex: bindings.popup.context.file.suggestionSelectionIndex,
  Line 49:         filePopupSuggestionsFocused: bindings.popup.context.file.suggestionsFocused,
  Line 50:         onFilePopupDraftChange: bindings.popup.context.file.onDraftChange,
  Line 51:         onAddFile: bindings.popup.context.file.onAdd,
  Line 53:         onUrlPopupDraftChange: bindings.popup.context.url.onDraftChange,
  Line 54:         onAddUrl: bindings.popup.context.url.onAdd,
  Line 56:         imagePopupSuggestions: bindings.popup.context.image.suggestions,
  Line 57:         imagePopupSuggestionSelectionIndex: bindings.popup.context.image.suggestionSelectionIndex,
  Line 58:         imagePopupSuggestionsFocused: bindings.popup.context.image.suggestionsFocused,
  Line 59:         onImagePopupDraftChange: bindings.popup.context.image.onDraftChange,
  Line 60:         onAddImage: bindings.popup.context.image.onAdd,
  Line 62:         videoPopupSuggestions: bindings.popup.context.video.suggestions,
  Line 63:         videoPopupSuggestionSelectionIndex: bindings.popup.context.video.suggestionSelectionIndex,
  Line 64:         videoPopupSuggestionsFocused: bindings.popup.context.video.suggestionsFocused,
  Line 65:         onVideoPopupDraftChange: bindings.popup.context.video.onDraftChange,
  Line 66:         onAddVideo: bindings.popup.context.video.onAdd,
  Line 69:         smartPopupSuggestions: bindings.popup.context.smart.suggestions,
  Line 70:         smartPopupSuggestionSelectionIndex: bindings.popup.context.smart.suggestionSelectionIndex,
  Line 71:         smartPopupSuggestionsFocused: bindings.popup.context.smart.suggestionsFocused,
  Line 72:         onSmartPopupDraftChange: bindings.popup.context.smart.onDraftChange,
  Line 73:         onSmartRootSubmit: bindings.popup.context.smart.onRootSubmit,
```
```

---

## Assistant

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupAndView.ts",
  "offset": 1,
  "limit": 120
}
```
**Output:**
```
<file>
00002| import { useCommandScreenViewModel } from './useCommandScreenViewModel'
00003| 
00004| import type {
00005|   UseCommandScreenPopupAndViewOptions,
00006|   UseCommandScreenPopupAndViewResult,
00007| } from './useCommandScreenPopupAndView.types'
00008| 
00009| export const useCommandScreenPopupAndView = (
00010|   options: UseCommandScreenPopupAndViewOptions,
00011| ): UseCommandScreenPopupAndViewResult => {
00012|   const { context, input, popup, history, generation } = options
00013| 
00014|   const { popupManager, shell, bindings, enhancedStatusChips } = useCommandScreenBindings(options)
00015| 
00016|   const viewModel = useCommandScreenViewModel({
00017|     transport: { isAwaitingTransportInput: shell.isAwaitingTransportInput },
00018|     panes: {
00019|       history: {
00020|         lines: shell.history,
00021|         visibleRows: shell.historyRows,
00022|         scrollOffset: shell.scrollOffset,
00023|       },
00024|       menu: {
00025|         isActive: shell.isCommandMenuActive,
00026|         height: shell.menuHeight,
00027|         commands: shell.visibleCommands,
00028|         selectedIndex: input.commandSelectionIndex,
00029|       },
00030|     },
00031|     popup: {
00032|       base: {
00033|         popupState: popupManager.popupState,
00034|         helpOpen: popup.helpOpen,
00035|         overlayHeight: shell.overlayHeight,
00036|       },
00037|       model: {
00038|         modelPopupOptions: bindings.popup.model.options,
00039|         modelPopupSelection: bindings.popup.model.selection,
00040|         modelPopupRecentCount: bindings.popup.model.recentCount,
00041|         providerStatuses: generation.providerStatuses,
00042|         onModelPopupQueryChange: bindings.popup.model.onQueryChange,
00043|         onModelPopupSubmit: popupManager.actions.handleModelPopupSubmit,
00044|       },
00045|       context: {
00046|         files: context.files,
00047|         filePopupSuggestions: bindings.popup.context.file.suggestions,
00048|         filePopupSuggestionSelectionIndex: bindings.popup.context.file.suggestionSelectionIndex,
00049|         filePopupSuggestionsFocused: bindings.popup.context.file.suggestionsFocused,
00050|         onFilePopupDraftChange: bindings.popup.context.file.onDraftChange,
00051|         onAddFile: bindings.popup.context.file.onAdd,
00052|         urls: context.urls,
00053|         onUrlPopupDraftChange: bindings.popup.context.url.onDraftChange,
00054|         onAddUrl: bindings.popup.context.url.onAdd,
00055|         images: context.images,
00056|         imagePopupSuggestions: bindings.popup.context.image.suggestions,
00057|         imagePopupSuggestionSelectionIndex: bindings.popup.context.image.suggestionSelectionIndex,
00058|         imagePopupSuggestionsFocused: bindings.popup.context.image.suggestionsFocused,
00059|         onImagePopupDraftChange: bindings.popup.context.image.onDraftChange,
00060|         onAddImage: bindings.popup.context.image.onAdd,
00061|         videos: context.videos,
00062|         videoPopupSuggestions: bindings.popup.context.video.suggestions,
00063|         videoPopupSuggestionSelectionIndex: bindings.popup.context.video.suggestionSelectionIndex,
00064|         videoPopupSuggestionsFocused: bindings.popup.context.video.suggestionsFocused,
00065|         onVideoPopupDraftChange: bindings.popup.context.video.onDraftChange,
00066|         onAddVideo: bindings.popup.context.video.onAdd,
00067|         smartContextEnabled: context.smartContextEnabled,
00068|         smartContextRoot: context.smartContextRoot,
00069|         smartPopupSuggestions: bindings.popup.context.smart.suggestions,
00070|         smartPopupSuggestionSelectionIndex: bindings.popup.context.smart.suggestionSelectionIndex,
00071|         smartPopupSuggestionsFocused: bindings.popup.context.smart.suggestionsFocused,
00072|         onSmartPopupDraftChange: bindings.popup.context.smart.onDraftChange,
00073|         onSmartRootSubmit: bindings.popup.context.smart.onRootSubmit,
00074|       },
00075|       history: {
00076|         historyPopupItems: bindings.popup.history.items,
00077|         onHistoryPopupDraftChange: bindings.popup.history.onDraftChange,
00078|         onHistoryPopupSubmit: bindings.popup.history.onSubmit,
00079|       },
00080|       resume: {
00081|         onResumePayloadPathDraftChange: bindings.popup.misc.onResumePayloadPathDraftChange,
00082|         onResumeSubmit: popupManager.actions.handleResumeSubmit,
00083|       },
00084|       export: {
00085|         onExportOutPathDraftChange: bindings.popup.misc.onExportOutPathDraftChange,
00086|         onExportSubmit: popupManager.actions.handleExportSubmit,
00087|       },
00088|       intent: {
00089|         intentPopupSuggestions: bindings.popup.intent.suggestions,
00090|         intentPopupSuggestionSelectionIndex: bindings.popup.intent.suggestionSelectionIndex,
00091|         intentPopupSuggestionsFocused: bindings.popup.intent.suggestionsFocused,
00092|         onIntentPopupDraftChange: bindings.popup.intent.onDraftChange,
00093|         onIntentFileSubmit: popupManager.actions.handleIntentFileSubmit,
00094|       },
00095|       instructions: {
00096|         onInstructionsDraftChange: bindings.popup.misc.onInstructionsDraftChange,
00097|         onInstructionsSubmit: popupManager.actions.handleInstructionsSubmit,
00098|       },
00099|       series: {
00100|         isGenerating: generation.isGenerating,
00101|         onSeriesDraftChange: bindings.popup.misc.onSeriesDraftChange,
00102|         onSeriesSubmit: bindings.submit.onSeriesSubmit,
00103|       },
00104|       test: {
00105|         isTestCommandRunning: history.isTestCommandRunning,
00106|         onTestDraftChange: bindings.popup.misc.onTestDraftChange,
00107|         onTestSubmit: history.onTestPopupSubmit,
00108|       },
00109|       tokens: {
00110|         tokenUsageRun: generation.tokenUsageRun,
00111|         tokenUsageBreakdown: generation.tokenUsageBreakdown,
00112|         maxContextTokens: context.maxContextTokens,
00113|         maxInputTokens: context.maxInputTokens,
00114|         contextOverflowStrategy: context.contextOverflowStrategy,
00115|         latestContextOverflow: generation.latestContextOverflow,
00116|       },
00117|       budgets: {
00118|         onBudgetsMaxContextTokensDraftChange:
00119|           bindings.popup.misc.onBudgetsMaxContextTokensDraftChange,
00120|         onBudgetsMaxInputTokensDraftChange: bindings.popup.misc.onBudgetsMaxInputTokensDraftChange,
00121|         onBudgetsSubmit: popupManager.actions.handleBudgetsSubmit,

(File has more lines. Use 'offset' parameter to read beyond line 121)
</file>
```
```

---

## Assistant

_Thinking:_

**Extending context for PDFs**

I need to add PDFs to the popup context by incorporating `pdfs: context.pdfs` and ensuring that suggestions are passed from `bindings.popup.context.pdf`. I’m wondering if the bindings actually include the PDF part. While we added PDF glue, I have to extend the `useCommandScreenPopupBindings` result types and structure. Right now, it only includes `context.video`, so I need to locate where the output object is built in `useCommandScreenPopupBindings.ts` around the relevant return point.

```
Tool: grep

**Input:**
```json
{
  "pattern": "popup:\\s*\\{\\s*context",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Output:**
```
No files found
```
```

---

## Assistant

```
Tool: grep

**Input:**
```json
{
  "pattern": "context:\\s*\\{\\s*file:",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Output:**
```
No files found
```
```

---

## Assistant

```
Tool: grep

**Input:**
```json
{
  "pattern": "popup\\.context|context: \\{\\s*file",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Output:**
```
No files found
```
```

---

## Assistant

_Thinking:_

**Inspecting object structure**

I think the return object structure might be different, so I need to take a closer look at the part where it's building the return object at the bottom. It seems wise to search for "return {" around offset 420 to find out what's going on there. This method should help me uncover any inconsistencies or errors in the current structure. Let’s see what I can find!

```
Tool: grep

**Input:**
```json
{
  "pattern": "return \\{|popup: \\{",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Output:**
```
Found 2 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts:
  Line 46:   popup: {
  Line 145:   popup: {
```
```

---

## Assistant

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "offset": 140,
  "limit": 90
}
```
**Output:**
```
<file>
00141|   submit: {
00142|     onSubmit: (value: string) => void
00143|     onSeriesSubmit: (value: string) => void
00144|   }
00145|   popup: {
00146|     model: {
00147|       options: ModelOption[]
00148|       recentCount: number
00149|       selection: number
00150|       onQueryChange: (next: string) => void
00151|     }
00152|     history: {
00153|       items: string[]
00154|       onDraftChange: (next: string) => void
00155|       onSubmit: (value: string) => void
00156|     }
00157|     intent: {
00158|       suggestions: string[]
00159|       suggestionSelectionIndex: number
00160|       suggestionsFocused: boolean
00161|       onDraftChange: (next: string) => void
00162|     }
00163|     context: {
00164|       file: {
00165|         suggestions: string[]
00166|         suggestionSelectionIndex: number
00167|         suggestionsFocused: boolean
00168|         onDraftChange: (next: string) => void
00169|         onAdd: (value: string) => void
00170|         onRemove: (index: number) => void
00171|       }
00172|       url: {
00173|         onDraftChange: (next: string) => void
00174|         onAdd: (value: string) => void
00175|         onRemove: (index: number) => void
00176|       }
00177|       image: {
00178|         suggestions: string[]
00179|         suggestionSelectionIndex: number
00180|         suggestionsFocused: boolean
00181|         onDraftChange: (next: string) => void
00182|         onAdd: (value: string) => void
00183|         onRemove: (index: number) => void
00184|       }
00185|       video: {
00186|         suggestions: string[]
00187|         suggestionSelectionIndex: number
00188|         suggestionsFocused: boolean
00189|         onDraftChange: (next: string) => void
00190|         onAdd: (value: string) => void
00191|         onRemove: (index: number) => void
00192|       }
00193|       smart: {
00194|         suggestions: string[]
00195|         suggestionSelectionIndex: number
00196|         suggestionsFocused: boolean
00197|         onDraftChange: (next: string) => void
00198|         onRootSubmit: (value: string) => void
00199|       }
00200|     }
00201|     misc: {
00202|       onSeriesDraftChange: (next: string) => void
00203|       onInstructionsDraftChange: (next: string) => void
00204|       onTestDraftChange: (next: string) => void
00205|       onBudgetsMaxContextTokensDraftChange: (next: string) => void
00206|       onBudgetsMaxInputTokensDraftChange: (next: string) => void
00207|       onResumePayloadPathDraftChange: (next: string) => void
00208|       onExportOutPathDraftChange: (next: string) => void
00209|     }
00210|     reasoning: {
00211|       lines: HistoryEntry[]
00212|       visibleRows: number
00213|     }
00214|   }
00215| }
00216| 
00217| export const useCommandScreenPopupBindings = (
00218|   options: UseCommandScreenPopupBindingsOptions,
00219| ): UseCommandScreenPopupBindingsResult => {
00220|   const notify = useStableCallback((message: string) => {
00221|     options.context.notify(message)
00222|   })
00223| 
00224|   const pushHistory = useStableCallback(
00225|     (content: string, kind: HistoryEntry['kind'] = 'system', format?: HistoryEntry['format']) => {
00226|       options.history.pushHistory(content, kind, format)
00227|     },
00228|   )
00229| 
00230|   const paste = useCommandScreenPasteBindings({

(File has more lines. Use 'offset' parameter to read beyond line 230)
</file>
```
```

---

## Assistant

_Thinking:_

**Patching the popup object**

I need to add a PDF in the `popup.context` type and make sure it’s included in the returned object at the bottom. So, what comes next is to patch this correctly. When I'm assembling the `popup` object, particularly around the return section at the end, I’ll need to locate the relevant part to do this. I think searching for `pdfPopupSuggestions` could help, and I might use grep for that.

```
Tool: grep

**Input:**
```json
{
  "pattern": "pdfPopupSuggestions|context: \\{\\n\\s*file|videoPopupSuggestions",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Error:**
```
Error: ripgrep failed: rg: the literal "\n" is not allowed in a regex

Consider enabling multiline mode with the --multiline flag (or -U for short).
When multiline mode is enabled, new line characters can be matched.

```
```

---

## Assistant

```
Tool: grep

**Input:**
```json
{
  "pattern": "videoPopupSuggestions|pdfPopupSuggestions|onAddPdf",
  "include": "src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts"
}
```
**Output:**
```
Found 7 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts:
  Line 365:       suggestions: context.videoPopupSuggestions,
  Line 372:       suggestions: context.pdfPopupSuggestions,
  Line 373:       onAdd: context.onAddPdf,
  Line 491:           suggestions: context.videoPopupSuggestions,
  Line 493:           suggestionsFocused: context.videoPopupSuggestionsFocused,
  Line 548:       context.videoPopupSuggestions,
  Line 550:       context.videoPopupSuggestionsFocused,
```
```

---

## Assistant

_Thinking:_

I’m noticing that the return object already has the PDF configuration, but it’s missing type definitions. To correct this, I’ll patch the type area around line 185. I need to edit the UseCommandScreenPopupBindingsResult type to include the PDF in the popup context. That’s located at the top, so my plan is to insert “pdf” right after “video.” This should help clarify things and ensure everything aligns correctly.

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "oldString": "      video: {\n        suggestions: string[]\n        suggestionSelectionIndex: number\n        suggestionsFocused: boolean\n        onDraftChange: (next: string) => void\n        onAdd: (value: string) => void\n        onRemove: (index: number) => void\n      }\n      smart: {\n",
  "newString": "      video: {\n        suggestions: string[]\n        suggestionSelectionIndex: number\n        suggestionsFocused: boolean\n        onDraftChange: (next: string) => void\n        onAdd: (value: string) => void\n        onRemove: (index: number) => void\n      }\n      pdf: {\n        suggestions: string[]\n        suggestionSelectionIndex: number\n        suggestionsFocused: boolean\n        onDraftChange: (next: string) => void\n        onAdd: (value: string) => void\n        onRemove: (index: number) => void\n      }\n      smart: {\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [577:3] Type '{ input: { tokenLabel: (token: string) => string | null; onChange: (next: string) => void; }; submit: { onSubmit: (value: string) => void; onSeriesSubmit: (value: string) => void; }; popup: { ...; }; }' is not assignable to type 'UseCommandScreenPopupBindingsResult'.
  The types of 'popup.context' are incompatible between these types.
    Property 'pdf' is missing in type '{ file: { suggestions: string[]; suggestionSelectionIndex: number; suggestionsFocused: boolean; onDraftChange: (next: string) => void; onAdd: (value: string) => void; onRemove: (index: number) => void; }; url: { ...; }; image: { ...; }; video: { ...; }; smart: { ...; }; }' but required in type '{ file: { suggestions: string[]; suggestionSelectionIndex: number; suggestionsFocused: boolean; onDraftChange: (next: string) => void; onAdd: (value: string) => void; onRemove: (index: number) => void; }; ... 4 more ...; smart: { ...; }; }'.
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_



```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "offset": 450,
  "limit": 160
}
```
**Output:**
```
<file>
00451|       onSubmit: submit.handleSubmit,
00452|       onSeriesSubmit: submit.onSeriesSubmit,
00453|     }),
00454|     [submit.handleSubmit, submit.onSeriesSubmit],
00455|   )
00456| 
00457|   const popup = useMemo(
00458|     () => ({
00459|       model: {
00460|         options: modelPopupOptions,
00461|         recentCount: modelPopupRecentCount,
00462|         selection: modelPopupSelection,
00463|         onQueryChange: miscDraftHandlers.onModelPopupQueryChange,
00464|       },
00465|       history: {
00466|         items: historyAndIntent.history.historyPopupItems,
00467|         onDraftChange: historyAndIntent.history.onHistoryPopupDraftChange,
00468|         onSubmit: historyAndIntent.history.onHistoryPopupSubmit,
00469|       },
00470|       intent: {
00471|         suggestions: historyAndIntent.intent.intentPopupSuggestions,
00472|         suggestionSelectionIndex: historyAndIntent.intent.intentPopupSuggestionSelectionIndex,
00473|         suggestionsFocused: historyAndIntent.intent.intentPopupSuggestionsFocused,
00474|         onDraftChange: historyAndIntent.intent.onIntentPopupDraftChange,
00475|       },
00476|       context: {
00477|         file: {
00478|           suggestions: context.filePopupSuggestions,
00479|           suggestionSelectionIndex: context.filePopupSuggestionSelectionIndex,
00480|           suggestionsFocused: context.filePopupSuggestionsFocused,
00481|           onDraftChange: context.onFilePopupDraftChange,
00482|           onAdd: context.onAddFile,
00483|           onRemove: context.onRemoveFile,
00484|         },
00485|         url: {
00486|           onDraftChange: context.onUrlPopupDraftChange,
00487|           onAdd: context.onAddUrl,
00488|           onRemove: context.onRemoveUrl,
00489|         },
00490|         image: {
00491|           suggestions: context.imagePopupSuggestions,
00492|           suggestionSelectionIndex: context.imagePopupSuggestionSelectionIndex,
00493|           suggestionsFocused: context.imagePopupSuggestionsFocused,
00494|           onDraftChange: context.onImagePopupDraftChange,
00495|           onAdd: context.onAddImage,
00496|           onRemove: context.onRemoveImage,
00497|         },
00498|         video: {
00499|           suggestions: context.videoPopupSuggestions,
00500|           suggestionSelectionIndex: context.videoPopupSuggestionSelectionIndex,
00501|           suggestionsFocused: context.videoPopupSuggestionsFocused,
00502|           onDraftChange: context.onVideoPopupDraftChange,
00503|           onAdd: context.onAddVideo,
00504|           onRemove: context.onRemoveVideo,
00505|         },
00506|         smart: {
00507|           suggestions: context.smartPopupSuggestions,
00508|           suggestionSelectionIndex: context.smartPopupSuggestionSelectionIndex,
00509|           suggestionsFocused: context.smartPopupSuggestionsFocused,
00510|           onDraftChange: context.onSmartPopupDraftChange,
00511|           onRootSubmit: context.onSmartRootSubmit,
00512|         },
00513|       },
00514|       misc: {
00515|         onSeriesDraftChange: miscDraftHandlers.onSeriesDraftChange,
00516|         onInstructionsDraftChange: miscDraftHandlers.onInstructionsDraftChange,
00517|         onTestDraftChange: miscDraftHandlers.onTestDraftChange,
00518|         onBudgetsMaxContextTokensDraftChange:
00519|           miscDraftHandlers.onBudgetsMaxContextTokensDraftChange,
00520|         onBudgetsMaxInputTokensDraftChange: miscDraftHandlers.onBudgetsMaxInputTokensDraftChange,
00521|         onResumePayloadPathDraftChange: miscDraftHandlers.onResumePayloadPathDraftChange,
00522|         onExportOutPathDraftChange: miscDraftHandlers.onExportOutPathDraftChange,
00523|       },
00524|       reasoning: {
00525|         lines: reasoningPopupLines,
00526|         visibleRows: reasoningPopupVisibleRows,
00527|       },
00528|     }),
00529|     [
00530|       modelPopupOptions,
00531|       modelPopupRecentCount,
00532|       modelPopupSelection,
00533|       miscDraftHandlers.onModelPopupQueryChange,
00534|       historyAndIntent.history.historyPopupItems,
00535|       historyAndIntent.history.onHistoryPopupDraftChange,
00536|       historyAndIntent.history.onHistoryPopupSubmit,
00537|       historyAndIntent.intent.intentPopupSuggestions,
00538|       historyAndIntent.intent.intentPopupSuggestionSelectionIndex,
00539|       historyAndIntent.intent.intentPopupSuggestionsFocused,
00540|       historyAndIntent.intent.onIntentPopupDraftChange,
00541|       context.filePopupSuggestions,
00542|       context.filePopupSuggestionSelectionIndex,
00543|       context.filePopupSuggestionsFocused,
00544|       context.onFilePopupDraftChange,
00545|       context.onAddFile,
00546|       context.onRemoveFile,
00547|       context.onUrlPopupDraftChange,
00548|       context.onAddUrl,
00549|       context.onRemoveUrl,
00550|       context.imagePopupSuggestions,
00551|       context.imagePopupSuggestionSelectionIndex,
00552|       context.imagePopupSuggestionsFocused,
00553|       context.onImagePopupDraftChange,
00554|       context.onAddImage,
00555|       context.onRemoveImage,
00556|       context.videoPopupSuggestions,
00557|       context.videoPopupSuggestionSelectionIndex,
00558|       context.videoPopupSuggestionsFocused,
00559|       context.onVideoPopupDraftChange,
00560|       context.onAddVideo,
00561|       context.onRemoveVideo,
00562|       context.smartPopupSuggestions,
00563|       context.smartPopupSuggestionSelectionIndex,
00564|       context.smartPopupSuggestionsFocused,
00565|       context.onSmartPopupDraftChange,
00566|       context.onSmartRootSubmit,
00567|       miscDraftHandlers.onSeriesDraftChange,
00568|       miscDraftHandlers.onInstructionsDraftChange,
00569|       miscDraftHandlers.onTestDraftChange,
00570|       miscDraftHandlers.onBudgetsMaxContextTokensDraftChange,
00571|       miscDraftHandlers.onBudgetsMaxInputTokensDraftChange,
00572|       reasoningPopupLines,
00573|       reasoningPopupVisibleRows,
00574|     ],
00575|   )
00576| 
00577|   return useMemo(
00578|     () => ({
00579|       input,
00580|       submit: submitGroup,
00581|       popup,
00582|     }),
00583|     [input, submitGroup, popup],
00584|   )
00585| }
00586| 

(End of file - total 586 lines)
</file>
```
```

---

## Assistant

_Thinking:_

**Updating PDF context**

I’m noticing that the return object isn’t including the PDF context block, which needs my attention. I have to add a `pdf` section to `popup.context` in the `useMemo` and make sure it's included in the dependencies list. We had the PDF in the keyboard shortcuts call but not in the returned values. I’ll patch this by adding the PDF block after the video block and adjusting the dependencies accordingly. Time to proceed with the edit!

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "oldString": "        video: {\n          suggestions: context.videoPopupSuggestions,\n          suggestionSelectionIndex: context.videoPopupSuggestionSelectionIndex,\n          suggestionsFocused: context.videoPopupSuggestionsFocused,\n          onDraftChange: context.onVideoPopupDraftChange,\n          onAdd: context.onAddVideo,\n          onRemove: context.onRemoveVideo,\n        },\n        smart: {\n",
  "newString": "        video: {\n          suggestions: context.videoPopupSuggestions,\n          suggestionSelectionIndex: context.videoPopupSuggestionSelectionIndex,\n          suggestionsFocused: context.videoPopupSuggestionsFocused,\n          onDraftChange: context.onVideoPopupDraftChange,\n          onAdd: context.onAddVideo,\n          onRemove: context.onRemoveVideo,\n        },\n        pdf: {\n          suggestions: context.pdfPopupSuggestions,\n          suggestionSelectionIndex: context.pdfPopupSuggestionSelectionIndex,\n          suggestionsFocused: context.pdfPopupSuggestionsFocused,\n          onDraftChange: context.onPdfPopupDraftChange,\n          onAdd: context.onAddPdf,\n          onRemove: context.onRemovePdf,\n        },\n        smart: {\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts",
  "oldString": "      context.videoPopupSuggestions,\n      context.videoPopupSuggestionSelectionIndex,\n      context.videoPopupSuggestionsFocused,\n      context.onVideoPopupDraftChange,\n      context.onAddVideo,\n      context.onRemoveVideo,\n      context.smartPopupSuggestions,\n",
  "newString": "      context.videoPopupSuggestions,\n      context.videoPopupSuggestionSelectionIndex,\n      context.videoPopupSuggestionsFocused,\n      context.onVideoPopupDraftChange,\n      context.onAddVideo,\n      context.onRemoveVideo,\n      context.pdfPopupSuggestions,\n      context.pdfPopupSuggestionSelectionIndex,\n      context.pdfPopupSuggestionsFocused,\n      context.onPdfPopupDraftChange,\n      context.onAddPdf,\n      context.onRemovePdf,\n      context.smartPopupSuggestions,\n"
}
```
```

---

## Assistant

_Thinking:_



```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupAndView.ts",
  "oldString": "        videos: context.videos,\n        videoPopupSuggestions: bindings.popup.context.video.suggestions,\n        videoPopupSuggestionSelectionIndex: bindings.popup.context.video.suggestionSelectionIndex,\n        videoPopupSuggestionsFocused: bindings.popup.context.video.suggestionsFocused,\n        onVideoPopupDraftChange: bindings.popup.context.video.onDraftChange,\n        onAddVideo: bindings.popup.context.video.onAdd,\n        smartContextEnabled: context.smartContextEnabled,\n",
  "newString": "        videos: context.videos,\n        videoPopupSuggestions: bindings.popup.context.video.suggestions,\n        videoPopupSuggestionSelectionIndex: bindings.popup.context.video.suggestionSelectionIndex,\n        videoPopupSuggestionsFocused: bindings.popup.context.video.suggestionsFocused,\n        onVideoPopupDraftChange: bindings.popup.context.video.onDraftChange,\n        onAddVideo: bindings.popup.context.video.onAdd,\n        pdfs: context.pdfs,\n        pdfPopupSuggestions: bindings.popup.context.pdf.suggestions,\n        pdfPopupSuggestionSelectionIndex: bindings.popup.context.pdf.suggestionSelectionIndex,\n        pdfPopupSuggestionsFocused: bindings.popup.context.pdf.suggestionsFocused,\n        onPdfPopupDraftChange: bindings.popup.context.pdf.onDraftChange,\n        onAddPdf: bindings.popup.context.pdf.onAdd,\n        smartContextEnabled: context.smartContextEnabled,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [67:9] Object literal may only specify known properties, and 'pdfs' does not exist in type '{ files: string[]; filePopupSuggestions: string[]; filePopupSuggestionSelectionIndex: number; filePopupSuggestionsFocused: boolean; onFilePopupDraftChange: (next: string) => void; onAddFile: (value: string) => void; ... 21 more ...; onSmartRootSubmit: (value: string) => void; }'.
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_

**Searching for options type**

I need to check `useCommandScreenViewModel.types`, possibly in the `useCommandScreenViewModel.ts` file, to see if it defines the options type. It seems like the same thought repeated, so I’ll definitely search for it. I just want to make sure I’m looking in the right file to find the information I need. Let's see what I can find in there!

```
Tool: grep

**Input:**
```json
{
  "pattern": "context:\\s*\\{|popup:\\s*\\{\\s*base|type UseCommandScreenViewModelOptions",
  "include": "src/tui/screens/command/hooks/useCommandScreenViewModel.ts"
}
```
**Output:**
```
Found 2 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenViewModel.ts:
  Line 12: export type UseCommandScreenViewModelOptions = {
  Line 38:     context: {
```
```

---

## Assistant

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenViewModel.ts",
  "offset": 1,
  "limit": 80
}
```
**Output:**
```
<file>
00002| 
00003| import type { DebugKeyEvent } from '../../../components/core/MultilineTextInput'
00004| import type { HistoryEntry, ModelOption, PopupState, ProviderStatusMap } from '../../../types'
00005| import type { TokenUsageBreakdown, TokenUsageRun } from '../../../token-usage-store'
00006| 
00007| import type { CommandInputProps } from '../components/CommandInput'
00008| import type { CommandMenuPaneProps } from '../components/CommandMenuPane'
00009| import type { HistoryPaneProps } from '../components/HistoryPane'
00010| import type { PopupAreaProps } from '../components/PopupArea'
00011| 
00012| export type UseCommandScreenViewModelOptions = {
00013|   transport: {
00014|     isAwaitingTransportInput: boolean
00015|   }
00016| 
00017|   panes: {
00018|     history: HistoryPaneProps
00019|     menu: CommandMenuPaneProps
00020|   }
00021| 
00022|   popup: {
00023|     base: {
00024|       popupState: PopupState
00025|       helpOpen: boolean
00026|       overlayHeight: number
00027|     }
00028| 
00029|     model: {
00030|       modelPopupOptions: ModelOption[]
00031|       modelPopupSelection: number
00032|       modelPopupRecentCount: number
00033|       providerStatuses: ProviderStatusMap
00034|       onModelPopupQueryChange: (next: string) => void
00035|       onModelPopupSubmit: (option: ModelOption | null | undefined) => void
00036|     }
00037| 
00038|     context: {
00039|       files: string[]
00040|       filePopupSuggestions: string[]
00041|       filePopupSuggestionSelectionIndex: number
00042|       filePopupSuggestionsFocused: boolean
00043|       onFilePopupDraftChange: (next: string) => void
00044|       onAddFile: (value: string) => void
00045| 
00046|       urls: string[]
00047|       onUrlPopupDraftChange: (next: string) => void
00048|       onAddUrl: (value: string) => void
00049| 
00050|       images: string[]
00051|       imagePopupSuggestions: string[]
00052|       imagePopupSuggestionSelectionIndex: number
00053|       imagePopupSuggestionsFocused: boolean
00054|       onImagePopupDraftChange: (next: string) => void
00055|       onAddImage: (value: string) => void
00056| 
00057|       videos: string[]
00058|       videoPopupSuggestions: string[]
00059|       videoPopupSuggestionSelectionIndex: number
00060|       videoPopupSuggestionsFocused: boolean
00061|       onVideoPopupDraftChange: (next: string) => void
00062|       onAddVideo: (value: string) => void
00063| 
00064|       smartContextEnabled: boolean
00065|       smartContextRoot: string | null
00066|       smartPopupSuggestions: string[]
00067|       smartPopupSuggestionSelectionIndex: number
00068|       smartPopupSuggestionsFocused: boolean
00069|       onSmartPopupDraftChange: (next: string) => void
00070|       onSmartRootSubmit: (value: string) => void
00071|     }
00072| 
00073|     history: {
00074|       historyPopupItems: string[]
00075|       onHistoryPopupDraftChange: (next: string) => void
00076|       onHistoryPopupSubmit: (value: string) => void
00077|     }
00078| 
00079|     resume: {
00080|       onResumePayloadPathDraftChange: (next: string) => void
00081|       onResumeSubmit: () => void

(File has more lines. Use 'offset' parameter to read beyond line 81)
</file>
```
```

---

## Assistant

_Thinking:_

I need to add a PDF section to the options type. It seems like I should patch this in after the videos section. So, I'll go ahead and edit the file to make sure everything fits together nicely. Let’s see, I need to make sure this change aligns with the overall structure and that it’s clear for anyone who will use it. I’m double-checking everything to avoid any hiccups later!

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenViewModel.ts",
  "oldString": "      videos: string[]\n      videoPopupSuggestions: string[]\n      videoPopupSuggestionSelectionIndex: number\n      videoPopupSuggestionsFocused: boolean\n      onVideoPopupDraftChange: (next: string) => void\n      onAddVideo: (value: string) => void\n\n      smartContextEnabled: boolean\n",
  "newString": "      videos: string[]\n      videoPopupSuggestions: string[]\n      videoPopupSuggestionSelectionIndex: number\n      videoPopupSuggestionsFocused: boolean\n      onVideoPopupDraftChange: (next: string) => void\n      onAddVideo: (value: string) => void\n\n      pdfs: string[]\n      pdfPopupSuggestions: string[]\n      pdfPopupSuggestionSelectionIndex: number\n      pdfPopupSuggestionsFocused: boolean\n      onPdfPopupDraftChange: (next: string) => void\n      onAddPdf: (value: string) => void\n\n      smartContextEnabled: boolean\n"
}
```
```

---

## Assistant

_Thinking:_

**Evaluating the editing process**

I'm attempting to use the edit tool again since the previous attempt failed due to a type issue. I think my adjustments might succeed this time, but I need to check if the file is still unchanged. It seems like the initial edit may have applied, even though I encountered an error. So, I'll go ahead and reapply the edit to the same block now and see if it works. Let's find out!

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenPopupAndView.ts",
  "oldString": "        videos: context.videos,\n        videoPopupSuggestions: bindings.popup.context.video.suggestions,\n        videoPopupSuggestionSelectionIndex: bindings.popup.context.video.suggestionSelectionIndex,\n        videoPopupSuggestionsFocused: bindings.popup.context.video.suggestionsFocused,\n        onVideoPopupDraftChange: bindings.popup.context.video.onDraftChange,\n        onAddVideo: bindings.popup.context.video.onAdd,\n        smartContextEnabled: context.smartContextEnabled,\n",
  "newString": "        videos: context.videos,\n        videoPopupSuggestions: bindings.popup.context.video.suggestions,\n        videoPopupSuggestionSelectionIndex: bindings.popup.context.video.suggestionSelectionIndex,\n        videoPopupSuggestionsFocused: bindings.popup.context.video.suggestionsFocused,\n        onVideoPopupDraftChange: bindings.popup.context.video.onDraftChange,\n        onAddVideo: bindings.popup.context.video.onAdd,\n        pdfs: context.pdfs,\n        pdfPopupSuggestions: bindings.popup.context.pdf.suggestions,\n        pdfPopupSuggestionSelectionIndex: bindings.popup.context.pdf.suggestionSelectionIndex,\n        pdfPopupSuggestionsFocused: bindings.popup.context.pdf.suggestionsFocused,\n        onPdfPopupDraftChange: bindings.popup.context.pdf.onDraftChange,\n        onAddPdf: bindings.popup.context.pdf.onAdd,\n        smartContextEnabled: context.smartContextEnabled,\n"
}
```
```

---

## Assistant

_Thinking:_

**Fixing typecheck errors**

I'm focusing on making sure the bindings.popup.context includes the PDF section. I’ve updated the useCommandScreenPopupBindingsResult type and the return object to incorporate PDF, which is good progress! Now, I need to tackle the remaining typecheck errors that might still be lurking around. So, let’s go ahead and rerun the typecheck to see what needs fixing next. It feels like I'm getting close to wrapping this up!

```
Tool: bash

**Input:**
```json
{
  "command": "npm run typecheck",
  "description": "Run TypeScript typecheck"
}
```
**Output:**
```

> @perceptron/prompt-maker-cli@0.1.0 typecheck
> tsc -p tsconfig.json --noEmit


```
```

---

## Assistant

_Thinking:_



```
Tool: grep

**Input:**
```json
{
  "pattern": "videos\\b|images\\b|pdfs\\b",
  "include": "src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts"
}
```
**Output:**
```
Found 6 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts:
  Line 23:   images: string[]
  Line 24:   videos: string[]
  Line 62:   images,
  Line 63:   videos,
  Line 125:     images,
  Line 126:     videos,
```
```

---

## Assistant

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts",
  "offset": 1,
  "limit": 160
}
```
**Output:**
```
<file>
00002| import { useCallback, useEffect, useRef, useState } from 'react'
00003| 
00004| import type { BudgetSettings } from '../../../budget-settings'
00005| import type { HistoryEntry, ModelOption } from '../../../types'
00006| 
00007| import { DEFAULT_MODEL_ID, getPreferredModelId } from '../../../model-options'
00008| import { resolveDefaultGenerateModel } from '../../../../prompt-generator-service'
00009| 
00010| import { useModelProviderState } from './useModelProviderState'
00011| import { useCommandGenerationPipeline } from './useCommandGenerationPipeline'
00012| 
00013| export type UseCommandScreenModelGenerationOptions = {
00014|   pushHistoryProxy: (
00015|     content: string,
00016|     kind?: HistoryEntry['kind'],
00017|     format?: HistoryEntry['format'],
00018|   ) => void
00019|   notify: (message: string, options?: NotifyOptions) => void
00020| 
00021|   files: string[]
00022|   urls: string[]
00023|   images: string[]
00024|   videos: string[]
00025| 
00026|   smartContextEnabled: boolean
00027|   smartContextRoot: string | null
00028| 
00029|   metaInstructions: string
00030|   budgets: BudgetSettings
00031|   interactiveTransportPath?: string | undefined
00032|   terminalColumns: number
00033| 
00034|   copyEnabled: boolean
00035|   chatGptEnabled: boolean
00036|   jsonOutputEnabled: boolean
00037| 
00038|   isTestCommandRunning: boolean
00039| 
00040|   setLastReasoning: (value: string | null) => void
00041|   setLastGeneratedPrompt: (value: string | null) => void
00042| }
00043| 
00044| export type UseCommandScreenModelGenerationResult = {
00045|   modelOptions: ReturnType<typeof useModelProviderState>['modelOptions']
00046|   currentModel: ReturnType<typeof useModelProviderState>['currentModel']
00047|   selectModel: ReturnType<typeof useModelProviderState>['selectModel']
00048|   polishModelId: ModelOption['id'] | null
00049|   selectPolishModel: (nextId: ModelOption['id'] | null) => void
00050|   currentTargetModel: ModelOption['id']
00051|   selectTargetModel: (nextId: ModelOption['id']) => void
00052|   providerStatuses: ReturnType<typeof useModelProviderState>['providerStatuses']
00053|   updateProviderStatus: ReturnType<typeof useModelProviderState>['updateProviderStatus']
00054|   pipeline: ReturnType<typeof useCommandGenerationPipeline>
00055| }
00056| 
00057| export const useCommandScreenModelGeneration = ({
00058|   pushHistoryProxy,
00059|   notify,
00060|   files,
00061|   urls,
00062|   images,
00063|   videos,
00064|   smartContextEnabled,
00065|   smartContextRoot,
00066|   metaInstructions,
00067|   budgets,
00068|   interactiveTransportPath,
00069|   terminalColumns,
00070|   copyEnabled,
00071|   chatGptEnabled,
00072|   jsonOutputEnabled,
00073|   isTestCommandRunning,
00074|   setLastReasoning,
00075|   setLastGeneratedPrompt,
00076| }: UseCommandScreenModelGenerationOptions): UseCommandScreenModelGenerationResult => {
00077|   const { modelOptions, currentModel, selectModel, providerStatuses, updateProviderStatus } =
00078|     useModelProviderState({ pushHistory: pushHistoryProxy })
00079| 
00080|   const [polishModelId, setPolishModelIdState] = useState<ModelOption['id'] | null>(null)
00081| 
00082|   const selectPolishModel = useCallback((nextId: ModelOption['id'] | null) => {
00083|     setPolishModelIdState((prev) => (prev === nextId ? prev : nextId))
00084|   }, [])
00085| 
00086|   const [currentTargetModel, setCurrentTargetModelState] =
00087|     useState<ModelOption['id']>(DEFAULT_MODEL_ID)
00088| 
00089|   const userSelectedTargetModelRef = useRef(false)
00090| 
00091|   const selectTargetModel = useCallback((nextId: ModelOption['id']) => {
00092|     userSelectedTargetModelRef.current = true
00093|     setCurrentTargetModelState((prev) => (prev === nextId ? prev : nextId))
00094|   }, [])
00095| 
00096|   useEffect(() => {
00097|     let cancelled = false
00098| 
00099|     const syncDefaultTargetModel = async (): Promise<void> => {
00100|       if (userSelectedTargetModelRef.current) {
00101|         return
00102|       }
00103| 
00104|       const resolvedDefault = await resolveDefaultGenerateModel().catch(() => null)
00105|       if (cancelled || userSelectedTargetModelRef.current) {
00106|         return
00107|       }
00108| 
00109|       const preferred = getPreferredModelId(modelOptions, resolvedDefault)
00110|       setCurrentTargetModelState((prev) => (prev === preferred ? prev : preferred))
00111|     }
00112| 
00113|     void syncDefaultTargetModel()
00114| 
00115|     return () => {
00116|       cancelled = true
00117|     }
00118|   }, [modelOptions])
00119| 
00120|   const pipeline = useCommandGenerationPipeline({
00121|     pushHistory: pushHistoryProxy,
00122|     notify,
00123|     files,
00124|     urls,
00125|     images,
00126|     videos,
00127|     smartContextEnabled,
00128|     smartContextRoot,
00129|     metaInstructions,
00130|     budgets,
00131|     currentModel,
00132|     targetModel: currentTargetModel,
00133|     interactiveTransportPath,
00134|     terminalColumns,
00135|     polishModelId,
00136|     jsonOutputEnabled,
00137| 
00138|     copyEnabled,
00139|     chatGptEnabled,
00140|     isTestCommandRunning,
00141|     onProviderStatusUpdate: updateProviderStatus,
00142|     onReasoningUpdate: setLastReasoning,
00143|     onLastGeneratedPromptUpdate: setLastGeneratedPrompt,
00144|   })
00145| 
00146|   return {
00147|     modelOptions,
00148|     currentModel,
00149|     selectModel,
00150|     polishModelId,
00151|     selectPolishModel,
00152|     currentTargetModel,
00153|     selectTargetModel,
00154| 
00155|     providerStatuses,
00156|     updateProviderStatus,
00157|     pipeline,
00158|   }
00159| }
00160| 

(End of file - total 160 lines)
</file>
```
```

---

## Assistant

_Thinking:_

**Inspecting command generation**

I need to add PDFs to the options for the command-generation pipeline. It looks like we're using `useCommandGenerationPipeline.ts`, so I’ll need to update that to accept PDFs and pass them along to generate the necessary arguments. It’s a good idea to quickly inspect the `useCommandGenerationPipeline.ts` file to see how we can efficiently implement this change. Let's get started!

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "offset": 1,
  "limit": 140
}
```
**Output:**
```
<file>
00002| 
00003| import type { BudgetSettings } from '../../../budget-settings'
00004| import { useGenerationPipeline } from '../../../hooks/useGenerationPipeline'
00005| import type { NotifyOptions } from '../../../notifier'
00006| import { createTokenUsageStore } from '../../../token-usage-store'
00007| import type { HistoryEntry, ProviderStatus } from '../../../types'
00008| import type { TokenUsageBreakdown, TokenUsageRun } from '../../../token-usage-store'
00009| 
00010| export type UseCommandGenerationPipelineOptions = {
00011|   pushHistory: (
00012|     content: string,
00013|     kind?: HistoryEntry['kind'],
00014|     format?: HistoryEntry['format'],
00015|   ) => void
00016|   notify?: (message: string, options?: NotifyOptions) => void
00017| 
00018|   files: string[]
00019|   urls: string[]
00020|   images: string[]
00021|   videos: string[]
00022| 
00023|   smartContextEnabled: boolean
00024|   smartContextRoot: string | null
00025| 
00026|   metaInstructions: string
00027|   budgets: BudgetSettings
00028|   currentModel: string
00029|   targetModel: string
00030|   interactiveTransportPath?: string | undefined
00031|   terminalColumns: number
00032| 
00033|   polishModelId: string | null
00034|   jsonOutputEnabled: boolean
00035|   copyEnabled: boolean
00036|   chatGptEnabled: boolean
00037| 
00038|   isTestCommandRunning: boolean
00039| 
00040|   onProviderStatusUpdate: (status: ProviderStatus) => void
00041|   onReasoningUpdate: (reasoning: string | null) => void
00042|   onLastGeneratedPromptUpdate: (prompt: string) => void
00043| }
00044| 
00045| export type UseCommandGenerationPipelineResult = {
00046|   isGenerating: boolean
00047|   runGeneration: ReturnType<typeof useGenerationPipeline>['runGeneration']
00048|   runSeriesGeneration: ReturnType<typeof useGenerationPipeline>['runSeriesGeneration']
00049|   statusChips: string[]
00050|   latestContextOverflow: ReturnType<typeof useGenerationPipeline>['latestContextOverflow']
00051|   isAwaitingRefinement: boolean
00052|   submitRefinement: ReturnType<typeof useGenerationPipeline>['submitRefinement']
00053|   awaitingInteractiveMode: ReturnType<typeof useGenerationPipeline>['awaitingInteractiveMode']
00054| 
00055|   tokenUsageRun: TokenUsageRun | null
00056|   tokenUsageBreakdown: TokenUsageBreakdown | null
00057| }
00058| 
00059| export const useCommandGenerationPipeline = ({
00060|   pushHistory,
00061|   notify,
00062|   files,
00063|   urls,
00064|   images,
00065|   videos,
00066|   smartContextEnabled,
00067|   smartContextRoot,
00068|   metaInstructions,
00069|   budgets,
00070|   currentModel,
00071|   targetModel,
00072|   interactiveTransportPath,
00073|   terminalColumns,
00074|   polishModelId,
00075|   jsonOutputEnabled,
00076|   copyEnabled,
00077|   chatGptEnabled,
00078|   isTestCommandRunning,
00079|   onProviderStatusUpdate,
00080|   onReasoningUpdate,
00081|   onLastGeneratedPromptUpdate,
00082| }: UseCommandGenerationPipelineOptions): UseCommandGenerationPipelineResult => {
00083|   const tokenUsageStoreRef = useRef<ReturnType<typeof createTokenUsageStore> | null>(null)
00084|   if (!tokenUsageStoreRef.current) {
00085|     tokenUsageStoreRef.current = createTokenUsageStore()
00086|   }
00087| 
00088|   const trimmedMetaInstructions = metaInstructions.trim()
00089| 
00090|   const pipeline = useGenerationPipeline({
00091|     pushHistory,
00092|     ...(notify ? { notify } : {}),
00093|     files,
00094|     urls,
00095|     images,
00096|     videos,
00097|     smartContextEnabled,
00098|     smartContextRoot,
00099|     currentModel,
00100|     targetModel,
00101|     ...(interactiveTransportPath ? { interactiveTransportPath } : {}),
00102|     terminalColumns,
00103|     metaInstructions: trimmedMetaInstructions,
00104|     budgets,
00105|     polishModelId,
00106|     jsonOutputEnabled,
00107| 
00108|     copyEnabled,
00109|     chatGptEnabled,
00110|     isTestCommandRunning,
00111|     tokenUsageStore: tokenUsageStoreRef.current,
00112|     onProviderStatusUpdate,
00113|     onReasoningUpdate,
00114|     onLastGeneratedPromptUpdate,
00115|   })
00116| 
00117|   const tokenUsageRun = tokenUsageStoreRef.current?.getLatestRun() ?? null
00118|   const tokenUsageBreakdown = tokenUsageStoreRef.current?.getLatestBreakdown() ?? null
00119| 
00120|   return {
00121|     isGenerating: pipeline.isGenerating,
00122|     runGeneration: pipeline.runGeneration,
00123|     runSeriesGeneration: pipeline.runSeriesGeneration,
00124|     statusChips: pipeline.statusChips,
00125|     latestContextOverflow: pipeline.latestContextOverflow,
00126|     isAwaitingRefinement: pipeline.isAwaitingRefinement,
00127|     submitRefinement: pipeline.submitRefinement,
00128|     awaitingInteractiveMode: pipeline.awaitingInteractiveMode,
00129|     tokenUsageRun,
00130|     tokenUsageBreakdown,
00131|   }
00132| }
00133| 

(End of file - total 133 lines)
</file>
```
```

---

## Assistant

_Thinking:_

**Updating pipeline for PDFs**

I need to add PDFs to both the option type and pass them into the `useGenerationPipeline`. Currently, the `useGenerationPipeline` hook in the specified file doesn’t handle PDFs. Earlier, we set `pdf: []` for arguments, but not for the pipeline context. My next step is updating that. The core pipeline builds `GenerateArgs` and calls the generate pipeline, so we have to ensure actual PDFs get included. I'll inspect the options type to incorporate images and videos as well.

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/hooks/useGenerationPipeline.ts",
  "offset": 40,
  "limit": 120
}
```
**Output:**
```
<file>
00041| import type { PromptGenerationRequest, SeriesResponse } from '../../prompt-generator-service'
00042| import { resolveFileContext } from '../../file-context'
00043| import { resolveSmartContextFiles } from '../../smart-context-service'
00044| import { resolveUrlContext } from '../../url-context'
00045| import type { UploadStateChange } from '../../prompt-generator-service'
00046| import { MODEL_PROVIDER_LABELS } from '../../model-providers'
00047| import { checkModelProviderStatus } from '../provider-status'
00048| import type { TokenUsageStore } from '../token-usage-store'
00049| import type { BudgetSettings } from '../budget-settings'
00050| import type { NotifyOptions } from '../notifier'
00051| import type { HistoryEntry, ProviderStatus, ResumeMode } from '../types'
00052| 
00053| export type UseGenerationPipelineOptions = {
00054|   pushHistory: (
00055|     content: string,
00056|     kind?: HistoryEntry['kind'],
00057|     format?: HistoryEntry['format'],
00058|   ) => void
00059|   notify?: (message: string, options?: NotifyOptions) => void
00060|   files: string[]
00061|   urls: string[]
00062|   images: string[]
00063|   videos: string[]
00064|   smartContextEnabled: boolean
00065|   smartContextRoot: string | null
00066|   metaInstructions: string
00067|   budgets: BudgetSettings
00068|   currentModel: string
00069|   targetModel?: string
00070|   interactiveTransportPath?: string | undefined
00071|   terminalColumns: number
00072|   polishModelId: string | null
00073|   jsonOutputEnabled: boolean
00074|   copyEnabled: boolean
00075|   chatGptEnabled: boolean
00076|   isTestCommandRunning: boolean
00077|   onProviderStatusUpdate?: (status: ProviderStatus) => void
00078|   tokenUsageStore?: TokenUsageStore
00079|   onReasoningUpdate?: (reasoning: string | null) => void
00080|   onLastGeneratedPromptUpdate?: (prompt: string) => void
00081| }
00082| 
00083| export const useGenerationPipeline = ({
00084|   pushHistory,
00085|   notify,
00086|   files,
00087|   urls,
00088|   images,
00089|   videos,
00090|   smartContextEnabled,
00091|   smartContextRoot,
00092|   metaInstructions,
00093|   budgets,
00094|   currentModel,
00095|   targetModel,
00096|   interactiveTransportPath,
00097|   terminalColumns,
00098|   polishModelId,
00099|   jsonOutputEnabled,
00100|   copyEnabled,
00101|   chatGptEnabled,
00102|   isTestCommandRunning,
00103|   onProviderStatusUpdate,
00104|   tokenUsageStore,
00105|   onReasoningUpdate,
00106|   onLastGeneratedPromptUpdate,
00107| }: UseGenerationPipelineOptions) => {
00108|   const [pipelineState, dispatch] = useReducer(
00109|     generationPipelineReducer,
00110|     INITIAL_GENERATION_PIPELINE_STATE,
00111|   )
00112| 
00113|   const {
00114|     isGenerating,
00115|     statusMessage,
00116|     isAwaitingRefinement,
00117|     awaitingInteractiveMode,
00118|     latestTelemetry,
00119|   } = pipelineState
00120|   const normalizedMetaInstructions = metaInstructions.trim()
00121| 
00122|   // “Stale closure” explanation (plain-English):
00123|   // React callbacks capture the variables that were in scope when they were created.
00124|   // If we keep a callback stable (so we don’t recreate it every render), it would
00125|   // otherwise keep using old values.
00126|   //
00127|   // Example: if `handleStreamEvent` closed over an old `terminalColumns`, it would
00128|   // keep wrapping text to the wrong width after the terminal is resized.
00129|   //
00130|   // Solution used here: keep the callback stable, but read changing values from refs
00131|   // (kept fresh via useLatestRef).
00132|   const pushHistoryRef = useLatestRef(pushHistory)
00133|   const tokenUsageStoreRef = useLatestRef(tokenUsageStore)
00134|   const terminalColumnsRef = useLatestRef(terminalColumns)
00135|   const interactiveTransportPathRef = useLatestRef(interactiveTransportPath)
00136|   const notifyRef = useLatestRef(notify)
00137| 
00138|   const activeRunIdRef = useRef<string | null>(null)
00139|   const lastGeneratedPromptUpdateRef = useLatestRef<((prompt: string) => void) | null>(
00140|     onLastGeneratedPromptUpdate ?? null,
00141|   )
00142| 
00143|   type PendingRefinement = {
00144|     requestId: number
00145|     resolveText: (text: string) => void
00146|   }
00147| 
00148|   const pendingRefinementRef = useRef<PendingRefinement | null>(null)
00149|   const refinementRequestIdRef = useRef(0)
00150|   const isGeneratingRef = useLatestRef(isGenerating)
00151|   const transportAwaitingHintShownRef = useRef(false)
00152| 
00153|   const setAwaitingInteractiveMode = useCallback(
00154|     (nextMode: InteractiveAwaitingMode | null, nextStatusMessage?: string): void => {
00155|       dispatch({
00156|         type: 'set-awaiting-interactive',
00157|         awaitingInteractiveMode: nextMode,
00158|         ...(nextStatusMessage ? { statusMessage: nextStatusMessage } : {}),
00159|       })
00160|     },

(File has more lines. Use 'offset' parameter to read beyond line 160)
</file>
```
```

---

## Assistant

_Thinking:_



```
Tool: grep

**Input:**
```json
{
  "pattern": "images:|videos:",
  "include": "src/tui/hooks/useGenerationPipeline.ts"
}
```
**Output:**
```
Found 5 matches
/Users/eroberts/Projects/prompt-maker/src/tui/hooks/useGenerationPipeline.ts:
  Line 62:   images: string[]
  Line 63:   videos: string[]
  Line 572:           images: [...images],
  Line 865:           images: [...images],
  Line 866:           videos: [...videos],
```
```

---

## Assistant

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/hooks/useGenerationPipeline.ts",
  "offset": 540,
  "limit": 80
}
```
**Output:**
```
<file>
00541|       setLatestTelemetry(null)
00542|       onReasoningUpdate?.(null)
00543| 
00544|       dispatch({ type: 'generation-start', statusMessage: 'Preparing generation…' })
00545|       transportAwaitingHintShownRef.current = false
00546|       pushHistoryRef.current('Starting generation…')
00547| 
00548|       let stopStatusMessage: string | undefined
00549| 
00550|       try {
00551|         const transportPath = interactiveTransportPathRef.current
00552|         const usesTransportInteractive = Boolean(transportPath)
00553| 
00554|         const usesTuiInteractiveDelegate = !usesTransportInteractive && !jsonOutputEnabled
00555| 
00556|         const shouldIgnoreContextForResume = Boolean(resume)
00557| 
00558|         const args: GenerateArgs = {
00559|           interactive: usesTransportInteractive || usesTuiInteractiveDelegate,
00560|           copy: false,
00561|           openChatGpt: false,
00562|           polish: polishEnabled,
00563|           json: jsonOutputEnabled,
00564|           quiet: true,
00565|           progress: false,
00566|           stream: 'none',
00567|           showContext: false,
00568|           contextFormat: 'text',
00569|           help: false,
00570|           context: shouldIgnoreContextForResume ? [] : [...files],
00571|           urls: shouldIgnoreContextForResume ? [] : [...urls],
00572|           images: [...images],
00573|           video: [...videos],
00574|           pdf: [],
00575|           smartContext: shouldIgnoreContextForResume ? false : smartContextEnabled,
00576|           model: generationModel,
00577|           target: normalizedTargetModel,
00578|           ...(budgets.maxInputTokens !== null ? { maxInputTokens: budgets.maxInputTokens } : {}),
00579|           ...(budgets.maxContextTokens !== null
00580|             ? { maxContextTokens: budgets.maxContextTokens }
00581|             : {}),
00582|           ...(budgets.contextOverflowStrategy !== null
00583|             ? { contextOverflow: budgets.contextOverflowStrategy }
00584|             : {}),
00585|         }
00586|         if (normalizedMetaInstructions) {
00587|           args.metaInstructions = normalizedMetaInstructions
00588|         }
00589|         if (trimmedIntentFile) {
00590|           args.intentFile = trimmedIntentFile
00591|         } else if (trimmedIntent) {
00592|           args.intent = trimmedIntent
00593|         }
00594| 
00595|         if (resume) {
00596|           args.resumeMode = resume.mode
00597|           if (resume.kind === 'history') {
00598|             args.resume = resume.selector
00599|           } else {
00600|             args.resumeFrom = resume.payloadPath
00601|           }
00602|         }
00603|         if (polishEnabled) {
00604|           args.polishModel = normalizedPolishModel
00605|         }
00606| 
00607|         if (!shouldIgnoreContextForResume && smartContextEnabled && smartContextRoot) {
00608|           args.smartContextRoot = smartContextRoot
00609|         }
00610|         if (transportPath) {
00611|           args.interactiveTransport = transportPath
00612|         }
00613| 
00614|         const options: GeneratePipelineOptions = {
00615|           onStreamEvent: handleStreamEvent,
00616|           ...(usesTuiInteractiveDelegate ? { interactiveDelegate } : {}),
00617|         }
00618| 
00619|         const result: GeneratePipelineResult = await runGeneratePipeline(args, options)
00620|         onReasoningUpdate?.(result.reasoning ?? null)

(File has more lines. Use 'offset' parameter to read beyond line 620)
</file>
```
```

---

## Assistant

_Thinking:_



Next I’ll wire `pdfs` through the TUI generation pipeline so PDFs selected in the UI are included in the actual generation requests (same as images/videos).

```
Tool: todowrite

**Input:**
```json
{
  "todos": [
    {
      "content": "Add PDF state to TUI context",
      "status": "completed",
      "priority": "high",
      "id": "1"
    },
    {
      "content": "Add PDF popup UI and commands",
      "status": "completed",
      "priority": "high",
      "id": "2"
    },
    {
      "content": "Wire TUI generation pipeline PDFs",
      "status": "in_progress",
      "priority": "high",
      "id": "3"
    },
    {
      "content": "Implement OpenAI PDF text extraction",
      "status": "pending",
      "priority": "high",
      "id": "4"
    },
    {
      "content": "Update tests for TUI and OpenAI",
      "status": "pending",
      "priority": "high",
      "id": "5"
    },
    {
      "content": "Run typecheck/tests/build",
      "status": "pending",
      "priority": "high",
      "id": "6"
    }
  ]
}
```
**Output:**
```
[
  {
    "content": "Add PDF state to TUI context",
    "status": "completed",
    "priority": "high",
    "id": "1"
  },
  {
    "content": "Add PDF popup UI and commands",
    "status": "completed",
    "priority": "high",
    "id": "2"
  },
  {
    "content": "Wire TUI generation pipeline PDFs",
    "status": "in_progress",
    "priority": "high",
    "id": "3"
  },
  {
    "content": "Implement OpenAI PDF text extraction",
    "status": "pending",
    "priority": "high",
    "id": "4"
  },
  {
    "content": "Update tests for TUI and OpenAI",
    "status": "pending",
    "priority": "high",
    "id": "5"
  },
  {
    "content": "Run typecheck/tests/build",
    "status": "pending",
    "priority": "high",
    "id": "6"
  }
]
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/hooks/useGenerationPipeline.ts",
  "oldString": "  images: string[]\n  videos: string[]\n",
  "newString": "  images: string[]\n  videos: string[]\n  pdfs: string[]\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/hooks/useGenerationPipeline.ts",
  "oldString": "  images,\n  videos,\n  smartContextEnabled,\n",
  "newString": "  images,\n  videos,\n  pdfs,\n  smartContextEnabled,\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/hooks/useGenerationPipeline.ts",
  "oldString": "          images: [...images],\n          video: [...videos],\n          pdf: [],\n",
  "newString": "          images: [...images],\n          video: [...videos],\n          pdf: [...pdfs],\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/hooks/useGenerationPipeline.ts",
  "oldString": "          images: [...images],\n          videos: [...videos],\n",
  "newString": "          images: [...images],\n          videos: [...videos],\n          pdfs: [...pdfs],\n"
}
```
```

---

## Assistant

_Thinking:_

**Updating arguments**

I'm considering whether I need to update the other arguments around the 865 mark. I previously inserted something in the prompt generation request, but there could be other instances where args.pdf is set to an empty list. So, I think it would be a good idea to do a search to find those occurrences and ensure everything is in line. Let's make sure all the necessary updates are consistent!

```
Tool: grep

**Input:**
```json
{
  "pattern": "pdf:\\s*\\[\\]|pdfs:\\s*\\[\\]",
  "include": "src/tui/hooks/useGenerationPipeline.ts"
}
```
**Output:**
```
No files found
```
```

---

## Assistant

_Thinking:_



```
Tool: todowrite

**Input:**
```json
{
  "todos": [
    {
      "content": "Add PDF state to TUI context",
      "status": "completed",
      "priority": "high",
      "id": "1"
    },
    {
      "content": "Add PDF popup UI and commands",
      "status": "completed",
      "priority": "high",
      "id": "2"
    },
    {
      "content": "Wire TUI generation pipeline PDFs",
      "status": "in_progress",
      "priority": "high",
      "id": "3"
    },
    {
      "content": "Implement OpenAI PDF text extraction",
      "status": "pending",
      "priority": "high",
      "id": "4"
    },
    {
      "content": "Update tests for TUI and OpenAI",
      "status": "pending",
      "priority": "high",
      "id": "5"
    },
    {
      "content": "Run typecheck/tests/build",
      "status": "pending",
      "priority": "high",
      "id": "6"
    }
  ]
}
```
**Output:**
```
[
  {
    "content": "Add PDF state to TUI context",
    "status": "completed",
    "priority": "high",
    "id": "1"
  },
  {
    "content": "Add PDF popup UI and commands",
    "status": "completed",
    "priority": "high",
    "id": "2"
  },
  {
    "content": "Wire TUI generation pipeline PDFs",
    "status": "in_progress",
    "priority": "high",
    "id": "3"
  },
  {
    "content": "Implement OpenAI PDF text extraction",
    "status": "pending",
    "priority": "high",
    "id": "4"
  },
  {
    "content": "Update tests for TUI and OpenAI",
    "status": "pending",
    "priority": "high",
    "id": "5"
  },
  {
    "content": "Run typecheck/tests/build",
    "status": "pending",
    "priority": "high",
    "id": "6"
  }
]
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "oldString": "  images: string[]\n  videos: string[]\n",
  "newString": "  images: string[]\n  videos: string[]\n  pdfs: string[]\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "oldString": "  images,\n  videos,\n  smartContextEnabled,\n",
  "newString": "  images,\n  videos,\n  pdfs,\n  smartContextEnabled,\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "oldString": "    images,\n    videos,\n    smartContextEnabled,\n",
  "newString": "    images,\n    videos,\n    pdfs,\n    smartContextEnabled,\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "oldString": "     images,\n     videos,\n     smartContextEnabled,\n",
  "newString": "     images,\n     videos,\n     pdfs,\n     smartContextEnabled,\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "oldString": "     images,\n     videos,\n     smartContextEnabled,\n",
  "newString": "     images,\n     videos,\n     pdfs,\n     smartContextEnabled,\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "oldString": "     images,\n     videos,\n     smartContextEnabled,\n",
  "newString": "     images,\n     videos,\n     pdfs,\n     smartContextEnabled,\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "oldString": "    images,\n    videos,\n    smartContextEnabled,\n",
  "newString": "    images,\n    videos,\n    pdfs,\n    smartContextEnabled,\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "oldString": "    images,\n    videos,\n    smartContextEnabled,\n",
  "newString": "    images,\n    videos,\n    pdfs,\n    smartContextEnabled,\n"
}
```
```

---

## Assistant

_Thinking:_

**Inspecting file for duplicates**

I think we might have made too many redundant edits, which could risk corrupting the file. I need to take a closer look at the file to ensure everything is correct and to remove any duplicates if I find them. It's important to read the relevant portion carefully to catch any mistakes. Let's make sure everything is in good shape before proceeding further!

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "offset": 10,
  "limit": 40
}
```
**Output:**
```
<file>
00011|   pushHistory: (
00012|     content: string,
00013|     kind?: HistoryEntry['kind'],
00014|     format?: HistoryEntry['format'],
00015|   ) => void
00016|   notify?: (message: string, options?: NotifyOptions) => void
00017| 
00018|   files: string[]
00019|   urls: string[]
00020|   images: string[]
00021|   videos: string[]
00022|   pdfs: string[]
00023| 
00024|   smartContextEnabled: boolean
00025|   smartContextRoot: string | null
00026| 
00027|   metaInstructions: string
00028|   budgets: BudgetSettings
00029|   currentModel: string
00030|   targetModel: string
00031|   interactiveTransportPath?: string | undefined
00032|   terminalColumns: number
00033| 
00034|   polishModelId: string | null
00035|   jsonOutputEnabled: boolean
00036|   copyEnabled: boolean
00037|   chatGptEnabled: boolean
00038| 
00039|   isTestCommandRunning: boolean
00040| 
00041|   onProviderStatusUpdate: (status: ProviderStatus) => void
00042|   onReasoningUpdate: (reasoning: string | null) => void
00043|   onLastGeneratedPromptUpdate: (prompt: string) => void
00044| }
00045| 
00046| export type UseCommandGenerationPipelineResult = {
00047|   isGenerating: boolean
00048|   runGeneration: ReturnType<typeof useGenerationPipeline>['runGeneration']
00049|   runSeriesGeneration: ReturnType<typeof useGenerationPipeline>['runSeriesGeneration']
00050|   statusChips: string[]

(File has more lines. Use 'offset' parameter to read beyond line 50)
</file>
```
```

---

## Assistant

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandGenerationPipeline.ts",
  "offset": 56,
  "limit": 70
}
```
**Output:**
```
<file>
00057|   tokenUsageBreakdown: TokenUsageBreakdown | null
00058| }
00059| 
00060| export const useCommandGenerationPipeline = ({
00061|   pushHistory,
00062|   notify,
00063|   files,
00064|   urls,
00065|   images,
00066|   videos,
00067|   pdfs,
00068|   smartContextEnabled,
00069| 
00070|   smartContextRoot,
00071|   metaInstructions,
00072|   budgets,
00073|   currentModel,
00074|   targetModel,
00075|   interactiveTransportPath,
00076|   terminalColumns,
00077|   polishModelId,
00078|   jsonOutputEnabled,
00079|   copyEnabled,
00080|   chatGptEnabled,
00081|   isTestCommandRunning,
00082|   onProviderStatusUpdate,
00083|   onReasoningUpdate,
00084|   onLastGeneratedPromptUpdate,
00085| }: UseCommandGenerationPipelineOptions): UseCommandGenerationPipelineResult => {
00086|   const tokenUsageStoreRef = useRef<ReturnType<typeof createTokenUsageStore> | null>(null)
00087|   if (!tokenUsageStoreRef.current) {
00088|     tokenUsageStoreRef.current = createTokenUsageStore()
00089|   }
00090| 
00091|   const trimmedMetaInstructions = metaInstructions.trim()
00092| 
00093|   const pipeline = useGenerationPipeline({
00094|     pushHistory,
00095|     ...(notify ? { notify } : {}),
00096|     files,
00097|     urls,
00098|     images,
00099|     videos,
00100|     pdfs,
00101|     smartContextEnabled,
00102|     smartContextRoot,
00103|     currentModel,
00104|     targetModel,
00105|     ...(interactiveTransportPath ? { interactiveTransportPath } : {}),
00106|     terminalColumns,
00107|     metaInstructions: trimmedMetaInstructions,
00108|     budgets,
00109|     polishModelId,
00110|     jsonOutputEnabled,
00111| 
00112|     copyEnabled,
00113|     chatGptEnabled,
00114|     isTestCommandRunning,
00115|     tokenUsageStore: tokenUsageStoreRef.current,
00116|     onProviderStatusUpdate,
00117|     onReasoningUpdate,
00118|     onLastGeneratedPromptUpdate,
00119|   })
00120| 
00121|   const tokenUsageRun = tokenUsageStoreRef.current?.getLatestRun() ?? null
00122|   const tokenUsageBreakdown = tokenUsageStoreRef.current?.getLatestBreakdown() ?? null
00123| 
00124|   return {
00125|     isGenerating: pipeline.isGenerating,
00126|     runGeneration: pipeline.runGeneration,

(File has more lines. Use 'offset' parameter to read beyond line 126)
</file>
```
```

---

## Assistant

_Thinking:_



```
Tool: grep

**Input:**
```json
{
  "pattern": "pdfs",
  "include": "src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts"
}
```
**Output:**
```
Found 1 matches
/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts:
  Line 25:   pdfs: string[]
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts",
  "oldString": "  images: string[]\n  videos: string[]\n",
  "newString": "  images: string[]\n  videos: string[]\n  pdfs: string[]\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts",
  "oldString": "  images,\n  videos,\n",
  "newString": "  images,\n  videos,\n  pdfs,\n"
}
```
```

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts",
  "oldString": "    images,\n    videos,\n    smartContextEnabled,\n",
  "newString": "    images,\n    videos,\n    pdfs,\n    smartContextEnabled,\n"
}
```
```

---

## Assistant

_Thinking:_

**Updating command screen controller**

I need to update `useCommandScreenController.ts` to ensure that PDFs are correctly passed to the model generation pipeline. I remember that earlier, we tried to insert these PDFs into the `modelGeneration` call, but I’m not certain if it was done correctly or if we just attempted it. I'll check around line 92 to find the relevant piece of code and confirm if the implementation is as intended. Let's make sure it’s working properly!

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 88,
  "limit": 30
}
```
**Output:**
```
<file>
00089|   } = useContextDispatch()
00090| 
00091|   const historyAndTests = useCommandScreenHistoryAndTests()
00092| 
00093|   const inputState = useCommandScreenInputState({
00094|     pushHistoryProxy: historyAndTests.pushHistoryProxy,
00095|   })
00096| 
00097|   const modelAndGeneration: UseCommandScreenModelGenerationResult = useCommandScreenModelGeneration(
00098|     {
00099|       pushHistoryProxy: historyAndTests.pushHistoryProxy,
00100|       notify,
00101|       files,
00102|       urls,
00103|       images,
00104|       videos,
00105|       smartContextEnabled,
00106|       smartContextRoot,
00107|       metaInstructions,
00108|       budgets: { maxContextTokens, maxInputTokens, contextOverflowStrategy },
00109|       ...(interactiveTransportPath ? { interactiveTransportPath } : {}),
00110|       terminalColumns: inputState.terminalColumns,
00111|       copyEnabled: inputState.copyEnabled,
00112|       chatGptEnabled: inputState.chatGptEnabled,
00113|       jsonOutputEnabled: inputState.jsonOutputEnabled,
00114|       isTestCommandRunning: historyAndTests.isTestCommandRunning,
00115|       setLastReasoning,
00116|       setLastGeneratedPrompt,
00117|     },
00118|   )

(File has more lines. Use 'offset' parameter to read beyond line 118)
</file>
```
```

---

## Assistant

_Thinking:_



```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [98:5] Argument of type '{ terminalColumns: number; copyEnabled: boolean; chatGptEnabled: boolean; jsonOutputEnabled: boolean; isTestCommandRunning: boolean; setLastReasoning: (value: string | null) => void; ... 11 more ...; budgets: { ...; }; }' is not assignable to parameter of type 'UseCommandScreenModelGenerationOptions'.
  Property 'pdfs' is missing in type '{ terminalColumns: number; copyEnabled: boolean; chatGptEnabled: boolean; jsonOutputEnabled: boolean; isTestCommandRunning: boolean; setLastReasoning: (value: string | null) => void; ... 11 more ...; budgets: { ...; }; }' but required in type 'UseCommandScreenModelGenerationOptions'.
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_

**Updating file context**

I see that the edit didn’t apply due to multiple matches. That’s likely why the diagnostic from the file is still missing the PDFs. I need to ensure everything is updated properly. So, it seems I need to patch within the object to include more context. I’m planning to edit that call and replace `images,\n      videos,` with the inclusion of PDFs. This should help cover what’s missing!

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      urls,\n      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "      urls,\n      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
**Output:**
```

This file has errors, please fix
<file_diagnostics>
ERROR [98:5] Argument of type '{ terminalColumns: number; copyEnabled: boolean; chatGptEnabled: boolean; jsonOutputEnabled: boolean; isTestCommandRunning: boolean; setLastReasoning: (value: string | null) => void; ... 11 more ...; budgets: { ...; }; }' is not assignable to parameter of type 'UseCommandScreenModelGenerationOptions'.
  Property 'pdfs' is missing in type '{ terminalColumns: number; copyEnabled: boolean; chatGptEnabled: boolean; jsonOutputEnabled: boolean; isTestCommandRunning: boolean; setLastReasoning: (value: string | null) => void; ... 11 more ...; budgets: { ...; }; }' but required in type 'UseCommandScreenModelGenerationOptions'.
</file_diagnostics>

```
```

---

## Assistant

_Thinking:_



```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "offset": 98,
  "limit": 12
}
```
**Output:**
```
<file>
00099|       pushHistoryProxy: historyAndTests.pushHistoryProxy,
00100|       notify,
00101|       files,
00102|       urls,
00103|       images,
00104|       videos,
00105|       smartContextEnabled,
00106|       smartContextRoot,
00107|       metaInstructions,
00108|       budgets: { maxContextTokens, maxInputTokens, contextOverflowStrategy },
00109|       ...(interactiveTransportPath ? { interactiveTransportPath } : {}),
00110|       terminalColumns: inputState.terminalColumns,

(File has more lines. Use 'offset' parameter to read beyond line 110)
</file>
```
```

---

## Assistant

_Thinking:_



```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/tui/screens/command/hooks/useCommandScreenController.ts",
  "oldString": "      pushHistoryProxy: historyAndTests.pushHistoryProxy,\n      notify,\n      files,\n      urls,\n      images,\n      videos,\n      smartContextEnabled,\n",
  "newString": "      pushHistoryProxy: historyAndTests.pushHistoryProxy,\n      notify,\n      files,\n      urls,\n      images,\n      videos,\n      pdfs,\n      smartContextEnabled,\n"
}
```
```

---

## Assistant

_Thinking:_



```
Tool: bash

**Input:**
```json
{
  "command": "npm run typecheck",
  "description": "Run TypeScript typecheck"
}
```
**Output:**
```

> @perceptron/prompt-maker-cli@0.1.0 typecheck
> tsc -p tsconfig.json --noEmit

src/__tests__/useGenerationPipeline.test.ts(143,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; onProviderStatusUpdate: jest.Mock<any, any, any>; files: string[]; urls: string[]; images: string[]; videos: string[]; ... 11 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; onProviderStatusUpdate: jest.Mock<any, any, any>; files: string[]; urls: string[]; images: string[]; videos: string[]; ... 11 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(174,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(201,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; onLastGeneratedPromptUpdate: jest.Mock<any, any, any>; files: string[]; urls: string[]; images: string[]; ... 12 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; onLastGeneratedPromptUpdate: jest.Mock<any, any, any>; files: string[]; urls: string[]; images: string[]; ... 12 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(228,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; smartContextEnabled: true; images: string[]; videos: string[]; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; smartContextEnabled: true; images: string[]; videos: string[]; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(271,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; polishModelId: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; polishModelId: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(296,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(345,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; notify: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; isTestCommandRunning: boolean; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; notify: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; isTestCommandRunning: boolean; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(403,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; interactiveTransportPath: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; interactiveTransportPath: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(467,31): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; isTestCommandRunning: boolean; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; isTestCommandRunning: boolean; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(505,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; metaInstructions: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; metaInstructions: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(532,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; images: string[]; videos: string[]; files: string[]; urls: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; images: string[]; videos: string[]; files: string[]; urls: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(559,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; videos: string[]; files: string[]; urls: string[]; images: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; videos: string[]; files: string[]; urls: string[]; images: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.
src/__tests__/useGenerationPipeline.test.ts(593,29): error TS2345: Argument of type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' is not assignable to parameter of type 'UseGenerationPipelineOptions'.
  Property 'pdfs' is missing in type '{ pushHistory: jest.Mock<any, any, any>; currentModel: string; files: string[]; urls: string[]; images: string[]; videos: string[]; smartContextEnabled: boolean; smartContextRoot: null; ... 9 more ...; notify: jest.Mock<...>; }' but required in type 'UseGenerationPipelineOptions'.

```
```

---

## Assistant

_Thinking:_

**Updating test options**

I'm planning to update the useGenerationPipeline tests to include `pdfs: []` in the createOptions objects. It seems there will be quite a few instances to replace in the test file, so I'll need to add `pdfs: []` to the base options in those objects. My next step is to open the file and get started on making those changes. This should help ensure everything functions smoothly!

```
Tool: read

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/useGenerationPipeline.test.ts",
  "offset": 60,
  "limit": 60
}
```
**Output:**
```
<file>
00061| 
00062| const urlContextModule = jest.requireMock('../url-context') as {
00063|   resolveUrlContext: jest.Mock
00064| }
00065| 
00066| const smartContextModule = jest.requireMock('../smart-context-service') as {
00067|   resolveSmartContextFiles: jest.Mock
00068| }
00069| 
00070| const fsPromisesModule = jest.requireMock('node:fs/promises') as {
00071|   mkdir: jest.Mock
00072|   writeFile: jest.Mock
00073| }
00074| 
00075| const dom = new JSDOM('<!doctype html><html><body></body></html>')
00076| const globalEnv = globalThis as typeof globalThis & {
00077|   window: Window & typeof globalThis
00078|   document: Document
00079|   navigator: Navigator
00080| }
00081| globalEnv.window = dom.window as typeof globalEnv.window
00082| globalEnv.document = dom.window.document as Document
00083| globalEnv.navigator = dom.window.navigator
00084| 
00085| describe('useGenerationPipeline', () => {
00086|   const baseOptions = {
00087|     files: [] as string[],
00088|     urls: [] as string[],
00089|     images: [] as string[],
00090|     videos: [] as string[],
00091|     smartContextEnabled: false,
00092|     smartContextRoot: null,
00093|     metaInstructions: '',
00094|     budgets: {
00095|       maxContextTokens: null,
00096|       maxInputTokens: null,
00097|       contextOverflowStrategy: null,
00098|     },
00099|     interactiveTransportPath: undefined as string | undefined,
00100|     terminalColumns: 80,
00101|     polishModelId: null,
00102|     jsonOutputEnabled: false,
00103|     copyEnabled: false,
00104|     chatGptEnabled: false,
00105|     isTestCommandRunning: false,
00106|     notify: jest.fn(),
00107|   }
00108| 
00109|   beforeEach(() => {
00110|     jest.clearAllMocks()
00111| 
00112|     generateCommandModule.runGeneratePipeline.mockResolvedValue({
00113|       finalPrompt: 'Prompt',
00114|       model: 'gpt-4o-mini',
00115|       iterations: 1,
00116|       telemetry: null,
00117|       payload: {},
00118|     })
00119| 
00120|     promptGeneratorModule.generatePromptSeries.mockResolvedValue({

(File has more lines. Use 'offset' parameter to read beyond line 120)
</file>
```
```

---

## Assistant

_Thinking:_

**Evaluating baseOptions for PDFs**

I notice that the baseOptions are missing PDFs. To fix this, I'll add `pdfs: [] as string[]`. That should address the issue since the tests utilize spread baseOptions. It seems like a simple patch, but I want to make sure I'm covering all bases! I want to ensure everything works smoothly and fulfills the requirements, so I’ll double-check before making any changes to confirm it all integrates properly.

```
Tool: edit

**Input:**
```json
{
  "filePath": "/Users/eroberts/Projects/prompt-maker/src/__tests__/useGenerationPipeline.test.ts",
  "oldString": "    images: [] as string[],\n    videos: [] as string[],\n",
  "newString": "    images: [] as string[],\n    videos: [] as string[],\n    pdfs: [] as string[],\n"
}
```
```

---

## Assistant

```
Tool: bash

**Input:**
```json
{
  "command": "npm run typecheck",
  "description": "Run TypeScript typecheck"
}
```
**Output:**
```

> @perceptron/prompt-maker-cli@0.1.0 typecheck
> tsc -p tsconfig.json --noEmit


```
```

---

## Assistant

---

## User

please continue

---

## Assistant

---
</file>

<file path="docs/tui-design.md">
# Prompt Maker CLI TUI Design

This document describes the _current_ Ink-based TUI: its UX goals, input routing invariants, and how the code is organized.

If you’re looking for a user guide, the fastest way to learn the TUI is to run it and type `/help`.

## Goals

- **Keyboard-first**: no mouse required; command palette + popups cover most actions.
- **Safe input routing**: help/popup layers must “own” the keyboard (no fallthrough).
- **Shared core logic**: the TUI calls the same generation/test pipeline as the CLI.
- **Automation-friendly**: the CLI continues to support JSON/JSONL output; the TUI is optional.

## Entry points

- CLI routing: `src/index.ts`
  - No args defaults to `ui`.
  - `test` runs the prompt test runner.
  - Everything else runs the generate workflow.
- TUI renderer: `src/tui/index.tsx` (parses `--interactive-transport`, then mounts `AppContainer`).

## Global controls

Global keybinds are defined by `src/tui/app-container-keymap.ts`. The help overlay is opened via the `/help` command.

- `Ctrl+G`: open command palette (Generate view)
- `Ctrl+T`: switch to Test Runner view
- `/help`: open help overlay
- `Ctrl+C`: exit
- `Esc`: dismiss UI elements (never exits)

## Input routing invariants

Priority order (highest wins):

1. **Help overlay**: when help is open, it consumes almost all keys.
2. **Popup input**: when a popup is open, it “owns” the keyboard.
3. **Screen input**: active screen handles keys (history scrolling, typing, submission).
4. **AppContainer global keys**: only truly-global shortcuts should run here.

This is enforced by:

- `AppContainer` tracking `isHelpOpen` + `isPopupOpen` and gating global actions.
- Screens using `useInput(..., { isActive })` to ensure only the top layer handles keys.

## Screen model

The TUI has two views:

- **Generate** (`src/tui/screens/command/CommandScreen.tsx`)
- **Test Runner** (`src/tui/screens/test-runner/TestRunnerScreen.tsx`)

The view is managed by `src/tui/AppContainer.tsx`.

## Generate view UX

The Generate view is a scrollable history pane plus a single input bar:

- Type normal text → treated as an intent.
- Type `/` → command mode (command palette opens and filters).
- Drag & drop an absolute file path, then press `Tab` to attach it.

Commands are defined in `src/tui/config.ts` (`COMMAND_DESCRIPTORS`). The palette uses matching helpers in `src/tui/command-filter.ts`.

### Popups

Most commands open a popup:

- `/model`, `/target` → model selection
- `/file`, `/url`, `/image`, `/video` → add context items
- `/smart`, `/smart-root` → smart context controls
- `/theme`, `/theme-mode` → theme controls
- `/tokens`, `/budgets`, `/resume`, `/export`, `/settings`, `/history`, `/reasoning` → informational/workflow views

Popup transitions are managed with a pure reducer (`src/tui/popup-reducer.ts`) and a hook for effects (`src/tui/hooks/usePopupManager.ts`).

### Series generation

`/series` (or pressing `Tab`) produces a set of standalone “atomic prompts” (no cross references between steps). It writes the artifacts under a timestamped folder (see `src/tui/hooks/useGenerationPipeline.ts`).

## Test Runner UX

The Test Runner view runs suites defined by `prompt-tests.yaml` (or another YAML file). It uses the same underlying runner exported by `src/test-command.ts`.

## Theming

- Theme definitions and loader: `src/tui/theme/*`
- Some TUI settings are persisted into CLI config via `src/config.ts` (theme, budgets, resume defaults, export defaults).

Theme JSON examples live in `src/tui/theme/examples/`.
</file>

<file path="docs/tui-updates.md">
# TUI Parity Updates (implemented in PR #15)

This document originally described a plan to update the Ink + React TUI to support CLI “auditable workflows” introduced in PR #14.

Those planned items are now implemented on `feature/tui-cli-parity-refacto` (PR #15).

Scope constraints (still apply):

- Preserve input routing priority (**Help overlay > Popup > Screen > AppContainer globals**) as described in `docs/tui-design.md` and `src/tui/DEVELOPER_NOTE.md`.
- Keep reducers pure (`*-reducer.ts`), and keep effects in hooks.

---

## Summary

User-facing TUI parity improvements:

- `/budgets`: configure token budgets + overflow strategy (persists to config).
- `/resume`: resume generation from history or an exported payload file (persists default mode/source).
- `/export`: export a selected history payload to JSON/YAML (persists default format/output directory).
- Help overlay includes a “Workflows” section to make these discoverable.

CLI behavior/UX improvements related to these workflows:

- `export --from-history <selector>` now validates schema compatibility for the selected history entry and fails with an actionable error if the selected entry has an unsupported `schemaVersion`.

---

## TUI Workflow Commands

All of these commands are listed in the command palette (`Ctrl+G` or type `/`) and open a popup (no inline args parsing).

### `/budgets`

- Popup fields:
  - `Max context tokens` (optional integer)
  - `Max input tokens` (optional integer)
  - `Overflow strategy` (`fail`, `drop-smart`, `drop-url`, `drop-largest`, `drop-oldest`, or unset)
- Behavior:
  - Budgets are enabled when either token field is non-empty.
  - If budgets are enabled and overflow strategy is unset, the effective strategy defaults to `fail`.
  - Clearing both token fields disables budgets.
- Persistence:
  - Writes to CLI config under `promptGenerator.maxInputTokens`, `promptGenerator.maxContextTokens`, and `promptGenerator.contextOverflowStrategy`.

Implementation entrypoints:

- UI: `src/tui/components/popups/BudgetsPopup.tsx`
- Parsing: `src/tui/budget-settings.ts`
- Persistence: `src/config.ts` (`updateCliPromptGeneratorSettings`)

### `/resume`

- Popup fields:
  - `Source`: `history` or `file`
  - `Mode`: `best-effort` or `strict`
  - History picker (for `history`) or payload path input (for `file`)
- Behavior:
  - Resume runs can start even when the typed intent is empty (the resumed payload supplies the intent).
  - Resume only reuses `source:"file"` context paths. URL/smart entries are treated as missing.
  - `best-effort` warns (and continues) if some resumed files are missing.
  - `strict` fails if any resumed `source:"file"` context paths are missing.
- Persistence:
  - Writes `resumeMode` and `resumeSourceKind` to CLI config.

Implementation entrypoints:

- UI: `src/tui/components/popups/ResumePopup.tsx`
- History list: `src/tui/resume-history.ts`
- Persistence: `src/config.ts` (`updateCliResumeSettings`)

### `/export`

- Popup fields:
  - `Format`: `json` or `yaml`
  - `Out`: output file path (relative to cwd or absolute)
  - History picker (newest-first list of `last`, `last:2`, …)
- Behavior:
  - Exports write the selected history payload to the chosen output path.
  - History entries with an unsupported `schemaVersion` are surfaced in the picker as unsupported; export fails if you attempt to export an unsupported entry.
- Persistence:
  - Writes `exportFormat` and `exportOutDir` (directory path) to CLI config.

Implementation entrypoints:

- UI: `src/tui/components/popups/ExportPopup.tsx`
- History picker + schema gating: `src/history/generate-history.ts`
- File writing helper: `src/export/export-generate-payload.ts`
- Persistence: `src/config.ts` (`updateCliExportSettings`)

---

## Config keys (relevant to parity workflows)

Config is loaded from (`src/config.ts`):

1. `PROMPT_MAKER_CLI_CONFIG=/path/to/config.json`
2. `~/.config/prompt-maker-cli/config.json`
3. `~/.prompt-maker-cli.json`

Relevant persisted keys:

- Budgets:
  - `promptGenerator.maxInputTokens`
  - `promptGenerator.maxContextTokens`
  - `promptGenerator.contextOverflowStrategy`
- Resume defaults:
  - `resumeMode` (`best-effort` | `strict`)
  - `resumeSourceKind` (`history` | `file`)
- Export defaults:
  - `exportFormat` (`json` | `yaml`)
  - `exportOutDir` (directory path)

---

## Help overlay

Help overlay text is generated in `src/tui/help-config.ts`.

PR #15 adds a “Workflows” section that calls out:

- `/budgets`
- `/resume`
- `/export`
- A note that `compose` is CLI-only (not implemented in the TUI)
</file>

<file path="src/__tests__/generate/resume-from-file.test.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
import { appendToHistory } from '../../history-logger'
import { runGeneratePipeline } from '../../generate/pipeline'
import {
  GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
  type GenerateArgs,
  type GenerateJsonPayload,
  type StreamEventInput,
} from '../../generate/types'
import { serializeGeneratePayload } from '../../generate/payload-io'
import { createPromptGeneratorService } from '../../prompt-generator-service'
import { resolveHistoryFilePath } from '../../history-logger'
const promptService = {
  generatePrompt: jest.fn<Promise<string>, []>(),
}
jest.mock('clipboardy', () => ({ write: jest.fn() }))
jest.mock('open', () => jest.fn())
jest.mock('../../config', () => ({
  loadCliConfig: jest.fn().mockResolvedValue(null),
}))
jest.mock('../../prompt-generator-service', () => ({
  createPromptGeneratorService: jest.fn(),
  resolveDefaultGenerateModel: jest.fn().mockResolvedValue('gpt-4o-mini'),
  isGemini: jest.fn(() => false),
}))
jest.mock('../../generate/models', () => ({
  resolveTargetModel: jest.fn(
    async ({
      defaultTargetModel,
      explicitTarget,
    }: {
      defaultTargetModel: string
      explicitTarget?: string
    }) => explicitTarget ?? defaultTargetModel,
  ),
  resolveGeminiVideoModel: jest.fn().mockResolvedValue('gemini-1.5-pro'),
}))
jest.mock('../../history-logger', () => ({
  appendToHistory: jest.fn().mockResolvedValue(undefined),
  resolveHistoryFilePath: jest.fn(() => 'history.jsonl'),
}))
jest.mock('../../io', () => ({
  readFromStdin: jest.fn().mockResolvedValue(null),
}))
const mockAppendToHistory = appendToHistory as jest.Mock
const mockResolveHistoryFilePath = resolveHistoryFilePath as jest.Mock
const mockCreatePromptGeneratorService = createPromptGeneratorService as jest.Mock
const createTempDir = async (): Promise<string> =>
  fs.mkdtemp(path.join(os.tmpdir(), 'prompt-maker-resume-from-file-'))
const createBaseArgs = (): GenerateArgs => ({
  interactive: false,
  copy: false,
  openChatGpt: false,
  polish: false,
  json: false,
  quiet: true,
  progress: false,
  stream: 'none',
  showContext: false,
  contextFormat: 'text',
  help: false,
  context: [],
  urls: [],
  images: [],
  video: [],
  pdf: [],
  smartContext: false,
})
describe('runGeneratePipeline --resume-from', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    mockCreatePromptGeneratorService.mockResolvedValue(promptService)
  })
  it('loads a JSON payload file, seeds state, and emits resume.loaded with source=file', async () => {
    const dir = await createTempDir()
    try {
      const existingPath = path.join(dir, 'existing.md')
      const missingPath = path.join(dir, 'missing.md')
      await fs.writeFile(existingPath, 'existing content', 'utf8')
      const payloadPath = path.join(dir, 'payload.json')
      const resumedPayload: GenerateJsonPayload = {
        schemaVersion: GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
        intent: 'resumed intent',
        model: 'gpt-4o-mini',
        targetModel: 'gpt-4o-mini',
        prompt: 'previous prompt',
        polishedPrompt: 'previous polished',
        polishModel: 'polish-model',
        metaInstructions: 'Be concise',
        refinements: ['prior refinement'],
        iterations: 2,
        interactive: true,
        timestamp: '2025-01-01T00:00:00.000Z',
        contextPaths: [
          { path: existingPath, source: 'file' },
          { path: missingPath, source: 'file' },
          { path: 'url:https://example.com', source: 'url' },
        ],
      }
      await fs.writeFile(payloadPath, serializeGeneratePayload(resumedPayload, 'json'), 'utf8')
      const events: StreamEventInput[] = []
      const warnSpy = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
      await runGeneratePipeline(
        {
          ...createBaseArgs(),
          resumeFrom: payloadPath,
          resumeMode: 'best-effort',
        },
        {
          onStreamEvent: (event) => {
            events.push(event)
          },
        },
      )
      warnSpy.mockRestore()
      const resumeEvent = events.find(
        (event): event is Extract<StreamEventInput, { event: 'resume.loaded' }> =>
          event.event === 'resume.loaded',
      )
      expect(resumeEvent).toBeDefined()
      expect(resumeEvent?.source).toBe('file')
      expect(resumeEvent?.reusedContextPaths).toEqual([{ path: existingPath, source: 'file' }])
      expect(resumeEvent?.missingContextPaths).toEqual(
        expect.arrayContaining([
          { path: missingPath, source: 'file' },
          { path: 'url:https://example.com', source: 'url' },
        ]),
      )
      expect(mockResolveHistoryFilePath).not.toHaveBeenCalled()
      expect(promptService.generatePrompt).not.toHaveBeenCalled()
      expect(mockAppendToHistory).toHaveBeenCalledWith(
        expect.objectContaining({ intent: 'resumed intent', prompt: 'previous polished' }),
      )
    } finally {
      await fs.rm(dir, { recursive: true, force: true })
    }
  })
  it('loads a YAML payload file and emits resume.loaded with source=file', async () => {
    const dir = await createTempDir()
    try {
      const existingPath = path.join(dir, 'existing.md')
      await fs.writeFile(existingPath, 'existing content', 'utf8')
      const payloadPath = path.join(dir, 'payload.yaml')
      const resumedPayload: GenerateJsonPayload = {
        schemaVersion: GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
        intent: 'resumed intent',
        model: 'gpt-4o-mini',
        targetModel: 'gpt-4o-mini',
        prompt: 'previous prompt',
        refinements: [],
        iterations: 1,
        interactive: false,
        timestamp: '2025-01-01T00:00:00.000Z',
        contextPaths: [{ path: existingPath, source: 'file' }],
      }
      await fs.writeFile(payloadPath, serializeGeneratePayload(resumedPayload, 'yaml'), 'utf8')
      const events: StreamEventInput[] = []
      await runGeneratePipeline(
        {
          ...createBaseArgs(),
          resumeFrom: payloadPath,
          resumeMode: 'best-effort',
        },
        {
          onStreamEvent: (event) => {
            events.push(event)
          },
        },
      )
      const resumeEvent = events.find(
        (event): event is Extract<StreamEventInput, { event: 'resume.loaded' }> =>
          event.event === 'resume.loaded',
      )
      expect(resumeEvent).toBeDefined()
      expect(resumeEvent?.source).toBe('file')
      expect(resumeEvent?.reusedContextPaths).toEqual([{ path: existingPath, source: 'file' }])
      expect(resumeEvent?.missingContextPaths).toEqual([])
      expect(mockResolveHistoryFilePath).not.toHaveBeenCalled()
      expect(promptService.generatePrompt).not.toHaveBeenCalled()
    } finally {
      await fs.rm(dir, { recursive: true, force: true })
    }
  })
  it('fails fast with a descriptive error when the payload file cannot be parsed', async () => {
    const dir = await createTempDir()
    try {
      const payloadPath = path.join(dir, 'payload.json')
      await fs.writeFile(payloadPath, '{ not-valid-json', 'utf8')
      await expect(
        runGeneratePipeline({
          ...createBaseArgs(),
          resumeFrom: payloadPath,
          resumeMode: 'best-effort',
        }),
      ).rejects.toThrow(/Failed to parse JSON/i)
      expect(mockCreatePromptGeneratorService).not.toHaveBeenCalled()
      expect(mockAppendToHistory).not.toHaveBeenCalled()
    } finally {
      await fs.rm(dir, { recursive: true, force: true })
    }
  })
})
</file>

<file path="src/__tests__/tui/buffered-history-writer.test.ts">
import { createBufferedHistoryWriter } from '../../tui/hooks/buffered-history-writer'
describe('buffered-history-writer', () => {
  it('buffers writes until flush, preserving order', () => {
    const push = jest.fn()
    let scheduleCalls = 0
    const writer = createBufferedHistoryWriter({
      push,
      scheduleFlush: () => {
        scheduleCalls += 1
      },
    })
    writer.pushBuffered('first')
    writer.pushBuffered('second', 'progress')
    expect(push).not.toHaveBeenCalled()
    expect(scheduleCalls).toBe(1)
    writer.flush()
    expect(push.mock.calls).toEqual([
      ['first', undefined, undefined],
      ['second', 'progress', undefined],
    ])
    writer.pushBuffered('third')
    expect(push.mock.calls).toHaveLength(2)
    expect(scheduleCalls).toBe(2)
    writer.flush()
    expect(push.mock.calls).toEqual([
      ['first', undefined, undefined],
      ['second', 'progress', undefined],
      ['third', undefined, undefined],
    ])
  })
  it('pushManyBuffered schedules once and preserves kinds', () => {
    const push = jest.fn()
    let scheduleCalls = 0
    const writer = createBufferedHistoryWriter({
      push,
      scheduleFlush: () => {
        scheduleCalls += 1
      },
    })
    writer.pushManyBuffered([
      { content: 'a', kind: 'system' },
      { content: 'b', kind: 'progress' },
      { content: 'c' },
    ])
    expect(push).not.toHaveBeenCalled()
    expect(scheduleCalls).toBe(1)
    writer.flush()
    expect(push.mock.calls).toEqual([
      ['a', 'system', undefined],
      ['b', 'progress', undefined],
      ['c', undefined, undefined],
    ])
  })
})
</file>

<file path="src/__tests__/tui/context-popup-glue-contract.test.ts">
import { renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
jest.mock('ink', () => ({
  useInput: jest.fn(),
}))
import {
  useContextPopupGlue,
  type UseContextPopupGlueOptions,
} from '../../tui/screens/command/hooks/useContextPopupGlue'
const dom = new JSDOM('<!doctype html><html><body></body></html>')
type GlobalDom = { window: Window; document: Document }
beforeAll(() => {
  const target = globalThis as unknown as GlobalDom
  target.window = dom.window as unknown as Window
  target.document = dom.window.document
})
afterAll(() => {
  const target = globalThis as unknown as Partial<GlobalDom>
  delete target.window
  delete target.document
})
describe('useContextPopupGlue (shape contract)', () => {
  it('returns stable keys and safe-to-call handlers', () => {
    const options: UseContextPopupGlueOptions = {
      inputValue: '',
      popupState: null,
      helpOpen: false,
      isPopupOpen: false,
      isCommandMode: false,
      isCommandMenuActive: false,
      isGenerating: false,
      droppedFilePath: null,
      files: [],
      urls: [],
      images: [],
      videos: [],
      pdfs: [],
      smartContextEnabled: false,
      smartContextRoot: null,
      addFile: jest.fn(),
      removeFile: jest.fn(),
      addUrl: jest.fn(),
      removeUrl: jest.fn(),
      updateUrl: jest.fn(),
      addImage: jest.fn(),
      removeImage: jest.fn(),
      addVideo: jest.fn(),
      removeVideo: jest.fn(),
      addPdf: jest.fn(),
      removePdf: jest.fn(),
      toggleSmartContext: jest.fn(),
      setSmartRoot: jest.fn(),
      setInputValue: jest.fn(),
      setPopupState: jest.fn(),
      suppressNextInput: jest.fn(),
      notify: jest.fn(),
      pushHistory: jest.fn(),
      addCommandHistoryEntry: jest.fn(),
      handleCommandSelection: jest.fn(),
      consumeSuppressedTextInputChange: jest.fn(() => false),
      isFilePath: jest.fn(() => true),
    }
    const { result } = renderHook(() => useContextPopupGlue(options))
    const expectedKeys = [
      'filePopupSuggestions',
      'filePopupSuggestionSelectionIndex',
      'filePopupSuggestionsFocused',
      'onFilePopupDraftChange',
      'onAddFile',
      'onRemoveFile',
      'onUrlPopupDraftChange',
      'onAddUrl',
      'onRemoveUrl',
      'imagePopupSuggestions',
      'imagePopupSuggestionSelectionIndex',
      'imagePopupSuggestionsFocused',
      'onImagePopupDraftChange',
      'onAddImage',
      'onRemoveImage',
      'videoPopupSuggestions',
      'videoPopupSuggestionSelectionIndex',
      'videoPopupSuggestionsFocused',
      'onVideoPopupDraftChange',
      'onAddVideo',
      'onRemoveVideo',
      'pdfPopupSuggestions',
      'pdfPopupSuggestionSelectionIndex',
      'pdfPopupSuggestionsFocused',
      'onPdfPopupDraftChange',
      'onAddPdf',
      'onRemovePdf',
      'smartPopupSuggestions',
      'smartPopupSuggestionSelectionIndex',
      'smartPopupSuggestionsFocused',
      'onSmartPopupDraftChange',
      'onSmartToggle',
      'onSmartRootSubmit',
    ].sort()
    expect(Object.keys(result.current).sort()).toEqual(expectedKeys)
    expect(() => result.current.onFilePopupDraftChange('x')).not.toThrow()
    expect(() => result.current.onAddFile('/tmp/file.txt')).not.toThrow()
    expect(() => result.current.onRemoveFile(0)).not.toThrow()
    expect(() => result.current.onUrlPopupDraftChange('https://example.com')).not.toThrow()
    expect(() => result.current.onAddUrl('https://example.com')).not.toThrow()
    expect(() => result.current.onRemoveUrl(0)).not.toThrow()
    expect(() => result.current.onImagePopupDraftChange('x')).not.toThrow()
    expect(() => result.current.onAddImage('/tmp/image.png')).not.toThrow()
    expect(() => result.current.onRemoveImage(0)).not.toThrow()
    expect(() => result.current.onVideoPopupDraftChange('x')).not.toThrow()
    expect(() => result.current.onAddVideo('/tmp/video.mp4')).not.toThrow()
    expect(() => result.current.onRemoveVideo(0)).not.toThrow()
    expect(() => result.current.onSmartPopupDraftChange('x')).not.toThrow()
    expect(() => result.current.onSmartToggle(true)).not.toThrow()
    expect(() => result.current.onSmartRootSubmit('/tmp')).not.toThrow()
  })
})
</file>

<file path="src/__tests__/tui/generation-history-formatters.test.ts">
import {
  buildIterationCompleteHistoryMessages,
  buildJsonPayloadHistoryMessages,
  wrapTextForHistory,
} from '../../tui/hooks/generation-history-formatters'
jest.mock('wrap-ansi', () => jest.fn())
const wrapAnsiMock = jest.requireMock('wrap-ansi') as jest.Mock
describe('generation-history-formatters', () => {
  beforeEach(() => {
    wrapAnsiMock.mockReset()
  })
  it('wrapTextForHistory preserves order and splits wrapped lines', () => {
    wrapAnsiMock.mockImplementation((text: string) => {
      if (text === 'abc') {
        return 'a\nbc'
      }
      return text
    })
    expect(wrapTextForHistory('abc\ndef', 12)).toEqual(['a', 'bc', 'def'])
    expect(wrapAnsiMock).toHaveBeenCalledTimes(2)
    expect(wrapAnsiMock).toHaveBeenCalledWith('abc', 12, expect.objectContaining({ hard: true }))
    expect(wrapAnsiMock).toHaveBeenCalledWith('def', 12, expect.objectContaining({ hard: true }))
  })
  it('buildIterationCompleteHistoryMessages matches existing text output', () => {
    wrapAnsiMock.mockImplementation((text: string) => text)
    expect(
      buildIterationCompleteHistoryMessages({
        iteration: 2,
        tokens: 123,
        prompt: 'Line 1\nLine 2',
        wrapWidth: 80,
      }),
    ).toEqual([
      { content: 'Iteration 2 complete (123 tokens)', kind: 'progress' },
      { content: 'Prompt (iteration 2):', kind: 'system' },
      { content: 'Line 1', kind: 'system', format: 'markdown' },
      { content: 'Line 2', kind: 'system', format: 'markdown' },
    ])
    expect(
      buildIterationCompleteHistoryMessages({
        iteration: 1,
        tokens: 100,
        reasoningTokens: 50,
        prompt: 'Only line',
        wrapWidth: 80,
      })[0],
    ).toEqual({
      content: 'Iteration 1 complete (100 prompt tokens · 50 reasoning tokens)',
      kind: 'progress',
    })
  })
  it('buildJsonPayloadHistoryMessages pretty prints the payload', () => {
    wrapAnsiMock.mockImplementation((text: string) => text)
    expect(buildJsonPayloadHistoryMessages({ a: 1 }, 80)).toEqual([
      { content: 'JSON payload:', kind: 'system' },
      { content: '{', kind: 'system' },
      { content: '  "a": 1', kind: 'system' },
      { content: '}', kind: 'system' },
    ])
  })
})
</file>

<file path="src/__tests__/tui/list-windowing.test.ts">
import {
  clampSelectionIndex,
  ensureLeadingHeaderVisible,
  resolveWindowedValues,
} from '../../tui/components/popups/list-windowing'
describe('list-windowing helpers', () => {
  describe('clampSelectionIndex', () => {
    it('returns 0 for empty lists', () => {
      expect(clampSelectionIndex(0, 0)).toBe(0)
      expect(clampSelectionIndex(0, 10)).toBe(0)
      expect(clampSelectionIndex(0, -5)).toBe(0)
    })
    it('clamps within list bounds', () => {
      expect(clampSelectionIndex(3, -1)).toBe(0)
      expect(clampSelectionIndex(3, 0)).toBe(0)
      expect(clampSelectionIndex(3, 2)).toBe(2)
      expect(clampSelectionIndex(3, 3)).toBe(2)
    })
  })
  describe('resolveWindowedValues', () => {
    it('matches resolveWindowedList boundaries', async () => {
      const { resolveWindowedList } = await import('../../tui/components/popups/list-window')
      const items = ['a', 'b', 'c', 'd', 'e', 'f']
      const result = resolveWindowedValues(items, 3, 4)
      const window = resolveWindowedList({
        itemCount: items.length,
        selectedIndex: 3,
        maxVisibleRows: 4,
      })
      expect(result.start).toBe(window.start)
      expect(result.end).toBe(window.end)
      expect(result.showBefore).toBe(window.showBefore)
      expect(result.showAfter).toBe(window.showAfter)
      expect(result.values).toEqual(items.slice(window.start, window.end))
    })
    it('handles empty items', () => {
      expect(resolveWindowedValues([], 0, 3)).toEqual({
        start: 0,
        end: 0,
        values: [],
        showBefore: false,
        showAfter: false,
      })
    })
    it('shows a single item without indicators', () => {
      expect(resolveWindowedValues(['only'], 0, 3)).toEqual({
        start: 0,
        end: 1,
        values: ['only'],
        showBefore: false,
        showAfter: false,
      })
    })
    it('windows with small row counts and indicators', () => {
      const items = ['a', 'b', 'c', 'd', 'e']
      expect(resolveWindowedValues(items, 0, 3)).toEqual({
        start: 0,
        end: 2,
        values: ['a', 'b'],
        showBefore: false,
        showAfter: true,
      })
      expect(resolveWindowedValues(items, 2, 3)).toEqual({
        start: 2,
        end: 3,
        values: ['c'],
        showBefore: true,
        showAfter: true,
      })
      expect(resolveWindowedValues(items, 4, 3)).toEqual({
        start: 3,
        end: 5,
        values: ['d', 'e'],
        showBefore: true,
        showAfter: false,
      })
    })
    it('supports a custom lead', () => {
      const items = Array.from({ length: 10 }, (_, index) => String(index))
      expect(resolveWindowedValues(items, 5, 5, { lead: 1 })).toEqual({
        start: 4,
        end: 7,
        values: ['4', '5', '6'],
        showBefore: true,
        showAfter: true,
      })
    })
    it('moves the window as the selection changes', () => {
      const items = Array.from({ length: 10 }, (_, index) => String(index))
      expect(resolveWindowedValues(items, 0, 5).start).toBe(0)
      expect(resolveWindowedValues(items, 5, 5).start).toBe(3)
      expect(resolveWindowedValues(items, 9, 5)).toEqual({
        start: 6,
        end: 10,
        values: ['6', '7', '8', '9'],
        showBefore: true,
        showAfter: false,
      })
    })
  })
  describe('ensureLeadingHeaderVisible', () => {
    type Row =
      | { type: 'header'; title: string }
      | { type: 'spacer' }
      | { type: 'option'; id: string }
    it('pulls the header into view when there is slack', () => {
      const rows: Row[] = [
        { type: 'header', title: 'Recent' },
        { type: 'option', id: 'a' },
        { type: 'option', id: 'b' },
      ]
      expect(ensureLeadingHeaderVisible(rows, { start: 1, end: 3 }, 5, 'header', 'option')).toEqual(
        {
          start: 0,
          end: 3,
        },
      )
    })
    it('does not shift the window when already full', () => {
      const rows: Row[] = [
        { type: 'header', title: 'Recent' },
        { type: 'option', id: 'a' },
        { type: 'option', id: 'b' },
        { type: 'option', id: 'c' },
        { type: 'option', id: 'd' },
        { type: 'option', id: 'e' },
      ]
      expect(ensureLeadingHeaderVisible(rows, { start: 1, end: 6 }, 5, 'header', 'option')).toEqual(
        {
          start: 1,
          end: 6,
        },
      )
    })
    it('does not shift when the previous row is not a header', () => {
      const rows: Row[] = [
        { type: 'spacer' },
        { type: 'option', id: 'a' },
        { type: 'option', id: 'b' },
      ]
      expect(ensureLeadingHeaderVisible(rows, { start: 1, end: 3 }, 5, 'header', 'option')).toEqual(
        {
          start: 1,
          end: 3,
        },
      )
    })
  })
})
</file>

<file path="src/__tests__/tui/model-popup-model.test.ts">
import {
  buildModelPopupRows,
  resolveModelPopupListRows,
  resolveModelPopupVisibleRows,
} from '../../tui/components/popups/model-popup-model'
import type { ModelProvider } from '../../model-providers'
import type { ModelOption } from '../../tui/types'
import type { ModelPopupRow } from '../../tui/components/popups/model-popup-model'
const makeOption = (id: string, provider: ModelProvider): ModelOption => ({
  id,
  label: id,
  provider,
  description: '',
  capabilities: [],
  source: 'builtin',
})
const describeRows = (rows: readonly ModelPopupRow[]): string[] =>
  rows.map((row) => {
    if (row.type === 'header') {
      return `H:${row.title}`
    }
    if (row.type === 'spacer') {
      return 'S'
    }
    return `O:${row.option.id}:${row.optionIndex}`
  })
describe('model-popup-model', () => {
  describe('resolveModelPopupListRows', () => {
    it('reserves padding and fixed rows', () => {
      expect(resolveModelPopupListRows({ popupHeight: 16, paddingY: 2, fixedRows: 6 })).toBe(6)
    })
    it('clamps to at least one row', () => {
      expect(resolveModelPopupListRows({ popupHeight: 0, paddingY: 2, fixedRows: 6 })).toBe(1)
    })
  })
  describe('buildModelPopupRows', () => {
    it('groups recents separately from provider sections', () => {
      const options = [
        makeOption('o0', 'openai'),
        makeOption('o1', 'openai'),
        makeOption('g2', 'gemini'),
        makeOption('g3', 'gemini'),
        makeOption('c4', 'other'),
      ]
      const rows = buildModelPopupRows(options, 2)
      expect(describeRows(rows)).toEqual([
        'H:Recent',
        'O:o0:0',
        'O:o1:1',
        'S',
        'H:Gemini',
        'O:g2:2',
        'O:g3:3',
        'H:Custom',
        'O:c4:4',
      ])
    })
    it('clamps recentCount to the option length', () => {
      const options = [makeOption('o0', 'openai'), makeOption('o1', 'openai')]
      const rows = buildModelPopupRows(options, 99)
      expect(describeRows(rows)).toEqual(['H:Recent', 'O:o0:0', 'O:o1:1'])
    })
  })
  describe('resolveModelPopupVisibleRows', () => {
    const options = [
      makeOption('openai-0', 'openai'),
      makeOption('openai-1', 'openai'),
      makeOption('openai-2', 'openai'),
      makeOption('openai-3', 'openai'),
      makeOption('openai-4', 'openai'),
      makeOption('openai-5', 'openai'),
      makeOption('gemini-6', 'gemini'),
      makeOption('gemini-7', 'gemini'),
      makeOption('gemini-8', 'gemini'),
      makeOption('gemini-9', 'gemini'),
    ]
    const rows = buildModelPopupRows(options, 0)
    it('pulls a section header into view when there is slack', () => {
      const result = resolveModelPopupVisibleRows({
        rows,
        selectedOptionIndex: 9,
        maxVisibleRows: 5,
      })
      expect(result.slice.start).toBe(7)
      expect(describeRows(result.visibleRows)).toEqual([
        'H:Gemini',
        'O:gemini-6:6',
        'O:gemini-7:7',
        'O:gemini-8:8',
        'O:gemini-9:9',
      ])
    })
    it('moves the window as the selection changes', () => {
      const top = resolveModelPopupVisibleRows({
        rows,
        selectedOptionIndex: 0,
        maxVisibleRows: 5,
      })
      const middle = resolveModelPopupVisibleRows({
        rows,
        selectedOptionIndex: 5,
        maxVisibleRows: 5,
      })
      const end = resolveModelPopupVisibleRows({
        rows,
        selectedOptionIndex: 9,
        maxVisibleRows: 5,
      })
      expect(top.slice.start).toBe(0)
      expect(describeRows(top.visibleRows)).toEqual([
        'H:OpenAI',
        'O:openai-0:0',
        'O:openai-1:1',
        'O:openai-2:2',
        'S',
      ])
      expect(middle.slice.start).toBe(4)
      expect(describeRows(middle.visibleRows)).toEqual([
        'O:openai-3:3',
        'O:openai-4:4',
        'O:openai-5:5',
        'S',
        'S',
      ])
      expect(end.slice.start).toBe(7)
    })
  })
})
</file>

<file path="src/__tests__/tui/tui-exit-clear-screen.test.ts">
import { runTuiCommand } from '../../tui'
type InkModule = {
  render: jest.Mock
}
jest.mock('ink', () => ({
  render: jest.fn(),
}))
jest.mock('../../tui/AppContainer', () => ({
  __esModule: true,
  AppContainer: () => null,
}))
describe('runTuiCommand exit clear', () => {
  const getInkMock = (): InkModule => jest.requireMock('ink') as InkModule
  const mockStdoutIsTTY = (value: boolean): (() => void) => {
    const proto = Object.getPrototypeOf(process.stdout) as object | null
    const descriptor =
      Object.getOwnPropertyDescriptor(process.stdout, 'isTTY') ??
      (proto ? Object.getOwnPropertyDescriptor(proto, 'isTTY') : undefined)
    if (descriptor?.get) {
      const spy = jest
        .spyOn(process.stdout as unknown as { isTTY: boolean }, 'isTTY', 'get')
        .mockReturnValue(value)
      return () => spy.mockRestore()
    }
    const original = process.stdout.isTTY
    Object.defineProperty(process.stdout, 'isTTY', { value, configurable: true })
    return () => {
      Object.defineProperty(process.stdout, 'isTTY', { value: original, configurable: true })
    }
  }
  it('clears terminal after Ink exits', async () => {
    const stdoutWrite = jest.spyOn(process.stdout, 'write').mockImplementation(() => true)
    const restoreIsTTY = mockStdoutIsTTY(true)
    try {
      const waitUntilExit = jest.fn(async () => {})
      getInkMock().render.mockReturnValue({ waitUntilExit })
      await runTuiCommand([])
      expect(waitUntilExit).toHaveBeenCalledTimes(1)
      expect(stdoutWrite).toHaveBeenCalledWith('\u001b[0m\u001b[2J\u001b[H')
    } finally {
      stdoutWrite.mockRestore()
      restoreIsTTY()
    }
  })
  it('skips clear when not a TTY', async () => {
    const stdoutWrite = jest.spyOn(process.stdout, 'write').mockImplementation(() => true)
    const restoreIsTTY = mockStdoutIsTTY(false)
    try {
      const waitUntilExit = jest.fn(async () => {})
      getInkMock().render.mockReturnValue({ waitUntilExit })
      await runTuiCommand([])
      expect(waitUntilExit).toHaveBeenCalledTimes(1)
      expect(stdoutWrite).not.toHaveBeenCalled()
    } finally {
      stdoutWrite.mockRestore()
      restoreIsTTY()
    }
  })
})
</file>

<file path="src/__tests__/generate-args.test.ts">
import { extractIntentArg, parseGenerateArgs, stripHelpFlags } from '../generate/args'
describe('generate args helpers', () => {
  describe('extractIntentArg', () => {
    it('detects positional intent after -i/--interactive', () => {
      const result = extractIntentArg(['-i', 'intent.md', '--json'])
      expect(result.positionalIntent).toBe('intent.md')
      expect(result.positionalIntentAfterInteractive).toBe(true)
      expect(result.optionArgs).toEqual(['-i', '--json'])
    })
    it('consumes value flags without treating them as intent', () => {
      const result = extractIntentArg(['--context', 'src/**/*.ts', 'do the thing'])
      expect(result.positionalIntent).toBe('do the thing')
      expect(result.optionArgs).toEqual(['--context', 'src/**/*.ts'])
    })
    it('treats resume selectors as value flags', () => {
      const fromHistory = extractIntentArg(['--resume', 'last:2', 'do the thing'])
      expect(fromHistory.positionalIntent).toBe('do the thing')
      expect(fromHistory.optionArgs).toEqual(['--resume', 'last:2'])
      const fromFile = extractIntentArg(['--resume-from', 'payload.jsonl', 'do the thing'])
      expect(fromFile.positionalIntent).toBe('do the thing')
      expect(fromFile.optionArgs).toEqual(['--resume-from', 'payload.jsonl'])
    })
  })
  describe('stripHelpFlags', () => {
    it('removes help flags before -- passthrough', () => {
      const result = stripHelpFlags(['--help', '-h', '--json'])
      expect(result.helpRequested).toBe(true)
      expect(result.optionArgs).toEqual(['--json'])
    })
    it('preserves help tokens after -- passthrough', () => {
      const result = stripHelpFlags(['--help', '--', '--help', '-h'])
      expect(result.helpRequested).toBe(true)
      expect(result.optionArgs).toEqual(['--', '--help', '-h'])
    })
  })
  describe('parseGenerateArgs', () => {
    it('parses token budget flags without consuming intent', () => {
      const parsed = parseGenerateArgs(['--max-input-tokens', '100', 'do the thing'])
      expect(parsed.args.maxInputTokens).toBe(100)
      expect(parsed.args.intent).toBe('do the thing')
    })
    it('rejects non-positive integer token budgets', () => {
      expect(() => parseGenerateArgs(['--max-input-tokens', '0'])).toThrow(
        '--max-input-tokens must be a positive integer.',
      )
      expect(() => parseGenerateArgs(['--max-context-tokens', '1.5'])).toThrow(
        '--max-context-tokens must be a positive integer.',
      )
    })
    it('parses validated context overflow strategy', () => {
      const parsed = parseGenerateArgs(['--context-overflow', 'drop-oldest'])
      expect(parsed.args.contextOverflow).toBe('drop-oldest')
    })
    it('rejects invalid context overflow strategy', () => {
      expect(() => parseGenerateArgs(['--context-overflow', 'nope'])).toThrow()
    })
    it('parses --resume selector without consuming intent', () => {
      const parsed = parseGenerateArgs(['--resume', 'last', 'do the thing'])
      expect(parsed.args.resume).toBe('last')
      expect(parsed.args.resumeMode).toBe('best-effort')
      expect(parsed.args.intent).toBe('do the thing')
    })
    it('rejects conflicting resume flags', () => {
      expect(() => parseGenerateArgs(['--resume', 'last', '--resume-last'])).toThrow(
        /--resume and --resume-last cannot be combined/i,
      )
      expect(() =>
        parseGenerateArgs(['--resume-from', 'payload.jsonl', '--resume', 'last']),
      ).toThrow(/--resume-from cannot be combined/i)
      expect(() => parseGenerateArgs(['--resume-from', 'payload.jsonl', '--resume-last'])).toThrow(
        /--resume-from cannot be combined/i,
      )
    })
    it('rejects invalid resume-mode choices', () => {
      expect(() => parseGenerateArgs(['--resume-mode', 'nope'])).toThrow(/resume-mode/i)
    })
  })
})
</file>

<file path="src/__tests__/media-loader.test.ts">
import {
  inferPdfMimeType,
  inferVideoMimeType,
  uploadFileForGemini,
  uploadFileForGeminiWithMimeType,
} from '../media-loader'
jest.mock('node:fs/promises', () => ({
  access: jest.fn(),
}))
jest.mock('@google/generative-ai/server', () => {
  const manager = {
    uploadFile: jest.fn(),
    getFile: jest.fn(),
  }
  return {
    GoogleAIFileManager: jest.fn().mockImplementation(() => manager),
    __managerMock: manager,
  }
})
const fs = jest.requireMock('node:fs/promises') as { access: jest.Mock }
const googleModule = jest.requireMock('@google/generative-ai/server') as {
  GoogleAIFileManager: jest.Mock
  __managerMock: { uploadFile: jest.Mock; getFile: jest.Mock }
}
const manager = googleModule.__managerMock
describe('media-loader', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    googleModule.GoogleAIFileManager.mockImplementation(() => manager)
    process.env.GEMINI_API_KEY = 'gem-key'
    manager.uploadFile.mockReset()
    manager.getFile.mockReset()
    manager.uploadFile.mockResolvedValue({ file: { name: 'files/123' } })
    manager.getFile.mockResolvedValue({ state: 'ACTIVE', uri: 'gs://files/123' })
    fs.access.mockResolvedValue(undefined)
  })
  it('infers known video mime types', () => {
    expect(inferVideoMimeType('clip.mp4')).toBe('video/mp4')
    expect(() => inferVideoMimeType('clip.txt')).toThrow('Unsupported media type')
  })
  it('infers PDF mime type', () => {
    expect(inferPdfMimeType('doc.pdf')).toBe('application/pdf')
    expect(() => inferPdfMimeType('doc.txt')).toThrow('Unsupported PDF type')
  })
  it('uploads file and waits for active state', async () => {
    process.env.GEMINI_API_KEY = 'env-key'
    const uri = await uploadFileForGemini('clip.mp4', 'injected-key')
    expect(googleModule.GoogleAIFileManager).toHaveBeenCalledWith('injected-key')
    expect(manager.uploadFile).toHaveBeenCalledWith('clip.mp4', {
      mimeType: 'video/mp4',
      displayName: 'clip.mp4',
    })
    expect(uri).toBe('gs://files/123')
  })
  it('uploads arbitrary mime types via uploadFileForGeminiWithMimeType', async () => {
    const uri = await uploadFileForGeminiWithMimeType('doc.pdf', 'application/pdf', 'injected-key')
    expect(googleModule.GoogleAIFileManager).toHaveBeenCalledWith('injected-key')
    expect(manager.uploadFile).toHaveBeenCalledWith('doc.pdf', {
      mimeType: 'application/pdf',
      displayName: 'doc.pdf',
    })
    expect(uri).toBe('gs://files/123')
  })
  it('falls back to GEMINI_API_KEY when no key is provided', async () => {
    const uri = await uploadFileForGemini('clip.mp4')
    expect(googleModule.GoogleAIFileManager).toHaveBeenCalledWith('gem-key')
    expect(uri).toBe('gs://files/123')
  })
  it('throws when file is not readable', async () => {
    fs.access.mockRejectedValue(new Error('denied'))
    await expect(uploadFileForGemini('missing.mp4')).rejects.toThrow('is not readable')
  })
  it('throws when Gemini reports failure', async () => {
    manager.getFile.mockResolvedValueOnce({ state: 'FAILED', error: { message: 'bad' } })
    await expect(uploadFileForGemini('clip.mp4')).rejects.toThrow('bad')
  })
  it('requires GEMINI_API_KEY to be present', async () => {
    process.env.GEMINI_API_KEY = ''
    await expect(uploadFileForGemini('clip.mp4')).rejects.toThrow('Gemini API key')
  })
})
</file>

<file path="src/generate/interactive.ts">
import chalk from 'chalk'
import enquirer from 'enquirer'
import type {
  createPromptGeneratorService,
  PromptGenerationRequest,
  UploadStateChange,
} from '../prompt-generator-service'
import { countTokens } from '../token-counter'
import { displayPrompt } from './display'
import type { StreamDispatcher } from './stream'
import { displayTokenSummary } from './token-telemetry'
import type { InteractiveDelegate, InteractiveMode, LoopContext, TokenTelemetry } from './types'
import type { InteractiveTransport } from './interactive-transport'
const { prompt } = enquirer as typeof import('enquirer')
type PromptGenerator = Awaited<ReturnType<typeof createPromptGeneratorService>>
type GenerateAndMaybeDisplayContext = LoopContext & {
  iteration: number
  previousPrompt?: string
  latestRefinement?: string
}
export const runGenerationWorkflow = async ({
  service,
  context,
  telemetry,
  interactiveMode,
  interactiveTransport,
  interactiveDelegate,
  display,
  stream,
  onUploadStateChange,
  resume,
}: {
  service: PromptGenerator
  context: LoopContext
  telemetry: TokenTelemetry
  interactiveMode: InteractiveMode
  interactiveTransport?: InteractiveTransport | null
  interactiveDelegate?: InteractiveDelegate | undefined
  display: boolean
  stream: StreamDispatcher
  onUploadStateChange?: UploadStateChange
  resume?: { prompt: string; iterations: number } | undefined
}): Promise<{ prompt: string; reasoning?: string; iterations: number }> => {
  let iteration = 0
  let currentPrompt = ''
  let currentReasoning: string | undefined
  if (display) {
    displayTokenSummary(telemetry)
  }
  const inputTokens = telemetry.totalTokens
  if (resume) {
    currentPrompt = resume.prompt
    iteration = Math.max(0, resume.iterations)
    if (display) {
      const displayedIteration = iteration > 0 ? iteration : 1
      displayPrompt(currentPrompt, displayedIteration, countTokens(currentPrompt))
    }
  } else {
    iteration += 1
    const initialGeneration = await generateAndMaybeDisplay(
      service,
      { ...context, iteration },
      display,
      stream,
      inputTokens,
      interactiveMode !== 'none',
      onUploadStateChange,
    )
    currentPrompt = initialGeneration.prompt
    currentReasoning = initialGeneration.reasoning
  }
  if (interactiveMode !== 'none') {
    stream.emit({ event: 'interactive.state', phase: 'start', iteration })
    stream.emit({ event: 'interactive.state', phase: 'prompt', iteration })
    if (interactiveMode === 'transport' && interactiveTransport) {
      // eslint-disable-next-line no-constant-condition
      while (true) {
        stream.emit({ event: 'interactive.awaiting', mode: interactiveMode })
        const command = await interactiveTransport.nextCommand()
        if (!command || command.type === 'finish') {
          break
        }
        const instruction = command.instruction.trim()
        if (!instruction) {
          continue
        }
        stream.emit({ event: 'interactive.state', phase: 'refine', iteration })
        context.refinements.push(instruction)
        iteration += 1
        const refinementGeneration = await generateAndMaybeDisplay(
          service,
          {
            ...context,
            iteration,
            previousPrompt: currentPrompt,
            latestRefinement: instruction,
          },
          display,
          stream,
          inputTokens,
          true,
          onUploadStateChange,
        )
        currentPrompt = refinementGeneration.prompt
        currentReasoning = refinementGeneration.reasoning
        stream.emit({ event: 'interactive.state', phase: 'prompt', iteration })
      }
    } else if (interactiveDelegate) {
      // eslint-disable-next-line no-constant-condition
      while (true) {
        stream.emit({ event: 'interactive.awaiting', mode: interactiveMode })
        const action = await interactiveDelegate.getNextAction({ iteration, currentPrompt })
        if (!action || action.type === 'finish') {
          break
        }
        const refinement = action.instruction.trim()
        if (!refinement) {
          continue
        }
        stream.emit({ event: 'interactive.state', phase: 'refine', iteration })
        context.refinements.push(refinement)
        iteration += 1
        const refinementGeneration = await generateAndMaybeDisplay(
          service,
          {
            ...context,
            iteration,
            previousPrompt: currentPrompt,
            latestRefinement: refinement,
          },
          display,
          stream,
          inputTokens,
          true,
          onUploadStateChange,
        )
        currentPrompt = refinementGeneration.prompt
        currentReasoning = refinementGeneration.reasoning
        stream.emit({ event: 'interactive.state', phase: 'prompt', iteration })
      }
    } else {
      // eslint-disable-next-line no-constant-condition
      while (true) {
        stream.emit({ event: 'interactive.awaiting', mode: interactiveMode })
        const wantsRefine = await askShouldRefine()
        if (!wantsRefine) {
          break
        }
        stream.emit({ event: 'interactive.state', phase: 'refine', iteration })
        const refinement = await collectRefinementInstruction()
        if (!refinement) {
          console.log(chalk.dim('No refinement provided. Ending interactive session.'))
          break
        }
        context.refinements.push(refinement)
        iteration += 1
        const refinementGeneration = await generateAndMaybeDisplay(
          service,
          {
            ...context,
            iteration,
            previousPrompt: currentPrompt,
            latestRefinement: refinement,
          },
          display,
          stream,
          inputTokens,
          true,
          onUploadStateChange,
        )
        currentPrompt = refinementGeneration.prompt
        currentReasoning = refinementGeneration.reasoning
        stream.emit({ event: 'interactive.state', phase: 'prompt', iteration })
      }
    }
    stream.emit({ event: 'interactive.state', phase: 'complete', iteration })
  }
  return {
    prompt: currentPrompt,
    ...(typeof currentReasoning === 'string' ? { reasoning: currentReasoning } : {}),
    iterations: iteration,
  }
}
const generateAndMaybeDisplay = async (
  service: PromptGenerator,
  context: GenerateAndMaybeDisplayContext,
  display: boolean,
  stream: StreamDispatcher,
  inputTokens: number,
  interactive: boolean,
  onUploadStateChange?: UploadStateChange,
): Promise<{ prompt: string; reasoning?: string }> => {
  const request: PromptGenerationRequest = {
    intent: context.intent,
    model: context.model,
    targetModel: context.targetModel,
    fileContext: context.fileContext,
    images: context.images,
    videos: context.videos,
    pdfs: context.pdfs,
    onPromptAutoRepairAttempt: (detail) => {
      stream.emit({
        event: 'progress.update',
        scope: 'generate',
        state: 'update',
        label: `Auto-repair (${detail.kind}) ${detail.reason} (${detail.attempt}/${detail.maxAttempts})`,
      })
    },
  }
  if (context.metaInstructions) {
    request.metaInstructions = context.metaInstructions
  }
  if (onUploadStateChange) {
    request.onUploadStateChange = onUploadStateChange
  }
  if (context.previousPrompt && context.latestRefinement) {
    request.previousPrompt = context.previousPrompt
    request.refinementInstruction = context.latestRefinement
  }
  stream.emit({
    event: 'generation.iteration.start',
    iteration: context.iteration,
    intent: context.intent,
    model: context.model,
    interactive,
    inputTokens,
    refinements: [...context.refinements],
    ...(context.latestRefinement ? { latestRefinement: context.latestRefinement } : {}),
  })
  const generator = service as unknown as {
    generatePrompt: (request: PromptGenerationRequest) => Promise<string>
    generatePromptDetailed?: (request: PromptGenerationRequest) => Promise<{
      prompt: string
      reasoning?: string
    }>
  }
  const detailed =
    typeof generator.generatePromptDetailed === 'function'
      ? await generator.generatePromptDetailed(request)
      : { prompt: await generator.generatePrompt(request) }
  const outputTokens = countTokens(detailed.prompt)
  const reasoningTokens = detailed.reasoning ? countTokens(detailed.reasoning) : 0
  stream.emit({
    event: 'generation.iteration.complete',
    iteration: context.iteration,
    prompt: detailed.prompt,
    tokens: outputTokens,
    ...(reasoningTokens > 0 ? { reasoningTokens } : {}),
  })
  if (display) {
    displayPrompt(detailed.prompt, context.iteration, outputTokens)
  }
  return {
    prompt: detailed.prompt,
    ...(typeof detailed.reasoning === 'string' ? { reasoning: detailed.reasoning } : {}),
  }
}
const askShouldRefine = async (): Promise<boolean> => {
  try {
    const response = await prompt<{ refine: boolean }>({
      type: 'confirm',
      name: 'refine',
      message: chalk.cyan('Refine the generated prompt?'),
      initial: false,
    })
    return Boolean(response.refine)
  } catch (error) {
    if (isPromptCancellation(error)) {
      console.log(chalk.dim('\nInteractive session cancelled.'))
      return false
    }
    throw error
  }
}
const collectRefinementInstruction = async (): Promise<string | null> => {
  try {
    const response = await prompt<{ refinement: string }>({
      type: 'input',
      name: 'refinement',
      message: chalk.cyan('Describe the refinement (blank to finish):'),
      multiline: true,
    })
    const refinement = response.refinement?.trim()
    return refinement || null
  } catch (error) {
    if (isPromptCancellation(error)) {
      console.log(chalk.dim('\nRefinement input cancelled.'))
      return null
    }
    throw error
  }
}
const isPromptCancellation = (error: unknown): boolean => {
  if (typeof error === 'string') {
    return true
  }
  if (error instanceof Error) {
    const message = error.message.toLowerCase()
    return message.includes('cancel') || message.includes('abort')
  }
  return false
}
</file>

<file path="src/prompt-generator/message-builders.ts">
import type { MessageContent, TextPart } from '@prompt-maker/core'
import { formatContextForPrompt, type FileContext } from '../file-context'
import { resolveImageParts } from '../image-loader'
import { resolvePdfParts } from './pdf-parts'
import type { UploadStateChange } from './types'
import { resolveVideoParts } from './video-parts'
const buildPdfAttachmentSection = (pdfPaths: string[]): string | null => {
  if (pdfPaths.length === 0) {
    return null
  }
  return [
    'PDF Attachments (already provided as context):',
    pdfPaths.join('\n'),
    '',
    'Non-negotiable requirements for the prompt contract you will generate:',
    '- Treat the attached PDF as the source document and as already-available input.',
    '- In the contract "Inputs" section, do NOT ask the user to paste the document or provide a file/path.',
    '- Proceed assuming the executing assistant has the PDF attached.',
    '- Only request OCR / extracted text if you cannot access readable text from the PDF and you explicitly say so.',
  ].join('\n')
}
export const buildInitialUserMessageText = (
  intent: string,
  files: FileContext[],
  pdfPaths: string[],
  metaInstructions?: string,
): string => {
  const sections: string[] = []
  if (files.length > 0) {
    sections.push('Context Files:\n' + formatContextForPrompt(files))
  }
  const pdfSection = buildPdfAttachmentSection(pdfPaths)
  if (pdfSection) {
    sections.push(pdfSection)
  }
  sections.push(`User Intent:\n${intent.trim()}`)
  const trimmedInstructions = metaInstructions?.trim()
  if (trimmedInstructions) {
    sections.push(`Meta-Instructions:\n${trimmedInstructions}`)
  }
  sections.push(
    [
      'Return the final structured prompt contract now.',
      'Do NOT perform the task yourself; only craft instructions for another assistant using the required sections.',
    ].join(' '),
  )
  return sections.join('\n\n')
}
export const buildInitialUserMessage = async (
  intent: string,
  files: FileContext[],
  imagePaths: string[],
  videoPaths: string[],
  pdfPaths: string[],
  metaInstructions?: string,
  onUploadStateChange?: UploadStateChange,
  apiKey?: string,
): Promise<MessageContent> => {
  const text = buildInitialUserMessageText(intent, files, pdfPaths, metaInstructions)
  return await mergeMediaWithText(
    text,
    imagePaths,
    videoPaths,
    pdfPaths,
    onUploadStateChange,
    apiKey,
  )
}
export const buildRefinementMessageText = (
  previousPrompt: string,
  refinementInstruction: string,
  intent: string,
  files: FileContext[],
  pdfPaths: string[],
  metaInstructions?: string,
): string => {
  const sections: string[] = []
  if (files.length > 0) {
    sections.push('Context Files:\n' + formatContextForPrompt(files))
  }
  const pdfSection = buildPdfAttachmentSection(pdfPaths)
  if (pdfSection) {
    sections.push(pdfSection)
  }
  sections.push(`Original Intent (for reference):\n${intent.trim()}`)
  sections.push(`Current Prompt Draft:\n${previousPrompt}`)
  sections.push(`Refinement Instruction:\n${refinementInstruction.trim()}`)
  const trimmedInstructions = metaInstructions?.trim()
  if (trimmedInstructions) {
    sections.push(`Meta-Instructions:\n${trimmedInstructions}`)
  }
  sections.push(
    [
      'Return the fully updated prompt contract.',
      'Maintain the required sections and continue to avoid performing the task yourself.',
    ].join(' '),
  )
  return sections.join('\n\n')
}
export const buildRefinementMessage = async (
  previousPrompt: string,
  refinementInstruction: string,
  intent: string,
  files: FileContext[],
  imagePaths: string[],
  videoPaths: string[],
  pdfPaths: string[],
  metaInstructions?: string,
  onUploadStateChange?: UploadStateChange,
  apiKey?: string,
): Promise<MessageContent> => {
  const text = buildRefinementMessageText(
    previousPrompt,
    refinementInstruction,
    intent,
    files,
    pdfPaths,
    metaInstructions,
  )
  return await mergeMediaWithText(
    text,
    imagePaths,
    videoPaths,
    pdfPaths,
    onUploadStateChange,
    apiKey,
  )
}
export const buildSeriesUserMessageText = (
  intent: string,
  files: FileContext[],
  pdfPaths: string[],
  metaInstructions?: string,
): string => {
  const sections: string[] = []
  if (files.length > 0) {
    sections.push('Context Files:\n' + formatContextForPrompt(files))
  }
  const pdfSection = buildPdfAttachmentSection(pdfPaths)
  if (pdfSection) {
    sections.push(pdfSection)
  }
  sections.push(`User Intent:\n${intent.trim()}`)
  const trimmedInstructions = metaInstructions?.trim()
  if (trimmedInstructions) {
    sections.push(`Meta-Instructions:\n${trimmedInstructions}`)
  }
  sections.push(
    [
      'Task:',
      'Design a planning artifact consisting of one overview prompt plus a set of atomic prompts.',
      'Each atomic prompt must be self-contained, target a specific verifiable state change, and include a "Validation" section describing how a human can confirm completion.',
      'Do not perform the tasks; only describe them.',
    ].join(' '),
  )
  sections.push(
    [
      'Output Requirements:',
      'Return strict JSON matching the schema { "reasoning": string, "overviewPrompt": string, "atomicPrompts": Array<{ "title": string; "content": string }> }.',
      'Never wrap the JSON in markdown code fences and never add extra keys.',
    ].join(' '),
  )
  return sections.join('\n\n')
}
export const buildSeriesUserMessage = async (
  intent: string,
  files: FileContext[],
  imagePaths: string[],
  videoPaths: string[],
  pdfPaths: string[],
  metaInstructions?: string,
  onUploadStateChange?: UploadStateChange,
  apiKey?: string,
): Promise<MessageContent> => {
  const text = buildSeriesUserMessageText(intent, files, pdfPaths, metaInstructions)
  return await mergeMediaWithText(
    text,
    imagePaths,
    videoPaths,
    pdfPaths,
    onUploadStateChange,
    apiKey,
  )
}
const isTextPart = (part: Exclude<MessageContent, string>[number]): part is TextPart => {
  return part.type === 'text'
}
export const mergeResolvedMediaWithText = (
  content: MessageContent,
  text: string,
): MessageContent => {
  if (typeof content === 'string') {
    return text
  }
  const mediaParts = content.filter((part) => !isTextPart(part))
  if (mediaParts.length === 0) {
    return text
  }
  return [...mediaParts, { type: 'text', text }]
}
const mergeMediaWithText = async (
  text: string,
  imagePaths: string[],
  videoPaths: string[],
  pdfPaths: string[],
  onUploadStateChange?: UploadStateChange,
  apiKey?: string,
): Promise<MessageContent> => {
  const [imageParts, videoParts, pdfParts] = await Promise.all([
    resolveImageParts(imagePaths, onUploadStateChange),
    resolveVideoParts(videoPaths, onUploadStateChange, apiKey),
    resolvePdfParts(pdfPaths, onUploadStateChange, apiKey),
  ])
  if (imageParts.length === 0 && videoParts.length === 0 && pdfParts.length === 0) {
    return text
  }
  // Deterministic ordering: images → videos → PDFs → text.
  return [...imageParts, ...videoParts, ...pdfParts, { type: 'text', text }]
}
</file>

<file path="src/tui/components/core/InputBar.tsx">
import React from 'react'
import { Box, Text, useStdout } from 'ink'
import { MultilineTextInput, type DebugKeyEvent } from './MultilineTextInput'
import { resolveIndicatorSegments } from './status-indicators-layout'
import { resolveInputBarPresentation, type InputBarMode } from './input-bar-presentation'
import type { TokenLabelLookup } from './tokenized-text'
import { OpencodeSpinner } from '../OpencodeSpinner'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
export { estimateInputBarRows } from './input-bar-layout'
export type { InputBarRowEstimateOptions } from './input-bar-layout'
const APP_CONTAINER_PADDING_X = 2
const COMMAND_SCREEN_PADDING_X = 1
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type InputBarProps = {
  value: string
  onChange: (next: string) => void
  onSubmit: (value: string) => void
  mode?: InputBarMode
  isDisabled?: boolean
  isPasteActive?: boolean
  isBusy?: boolean
  statusChips: readonly string[]
  placeholder?: string
  hint?: string | undefined
  debugLine?: string | undefined
  tokenLabel?: TokenLabelLookup | undefined
  onDebugKeyEvent?: ((event: DebugKeyEvent) => void) | undefined
}
export const InputBar: React.FC<InputBarProps> = ({
  value,
  onChange,
  onSubmit,
  mode = 'intent',
  isDisabled = false,
  isPasteActive = false,
  isBusy = false,
  statusChips,
  placeholder,
  hint,
  debugLine,
  tokenLabel,
  onDebugKeyEvent,
}) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  // `resolveInputBarPresentation` is pure but involves string/config mapping.
  // Memoizing it keeps the render path a bit more predictable.
  const presentation = React.useMemo(() => resolveInputBarPresentation(mode), [mode])
  const summary = React.useMemo(() => {
    const segments = resolveIndicatorSegments(statusChips)
    const status = segments.find((segment) => segment.label === 'Status')
    const model = segments.find((segment) => segment.label === 'Model')
    const polish = segments.find((segment) => segment.label === 'Polish')
    const target = segments.find((segment) => segment.label === 'Target')
    return { status, model, polish, target }
  }, [statusChips])
  const borderColor = presentation.borderTone === 'warning' ? theme.warning : theme.border
  const labelColor = presentation.labelTone === 'warning' ? theme.warning : theme.mutedText
  const terminalColumns = stdout?.columns ?? 80
  const barWidth = Math.max(
    0,
    terminalColumns - 2 * (APP_CONTAINER_PADDING_X + COMMAND_SCREEN_PADDING_X),
  )
  const backgroundProps = inkBackgroundColorProps(theme.panelBackground)
  const statusLineColumns = React.useMemo(() => {
    const joinerColumns = ' · '.length
    let columns = 0
    let partCount = 0
    const addPart = (value: string, extraColumns = 0): void => {
      if (partCount > 0) {
        columns += joinerColumns
      }
      columns += value.length + extraColumns
      partCount += 1
    }
    if (summary.status) {
      const spinnerColumns = isBusy ? 12 + 1 : 0
      addPart(`Status: ${summary.status.value}`, spinnerColumns)
    }
    if (summary.model) {
      addPart(`Model: ${summary.model.value}`)
    }
    if (summary.polish) {
      addPart(`Polish: ${summary.polish.value}`)
    }
    if (summary.target) {
      addPart(`Target: ${summary.target.value}`)
    }
    return columns
  }, [
    isBusy,
    summary.model?.value,
    summary.polish?.value,
    summary.status?.value,
    summary.target?.value,
  ])
  const BORDER_GLYPH = '▌'
  const renderBorderPrefix = (): React.ReactNode => (
    <>
      <Text {...backgroundProps} {...inkColorProps(borderColor)}>
        {BORDER_GLYPH}
      </Text>
      <Text {...backgroundProps}> </Text>
    </>
  )
  return (
    <Box
      flexDirection="column"
      paddingLeft={0}
      paddingRight={0}
      paddingY={0}
      width="100%"
      {...inkBackgroundColorProps(theme.panelBackground)}
    >
      <Box flexDirection="row" width="100%">
        {renderBorderPrefix()}
        <Text {...backgroundProps} {...inkColorProps(labelColor)} bold={presentation.labelBold}>
          {presentation.label}
        </Text>
        <Text {...backgroundProps}>
          {padRight('', Math.max(0, barWidth - 2 - presentation.label.length))}
        </Text>
      </Box>
      {hint ? (
        <Box flexDirection="row" width="100%">
          {renderBorderPrefix()}
          <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
            {hint}
          </Text>
          <Text {...backgroundProps}>{padRight('', Math.max(0, barWidth - 2 - hint.length))}</Text>
        </Box>
      ) : null}
      {debugLine ? (
        <Box flexDirection="row" width="100%">
          {renderBorderPrefix()}
          <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
            {debugLine}
          </Text>
          <Text {...backgroundProps}>
            {padRight('', Math.max(0, barWidth - 2 - debugLine.length))}
          </Text>
        </Box>
      ) : null}
      <MultilineTextInput
        value={value}
        onChange={onChange}
        onSubmit={onSubmit}
        placeholder={placeholder ?? 'Describe your goal or type /command'}
        focus={!isDisabled}
        isDisabled={isDisabled}
        isPasteActive={isPasteActive}
        tokenLabel={tokenLabel}
        onDebugKeyEvent={onDebugKeyEvent}
        gutter={{ glyph: BORDER_GLYPH, color: borderColor, spacer: 1 }}
        width={barWidth}
        backgroundColor={theme.panelBackground}
      />
      {summary.status || summary.model || summary.polish || summary.target ? (
        <Box flexDirection="row" width="100%">
          {renderBorderPrefix()}
          {summary.status ? (
            <>
              <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
                Status:{' '}
              </Text>
              {isBusy ? (
                <>
                  <OpencodeSpinner backgroundColor={theme.panelBackground} />
                  <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
                    {' '}
                  </Text>
                </>
              ) : null}
              <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
                {summary.status.value}
              </Text>
            </>
          ) : null}
          {summary.status && (summary.model || summary.polish || summary.target) ? (
            <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
              {' · '}
            </Text>
          ) : null}
          {summary.model ? (
            <>
              <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
                Model:{' '}
              </Text>
              <Text {...backgroundProps} {...inkColorProps(theme.text)}>
                {summary.model.value}
              </Text>
            </>
          ) : null}
          {summary.model && (summary.polish || summary.target) ? (
            <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
              {' · '}
            </Text>
          ) : null}
          {summary.polish ? (
            <>
              <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
                Polish:{' '}
              </Text>
              <Text {...backgroundProps} {...inkColorProps(theme.text)}>
                {summary.polish.value}
              </Text>
            </>
          ) : null}
          {summary.polish && summary.target ? (
            <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
              {' · '}
            </Text>
          ) : null}
          {summary.target ? (
            <>
              <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
                Target:{' '}
              </Text>
              <Text {...backgroundProps} {...inkColorProps(theme.text)}>
                {summary.target.value}
              </Text>
            </>
          ) : null}
          <Text {...backgroundProps}>
            {padRight('', Math.max(0, barWidth - 2 - statusLineColumns))}
          </Text>
        </Box>
      ) : null}
    </Box>
  )
}
</file>

<file path="src/tui/components/popups/model-popup-model.ts">
import { MODEL_PROVIDER_LABELS } from '../../../model-providers'
import { ensureLeadingHeaderVisible, resolveWindowedValues } from './list-windowing'
import type { WindowBounds } from './list-windowing'
import type { ModelOption } from '../../types'
export type ModelPopupRow =
  | { type: 'header'; title: string }
  | { type: 'spacer' }
  | { type: 'option'; option: ModelOption; optionIndex: number }
export const resolveModelPopupListRows = ({
  popupHeight,
  paddingY,
  fixedRows,
}: {
  popupHeight: number
  paddingY: number
  fixedRows: number
}): number => {
  const paddingRows = 2 * paddingY
  const availableRows = Math.max(1, popupHeight - paddingRows - fixedRows)
  return availableRows
}
export const buildModelPopupRows = (
  options: readonly ModelOption[],
  recentCount: number,
): ModelPopupRow[] => {
  if (options.length === 0) {
    return []
  }
  const rows: ModelPopupRow[] = []
  const safeRecentCount = Math.max(0, Math.min(recentCount, options.length))
  if (safeRecentCount > 0) {
    rows.push({ type: 'header', title: 'Recent' })
    for (let index = 0; index < safeRecentCount; index += 1) {
      const option = options[index]
      if (!option) {
        continue
      }
      rows.push({ type: 'option', option, optionIndex: index })
    }
    if (safeRecentCount < options.length) {
      rows.push({ type: 'spacer' })
    }
  }
  let lastProvider: string | null = null
  for (let index = safeRecentCount; index < options.length; index += 1) {
    const option = options[index]
    if (!option) {
      continue
    }
    const providerLabel = MODEL_PROVIDER_LABELS[option.provider]
    if (providerLabel !== lastProvider) {
      rows.push({ type: 'header', title: providerLabel })
      lastProvider = providerLabel
    }
    rows.push({ type: 'option', option, optionIndex: index })
  }
  return rows
}
export const resolveModelPopupSelectedRowIndex = (
  rows: readonly ModelPopupRow[],
  selectedOptionIndex: number,
): number => {
  if (rows.length === 0) {
    return 0
  }
  const index = rows.findIndex(
    (row) => row.type === 'option' && row.optionIndex === selectedOptionIndex,
  )
  return index >= 0 ? index : 0
}
export type ModelPopupVisibleRows = {
  slice: WindowBounds
  selectedRowIndex: number
  visibleRows: readonly ModelPopupRow[]
}
export const resolveModelPopupVisibleRows = ({
  rows,
  selectedOptionIndex,
  maxVisibleRows,
}: {
  rows: readonly ModelPopupRow[]
  selectedOptionIndex: number
  maxVisibleRows: number
}): ModelPopupVisibleRows => {
  const selectedRowIndex = resolveModelPopupSelectedRowIndex(rows, selectedOptionIndex)
  const window = resolveWindowedValues(rows, selectedRowIndex, maxVisibleRows)
  const slice = ensureLeadingHeaderVisible(
    rows,
    { start: window.start, end: window.end },
    maxVisibleRows,
    'header',
    'option',
  )
  const base = rows.slice(slice.start, slice.end)
  if (base.length >= maxVisibleRows) {
    return { slice, selectedRowIndex, visibleRows: base }
  }
  const padded: ModelPopupRow[] = [...base]
  while (padded.length < maxVisibleRows) {
    padded.push({ type: 'spacer' })
  }
  return { slice, selectedRowIndex, visibleRows: padded }
}
</file>

<file path="src/tui/hooks/buffered-history-writer.ts">
import type { HistoryEntry } from '../types'
export type HistoryPush = (
  content: string,
  kind?: HistoryEntry['kind'],
  format?: HistoryEntry['format'],
) => void
export type BufferedHistoryWriter = {
  pushBuffered: (
    content: string,
    kind?: HistoryEntry['kind'],
    format?: HistoryEntry['format'],
  ) => void
  pushManyBuffered: (
    entries: Array<{
      content: string
      kind?: HistoryEntry['kind']
      format?: HistoryEntry['format']
    }>,
  ) => void
  flush: () => void
}
type FlushScheduler = (flush: () => void) => void
type BufferedHistoryEntry = {
  content: string
  kind: HistoryEntry['kind'] | undefined
  format: HistoryEntry['format'] | undefined
}
const scheduleMicrotaskFlush: FlushScheduler = (flush) => {
  queueMicrotask(flush)
}
export const createBufferedHistoryWriter = (options: {
  push: HistoryPush
  scheduleFlush?: FlushScheduler
}): BufferedHistoryWriter => {
  const scheduleFlush = options.scheduleFlush ?? scheduleMicrotaskFlush
  let scheduled = false
  let queue: BufferedHistoryEntry[] = []
  const flush = () => {
    scheduled = false
    const entries = queue
    queue = []
    entries.forEach((entry) => {
      options.push(entry.content, entry.kind, entry.format)
    })
  }
  const schedule = () => {
    if (scheduled) {
      return
    }
    scheduled = true
    scheduleFlush(flush)
  }
  const pushBuffered = (
    content: string,
    kind?: HistoryEntry['kind'],
    format?: HistoryEntry['format'],
  ) => {
    queue.push({ content, kind, format })
    schedule()
  }
  const pushManyBuffered = (
    entries: Array<{
      content: string
      kind?: HistoryEntry['kind']
      format?: HistoryEntry['format']
    }>,
  ) => {
    entries.forEach((entry) => {
      queue.push({ content: entry.content, kind: entry.kind, format: entry.format })
    })
    if (entries.length > 0) {
      schedule()
    }
  }
  return {
    pushBuffered,
    pushManyBuffered,
    flush,
  }
}
</file>

<file path="src/tui/hooks/generation-history-formatters.ts">
import wrapAnsi from 'wrap-ansi'
import type { HistoryEntry } from '../types'
export type HistoryMessage = {
  content: string
  kind: HistoryEntry['kind']
  format?: HistoryEntry['format']
}
export const formatCompactTokens = (count: number): string => {
  if (count < 1000) {
    return String(count)
  }
  if (count < 10_000) {
    return `${(count / 1000).toFixed(1)}k`
  }
  if (count < 1_000_000) {
    return `${Math.round(count / 1000)}k`
  }
  return `${(count / 1_000_000).toFixed(1)}m`
}
export const extractValidationSection = (content: string): string | null => {
  const markerRegex = /^(?:#{1,6}\s*Validation\b.*|Validation\s*:.*)$/im
  const match = markerRegex.exec(content)
  if (!match) {
    return null
  }
  return content.slice(match.index).trim()
}
export const getHistoryWrapWidth = (terminalColumns: number): number => {
  return Math.max(40, terminalColumns - 6)
}
export const wrapTextForHistory = (content: string, wrapWidth: number): string[] => {
  const output: string[] = []
  content.split('\n').forEach((line) => {
    const wrapped = wrapAnsi(line, wrapWidth, { trim: false, hard: true })
    wrapped.split('\n').forEach((wrappedLine) => {
      output.push(wrappedLine)
    })
  })
  return output
}
const formatIterationTokenLabel = (tokens: number, reasoningTokens?: number): string => {
  const normalizedReasoningTokens = reasoningTokens ?? 0
  if (normalizedReasoningTokens > 0) {
    return ` (${tokens} prompt tokens · ${normalizedReasoningTokens} reasoning tokens)`
  }
  return ` (${tokens} tokens)`
}
export const buildIterationCompleteHistoryMessages = (options: {
  iteration: number
  tokens: number
  reasoningTokens?: number
  prompt: string
  wrapWidth: number
}): HistoryMessage[] => {
  const tokenLabel = formatIterationTokenLabel(options.tokens, options.reasoningTokens)
  const messages: HistoryMessage[] = [
    {
      content: `Iteration ${options.iteration} complete${tokenLabel}`,
      kind: 'progress',
    },
    {
      content: `Prompt (iteration ${options.iteration}):`,
      kind: 'system',
    },
  ]
  wrapTextForHistory(options.prompt, options.wrapWidth).forEach((line) => {
    messages.push({ content: line, kind: 'system', format: 'markdown' })
  })
  return messages
}
export const buildJsonPayloadHistoryMessages = (
  payload: unknown,
  wrapWidth: number,
): HistoryMessage[] => {
  const prettyPayload = JSON.stringify(payload, null, 2)
  const messages: HistoryMessage[] = [{ content: 'JSON payload:', kind: 'system' }]
  wrapTextForHistory(prettyPayload, wrapWidth).forEach((line) => {
    messages.push({ content: line, kind: 'system' })
  })
  return messages
}
</file>

<file path="src/tui/screens/command/hooks/popup-shortcuts/suggested-selected-list.ts">
import type { PopupState } from '../../../../types'
import { clampIndex } from './selection'
import {
  NO_EFFECT,
  guardPopupUpdater,
  type PopupKey,
  type PopupShortcutEffect,
  type PopupStateFor,
  type PopupType,
} from './types'
export type SuggestedSelectedListPopupType = Extract<PopupType, 'file' | 'image' | 'video' | 'pdf'>
export type SuggestedSelectedListPopupState =
  | PopupStateFor<'file'>
  | PopupStateFor<'image'>
  | PopupStateFor<'video'>
  | PopupStateFor<'pdf'>
export type GetSuggestedSelectedListEffectOptions = {
  popupType: SuggestedSelectedListPopupType
  popupState: SuggestedSelectedListPopupState
  itemsLength: number
  suggestions: readonly string[]
  key: PopupKey
  isBackspace: boolean
}
const guardSuggestedSelectedListUpdater = (
  popupType: SuggestedSelectedListPopupType,
  updater: (prev: SuggestedSelectedListPopupState) => SuggestedSelectedListPopupState,
): ((prev: PopupState) => PopupState) => {
  return guardPopupUpdater(
    popupType,
    updater as (prev: PopupStateFor<typeof popupType>) => PopupStateFor<typeof popupType>,
  )
}
export const getSuggestedSelectedListPopupEffect = ({
  popupType,
  popupState,
  itemsLength,
  suggestions,
  key,
  isBackspace,
}: GetSuggestedSelectedListEffectOptions): PopupShortcutEffect => {
  const hasSuggestions = suggestions.length > 0
  const maxSuggestedIndex = Math.max(suggestions.length - 1, 0)
  const draftIsEmpty = popupState.draft.trim().length === 0
  if (key.escape) {
    return { type: 'close' }
  }
  if (popupState.suggestedFocused && hasSuggestions) {
    if (key.tab) {
      return {
        type: 'set',
        updater: guardSuggestedSelectedListUpdater(popupType, (prev) => ({
          ...prev,
          suggestedFocused: false,
        })),
      }
    }
    if (key.upArrow) {
      return {
        type: 'set',
        updater: guardSuggestedSelectedListUpdater(popupType, (prev) => {
          const effectiveIndex = clampIndex(prev.suggestedSelectionIndex, suggestions.length)
          if (effectiveIndex === 0) {
            return {
              ...prev,
              suggestedFocused: false,
              selectedFocused: itemsLength > 0,
            }
          }
          return {
            ...prev,
            suggestedSelectionIndex: Math.max(prev.suggestedSelectionIndex - 1, 0),
          }
        }),
      }
    }
    if (key.downArrow) {
      return {
        type: 'set',
        updater: guardSuggestedSelectedListUpdater(popupType, (prev) => ({
          ...prev,
          suggestedSelectionIndex: Math.min(prev.suggestedSelectionIndex + 1, maxSuggestedIndex),
        })),
      }
    }
    if (key.return) {
      const effectiveIndex = clampIndex(popupState.suggestedSelectionIndex, suggestions.length)
      const selection = suggestions[effectiveIndex]
      return selection ? { type: 'selectSuggestion', value: selection } : NO_EFFECT
    }
    return NO_EFFECT
  }
  if (key.tab && !key.shift && hasSuggestions) {
    return {
      type: 'set',
      updater: guardSuggestedSelectedListUpdater(popupType, (prev) => ({
        ...prev,
        suggestedFocused: true,
        selectedFocused: false,
        suggestedSelectionIndex: 0,
      })),
    }
  }
  if (!popupState.selectedFocused && (key.upArrow || key.downArrow) && itemsLength > 0) {
    return {
      type: 'set',
      updater: guardSuggestedSelectedListUpdater(popupType, (prev) => ({
        ...prev,
        selectedFocused: true,
        selectionIndex: clampIndex(prev.selectionIndex, itemsLength),
      })),
    }
  }
  if (popupState.selectedFocused) {
    if (key.upArrow) {
      return {
        type: 'set',
        updater: guardSuggestedSelectedListUpdater(popupType, (prev) => {
          if (prev.selectionIndex === 0) {
            return { ...prev, selectedFocused: false }
          }
          return { ...prev, selectionIndex: Math.max(prev.selectionIndex - 1, 0) }
        }),
      }
    }
    if (key.downArrow) {
      return {
        type: 'set',
        updater: guardSuggestedSelectedListUpdater(popupType, (prev) => {
          if (itemsLength === 0) {
            return { ...prev, selectedFocused: false }
          }
          if (prev.selectionIndex >= itemsLength - 1) {
            return hasSuggestions
              ? {
                  ...prev,
                  suggestedFocused: true,
                  selectedFocused: false,
                  suggestedSelectionIndex: 0,
                }
              : prev
          }
          return {
            ...prev,
            selectionIndex: Math.min(prev.selectionIndex + 1, itemsLength - 1),
          }
        }),
      }
    }
    if (key.delete || isBackspace) {
      return itemsLength > 0 ? { type: 'remove', index: popupState.selectionIndex } : NO_EFFECT
    }
    return NO_EFFECT
  }
  if (key.downArrow && itemsLength === 0 && hasSuggestions) {
    return {
      type: 'set',
      updater: guardSuggestedSelectedListUpdater(popupType, (prev) => ({
        ...prev,
        suggestedFocused: true,
        selectedFocused: false,
        suggestedSelectionIndex: 0,
      })),
    }
  }
  // Backspace-remove remains available when the input is empty.
  if (draftIsEmpty && isBackspace && itemsLength > 0) {
    return { type: 'remove', index: popupState.selectionIndex }
  }
  return NO_EFFECT
}
export const applySuggestedSelectedListEffect = (
  effect: PopupShortcutEffect,
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void,
  closePopup: () => void,
  onRemove: (index: number) => void,
  onSelectSuggestion: (value: string) => void,
): void => {
  switch (effect.type) {
    case 'close':
      closePopup()
      return
    case 'set':
      setPopupState(effect.updater)
      return
    case 'remove':
      onRemove(effect.index)
      return
    case 'selectSuggestion':
      onSelectSuggestion(effect.value)
      return
    case 'none':
      return
    default: {
      const exhaustive: never = effect
      return exhaustive
    }
  }
}
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenSubmitBindings.ts">
import { useCallback } from 'react'
import type { HistoryEntry, PopupState } from '../../../types'
import { useIntentSubmitHandler } from './useIntentSubmitHandler'
export type UseCommandScreenSubmitBindingsOptions = {
  popupState: PopupState
  isAwaitingRefinement: boolean
  submitRefinement: (value: string) => void
  isCommandMenuActive: boolean
  selectedCommandId: import('../../../types').CommandDescriptor['id'] | null
  commandMenuArgsRaw: string
  isCommandMode: boolean
  intentFilePath: string
  isGenerating: boolean
  expandInputForSubmit: (value: string) => string
  setInputValue: (value: string | ((prev: string) => string)) => void
  pushHistory: (content: string, kind?: HistoryEntry['kind']) => void
  addCommandHistoryEntry: (value: string) => void
  runGeneration: (payload: {
    intent?: string
    intentFile?: string
    resume?:
      | { kind: 'history'; selector: string; mode: import('../../../types').ResumeMode }
      | { kind: 'file'; payloadPath: string; mode: import('../../../types').ResumeMode }
  }) => Promise<void>
  handleCommandSelection: (
    commandId: import('../../../types').CommandDescriptor['id'],
    argsRaw?: string,
  ) => void
  handleNewCommand: (argsRaw: string) => void
  handleReuseCommand: () => void
  lastUserIntentRef: import('react').MutableRefObject<string | null>
  handleSeriesIntentSubmit: (value: string) => void
  openHelp?: () => void
}
export type UseCommandScreenSubmitBindingsResult = {
  handleSubmit: (value: string) => void
  onSeriesSubmit: (value: string) => void
}
export const useCommandScreenSubmitBindings = ({
  popupState,
  isAwaitingRefinement,
  submitRefinement,
  isCommandMenuActive,
  selectedCommandId,
  commandMenuArgsRaw,
  isCommandMode,
  intentFilePath,
  isGenerating,
  expandInputForSubmit,
  setInputValue,
  pushHistory,
  addCommandHistoryEntry,
  runGeneration,
  handleCommandSelection,
  handleNewCommand,
  handleReuseCommand,
  lastUserIntentRef,
  handleSeriesIntentSubmit,
  openHelp,
}: UseCommandScreenSubmitBindingsOptions): UseCommandScreenSubmitBindingsResult => {
  const handleSubmit = useIntentSubmitHandler({
    popupState,
    isAwaitingRefinement,
    submitRefinement,
    isCommandMenuActive,
    selectedCommandId,
    commandMenuArgsRaw,
    isCommandMode,
    intentFilePath,
    isGenerating,
    expandInputForSubmit,
    setInputValue,
    pushHistory,
    addCommandHistoryEntry,
    runGeneration,
    handleCommandSelection,
    handleNewCommand,
    handleReuseCommand,
    lastUserIntentRef,
    ...(openHelp ? { openHelp } : {}),
  })
  const onSeriesSubmit = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      if (trimmed) {
        addCommandHistoryEntry(`/series ${trimmed}`)
      }
      handleSeriesIntentSubmit(value)
    },
    [addCommandHistoryEntry, handleSeriesIntentSubmit],
  )
  return {
    handleSubmit,
    onSeriesSubmit,
  }
}
</file>

<file path="src/tui/screens/command/hooks/useIntentSubmitHandler.ts">
import type { MutableRefObject } from 'react'
import { useCallback } from 'react'
import type { CommandDescriptor, HistoryEntry, PopupState } from '../../../types'
import { resolveSubmitPlan } from '../utils/submit-plan'
type PushHistory = (content: string, kind?: HistoryEntry['kind']) => void
type AddCommandHistoryEntry = (value: string) => void
type RunGeneration = (payload: {
  intent?: string
  intentFile?: string
  resume?:
    | { kind: 'history'; selector: string; mode: import('../../../types').ResumeMode }
    | { kind: 'file'; payloadPath: string; mode: import('../../../types').ResumeMode }
}) => Promise<void>
export type UseIntentSubmitHandlerOptions = {
  popupState: PopupState
  isAwaitingRefinement: boolean
  submitRefinement: (value: string) => void
  isCommandMenuActive: boolean
  selectedCommandId: CommandDescriptor['id'] | null
  commandMenuArgsRaw: string
  isCommandMode: boolean
  intentFilePath: string
  isGenerating: boolean
  expandInputForSubmit: (value: string) => string
  setInputValue: (value: string) => void
  pushHistory: PushHistory
  addCommandHistoryEntry: AddCommandHistoryEntry
  runGeneration: RunGeneration
  handleCommandSelection: (commandId: CommandDescriptor['id'], argsRaw?: string) => void
  handleNewCommand: (argsRaw: string) => void
  handleReuseCommand: () => void
  lastUserIntentRef: MutableRefObject<string | null>
  openHelp?: () => void
}
export const useIntentSubmitHandler = ({
  popupState,
  isAwaitingRefinement,
  submitRefinement,
  isCommandMenuActive,
  selectedCommandId,
  commandMenuArgsRaw,
  isCommandMode,
  intentFilePath,
  isGenerating,
  expandInputForSubmit,
  setInputValue,
  pushHistory,
  addCommandHistoryEntry,
  runGeneration,
  handleCommandSelection,
  handleNewCommand,
  handleReuseCommand,
  lastUserIntentRef,
  openHelp,
}: UseIntentSubmitHandlerOptions): ((value: string) => void) => {
  return useCallback(
    (value: string) => {
      const expandedValue = expandInputForSubmit(value)
      const plan = resolveSubmitPlan({
        expandedValue,
        isAwaitingRefinement,
        popupOpen: Boolean(popupState),
        isCommandMenuActive,
        selectedCommandId,
        commandMenuArgsRaw,
        isCommandMode,
        intentFilePath,
        isGenerating,
      })
      for (const action of plan) {
        switch (action.type) {
          case 'set-input': {
            setInputValue(action.value)
            break
          }
          case 'push-history': {
            pushHistory(action.content, action.kind)
            break
          }
          case 'add-command-history': {
            addCommandHistoryEntry(action.value)
            break
          }
          case 'set-last-user-intent': {
            lastUserIntentRef.current = action.value
            break
          }
          case 'submit-refinement': {
            submitRefinement(action.value)
            break
          }
          case 'run-generation': {
            const payload: {
              intent?: string
              intentFile?: string
              resume?:
                | { kind: 'history'; selector: string; mode: import('../../../types').ResumeMode }
                | { kind: 'file'; payloadPath: string; mode: import('../../../types').ResumeMode }
            } = {}
            if (action.intent) {
              payload.intent = action.intent
            }
            if (action.intentFile) {
              payload.intentFile = action.intentFile
            }
            if (action.resume) {
              payload.resume = action.resume
            }
            void runGeneration(payload)
            break
          }
          case 'run-new': {
            handleNewCommand(action.argsRaw)
            break
          }
          case 'run-reuse': {
            handleReuseCommand()
            break
          }
          case 'run-command': {
            if (action.commandId === 'help') {
              if (openHelp) {
                openHelp()
              } else {
                pushHistory('Help overlay is unavailable in this view.', 'system')
              }
              break
            }
            handleCommandSelection(action.commandId, action.argsRaw)
            break
          }
          default: {
            throw new Error('Unhandled submit plan action')
          }
        }
      }
    },
    [
      addCommandHistoryEntry,
      commandMenuArgsRaw,
      expandInputForSubmit,
      handleCommandSelection,
      handleNewCommand,
      handleReuseCommand,
      intentFilePath,
      isAwaitingRefinement,
      isCommandMenuActive,
      isCommandMode,
      isGenerating,
      lastUserIntentRef,
      openHelp,
      popupState,
      pushHistory,
      runGeneration,
      selectedCommandId,
      setInputValue,
      submitRefinement,
    ],
  )
}
</file>

<file path="src/tui/screens/command/CommandScreenImpl.tsx">
import { forwardRef, memo, useImperativeHandle } from 'react'
import { Box, Text, useStdout } from 'ink'
import { BackgroundFill } from '../../components/core/BackgroundFill'
import { CommandInput } from './components/CommandInput'
import { CommandMenuPane } from './components/CommandMenuPane'
import { HistoryPane } from './components/HistoryPane'
import { PopupArea } from './components/PopupArea'
import { useCommandScreenController } from './hooks/useCommandScreenController'
import type { NotifyOptions } from '../../notifier'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
type CommandScreenProps = {
  interactiveTransportPath?: string | undefined
  onPopupVisibilityChange?: (isOpen: boolean) => void
  commandMenuSignal?: number
  helpOpen?: boolean
  reservedRows?: number
  onOpenHelp?: () => void
  notify: (message: string, options?: NotifyOptions) => void
}
export type CommandScreenHandle = {
  suppressNextInput: () => void
}
export const CommandScreen = memo(
  forwardRef<CommandScreenHandle, CommandScreenProps>(
    (
      {
        interactiveTransportPath,
        onPopupVisibilityChange,
        commandMenuSignal,
        helpOpen = false,
        reservedRows = 0,
        onOpenHelp,
        notify,
      },
      ref,
    ) => {
      const {
        view: {
          transportMessage,
          historyPaneProps,
          popupAreaProps,
          commandMenuPaneProps,
          commandInputProps,
        },
        actions: { suppressNextInput },
      } = useCommandScreenController({
        ...(interactiveTransportPath ? { transport: { interactiveTransportPath } } : {}),
        popup: {
          ...(onPopupVisibilityChange ? { onPopupVisibilityChange } : {}),
          ...(commandMenuSignal !== undefined ? { commandMenuSignal } : {}),
          helpOpen,
          reservedRows,
          ...(onOpenHelp ? { onOpenHelp } : {}),
        },
        notify,
      })
      useImperativeHandle(ref, () => ({ suppressNextInput }), [suppressNextInput])
      const { theme } = useTheme()
      const { stdout } = useStdout()
      const terminalRows = stdout?.rows ?? 24
      const terminalColumns = stdout?.columns ?? 80
      // AppContainer applies `paddingX={2}` (left + right), which reduces the
      // actual renderable width available to CommandScreen. If we try to render
      // a full-width `BackgroundFill` at `stdout.columns`, Ink will truncate the
      // line and paint `...` in the last cells.
      const backdropColumns = Math.max(0, terminalColumns - 4)
      const showPopupOverlay = Boolean(popupAreaProps.popupState) && !popupAreaProps.helpOpen
      return (
        <Box flexGrow={1} width="100%" {...inkBackgroundColorProps(theme.background)}>
          <Box flexDirection="column" flexGrow={1} paddingX={1} paddingY={1} width="100%">
            {transportMessage ? (
              <Box flexShrink={0}>
                <Text {...inkColorProps(theme.warning)}>{transportMessage}</Text>
              </Box>
            ) : null}
            <HistoryPane {...historyPaneProps} />
            <CommandMenuPane {...commandMenuPaneProps} />
            <CommandInput {...commandInputProps} />
          </Box>
          {showPopupOverlay ? (
            <Box position="absolute" width="100%" height="100%">
              <Box position="absolute" width="100%" height="100%" overflow="hidden">
                <BackgroundFill
                  rows={terminalRows}
                  columns={backdropColumns}
                  background={theme.background}
                />
              </Box>
              <Box
                position="absolute"
                width="100%"
                height="100%"
                justifyContent="center"
                alignItems="center"
              >
                <PopupArea {...popupAreaProps} />
              </Box>
            </Box>
          ) : null}
        </Box>
      )
    },
  ),
)
CommandScreen.displayName = 'CommandScreen'
</file>

<file path="src/tui/theme/theme-validate.ts">
import {
  REQUIRED_THEME_SLOTS,
  type RequiredThemeSlot,
  type ThemeColorValue,
  type ThemeJson,
} from './theme-types'
type ThemeValidationResult = { ok: true; theme: ThemeJson } | { ok: false; message: string }
const isRecord = (value: unknown): value is Record<string, unknown> =>
  typeof value === 'object' && value !== null
const isThemeColorVariant = (value: unknown): value is { dark: unknown; light: unknown } => {
  if (!isRecord(value)) {
    return false
  }
  return 'dark' in value && 'light' in value
}
const isThemeColorValue = (value: unknown): value is ThemeColorValue => {
  if (typeof value === 'string') {
    return true
  }
  if (typeof value === 'number') {
    return Number.isFinite(value)
  }
  if (isThemeColorVariant(value)) {
    return isThemeColorValue(value.dark) && isThemeColorValue(value.light)
  }
  return false
}
export const validateThemeJson = (value: unknown): ThemeValidationResult => {
  if (!isRecord(value)) {
    return { ok: false, message: 'Theme JSON must be an object.' }
  }
  const theme = value.theme
  if (!isRecord(theme)) {
    return { ok: false, message: 'Theme JSON must include a `theme` object.' }
  }
  const defs = value.defs
  if (defs !== undefined) {
    if (!isRecord(defs)) {
      return { ok: false, message: '`defs` must be an object when present.' }
    }
    for (const [key, defValue] of Object.entries(defs)) {
      if (!isThemeColorValue(defValue)) {
        return { ok: false, message: `Invalid defs color value for key: ${key}` }
      }
    }
  }
  const missingSlots: RequiredThemeSlot[] = []
  for (const slot of REQUIRED_THEME_SLOTS) {
    const slotValue = theme[slot]
    if (slotValue === undefined) {
      missingSlots.push(slot)
      continue
    }
    if (!isThemeColorValue(slotValue)) {
      return { ok: false, message: `Invalid theme color value for slot: ${slot}` }
    }
  }
  if (missingSlots.length > 0) {
    return {
      ok: false,
      message: `Theme JSON missing required slots: ${missingSlots.join(', ')}`,
    }
  }
  for (const [key, slotValue] of Object.entries(theme)) {
    if (!isThemeColorValue(slotValue)) {
      return { ok: false, message: `Invalid theme color value for key: ${key}` }
    }
  }
  return { ok: true, theme: value as ThemeJson }
}
</file>

<file path="src/export-command.ts">
import path from 'node:path'
import yargs from 'yargs'
import type { ArgumentsCamelCase } from 'yargs'
import type { PayloadFormat } from './generate/payload-io'
import { writeGeneratePayloadExport } from './export/export-generate-payload'
import { loadGeneratePayloadFromHistory } from './history/generate-history'
type ExportArgs = {
  fromHistory: string
  format: PayloadFormat
  out: string
  quiet: boolean
  help: boolean
}
export const runExportCommand = async (argv: string[]): Promise<void> => {
  try {
    const { args, showHelp } = parseExportArgs(argv)
    if (args.help) {
      showHelp()
      return
    }
    await exportHistoryEntry(args)
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown export error.'
    console.error(message)
    process.exitCode = 1
  }
}
const HELP_FLAGS = new Set(['--help', '-h'])
const parseExportArgs = (argv: string[]): { args: ExportArgs; showHelp: () => void } => {
  const { optionArgs, helpRequested } = stripHelpFlags(argv)
  const parser = yargs(optionArgs)
    .scriptName('prompt-maker-cli export')
    .usage(
      'Usage:\n  prompt-maker-cli export [--from-history <selector>] --format json|yaml --out <path>',
    )
    .option('from-history', {
      type: 'string',
      default: 'last',
      describe: 'History selector (last, last:N, or N-th from end)',
    })
    .option('format', {
      alias: 'f',
      type: 'string',
      choices: ['json', 'yaml'] as const,
      describe: 'Output format',
    })
    .option('out', {
      alias: 'o',
      type: 'string',
      describe: 'Output file path',
    })
    .option('quiet', {
      type: 'boolean',
      default: false,
      describe: 'Suppress human-readable stderr logs',
    })
    .check((argv) => {
      if (helpRequested) {
        return true
      }
      const format = argv.format
      if (format !== 'json' && format !== 'yaml') {
        throw new Error('--format is required (json|yaml).')
      }
      const outPath = argv.out
      if (typeof outPath !== 'string' || outPath.trim().length === 0) {
        throw new Error('--out is required.')
      }
      return true
    })
    .help('help')
    .alias('help', 'h')
    .exitProcess(false)
    .showHelpOnFail(false)
    .parserConfiguration({ 'halt-at-non-option': true })
    .strict(false)
    .fail((msg, err) => {
      throw err ?? new Error(msg ?? 'Invalid CLI arguments.')
    })
  const parsed = parser.parseSync() as ArgumentsCamelCase<{
    fromHistory?: unknown
    format?: PayloadFormat
    out?: string
    quiet: boolean
    help?: boolean
  }>
  const fromHistoryValue = parsed.fromHistory
  const fromHistory =
    typeof fromHistoryValue === 'string'
      ? fromHistoryValue.trim() || 'last'
      : typeof fromHistoryValue === 'number'
        ? String(fromHistoryValue)
        : 'last'
  const help = helpRequested || Boolean(parsed.help)
  const format = parsed.format ?? 'json'
  const out = parsed.out?.trim() ?? ''
  if (!help && !out) {
    throw new Error('--out is required.')
  }
  return {
    args: {
      fromHistory,
      format,
      out,
      quiet: parsed.quiet ?? false,
      help,
    },
    showHelp: () => parser.showHelp(),
  }
}
const exportHistoryEntry = async (args: ExportArgs): Promise<void> => {
  const payload = await loadGeneratePayloadFromHistory({ selector: args.fromHistory })
  const { absolutePath } = await writeGeneratePayloadExport({
    payload,
    format: args.format,
    outPath: args.out,
  })
  if (!args.quiet) {
    console.error(
      `Wrote ${args.format.toUpperCase()} export to ${formatDisplayPath(absolutePath)}.`,
    )
  }
}
const formatDisplayPath = (absolutePath: string): string => {
  const relative = path.relative(process.cwd(), absolutePath)
  return relative && !relative.startsWith('..') ? relative : absolutePath
}
const stripHelpFlags = (tokens: string[]): { optionArgs: string[]; helpRequested: boolean } => {
  const optionArgs: string[] = []
  let helpRequested = false
  let passthrough = false
  tokens.forEach((token) => {
    if (passthrough) {
      optionArgs.push(token)
      return
    }
    if (token === '--') {
      optionArgs.push(token)
      passthrough = true
      return
    }
    if (HELP_FLAGS.has(token)) {
      helpRequested = true
      return
    }
    optionArgs.push(token)
  })
  return { optionArgs, helpRequested }
}
</file>

<file path="src/index.ts">
#!/usr/bin/env node
import { runComposeCommand } from './compose-command'
import { runExportCommand } from './export-command'
import { runGenerateCommand } from './generate-command'
import { runTestCommand } from './test-command'
type CliCommand = 'compose' | 'export' | 'generate' | 'test' | 'ui'
const { command, args } = resolveCommand(process.argv.slice(2))
switch (command) {
  case 'test':
    void runTestCommand(args)
    break
  case 'ui':
    void loadAndRunTui(args)
    break
  case 'export':
    void runExportCommand(args)
    break
  case 'compose':
    void runComposeCommand(args)
    break
  case 'generate':
  default:
    void runGenerateCommand(args)
}
async function loadAndRunTui(args: string[]): Promise<void> {
  const { runTuiCommand } = await import('./tui')
  await runTuiCommand(args)
}
function resolveCommand(args: string[]): { command: CliCommand; args: string[] } {
  if (args.length === 0) {
    return { command: 'ui', args }
  }
  const [first, ...rest] = args
  if (!first) {
    return { command: 'ui', args }
  }
  if (first === 'test') {
    return { command: 'test', args: rest }
  }
  if (first === 'ui') {
    return { command: 'ui', args: rest }
  }
  if (first === 'export') {
    return { command: 'export', args: rest }
  }
  if (first === 'compose') {
    return { command: 'compose', args: rest }
  }
  if (!first.startsWith('-') && (first === 'generate' || first === 'expand')) {
    return { command: 'generate', args: rest }
  }
  if (first.startsWith('-')) {
    return { command: 'generate', args }
  }
  return { command: 'generate', args }
}
</file>

<file path="src/media-loader.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import { GoogleAIFileManager } from '@google/generative-ai/server'
const POLL_INTERVAL_MS = 3_000
const PROCESSING_TIMEOUT_MS = 5 * 60_000
const VIDEO_MIME_TYPES: Record<string, string> = {
  '.mp4': 'video/mp4',
  '.mov': 'video/quicktime',
  '.m4v': 'video/x-m4v',
  '.webm': 'video/webm',
  '.avi': 'video/x-msvideo',
  '.mpeg': 'video/mpeg',
  '.mpg': 'video/mpeg',
  '.gif': 'image/gif',
}
const PDF_MIME_TYPE = 'application/pdf'
type GeminiFile = Awaited<ReturnType<GoogleAIFileManager['getFile']>>
type FileState = 'STATE_UNSPECIFIED' | 'PROCESSING' | 'ACTIVE' | 'FAILED'
export const uploadFileForGeminiWithMimeType = async (
  filePath: string,
  mimeType: string,
  apiKey?: string,
): Promise<string> => {
  await assertReadableFile(filePath)
  const manager = createFileManager(apiKey)
  const uploadResponse = await manager.uploadFile(filePath, {
    mimeType,
    displayName: path.basename(filePath),
  })
  const uploadedFile = uploadResponse.file
  if (!uploadedFile?.name) {
    throw new Error('Gemini Files API did not return a file name.')
  }
  const readyFile = await waitForActiveFile(manager, uploadedFile.name)
  if (!readyFile.uri) {
    throw new Error(`Gemini file ${uploadedFile.name} became active without a URI.`)
  }
  return readyFile.uri
}
export const uploadFileForGemini = async (filePath: string, apiKey?: string): Promise<string> => {
  const mimeType = inferVideoMimeType(filePath)
  return await uploadFileForGeminiWithMimeType(filePath, mimeType, apiKey)
}
const assertReadableFile = async (filePath: string): Promise<void> => {
  try {
    await fs.access(filePath)
  } catch {
    throw new Error(`Media file ${filePath} is not readable.`)
  }
}
export const inferVideoMimeType = (filePath: string): string => {
  const ext = path.extname(filePath).toLowerCase()
  const mimeType = VIDEO_MIME_TYPES[ext]
  if (!mimeType) {
    throw new Error(`Unsupported media type for ${filePath}.`)
  }
  return mimeType
}
export const inferPdfMimeType = (filePath: string): 'application/pdf' => {
  const ext = path.extname(filePath).toLowerCase()
  if (ext !== '.pdf') {
    throw new Error(`Unsupported PDF type for ${filePath}.`)
  }
  return PDF_MIME_TYPE
}
const createFileManager = (apiKey?: string): GoogleAIFileManager => {
  const resolvedApiKey = apiKey?.trim() || process.env.GEMINI_API_KEY?.trim()
  if (!resolvedApiKey) {
    throw new Error(
      'Gemini API key is required to upload media files. Pass apiKey or set GEMINI_API_KEY.',
    )
  }
  return new GoogleAIFileManager(resolvedApiKey)
}
const waitForActiveFile = async (
  manager: GoogleAIFileManager,
  fileName: string,
  timeoutMs = PROCESSING_TIMEOUT_MS,
): Promise<GeminiFile> => {
  const start = Date.now()
  // eslint-disable-next-line no-constant-condition
  while (true) {
    const file = await manager.getFile(fileName)
    const state = normalizeState(file.state)
    if (state === 'ACTIVE') {
      return file
    }
    if (state === 'FAILED') {
      const message = file.error?.message ?? 'The Gemini Files API reported a failure.'
      throw new Error(`Failed to process media file ${fileName}: ${message}`)
    }
    if (Date.now() - start > timeoutMs) {
      throw new Error(`Timed out while waiting for Gemini to process file ${fileName}.`)
    }
    await delay(POLL_INTERVAL_MS)
  }
}
const normalizeState = (state: GeminiFile['state']): FileState => {
  if (!state) {
    return 'STATE_UNSPECIFIED'
  }
  const normalized = state.toUpperCase() as FileState
  if (normalized === 'ACTIVE' || normalized === 'FAILED' || normalized === 'PROCESSING') {
    return normalized
  }
  return 'STATE_UNSPECIFIED'
}
const delay = (ms: number): Promise<void> => {
  return new Promise((resolve) => {
    setTimeout(resolve, ms)
  })
}
</file>

<file path="src/prompt-generator-service.ts">
import { callLLM, type Message, type MessageContent } from '@prompt-maker/core'
import path from 'node:path'
import {
  GEN_SYSTEM_PROMPT,
  REFINE_SYSTEM_PROMPT,
  SERIES_REPAIR_SYSTEM_PROMPT,
  SERIES_SYSTEM_PROMPT,
} from './prompt-generator/prompts'
import {
  buildInitialUserMessage,
  buildRefinementMessage,
  buildRefinementMessageText,
  buildSeriesUserMessage,
  mergeResolvedMediaWithText,
} from './prompt-generator/message-builders'
import { ensureModelCredentials, isGemini } from './prompt-generator/model-credentials'
import { parseLLMJson } from './prompt-generator/parse-llm-json'
import {
  buildSeriesRepairUserMessage,
  isRepairableSeriesValidationError,
} from './prompt-generator/series-repair'
import { validateSeriesResponse } from './prompt-generator/series-validation'
import {
  buildTargetRuntimeModelGuidance,
  sanitizePromptForTargetModelLeakage,
} from './prompt-generator/target-model-guidance'
import type {
  PromptGenerationRequest,
  PromptGenerationResult,
  SeriesResponse,
} from './prompt-generator/types'
export {
  GEN_SYSTEM_PROMPT,
  REFINE_SYSTEM_PROMPT,
  SERIES_SYSTEM_PROMPT,
  SERIES_REPAIR_SYSTEM_PROMPT,
}
export { sanitizePromptForTargetModelLeakage }
export {
  ensureModelCredentials,
  isGemini,
  resolveDefaultGenerateModel,
} from './prompt-generator/model-credentials'
export type {
  PromptGenerationRequest,
  PromptGenerationResult,
  SeriesRepairAttemptDetail,
  SeriesResponse,
  UploadDetail,
  UploadState,
  UploadStateChange,
} from './prompt-generator/types'
type CoTResponse = {
  reasoning: string
  prompt: string
}
type PdfGroundingAssessment =
  | { ok: true }
  | {
      ok: false
      reason: 'missing-pdf-filename' | 'asked-for-document' | 'missing-document-snapshot'
    }
const normalizeForHeuristic = (value: string): string => value.toLowerCase()
const getPdfBasenames = (pdfPaths: readonly string[]): string[] => {
  return pdfPaths.map((pdfPath) => path.basename(pdfPath)).filter((name) => name.length > 0)
}
const assessPdfGrounding = (
  prompt: string,
  pdfPaths: readonly string[],
): PdfGroundingAssessment => {
  if (pdfPaths.length === 0) {
    return { ok: true }
  }
  const lowered = normalizeForHeuristic(prompt)
  const forbiddenAsks = [
    'paste the',
    'paste the document',
    'paste document',
    'provide the document',
    'provide document',
    'upload the document',
    'upload the pdf',
    'provide as a file',
    'provide as file',
  ]
  if (forbiddenAsks.some((needle) => lowered.includes(needle))) {
    return { ok: false, reason: 'asked-for-document' }
  }
  const basenames = getPdfBasenames(pdfPaths)
  const mentionsAnyBasename = basenames.some((name) =>
    normalizeForHeuristic(prompt).includes(name.toLowerCase()),
  )
  if (!mentionsAnyBasename) {
    return { ok: false, reason: 'missing-pdf-filename' }
  }
  const hasSnapshot = lowered.includes('document snapshot') || lowered.includes('document outline')
  if (!hasSnapshot) {
    return { ok: false, reason: 'missing-document-snapshot' }
  }
  return { ok: true }
}
const buildPdfGroundingRepairInstruction = (pdfPaths: readonly string[]): string => {
  const basenames = getPdfBasenames(pdfPaths)
  return [
    'Make this prompt contract non-generic and explicitly grounded in the attached PDF(s).',
    '',
    'Hard requirements:',
    `- Mention the attached PDF filename(s) verbatim somewhere in the contract: ${
      basenames.join(', ') || '(unknown filename)'
    }.`,
    '- In "Inputs", do NOT ask the user to paste/upload/provide the PDF/path. The PDF is already attached and must be used directly.',
    '- Add a new section (in the contract) titled "Document Snapshot" that proves you read the attached PDF:',
    '  - 5-10 bullet points capturing the document’s specific topics/sections.',
    '  - 3 short verbatim quotes (10-25 words each) from the PDF (wrap each quote in double quotes).',
    '  - If the PDF appears image-only/scanned and you cannot quote text, state that explicitly and request OCR as the only missing input.',
    '',
    'Do not change the overall prompt-contract format requirements; just make it grounded and actionable.',
  ].join('\n')
}
export class PromptGeneratorService {
  async generatePromptDetailed(request: PromptGenerationRequest): Promise<PromptGenerationResult> {
    await ensureModelCredentials(request.model)
    const isRefinement = Boolean(request.previousPrompt && request.refinementInstruction)
    const systemContent = isRefinement ? REFINE_SYSTEM_PROMPT : GEN_SYSTEM_PROMPT
    const geminiApiKey = isGemini(request.model) ? process.env.GEMINI_API_KEY?.trim() : undefined
    let userContent: MessageContent
    if (isRefinement) {
      const previousPrompt = request.previousPrompt
      const refinementInstruction = request.refinementInstruction
      if (!previousPrompt || !refinementInstruction) {
        throw new Error('Refinement requests require previousPrompt and refinementInstruction.')
      }
      userContent = await buildRefinementMessage(
        previousPrompt,
        refinementInstruction,
        request.intent,
        request.fileContext,
        request.images,
        request.videos,
        request.pdfs ?? [],
        request.metaInstructions,
        request.onUploadStateChange,
        geminiApiKey,
      )
    } else {
      userContent = await buildInitialUserMessage(
        request.intent,
        request.fileContext,
        request.images,
        request.videos,
        request.pdfs ?? [],
        request.metaInstructions,
        request.onUploadStateChange,
        geminiApiKey,
      )
    }
    const targetGuidance = buildTargetRuntimeModelGuidance(request.targetModel)
    const messages: Message[] = [
      { role: 'system', content: systemContent },
      ...(targetGuidance ? [{ role: 'system' as const, content: targetGuidance }] : []),
      { role: 'user', content: userContent },
    ]
    const rawResponse = await callLLM(messages, request.model)
    const attemptRepair = async (
      prompt: string,
      reasoning?: string,
    ): Promise<PromptGenerationResult> => {
      const pdfPaths = request.pdfs ?? []
      const assessment = assessPdfGrounding(prompt, pdfPaths)
      if (assessment.ok) {
        return { prompt, ...(reasoning ? { reasoning } : {}) }
      }
      // Avoid infinite repair loops.
      const MAX_PDF_GROUNDING_REPAIR_ATTEMPTS = 1
      for (let attempt = 0; attempt < MAX_PDF_GROUNDING_REPAIR_ATTEMPTS; attempt += 1) {
        const refinementInstruction = buildPdfGroundingRepairInstruction(pdfPaths)
        request.onPromptAutoRepairAttempt?.({
          kind: 'pdf-grounding',
          reason: assessment.reason,
          attempt: 1,
          maxAttempts: MAX_PDF_GROUNDING_REPAIR_ATTEMPTS,
          pdfs: [...pdfPaths],
        })
        const repairText = buildRefinementMessageText(
          prompt,
          refinementInstruction,
          request.intent,
          request.fileContext,
          pdfPaths,
          request.metaInstructions,
        )
        const repairUserContent = mergeResolvedMediaWithText(userContent, repairText)
        const repairMessages: Message[] = [
          { role: 'system', content: REFINE_SYSTEM_PROMPT },
          ...(targetGuidance ? [{ role: 'system' as const, content: targetGuidance }] : []),
          { role: 'user', content: repairUserContent },
        ]
        const repairedRaw = await callLLM(repairMessages, request.model)
        try {
          const repaired = parseLLMJson<CoTResponse>(repairedRaw)
          const repairedPrompt = sanitizePromptForTargetModelLeakage({
            prompt: repaired.prompt,
            intent: request.intent,
            targetModel: request.targetModel,
          })
          const repairedAssessment = assessPdfGrounding(repairedPrompt, pdfPaths)
          if (repairedAssessment.ok) {
            return {
              prompt: repairedPrompt,
              ...(repaired.reasoning ? { reasoning: repaired.reasoning } : {}),
            }
          }
          // If still failing, return the repaired prompt anyway (better than original).
          return {
            prompt: repairedPrompt,
            ...(repaired.reasoning ? { reasoning: repaired.reasoning } : {}),
          }
        } catch {
          // If JSON parse fails, fall back to raw repaired text.
          const repairedPrompt = sanitizePromptForTargetModelLeakage({
            prompt: repairedRaw,
            intent: request.intent,
            targetModel: request.targetModel,
          })
          return { prompt: repairedPrompt }
        }
      }
      return { prompt, ...(reasoning ? { reasoning } : {}) }
    }
    try {
      const result = parseLLMJson<CoTResponse>(rawResponse)
      if (process.env.DEBUG || process.env.VERBOSE) {
        console.error('\n--- AI Reasoning ---')
        console.error(result.reasoning)
        console.error('--------------------\n')
      }
      const sanitizedPrompt = sanitizePromptForTargetModelLeakage({
        prompt: result.prompt,
        intent: request.intent,
        targetModel: request.targetModel,
      })
      return await attemptRepair(sanitizedPrompt, result.reasoning)
    } catch {
      const sanitizedPrompt = sanitizePromptForTargetModelLeakage({
        prompt: rawResponse,
        intent: request.intent,
        targetModel: request.targetModel,
      })
      return await attemptRepair(sanitizedPrompt)
    }
  }
  async generatePrompt(request: PromptGenerationRequest): Promise<string> {
    const result = await this.generatePromptDetailed(request)
    return result.prompt
  }
  async generatePromptSeries(request: PromptGenerationRequest): Promise<SeriesResponse> {
    await ensureModelCredentials(request.model)
    const geminiApiKey = isGemini(request.model) ? process.env.GEMINI_API_KEY?.trim() : undefined
    const userContent = await buildSeriesUserMessage(
      request.intent,
      request.fileContext,
      request.images,
      request.videos,
      request.pdfs ?? [],
      request.metaInstructions,
      request.onUploadStateChange,
      geminiApiKey,
    )
    const targetGuidance = buildTargetRuntimeModelGuidance(request.targetModel)
    const messages: Message[] = [
      { role: 'system', content: SERIES_SYSTEM_PROMPT },
      ...(targetGuidance ? [{ role: 'system' as const, content: targetGuidance }] : []),
      { role: 'user', content: userContent },
    ]
    const MAX_SERIES_REPAIR_ATTEMPTS = 2
    let rawResponse = await callLLM(messages, request.model)
    let series: SeriesResponse
    try {
      series = parseLLMJson<SeriesResponse>(rawResponse)
    } catch {
      throw new Error('LLM did not return valid SeriesResponse JSON.')
    }
    for (let attempt = 0; attempt <= MAX_SERIES_REPAIR_ATTEMPTS; attempt++) {
      try {
        validateSeriesResponse(series)
        break
      } catch (error) {
        if (attempt === MAX_SERIES_REPAIR_ATTEMPTS || !isRepairableSeriesValidationError(error)) {
          throw error
        }
        const validationError = error instanceof Error ? error.message : String(error)
        request.onSeriesRepairAttempt?.({
          attempt: attempt + 1,
          maxAttempts: MAX_SERIES_REPAIR_ATTEMPTS,
          validationError,
        })
        const repairUserContent = buildSeriesRepairUserMessage({
          intent: request.intent,
          validationError,
          previousSeries: series,
        })
        const repairMessages: Message[] = [
          { role: 'system', content: SERIES_REPAIR_SYSTEM_PROMPT },
          ...(targetGuidance ? [{ role: 'system' as const, content: targetGuidance }] : []),
          { role: 'user', content: repairUserContent },
        ]
        rawResponse = await callLLM(repairMessages, request.model)
        try {
          series = parseLLMJson<SeriesResponse>(rawResponse)
        } catch {
          throw new Error('LLM did not return valid SeriesResponse JSON.')
        }
      }
    }
    const sanitizedOverviewPrompt = sanitizePromptForTargetModelLeakage({
      prompt: series.overviewPrompt,
      intent: request.intent,
      targetModel: request.targetModel,
    })
    const sanitizedAtomicPrompts = series.atomicPrompts.map((step) => ({
      ...step,
      content: sanitizePromptForTargetModelLeakage({
        prompt: step.content,
        intent: request.intent,
        targetModel: request.targetModel,
      }),
    }))
    if (process.env.DEBUG || process.env.VERBOSE) {
      console.error('\n--- Series Reasoning ---')
      console.error(series.reasoning)
      console.error('------------------------\n')
    }
    return {
      ...series,
      overviewPrompt: sanitizedOverviewPrompt,
      atomicPrompts: sanitizedAtomicPrompts,
    }
  }
}
export const createPromptGeneratorService = async (): Promise<PromptGeneratorService> => {
  return new PromptGeneratorService()
}
export const generatePromptSeries = async (
  request: PromptGenerationRequest,
): Promise<SeriesResponse> => {
  const service = await createPromptGeneratorService()
  return await service.generatePromptSeries(request)
}
</file>

<file path="package.json">
{
  "name": "@perceptron/prompt-maker-cli",
  "version": "0.1.0",
  "description": "Terminal workflow for diagnosing and improving prompts via prompt-maker.",
  "license": "MIT",
  "type": "module",
  "bin": {
    "prompt-maker-cli": "./dist/index.js"
  },
  "files": [
    "dist"
  ],
  "scripts": {
    "build": "node scripts/build.mjs",
    "dev": "node scripts/dev.mjs",
    "start": "node dist/index.js",
    "typecheck": "tsc -p tsconfig.json --noEmit",
    "test": "jest",
    "format": "prettier -w .",
    "lint": "prettier -c \"{src,packages,scripts,tests}/**/*.{ts,tsx,js,jsx,cjs,mjs,json,md}\"",
    "refresh": "npm run lint && npm run typecheck && npm test && npm run build && npm install -g .",
    "prepare": "npm run build",
    "prepublishOnly": "npm run build"
  },
  "keywords": [
    "prompt",
    "cli",
    "prompt-maker",
    "neovim"
  ],
  "engines": {
    "node": ">=18"
  },
  "dependencies": {
    "@google/generative-ai": "^0.24.1",
    "boxen": "^8.0.1",
    "chalk": "^5.3.0",
    "cli-cursor": "^4.0.0",
    "cli-table3": "^0.6.5",
    "clipboardy": "^5.0.1",
    "enquirer": "^2.4.1",
    "fast-glob": "^3.3.3",
    "fzf": "^0.5.2",
    "html-to-text": "^9.0.5",
    "ink": "^4.4.1",
    "js-tiktoken": "^1.0.21",
    "js-yaml": "^4.1.0",
    "open": "^11.0.0",
    "ora": "^8.1.0",
    "pdfjs-dist": "^4.10.38",
    "react": "^18.3.1",
    "react-devtools-core": "^4.28.5",
    "react-dom": "^18.3.1",
    "wrap-ansi": "^8.1.0",
    "yargs": "^18.0.0",
    "zod": "^3.23.8"
  },
  "devDependencies": {
    "@testing-library/react": "^16.1.0",
    "@types/html-to-text": "^9.0.4",
    "@types/jest": "^29.5.12",
    "@types/js-yaml": "^4.0.9",
    "@types/node": "^20.12.12",
    "@types/react": "^18.3.11",
    "@types/react-dom": "^18.3.0",
    "@types/yargs": "^17.0.33",
    "@typescript-eslint/eslint-plugin": "^8.51.0",
    "@typescript-eslint/parser": "^8.51.0",
    "esbuild": "^0.24.2",
    "eslint": "^8.57.1",
    "jest": "^29.7.0",
    "jsdom": "^24.0.0",
    "prettier": "^3.3.3",
    "ts-jest": "^29.2.5",
    "typescript": "^5.6.3"
  }
}
</file>

<file path="docs/neovim-plugin-integration.md">
# prompt-maker-cli × NeoVim Integration Guide

_Comprehensive reference for building a NeoVim plugin that orchestrates prompt-maker-cli runs, context capture, iterative refinements, and artifact delivery inside the editor._

## Quick Checklist

- Ensure `prompt-maker-cli` binary is built/installed and discoverable in `$PATH`.
- Confirm `OPENAI_API_KEY`/`GEMINI_API_KEY` (plus optional `GITHUB_TOKEN`) are set or present in `~/.config/prompt-maker-cli/config.json`.
- Capture intent via inline text, temp files, or buffer exports—never launch without validated intent input.
- Attach context through `--context`, `--url`, `--image`/`--video`, and `--smart-context-root` as needed; guard against oversized or duplicate files.
- Prefer `--json --quiet --stream jsonl` for editor integrations; parse `generation.final`, `context.telemetry`, and `context.overflow` to drive UI.
- Set token budgets via `--max-input-tokens` and/or `--max-context-tokens`, and choose an overflow strategy via `--context-overflow`.
- Use `telemetry.totalTokens` and `telemetry.fileTokens` from the `context.telemetry` event as guardrails (warn early, and keep a safety margin).
- Use `--interactive-transport <socket>` for refinement loops; send `{"type":"refine"}` / `{"type":"finish"}` messages and mirror streamed events.
- Apply `--context-template nvim` (or user template) before writing to buffers; fall back to `polishedPrompt`/`prompt` if `renderedPrompt` absent.
- Surface warnings/errors from stderr immediately (credentials, context fetch failures, upload issues) and offer corrective prompts.
- Append runs to history or expose pickers by tailing `~/.config/prompt-maker-cli/history.jsonl`.
- Clean up sockets/temp files when jobs end; cancel outstanding refinements on disconnect.

## 1. Purpose & Audience

- **Audience**: developers of a NeoVim plugin (Lua or TypeScript via `deno-nvim`/`node-host`) and accompanying coding agents collaborating on that plugin.
- **Goal**: expose `prompt-maker-cli` from inside NeoVim without reimplementing its business logic—use the CLI as the single source of truth for prompt contracts, telemetry, history logging, and transports.
- **Scope**: everything from installation and configuration through synchronous command execution, streaming refinement loops, media/context ingestion, diagnostics, and fallback handling.

## 2. Build & Runtime Prerequisites

- **Repository build**: from repo root run `npm ci` then `npm run build`. Artifacts land in `dist/`.
- **Global install**: `npm install -g .` (or `npm link` for iterative local development). The binary name is `prompt-maker-cli` (aliasable to `pmc`).
- **Direct execution**: during development you can run `node dist/index.js ...` (or `npm start -- ...`) without reinstalling.
- **Node tooling**: plugin should locate the binary via `which prompt-maker-cli` or respect user-configured aliases.
- **Config directory**: `$HOME/.config/prompt-maker-cli/` hosts `config.json`, `history.jsonl`, and the embeddings cache; the plugin must never write arbitrary files here unless mirroring CLI behavior (e.g., context templates, transport sockets under `/tmp`).
- **Environment**: CLI relies on `OPENAI_API_KEY`, `GEMINI_API_KEY`, optional `*_BASE_URL` overrides, and `PROMPT_MAKER_*` vars. Plugin settings UI should surface these but avoid storing secrets in git.

## 3. Command Surfaces & Flag Matrix

- **Primary command**: `prompt-maker-cli [intent] [options]` maps to the “generate” workflow; `prompt-maker-cli test ...` is unrelated to prompt generation.
- **Key positional/flag inputs**: summarized below (full list in Appendix A).
  - Intent: inline argument, `--intent-file`, or stdin.
  - Context: repeated `--context` globs, `--url`, `--image`, `--video`, `--smart-context` (+ `--smart-context-root`).
  - Output controls: `--json`, `--copy`, `--open-chatgpt`, `--context-template`, `--context-file`, `--context-format`.
  - Token budgets: `--max-input-tokens`, `--max-context-tokens`, `--context-overflow`.
  - Models: `--model <name>` (generation) and `--target <name>` (runtime optimization; recorded in JSON/history, not included in the generated prompt text by default).
  - Interaction: `-i/--interactive`, `--stream jsonl`, `--interactive-transport <path>`, `--quiet`, `--no-progress`.
  - Post-processing: `--polish`, `--polish-model`.

## 4. Intent & Context Ingestion

- **Intent sources**: exactly one of inline argument, `--intent-file`, or stdin must provide content. Files are capped at 512 KB, must be UTF‑8 text, and cannot contain NUL bytes (`readIntentFile`).
- **Interactive intent disambiguation**: placing a path immediately after `-i/--interactive` is treated as an implicit intent file with a warning—plugin should avoid this ambiguity by emitting explicit flags.
- **Context files**: `resolveFileContext` expands globs with `fast-glob` (`dot: true`). Missing matches trigger warnings but are not fatal. Each file becomes `<file path="…">…</file>` inside prompts.
- **Context dedupe**: plugin should keep track of already attached buffers/files to avoid redundant globbing, especially when also using smart context.
- **Context exports**: use `--show-context`, `--context-file <path>`, and `--context-format text|json` to preview or persist the blocks that feed the LLM—handy for preview windows inside NeoVim.

## 5. Remote Context Resolution

- **HTTP/HTTPS URLs**: `resolveUrlContext` downloads each URL (≤1 MB). HTML is converted to text via `html-to-text`, stripping scripts/styles. Failures log warnings but continue.
- **Duplicate suppression**: identical URLs are skipped. Only http(s) protocols allowed.
- **GitHub URLs**: `github-context.ts` recognizes `blob`, `tree`, and repo root URLs. Safeguards include file caps (≤64 KB per file, ≤60 files), ignore lists (`node_modules`, `dist`, lockfiles, archives), and automatic path prefix filtering. Responses stream progress via the `onProgress` callback—plugins can map these to NeoVim notifications.
- **Authentication**: honors `GITHUB_TOKEN` if set; otherwise uses unauthenticated requests with strict rate limits.

## 6. Smart Context (Local RAG)

- **Trigger**: `--smart-context` optionally paired with `--smart-context-root <dir>`.
- **Scan**: `fast-glob` searches `**/*.{ts,tsx,js,jsx,py,md,json}` excluding `node_modules`, build outputs, lockfiles, git metadata. Files >25 KB are skipped.
- **Embeddings**: `src/rag/vector-store.ts` stores SHA256 hashes + embeddings in `$HOME/.config/prompt-maker-cli/embeddings_cache.json` via `@prompt-maker/core#getEmbedding`.
- **Workflow**: index (with caching) → top‑k search (default 5) against intent string → read + append unique files not already in the user-provided context list.
- **Progress reporting**: `resolveSmartContextFiles` emits messages (“Scanning…”, “Indexed…”, “Smart context ready”) that surface through progress spinners and `--stream jsonl` events; the plugin should surface these as status lines.

## 7. Media Attachments

- **Images** (`--image`): Accepts PNG/JPG/JPEG/WEBP/GIF up to 20 MB. Files are Base64 encoded into `@prompt-maker/core` image parts. Unsupported extensions or oversize files generate warnings and are skipped.
- **Videos** (`--video`): Requires Gemini models. If any `--video` flag is present and the requested model is not Gemini, the CLI auto-switches to `gemini-3-pro-preview` (or configured default). Uploads use Google’s Files API via `GoogleAIFileManager`, polling until `ACTIVE` or failing with a detailed error.
- **Upload telemetry**: `upload.state` events emit `start/finish` for each file; when spinners are visible the label swaps to “Uploading…” until transfers finish. Plugin integrations should reflect this to prevent users from closing buffers mid-upload.

## 8. Model Resolution & Credentials

- **Default model**: `resolveDefaultGenerateModel` picks `promptGenerator.defaultModel`, `PROMPT_MAKER_GENERATE_MODEL`, or `gpt-4o-mini`.
- **Gemini fallback**: `resolveGeminiVideoModel` prefers `promptGenerator.defaultGeminiModel`, otherwise `gemini-3-pro-preview`.
- **Credential loading**: `ensureModelCredentials` pulls env vars first, then falls back to `~/.config/prompt-maker-cli/config.json`. Missing keys throw descriptive errors before API calls, so the plugin should capture stderr and surface the message.
- **Polish model**: defaults to the generation model unless `--polish-model` or `PROMPT_MAKER_POLISH_MODEL` overrides it.
- **Target model**: set via `--target <name>` (or `/target` in the TUI). If omitted, it defaults to the resolved default generation model. The CLI uses it as internal optimization guidance (and emits it as `targetModel` in payloads/history), but it must not be echoed into the user-facing prompt text unless the intent explicitly asks for it.

## 9. Generation & Refinement Workflow

1. **Telemetry prep**: count tokens for intent + context (using `js-tiktoken`). Results print in a cyan box/table unless `--quiet` or `--json` suppresses UI. Plugin can parse `generation.iteration.start` events for token counts when running quiet.
2. **Initial iteration**: `PromptGeneratorService.generatePrompt` builds CoT JSON prompts with `GEN_SYSTEM_PROMPT` and `buildInitialUserMessage`. Responses must be JSON containing `reasoning` and `prompt`; if parsing fails, raw text is passed through after logging a warning.
3. **Interactive mode**:
   - **TTY loop**: uses Enquirer to ask “Refine?” and prompt for instructions. Plugin integrations typically avoid this mode and prefer transports.
   - **Transport loop**: `--interactive-transport /tmp/pmc.sock` opens a Unix socket (or Windows pipe) that accepts newline-delimited JSON commands: `{"type":"refine","instruction":"..."}` or `{"type":"finish"}`. The CLI streams events back over the same socket (mirroring `--stream jsonl` output) so the plugin can update UI without parsing stdout.
4. **Refinements**: each instruction triggers a new iteration, passing the prior prompt, original intent, context, and the latest instruction into `buildRefinementMessage` (system prompt switches to `REFINE_SYSTEM_PROMPT`).
5. **Completion**: when the user stops refining, the CLI emits `interactive.state`=`complete` and returns the final prompt plus iteration count.
6. **Polish pass** (optional): `--polish` invokes `polishPrompt` with a fixed system instruction emphasizing formatting fidelity.

## 10. Streaming Events & IPC

- **Modes**: `--stream none|jsonl` controls whether events print to stdout. Even with `none`, taps can receive events (e.g., interactive transport attaches as a tap).
- **Schema** (see `StreamEvent` in `src/generate/types.ts`):
  - All stream events include `event` and `timestamp`.
  - `context.telemetry` → `{ telemetry:{ files:[{path,tokens}], intentTokens, fileTokens, systemTokens, totalTokens } }`
  - `context.overflow` → `{ strategy, before, after, droppedPaths:[{path, source}] }`
  - `resume.loaded` → `{ source:'history'|'file', reusedContextPaths:[{path,source}], missingContextPaths:[{path,source}] }`
  - `progress.update` → `{ label, state:'start|update|stop', scope?:'url|smart|generate|polish|generic' }`
  - `upload.state` → `{ state:'start|finish', detail:{kind:'image|video', filePath} }`
  - `generation.iteration.start` → `{ iteration, intent, model, interactive, inputTokens, refinements[], latestRefinement? }`
  - `generation.iteration.complete` → `{ iteration, prompt, tokens }`
  - `interactive.state` → `{ phase:'start|prompt|refine|complete', iteration }`
  - `interactive.awaiting` → `{ mode:'transport|tty|none' }`
  - `transport.listening|transport.client.connected|transport.client.disconnected`
  - `generation.final` → `{ result: GenerateJsonPayload }`
- **Transport lifecycle**: server cleans up Unix sockets on exit (`SIGINT`, `SIGTERM`, `process.exit`). Plugin must handle reconnects and `transport.error` messages.

## 11. Output Assembly & Delivery

- **GenerateJsonPayload**: includes `schemaVersion`, `intent`, `model` (generation model), `targetModel` (runtime model), `prompt`, `refinements`, `iterations`, `interactive`, `timestamp`, `contextPaths`, optional `outputPath`, `polishedPrompt`, `polishModel`, `contextTemplate`, `renderedPrompt`.
- **Context templates**: built-in `nvim` template injects a header and instructions before inserting `{{prompt}}`. User-defined templates live under `contextTemplates` in `config.json`. Plugin should let users select templates per run or default to `nvim` when targeting scratch buffers.
- **Clipboard/browser**: `--copy` uses `clipboardy`; `--open-chatgpt` opens `https://chatgpt.com/?q=...`. In headless editor environments these options should default to false unless explicitly enabled.
- **History logging**: every run (JSON payload) appends to `$HOME/.config/prompt-maker-cli/history.jsonl`. Plugins can tail this file to show recent prompts or rehydrate drafts.

## 12. Telemetry & Status Presentation

- **Token summaries**: highlight total, intent, and file tokens plus top 10 files with counts. When running `--quiet` the plugin should parse the `telemetry` object from the `context.telemetry` event and render it in NeoVim (virtual text, floating window, etc.).
- **Spinners**: `ora` spinners appear only when `--progress` is true and stdout is a TTY. Plugins using `--quiet --stream jsonl` get deterministic `progress.update` events instead of spinners.
- **Reasoning**: setting `DEBUG=1` or `VERBOSE=1` prints `reasoning` text to stderr after each LLM call. Plugins can capture stderr to display reasoning logs or route them to a diagnostics pane.

## 13. Error Handling & Edge Cases

- **Missing intent**: CLI throws `Intent text is required…`. Plugin should prompt the user to provide text before spawning the process.
- **Invalid path/URL**: warnings are emitted but runs continue. Capture stderr so the user sees skipped files.
- **Oversized/binary intent files**: fail fast with descriptive errors (size in KB, binary detection). Provide guardrails in the plugin UI (file picker + size hint).
- **Credential errors**: `ensureModelCredentials` throws if API keys are absent—surface these immediately with actionable messaging (e.g., open plugin settings).
- **Schema mismatch (history exports)**: `prompt-maker-cli export --from-history ...` validates `schemaVersion` and fails if the selected entry is unsupported. Surface the error and suggest upgrading/downgrading prompt-maker-cli or regenerating/exporting with a matching version (`src/history/generate-history.ts`).
- **Transport disconnects**: CLI emits `transport.client.disconnected` and drains pending refinement promises; the plugin should reconnect or end the session gracefully.
- **Upload failures**: warnings identify the file and root cause. Consider showing a quickfix entry pointing at the asset path.
- **Smart context indexing failure**: CLI logs warnings but continues. Plugin may retry with reduced scope or inform the user to run without smart context.

## 14. Integration Playbooks

1. **One-shot prompt into buffer**
   - Command: `prompt-maker-cli <intent> --json --quiet --context-template nvim --context ...`
   - Steps: run job → capture stdout JSON → insert `renderedPrompt` (fallback to `polishedPrompt`/`prompt`).
2. **Interactive refinement split**
   - Start CLI with `--quiet --stream jsonl --interactive-transport /tmp/pmc.sock` inside a background job.
   - Plugin tails stdout for JSONL events, displays prompts in a buffer, and sends `refine`/`finish` commands over the socket based on user input.
3. **Context preview workflow**
   - Use `--context-file /tmp/pmc-context.json --context-format json --show-context`.
   - Plugin parses the JSON for an expandable tree view and optionally caches context for auditing.
4. **History-driven picker**
   - Tail `$HOME/.config/prompt-maker-cli/history.jsonl` (or periodically parse) to feed a Telescope picker listing recent intents/prompts.
   - Selecting an entry can repopulate buffers or re-run with `--intent-file` referencing saved drafts.
5. **Media-assisted runs**
   - Validate selected images/videos before invoking CLI (check extension + size).
   - Display upload progress based on `upload.state` events and prevent user from interrupting until finish.
6. **Automated guardrails**
   - Monitor `telemetry.totalTokens` from the `context.telemetry` event; if above a threshold (e.g., 30k), warn the user or prompt to trim context.
   - Set budgets proactively: start with `--max-context-tokens` (for predictable trimming), and add `--max-input-tokens` once you also account for system/intent overhead.
   - Prefer `--context-overflow drop-smart` when using `--smart-context` so auto-attached files are dropped before user-selected context.
   - Budgets apply only to text context entries (file/url/smart); images/videos are not trimmed by these strategies.
   - React to `context.overflow` by showing a notice that lists `droppedPaths` (and consider offering a one-click re-run with a larger budget).
   - Enforce linear command queueing when multiple runs share the same socket path to avoid collisions.

## 15. Appendix

### A. High-value Flags

| Flag                                       | Purpose                                                                                  |
| ------------------------------------------ | ---------------------------------------------------------------------------------------- |
| `<intent>` / `--intent-file` / stdin       | Provide rough intent text.                                                               |
| `-c, --context <glob>`                     | Attach local files. Repeatable.                                                          |
| `--url <https://…>`                        | Pull remote docs or GitHub repos.                                                        |
| `--image <path>` / `--video <path>`        | Inline media (image Base64, video uploads).                                              |
| `--smart-context` / `--smart-context-root` | Enable embedding-based file selection.                                                   |
| `-i, --interactive`                        | Enable TTY refinement loop.                                                              |
| `--interactive-transport <path>`           | Socket/pipe commands + streaming events.                                                 |
| `--stream jsonl`                           | Emit newline-delimited structured events.                                                |
| `--max-input-tokens <n>`                   | Cap total input tokens (intent + system + text context).                                 |
| `--max-context-tokens <n>`                 | Cap tokens reserved for text context entries.                                            |
| `--context-overflow <strategy>`            | Handle budget overflow: `fail`, `drop-smart`, `drop-url`, `drop-largest`, `drop-oldest`. |
| `--json`                                   | Print final payload for programmatic consumption.                                        |
| `--context-template <name>`                | Wrap final prompt (built-in `nvim`).                                                     |
| `--model <name>`                           | Generation model used by the CLI.                                                        |
| `--target <name>`                          | Runtime model the prompt is optimized for.                                               |
| `--copy`, `--open-chatgpt`                 | Clipboard/browser handoff.                                                               |
| `--polish`, `--polish-model`               | Post-generation refinement.                                                              |
| `--quiet`, `--no-progress`                 | Suppress UI spinners/banners.                                                            |

### B. Environment Variables & Config Keys

| Key                                  | Meaning                                    |
| ------------------------------------ | ------------------------------------------ |
| `OPENAI_API_KEY` / `OPENAI_BASE_URL` | OpenAI credentials (env overrides config). |
| `GEMINI_API_KEY` / `GEMINI_BASE_URL` | Gemini credentials + custom endpoints.     |
| `GITHUB_TOKEN`                       | Optional token for GitHub API rate limits. |
| `PROMPT_MAKER_GENERATE_MODEL`        | Default generation model override.         |
| `PROMPT_MAKER_POLISH_MODEL`          | Default polishing model.                   |
| `PROMPT_MAKER_CLI_CONFIG`            | Custom config file path.                   |
| `DEBUG` / `VERBOSE`                  | Enable reasoning logs to stderr.           |

Config file example (`~/.config/prompt-maker-cli/config.json`):

```json
{
  "openaiApiKey": "sk-…",
  "geminiApiKey": "gk-…",
  "promptGenerator": {
    "defaultModel": "gemini-1.5-flash",
    "defaultGeminiModel": "gemini-3-pro-preview"
  },
  "contextTemplates": {
    "nvim": "## NeoVim Prompt Buffer\n\n{{prompt}}",
    "scratch": "# Prompt Vault\n\n{{prompt}}"
  }
}
```

### C. File & Socket Locations

- CLI binary: global npm prefix (e.g., `~/.nvm/versions/node/v22.15.0/bin/prompt-maker-cli`).
- Config/history: `$HOME/.config/prompt-maker-cli/` (`config.json`, `history.jsonl`, `embeddings_cache.json`).
- Transport sockets: plugin chooses path (e.g., `/tmp/pmc.nvim.sock`). Clean up stale sockets before binding.
- Context exports: plugin-controlled temp dir (respect user preferences, clean up automatically).

### D. Event Handling Checklist

1. Always parse stdout as JSONL when `--stream jsonl` is active.
2. Ignore unknown `event` values (forward-compatible); treat them as informational unless your integration opts in.
3. Treat stderr as human-readable diagnostics (warnings, reasoning, spinner fallbacks).
4. Handle `context.overflow` by notifying the user and reconciling any UI “attached context” lists with `droppedPaths`.
5. Handle `transport.error` messages by notifying the user and prompting for corrected commands.
6. When the CLI exits, stop sending commands, close socket handles, and clean up temp files.

---

Use this guide as the single reference for coding agents and humans collaborating on the NeoVim plugin. The CLI already provides robust orchestration primitives; the plugin’s job is to present them elegantly inside NeoVim, protect users from misconfiguration, and surface rich telemetry in real time.
</file>

<file path="docs/pmc-tutorial.md">
# Comprehensive, Progressive Lesson Plan for Using prompt-maker-cli (PMC)

This tutorial is a progressive, lab-first curriculum that teaches a developer to use **prompt-maker-cli (PMC)** from beginner to advanced.

It’s grounded in the repository’s docs and source-of-truth behavior:

- `docs/cookbook.md`
- `docs/tui-styling-guide.md`
- `docs/prompt-maker-cli-tui-encyclopedia.md`
- `docs/neovim-plugin-integration.md`
- `src/prompt-generator/message-builders.ts`

> Note: there is also a legacy tutorial at `legacy docs/tutorial.md`. It contains older path references, but its command patterns are still broadly consistent with the modern CLI.

---

## Assumptions

- You are in the repo root (`prompt-maker/`) with Node.js **18+**.
- You have at least one provider configured:
  - OpenAI: `OPENAI_API_KEY`
  - Gemini: `GEMINI_API_KEY` (required for video)
- Optional: `GITHUB_TOKEN` for GitHub URL context (improves API rate limits).
- OS defaults: macOS or Linux. Windows works for most CLI features, but interactive transport uses Windows named pipes (see Lesson 8).

---

## Syllabus (Progressive Lessons)

1. **Setup & Mental Model** (Beginner)
2. **TUI Basics: Generate View + Command Palette** (Beginner)
3. **TUI Power Features: Context, Series, History, Tokens, Themes** (Intermediate)
4. **Generate Mode Fundamentals: Intent + Context + Output** (Beginner → Intermediate)
5. **Remote Context: URLs + GitHub URLs** (Intermediate)
6. **Smart Context (Local RAG / Embeddings)** (Intermediate)
7. **Media Attachments: Images + Video (Gemini)** (Intermediate)
8. **Refinement Workflows: Interactive TTY + Interactive Transport** (Advanced)
9. **Automation Outputs: `--json`, `--stream jsonl`, quiet/progress controls** (Advanced)
10. **Prompt Testing: `prompt-maker-cli test` + YAML suites** (Intermediate)
11. **NeoVim Integration Playbooks (including sidekick.nvim terminal workflows)** (Advanced)

Capstones:

- **Capstone A**: Non-interactive automation run → JSON → extract prompt
- **Capstone B**: JSONL streaming run → parse events
- **Capstone C**: NeoVim workflow → one-shot + interactive transport loop

---

## Shared Reference: Run Methods

Use whichever matches your situation:

### From source (recommended while working in this repo)

```bash
npm ci
npm run build

# TUI (default when no args)
npm start

# Generate
npm start -- "Draft a confident onboarding-bot spec" --model gpt-4o-mini

# Test runner
npm start -- test prompt-tests.yaml
```

### Installed CLI binary

```bash
# TUI
prompt-maker-cli

# Generate
prompt-maker-cli "Summarize src/tui/ in 5 bullets" --context "src/tui/**/*.ts*" --polish

# Tests
prompt-maker-cli test
```

---

# Lesson 1 — Setup & Mental Model (Beginner)

## Objectives

- Build and run PMC from this repo.
- Understand the three top-level modes: `ui`, `generate`, `test`.
- Configure credentials via env vars or config file.

## Prerequisites

- Node.js 18+
- Repo clone

## Concepts

- **Routing rules** (`docs/prompt-maker-cli-tui-encyclopedia.md`):
  - No args → `ui`
  - `ui` → TUI
  - `test` → prompt test runner
  - `generate` / `expand` → generate pipeline
  - Anything else (including flags like `--json`) → generate pipeline
- **Config resolution order** (`docs/prompt-maker-cli-tui-encyclopedia.md`, `docs/cookbook.md`):
  1. `PROMPT_MAKER_CLI_CONFIG=/path/to/config.json`
  2. `~/.config/prompt-maker-cli/config.json`
  3. `~/.prompt-maker-cli.json`
- **Credential precedence** (`docs/cookbook.md`, `docs/neovim-plugin-integration.md`):
  - Env vars override config keys:
    - `OPENAI_API_KEY` (optional `OPENAI_BASE_URL`)
    - `GEMINI_API_KEY` (optional `GEMINI_BASE_URL`)

## Walkthrough (Commands)

```bash
# 1) Install deps + build
npm ci
npm run build

# 2) Confirm the CLI runs (TUI)
npm start
```

If you want to use the `prompt-maker-cli` binary directly:

```bash
# From the repo root
npm install -g .

# Sanity check
prompt-maker-cli --help
```

## Lab

1. Start the TUI via `npm start`.
2. Exit cleanly with `Ctrl+C`.
3. Create a config file (choose one):
   - `~/.config/prompt-maker-cli/config.json` (recommended)
   - or set `PROMPT_MAKER_CLI_CONFIG` to point at a custom path.

Example config skeleton (do not commit secrets):

```json
{
  "openaiApiKey": "sk-...",
  "geminiApiKey": "gk-...",
  "promptGenerator": {
    "defaultModel": "gpt-4o-mini",
    "defaultGeminiModel": "gemini-3-pro-preview"
  },
  "contextTemplates": {
    "scratch": "# Scratch\n\n{{prompt}}"
  },
  "theme": "ocean",
  "themeMode": "system"
}
```

## Validation

- `npm start` launches an Ink UI (not a stack trace).
- `prompt-maker-cli` is on your `$PATH` if you installed globally.
- With credentials configured, later lessons won’t fail with “Missing OpenAI credentials” / “Missing Gemini credentials”.

## Troubleshooting

- **Ink raw mode error**: you’re not in a real TTY (e.g. piping output); use generate mode instead.
- **Credential errors**: set env vars or config keys (`openaiApiKey` / `geminiApiKey`).

## Video Script (Lesson 1)

- On screen: repo root → run `npm ci` → run `npm run build`.
- Show: `npm start` opens the TUI and exits with `Ctrl+C`.
- Narration beats:
  - “Three modes: `ui`, `generate`, `test`.”
  - “Config precedence and env var override.”
  - “We’re using repo build first; global install is optional.”

---

# Lesson 2 — TUI Basics: Generate View + Command Palette (Beginner)

## Objectives

- Navigate the TUI and use core global keys.
- Use the command palette to discover TUI commands.
- Generate a basic prompt from typed intent.

## Prerequisites

- Lesson 1

## Concepts

- TUI global keys (`docs/cookbook.md`, `docs/prompt-maker-cli-tui-encyclopedia.md`):
  - `Ctrl+G`: open command palette
  - `Ctrl+T`: switch to Test Runner view
  - `/help`: open help overlay (definitive keybind list)
  - `Esc`: dismiss popups
- The TUI is “keyboard-first” and follows strict input routing:
  1. Help overlay
  2. Popup input
  3. Screen input
  4. Global keys

## Walkthrough (Commands)

```bash
# Start the TUI
prompt-maker-cli
# (or: npm start)
```

In the UI:

1. Type `/help` to open help.
2. Press `Ctrl+G` to open the command palette.
3. Type `/` (or keep palette open) and browse commands.

## Lab

1. In the Generate view, type a short intent (example):
   - “Summarize this repository in 5 bullets.”
2. Press `Enter` to generate.
3. Open the command palette (`Ctrl+G`) and run:
   - `/settings`
   - `/history`

## Validation

- You can open help (`/help`) and the command palette (`Ctrl+G`).
- A prompt is generated and appears in the history pane.
- `/history` shows prior commands/runs.

## Troubleshooting

- **Keys don’t work**: a popup or help overlay likely owns input—press `Esc`.
- **Generation fails**: check credential setup (Lesson 1).

## Video Script (Lesson 2)

- Show `prompt-maker-cli` launch.
- Demo: `/help`, `Ctrl+G`, run `/settings`, run `/history`.
- Generate one intent and point out history updates.

---

# Lesson 3 — TUI Power Features: Context, Series, History, Tokens, Themes (Intermediate)

## Objectives

- Add context (files, URLs, images, video, smart context) via the TUI.
- Generate a **series** (atomic prompt plan) and find the saved files.
- Understand token telemetry and history in the UI.
- Change themes and theme mode, and understand where themes are loaded from.

## Prerequisites

- Lessons 1–2

## Concepts

### Adding context via TUI commands

From `docs/cookbook.md` and `docs/prompt-maker-cli-tui-encyclopedia.md`:

- `/file`: add file globs
- `/url`: add URL context
- `/image`: attach image paths
- `/video`: attach video paths (forces Gemini at generation time)
- `/smart`: toggle smart context
- `/smart-root`: set/clear smart-context root

### Series generation and its constraints

- TUI generates a “series” and writes markdown files under:
  - `generated/series/<timestamped-folder>/`
  - Includes `00-overview.md` plus one file per atomic prompt (`docs/cookbook.md`).
- Under the hood, series generation asks the model to return **strict JSON** matching:

```json
{
  "reasoning": "...",
  "overviewPrompt": "...",
  "atomicPrompts": [{ "title": "...", "content": "..." }]
}
```

(See `src/prompt-generator/message-builders.ts`.)

- Series and regular prompt generation share a constraint: **the model must not execute tasks**, only craft instruction artifacts.

### Themes and theme discovery

From `docs/prompt-maker-cli-tui-encyclopedia.md` and `docs/tui-styling-guide.md`:

- Built-in themes live under `src/tui/theme/builtins/`.
- Custom themes are discovered from:
  - `~/.config/prompt-maker-cli/themes/*.json`
  - Project: `.prompt-maker-cli/themes/*.json` (searches ancestor directories)
- Theme selection persists back to your CLI config as `theme` and `themeMode`.

## Walkthrough (Commands)

```bash
prompt-maker-cli
```

In the UI:

1. Add file context: open palette → `/file` → enter `docs/*.md`.
2. Add URL context: `/url` → paste `https://github.com/<owner>/<repo>` (any repo URL is fine).
3. Open token breakdown popup: `/tokens`.
4. Create a series:
   - type an intent, then press `Tab`, or
   - run `/series`.
5. Change theme:
   - `/theme` → choose a theme
   - `/theme-mode` → pick `dark`, `light`, or `system`

## Lab

1. Run `/series` for an intent like:
   - “Create an implementation plan to add a new command to the TUI.”
2. Find the generated folder:
   - `generated/series/<timestamped-folder>/`
3. Open `/tokens` after a run and note:
   - total tokens
   - which files contributed the most tokens
4. (Optional) Install a custom theme by copying the example theme:

```bash
mkdir -p .prompt-maker-cli/themes
cp src/tui/theme/examples/ocean-example.json .prompt-maker-cli/themes/ocean.json
```

Restart the TUI and select `ocean` via `/theme`.

## Validation

- A new folder exists under `generated/series/` after `/series`.
- `/tokens` shows a breakdown (even if values differ).
- Theme selection persists across TUI restarts.

## Troubleshooting

- **Series folder not written**: filesystem permissions issue; the TUI should still generate but will report write errors.
- **Theme not found**: confirm theme JSON is in one of the discovery directories and restart the TUI.

## Video Script (Lesson 3)

- Demo: `/file` with `docs/*.md` and `/tokens`.
- Demo: `/series` → show `generated/series/...` folder.
- Demo: copy `ocean-example.json` into `.prompt-maker-cli/themes/` and select it.

---

# Lesson 4 — Generate Mode Fundamentals: Intent + Context + Output (Beginner → Intermediate)

## Objectives

- Use generate mode via `prompt-maker-cli "..."` and `prompt-maker-cli generate ...`.
- Provide intent via positional arg, `--intent-file`, or stdin.
- Attach file context with `--context` globs (including `!` excludes).
- Use `--show-context` / `--context-file` / `--context-format`.

## Prerequisites

- Lesson 1

## Concepts

### Intent ingestion methods

From `docs/prompt-maker-cli-tui-encyclopedia.md`:

- Positional intent: `prompt-maker-cli "..."`
- Intent file: `--intent-file path/to/intent.md` (alias `-f`)
- stdin: pipe into the command

Rule of thumb:

- Provide intent exactly one way (avoid ambiguity).

### Local context ingestion

From `docs/prompt-maker-cli-tui-encyclopedia.md` and `docs/cookbook.md`:

- `-c, --context <glob>` is repeatable.
- Globs use `fast-glob` and support excludes via `!` prefixes.

### Context preview/export

- `--show-context` prints resolved context before generation.
- `--context-file <path>` writes resolved context to a file.
- `--context-format text|json` controls rendering.

## Walkthrough (Commands)

```bash
# Positional intent
prompt-maker-cli "Summarize the TUI architecture in 5 bullets" \
  --context "docs/prompt-maker-cli-tui-encyclopedia.md"

# Intent from file (keep it small; intent files are validated and size-capped)
cat > /tmp/pmc-intent.md <<'EOF'
Summarize the key PMC run modes and how routing works.
EOF

prompt-maker-cli --intent-file /tmp/pmc-intent.md \
  --context "docs/prompt-maker-cli-tui-encyclopedia.md" \
  --show-context

# Intent from stdin
printf "%s" "List key CLI flags and what they do" | prompt-maker-cli \
  --context docs/prompt-maker-cli-tui-encyclopedia.md
```

## Lab

1. Run a context preview export:

```bash
prompt-maker-cli "Explain PMC context ingestion" \
  --context "docs/prompt-maker-cli-tui-encyclopedia.md" \
  --context-file /tmp/pmc-context.json \
  --context-format json
```

2. Inspect that file and confirm it’s structured JSON.

## Validation

- `--show-context` prints `<file ...>` blocks (text mode).
- `/tmp/pmc-context.json` is a JSON array (json mode).

## Troubleshooting

- **No files matched**: your glob didn’t match any files; PMC warns and continues.
- **Unreadable files**: PMC warns and skips those files.

## Video Script (Lesson 4)

- Show 3 intent ingestion methods (positional, `--intent-file`, stdin).
- Show `--context` excludes using `!`.
- Show `--context-file --context-format json` and open the output.

---

# Lesson 5 — Remote Context: URLs + GitHub URLs (Intermediate)

## Objectives

- Attach remote docs using `--url`.
- Use GitHub URLs (`blob`, `tree`, repo root) as context.
- Understand size/file limits and authentication behaviors.

## Prerequisites

- Lesson 4

## Concepts

From `docs/prompt-maker-cli-tui-encyclopedia.md` and `docs/neovim-plugin-integration.md`:

- `--url` only accepts `http:` / `https:`.
- HTML pages are downloaded (≤ 1MB) and converted to text.
- GitHub URLs are special-cased:
  - Repo root: `https://github.com/<owner>/<repo>`
  - Tree: `https://github.com/<owner>/<repo>/tree/<ref>/<path?>`
  - Blob: `https://github.com/<owner>/<repo>/blob/<ref>/<path>`
- GitHub safety limits:
  - max files: 60
  - max size per file: 64KB
  - common large/binary paths skipped (`node_modules`, `dist`, lockfiles, archives, etc.)
- Optional auth: set `GITHUB_TOKEN` to reduce rate-limit pain.

## Walkthrough (Commands)

```bash
# Basic web URL context
prompt-maker-cli "Summarize this page into prompt instructions" \
  --url https://example.com

# GitHub repo root as context
prompt-maker-cli "Summarize this repo" \
  --url https://github.com/folke/sidekick.nvim

# GitHub tree path as context
prompt-maker-cli "Summarize the docs folder" \
  --url https://github.com/folke/sidekick.nvim/tree/main/doc
```

## Lab

1. Run a GitHub URL ingest and confirm it keeps running even if some files are skipped.
2. Repeat with `GITHUB_TOKEN` set (if you have one) and observe improved reliability under rate limits.

## Validation

- Output includes content that clearly came from the URL.
- If any URL fails, PMC emits warnings but continues.

## Troubleshooting

- **HTTP errors**: confirm URL is reachable and uses http(s).
- **GitHub rate limiting**: set `GITHUB_TOKEN`.

## Video Script (Lesson 5)

- Demo `--url` against a normal page and a GitHub repo tree.
- Point out safety limits and why PMC skips heavy paths.

---

# Lesson 6 — Smart Context (Local RAG / Embeddings) (Intermediate)

## Objectives

- Turn on smart context with `--smart-context`.
- Scope scanning with `--smart-context-root`.
- Understand what gets indexed, where it’s cached, and common failure modes.

## Prerequisites

- Lesson 4

## Concepts

From `docs/prompt-maker-cli-tui-encyclopedia.md` and `docs/neovim-plugin-integration.md`:

- Smart context:
  - scans local files using `fast-glob` patterns like `**/*.{ts,tsx,js,jsx,py,md,json}`
  - skips `node_modules`, `dist`, `.git`, lockfiles, etc.
  - skips files > 25KB
  - caches embeddings in `~/.config/prompt-maker-cli/embeddings_cache.json`
  - searches top-k results (k=5) based on the intent

## Walkthrough (Commands)

```bash
# Smart context with default scan root (CWD)
prompt-maker-cli "Explain how the TUI theme system works" \
  --smart-context

# Smart context with an explicit root
prompt-maker-cli "Explain how token telemetry is computed" \
  --smart-context \
  --smart-context-root src
```

## Lab

1. Run smart context once.
2. Confirm the cache file exists:

```bash
ls -la ~/.config/prompt-maker-cli/embeddings_cache.json
```

3. Run again with the same intent and root; it should be faster due to caching.

## Validation

- `~/.config/prompt-maker-cli/embeddings_cache.json` exists after the first run.
- Output references local files you didn’t explicitly pass via `--context`.

## Troubleshooting

- **Indexing errors**: PMC logs warnings but continues; reduce scope with `--smart-context-root`.
- **Unexpected file choices**: tighten your intent (embeddings search is intent-driven).

## Video Script (Lesson 6)

- Run one command without smart context, one with smart context.
- Show embeddings cache file creation.
- Explain when to use `--smart-context-root`.

---

# Lesson 7 — Media Attachments: Images + Video (Gemini) (Intermediate)

## Objectives

- Attach images via `--image` and understand supported formats and size caps.
- Attach video via `--video` and understand Gemini-only requirements.
- Understand model auto-switch behavior when `--video` is present.

## Prerequisites

- Lesson 4
- Gemini credentials for video: `GEMINI_API_KEY`

## Concepts

From `docs/prompt-maker-cli-tui-encyclopedia.md` and `docs/neovim-plugin-integration.md`:

- Images (`--image`):
  - supported: PNG/JPG/JPEG/WEBP/GIF
  - max 20MB each
  - unsupported/oversize → warning and skipped
- Video (`--video`):
  - requires Gemini
  - uploads via Google Files API and polls until `ACTIVE`
  - if you selected a non-Gemini model, PMC auto-switches to a Gemini video-capable model

## Walkthrough (Commands)

```bash
# Image attach (repeatable)
prompt-maker-cli "Critique this UI for accessibility" \
  --image path/to/screenshot.png \
  --polish

# Video attach (Gemini required; model may auto-switch)
prompt-maker-cli "Analyze this demo recording and list usability issues" \
  --video path/to/demo.mp4 \
  --progress=false
```

## Lab

1. Run an image-based prompt generation.
2. Run a video-based prompt generation and observe:
   - an upload phase
   - `--progress=false` reduces spinner noise (or use `--no-progress`)

## Validation

- For image: the run succeeds without warnings about unsupported extensions.
- For video: the run only succeeds when Gemini credentials are configured.

## Troubleshooting

- **Image skipped**: verify extension and file size.
- **Video fails immediately**: missing `GEMINI_API_KEY` or unreadable file.
- **Upload appears stuck**: keep the process alive; uploads poll until `ACTIVE`.

## Video Script (Lesson 7)

- Show one run with `--image` and one run with `--video`.
- Call out: “Video forces Gemini; PMC may auto-switch the model.”

---

# Lesson 8 — Refinement Workflows: Interactive TTY + Interactive Transport (Advanced)

## Objectives

- Use `--interactive` for a TTY-based refinement loop.
- Use `--interactive-transport` for programmatic refinement (editor/plugin workflows).
- Understand what commands transport accepts and what events you can observe.

## Prerequisites

- Lesson 4

## Concepts

### Interactive TTY mode

From `docs/prompt-maker-cli-tui-encyclopedia.md`:

- `--interactive` enables a refinement loop **only when a TTY is available**.
- If no TTY is detected, PMC warns and proceeds non-interactively.

### Interactive transport mode

From `docs/prompt-maker-cli-tui-encyclopedia.md` and `docs/neovim-plugin-integration.md`:

- `--interactive-transport <path>` opens a local server:
  - Unix socket path like `/tmp/pmc.sock` (macOS/Linux)
  - Windows named pipe path must start with `\\.\pipe\...`
- Client sends **newline-delimited JSON** commands:
  - `{"type":"refine","instruction":"..."}`
  - `{"type":"finish"}`
- Transport also receives JSONL events emitted by the pipeline.

### Hard constraints

- `--json` cannot be combined with `--interactive` or `--interactive-transport`.

## Walkthrough (Commands)

### A) Interactive TTY

```bash
prompt-maker-cli "Draft a code review prompt for this repo" \
  --context "docs/prompt-maker-cli-tui-encyclopedia.md" \
  --interactive
```

Follow the prompts to refine; finish by choosing not to refine.

### B) Interactive transport (two terminals)

Terminal A (start PMC):

```bash
prompt-maker-cli "Draft a refactor plan for the token telemetry system" \
  --context "src/generate/token-telemetry.ts" \
  --stream jsonl \
  --quiet \
  --interactive-transport /tmp/pmc.sock
```

Terminal B (send commands using Node.js):

```bash
node -e "
const net = require('node:net');
const socketPath = '/tmp/pmc.sock';
const c = net.createConnection(socketPath);
c.on('data', (d) => process.stdout.write(d));
c.on('connect', () => {
  c.write(JSON.stringify({ type: 'refine', instruction: 'Add a Validation section and keep steps testable.' }) + '\n');
  setTimeout(() => c.write(JSON.stringify({ type: 'finish' }) + '\n'), 500);
});
"
```

## Lab

1. Run the transport workflow above.
2. Observe the JSONL stream in Terminal A.
3. Confirm you see at least:
   - `interactive.awaiting` events
   - `generation.iteration.start` / `generation.iteration.complete`
   - `generation.final`

## Validation

- Refinement changes the output (you should see at least two iterations).
- The process exits after `finish`.

## Troubleshooting

- **Socket path already exists**: remove the stale file and retry.
- **No `interactive.awaiting`**: ensure you started with `--interactive-transport`.
- **Mixed output**: use `--quiet` so stdout is JSONL-only (plus any final prompt text depending on mode).

## Video Script (Lesson 8)

- Split-screen recording (two terminals):
  - Left: PMC running with `--interactive-transport`.
  - Right: Node one-liner sending `refine` then `finish`.
- Narrate: “Transport is how editor plugins drive refinement loops.”

---

# Lesson 9 — Automation Outputs: `--json`, `--stream jsonl`, quiet/progress controls (Advanced)

## Objectives

- Produce machine-readable JSON output with `--json`.
- Stream JSONL events with `--stream jsonl`.
- Use `--quiet` and `--progress=false` / `--no-progress` to keep stdout parseable.
- Understand context templates and `renderedPrompt` fallback logic.
- Learn where history is stored and how to use it.

## Prerequisites

- Lesson 4

## Concepts

### JSON output

From `docs/prompt-maker-cli-tui-encyclopedia.md` and `docs/cookbook.md`:

- `--json` prints a pretty JSON payload to **stdout**.
- `--show-context` + `--json` prints context to **stderr** (to protect stdout JSON).
- `--json` cannot be combined with interactive modes.

### Streaming JSONL

From `docs/neovim-plugin-integration.md`:

- `--stream jsonl` writes newline-delimited JSON events to stdout.
- Use `--quiet` to suppress human-oriented output so stdout is parseable.

Event names you can rely on include (each JSONL line includes an `event` field plus event-specific payload fields):

- `context.telemetry`
- `progress.update`
- `upload.state`
- `generation.iteration.start`
- `generation.iteration.complete`
- `interactive.state`
- `interactive.awaiting`
- `transport.listening`, `transport.client.connected`, `transport.client.disconnected`
- `generation.final`

### `--quiet`, progress flags, and telemetry presentation

- `--quiet` suppresses banners/telemetry UI.
- Progress spinners are controlled via:
  - `--progress=false` (boolean form)
  - or `--no-progress` (CLI alias)

### Context templates and fallback fields

From `docs/neovim-plugin-integration.md`:

- `--context-template nvim` wraps output into an editor-friendly template.
- JSON payload may include:
  - `renderedPrompt` (if template applied)
  - `polishedPrompt` (if `--polish`)
  - otherwise `prompt`

Fallback logic for consumers:

1. Use `renderedPrompt` if present.
2. Else use `polishedPrompt` if present.
3. Else use `prompt`.

### History

- Every generate run appends JSONL to:
  - `~/.config/prompt-maker-cli/history.jsonl`

## Walkthrough (Commands)

### A) JSON output (recommended for scripting)

```bash
prompt-maker-cli "Summarize the PMC generate flags" \
  --context docs/prompt-maker-cli-tui-encyclopedia.md \
  --json > /tmp/pmc-run.json

# Extract the generated prompt
node -e "
const fs = require('node:fs');
const run = JSON.parse(fs.readFileSync('/tmp/pmc-run.json','utf8'));
console.log(run.renderedPrompt ?? run.polishedPrompt ?? run.prompt);
"
```

### B) JSONL stream output (recommended for live UIs)

```bash
prompt-maker-cli "Explain smart context and where it caches embeddings" \
  --context docs/neovim-plugin-integration.md \
  --stream jsonl \
  --quiet \
  --progress=false > /tmp/pmc-events.jsonl
```

## Lab

1. Produce JSON output and extract the `prompt` using the Node snippet above.
2. Produce JSONL stream output and confirm you have newline-delimited JSON objects.
3. Confirm history appended:

```bash
ls -la ~/.config/prompt-maker-cli/history.jsonl
```

## Validation

- `/tmp/pmc-run.json` parses as JSON and contains keys like `schemaVersion`, `intent`, `model`, `targetModel`, and `prompt`.
- `/tmp/pmc-events.jsonl` contains multiple JSON objects separated by newlines.
- `~/.config/prompt-maker-cli/history.jsonl` grows over time.

## Troubleshooting

- **Stdout not parseable**: add `--quiet` and disable progress (`--progress=false` or `--no-progress`).
- **Need both JSONL and final JSON**: be careful—combining `--stream jsonl` with `--json` mixes formats on stdout; prefer reading the final payload from `~/.config/prompt-maker-cli/history.jsonl` instead.

## Video Script (Lesson 9)

- Show `--json > run.json` and extract via `node -e`.
- Show `--stream jsonl --quiet` and open the JSONL file.
- Explain the `renderedPrompt → polishedPrompt → prompt` fallback order.

---

# Lesson 10 — Prompt Testing: `prompt-maker-cli test` + YAML suites (Intermediate)

## Objectives

- Run prompt tests with `prompt-maker-cli test`.
- Understand the YAML suite concept and how failures are reported.
- Run tests in the TUI Test Runner view.

## Prerequisites

- Lesson 1

## Concepts

From `docs/prompt-maker-cli-tui-encyclopedia.md` and `docs/cookbook.md`:

- CLI test runner:
  - `prompt-maker-cli test` uses `prompt-tests.yaml` by default.
  - `prompt-maker-cli test path/to/suite.yaml` runs a specific file.
- The suite is validated (Zod schema) and tests result in a non-zero exit code on failures.

## Walkthrough (Commands)

```bash
# Default suite
prompt-maker-cli test

# Explicit suite
prompt-maker-cli test prompt-tests.yaml
```

TUI:

- `Ctrl+T` → Test Runner view
- Select file (default `prompt-tests.yaml`) → Run

## Lab

1. Run `prompt-maker-cli test`.
2. Observe PASS/FAIL results and ensure exit code reflects failures.

```bash
prompt-maker-cli test
echo $?
```

## Validation

- Tests print PASS/FAIL per test.
- Exit code is non-zero if any test fails.

## Troubleshooting

- **`test --help` still runs tests**: known quirk (`docs/prompt-maker-cli-tui-encyclopedia.md`).
- **Credential errors**: tests still require provider access.

## Video Script (Lesson 10)

- Run `prompt-maker-cli test`.
- Show a failing test case (if present) and the exit code.
- Open the TUI test runner (`Ctrl+T`) and run the same suite.

---

# Lesson 11 — NeoVim Integration Playbooks (including sidekick.nvim) (Advanced)

## Objectives

- Use the docs-backed integration patterns for:
  - one-shot prompt into a buffer
  - interactive transport refinement loop
  - context preview workflow
  - history-driven picker concept
  - media-assisted runs
- Run PMC from inside NeoVim terminals.
- Use **sidekick.nvim** as a convenient “integrated AI CLI terminal” to host PMC sessions.

## Prerequisites

- Lessons 8–9
- NeoVim installed
- Optional: sidekick.nvim installed (see https://github.com/folke/sidekick.nvim)

## Concepts

From `docs/neovim-plugin-integration.md`:

- For editor automation, prefer:
  - `--json --quiet --stream jsonl` (for programmatic consumption)
  - `--interactive-transport` for refinement loops
  - `--context-template nvim` for scratch-buffer-friendly formatting
- Template fallback logic for buffer insertion:
  - `renderedPrompt` → `polishedPrompt` → `prompt`

From sidekick.nvim’s README:

- sidekick.nvim provides an integrated terminal wrapper for arbitrary AI CLIs.
- It can “select” or “toggle” a tool session and send text to it.

## Walkthrough (Commands)

### Playbook 1: One-shot prompt into a buffer (CLI-first)

```bash
prompt-maker-cli "Write a refactor prompt for this file" \
  --context src/index.ts \
  --context-template nvim \
  --json --quiet > /tmp/pmc.json
```

Then in NeoVim, paste the output you want (`renderedPrompt` preferred).

### Playbook 2: Interactive transport loop (CLI + editor)

Start PMC:

```bash
prompt-maker-cli "Draft a refactor plan for the active file" \
  --context src/index.ts \
  --quiet \
  --stream jsonl \
  --interactive-transport /tmp/pmc.nvim.sock
```

Send `refine` / `finish` commands from another process (e.g. via Lua plugin code or the Node one-liner from Lesson 8).

### Playbook 3: Context preview (editor preview windows)

```bash
prompt-maker-cli "Explain what this context contains" \
  --context "src/**/*.ts" \
  --context "!src/**/__tests__/**" \
  --context-file /tmp/pmc-context.json \
  --context-format json \
  --show-context
```

### Playbook 4: History-driven picker (concept)

- Read and parse:
  - `~/.config/prompt-maker-cli/history.jsonl`
- Show recent entries by `intent`, `timestamp`, and a snippet of `prompt`.

### Running the PMC TUI inside NeoVim

- In NeoVim, use a builtin terminal:

```vim
:terminal prompt-maker-cli ui
```

- For transport-enabled sessions:

```vim
:terminal prompt-maker-cli ui --interactive-transport /tmp/pmc.nvim.sock
```

### Using sidekick.nvim to host PMC sessions

sidekick.nvim’s “AI CLI Integration” is a general terminal wrapper; you can add a custom tool for PMC.

Example `lazy.nvim` snippet (adapted from sidekick.nvim README):

```lua
{
  "folke/sidekick.nvim",
  opts = {
    cli = {
      tools = {
        pmc = {
          cmd = { "prompt-maker-cli" },
        },
      },
    },
  },
  keys = {
    {
      "<leader>ap",
      function() require("sidekick.cli").toggle({ name = "pmc", focus = true }) end,
      desc = "Sidekick: PMC",
    },
  },
}
```

From NeoVim:

- `:Sidekick cli show name=pmc focus=true` (start or attach)
- Run the TUI or generate commands inside the sidekick terminal:
  - `prompt-maker-cli ui`
  - `prompt-maker-cli "..." --json --quiet --stream jsonl`

## Lab

1. Do a one-shot run from NeoVim that produces `--json --quiet` output.
2. Start a transport session and send a refine command.
3. If using sidekick.nvim:
   - start the `pmc` tool terminal
   - run `prompt-maker-cli ui` inside it

## Validation

- NeoVim terminal can run `prompt-maker-cli ui` without Ink raw-mode errors.
- Transport socket workflow works end-to-end (refine then finish).
- sidekick.nvim can open a terminal session and keep it accessible.

## Troubleshooting

- **Ink raw mode errors inside NeoVim**: your terminal buffer might not support raw mode (or you’re in a headless environment); use generate mode instead.
- **Socket conflicts**: ensure `/tmp/pmc.nvim.sock` is unique per session.

## Video Script (Lesson 11)

- Record NeoVim:
  - open `:terminal prompt-maker-cli ui`
  - show `--interactive-transport` variant
- If sidekick.nvim installed:
  - show `:Sidekick cli show name=pmc focus=true`
  - run a generate command in that terminal

---

# Capstone A — Automation Run: JSON Output → Extract Prompt (Advanced)

## Goal

Produce a non-interactive JSON artifact and programmatically extract the final prompt text.

## Steps

```bash
prompt-maker-cli "Create a release checklist for this repo" \
  --context README.md \
  --context docs/cookbook.md \
  --context-template nvim \
  --polish \
  --json --quiet > /tmp/release-checklist.json

node -e "
const fs = require('node:fs');
const payload = JSON.parse(fs.readFileSync('/tmp/release-checklist.json','utf8'));
const text = payload.renderedPrompt ?? payload.polishedPrompt ?? payload.prompt;
console.log(text);
"
```

## Validation

- `/tmp/release-checklist.json` parses as JSON.
- Output is the template-wrapped prompt (when `renderedPrompt` is present).

## Common failure modes

- Missing credentials.
- Accidentally using `--interactive` with `--json` (invalid combination).

---

# Capstone B — Streaming Run: JSONL Events → Parse & React (Advanced)

## Goal

Capture a JSONL event stream and parse key milestones like `context.telemetry` and `generation.final`.

## Steps

```bash
prompt-maker-cli "Summarize how the TUI theme system loads custom themes" \
  --context docs/tui-styling-guide.md \
  --stream jsonl \
  --quiet \
  --progress=false > /tmp/pmc-theme-events.jsonl

node -e "
const fs = require('node:fs');
const lines = fs.readFileSync('/tmp/pmc-theme-events.jsonl','utf8').trim().split(/\n+/);
for (const line of lines) {
  const evt = JSON.parse(line);

  if (evt.event === 'context.telemetry') {
    console.error('telemetry totalTokens=', evt.telemetry.totalTokens);
  }

  if (evt.event === 'generation.final') {
    const result = evt.result;
    console.log(result.renderedPrompt ?? result.polishedPrompt ?? result.prompt);
  }
}
"
```

## Validation

- The JSONL file contains multiple lines of JSON.
- You observe a `generation.final` event and print the final prompt.

## Common failure modes

- Missing `--quiet` causes non-JSON output to mix into stdout.

---

# Capstone C — NeoVim Scenario: One-shot + Interactive Transport Loop (Advanced)

## Goal

Inside NeoVim, run a one-shot prompt build for a file, then run an interactive transport refinement session.

## Steps

1. NeoVim one-shot:

```vim
:terminal prompt-maker-cli "Review this file and propose tests" --context % --context-template nvim --json --quiet
```

2. NeoVim transport session:

```vim
:terminal prompt-maker-cli "Draft a refactor plan for this file" --context % --quiet --stream jsonl --interactive-transport /tmp/pmc.nvim.sock
```

3. Send commands using a second terminal or a small Lua helper (conceptually the same as Lesson 8’s Node client):
   - send `{"type":"refine","instruction":"..."}`
   - then `{"type":"finish"}`

## Validation

- NeoVim terminal displays a prompt output (one-shot).
- Transport session performs at least one refinement iteration before finishing.

---

# Feature Coverage Checklist (Docs-backed)

| Feature / Capability                                                                                                                  | Where covered            |
| :------------------------------------------------------------------------------------------------------------------------------------ | :----------------------- |
| Install/build from source (`npm ci`, `npm run build`, `npm start`)                                                                    | Lessons 1–2              |
| Global install + binary usage (`prompt-maker-cli`)                                                                                    | Lesson 1                 |
| Three modes: `ui`, `generate` (default), `test`                                                                                       | Lesson 1                 |
| TUI basics: help, command palette, navigation                                                                                         | Lessons 2–3              |
| TUI commands: `/file`, `/url`, `/image`, `/video`, `/smart`, `/smart-root`, `/tokens`, `/history`, `/theme`, `/theme-mode`, `/series` | Lesson 3                 |
| Intent ingestion: positional, `--intent-file/-f`, stdin                                                                               | Lesson 4                 |
| Local context: `--context` globs + excludes (`!`)                                                                                     | Lesson 4                 |
| Context preview/export: `--show-context`, `--context-file`, `--context-format text\|json`                                             | Lessons 4, 11            |
| URL context (`--url`) + GitHub URLs                                                                                                   | Lesson 5                 |
| GitHub token usage (`GITHUB_TOKEN`)                                                                                                   | Lesson 5                 |
| Smart context: `--smart-context`, `--smart-context-root`, embeddings cache                                                            | Lesson 6                 |
| Media: `--image` constraints                                                                                                          | Lesson 7                 |
| Media: `--video` Gemini requirement + upload behavior + model auto-switch                                                             | Lesson 7                 |
| Output: `--json` and non-combinability with interactive modes                                                                         | Lessons 8–9, Capstone A  |
| Output: `--stream jsonl` events and parsing                                                                                           | Lesson 9, Capstone B     |
| `--quiet`, `--progress=false`, `--no-progress` (clean stdout)                                                                         | Lessons 7–9              |
| Context templates: `--context-template nvim`, `renderedPrompt` fallback                                                               | Lessons 9, 11            |
| Refinement: `--interactive` (TTY)                                                                                                     | Lesson 8                 |
| Refinement: `--interactive-transport` (IPC)                                                                                           | Lesson 8, Capstone C     |
| Polishing: `--polish`, `--polish-model`                                                                                               | Lessons 7, 9             |
| `--model` vs `--target` semantics (target not echoed in prompt text by default)                                                       | Lessons 9, 11            |
| History: `~/.config/prompt-maker-cli/history.jsonl`                                                                                   | Lesson 9, 11             |
| Token telemetry (`/tokens` and `context.telemetry` events)                                                                            | Lessons 3, 9, Capstone B |
| Prompt tests: `prompt-maker-cli test`, `prompt-tests.yaml` default                                                                    | Lesson 10                |
| Run tests in the TUI test runner view                                                                                                 | Lessons 2, 10            |
| NeoVim integration playbooks (one-shot, transport loop, context preview, history)                                                     | Lesson 11                |
| sidekick.nvim integration (run PMC in integrated terminal)                                                                            | Lesson 11                |

---

## Next steps

- If you want, extend this curriculum by creating a personal “prompt suite” in `prompt-tests.yaml` and running it via:
  - `prompt-maker-cli test`
  - TUI Test Runner (`Ctrl+T`)
</file>

<file path="docs/sidekick.nvim-prompt-maker-tui-integration.md">
# Running Prompt Maker CLI TUI inside Neovim via `sidekick.nvim` (Streaming + Transport)

## 1. Title + Purpose

This document is a **design + implementation hand-off guide** for building a Neovim integration that runs **Prompt Maker CLI’s Ink TUI** (`prompt-maker-cli ui`) inside a **sidekick.nvim-managed terminal**, while also using Prompt Maker’s **interactive transport** and **JSONL streaming events** for a richer UX outside the terminal.

The target result is a plugin/module in your Neovim repo that:

- Opens the Prompt Maker Ink TUI in a Sidekick terminal window.
- Creates (and owns) a per-session `--interactive-transport` socket/pipe.
- Connects a Neovim-side transport client to:
  - parse streaming JSONL events for progress/telemetry/results
  - send newline-delimited JSON commands (`refine` / `finish`) to drive refinement loops
- Surfaces structured status and the final prompt into buffers/windows (without scraping terminal output).

**Authoritative Prompt Maker references in this repo**:

- `docs/neovim-plugin-integration.md` (integration flags and workflow guidance)
- `docs/prompt-maker-cli-tui-encyclopedia.md` (TUI behavior + transport notes)
- `docs/tui-design.md` (TUI input routing invariants)
- `src/generate/types.ts` (exact JSONL event schema)
- `src/generate/interactive-transport.ts` (exact transport command schema + lifecycle)

**Relevant Sidekick references (upstream)**:

- `folke/sidekick.nvim/lua/sidekick/cli/terminal.lua` (terminal job lifecycle + keymaps)
- `folke/sidekick.nvim/lua/sidekick/cli/session/init.lua` (session backend model)
- `folke/sidekick.nvim/lua/sidekick/cli/watch.lua` (file watching for external changes)
- `folke/sidekick.nvim/lua/sidekick/cli/actions.lua` (how terminal keymaps dispatch actions)

---

## 2. Glossary

- **Prompt Maker / PMC**: `prompt-maker-cli`, the Node-based CLI.
- **Ink TUI**: Prompt Maker’s terminal UI (`prompt-maker-cli ui`).
- **Sidekick Terminal**: Neovim terminal window created/managed by sidekick.nvim.
- **JSONL stream**: newline-delimited JSON “events” emitted by Prompt Maker (`--stream jsonl`) or mirrored over the transport tap.
- **Interactive transport**: local IPC channel created by PMC server (`--interactive-transport <path>`) that accepts newline-delimited JSON commands and mirrors events.
- **Session**: one running instance of Prompt Maker + its transport connection.
- **Rendered prompt**: the final prompt string after applying a context template.

---

## 3. Requirements & Non-Goals

### Requirements

- Run **Ink TUI** inside Neovim using Sidekick’s terminal/session abstractions.
- Use **interactive transport** to avoid scraping the terminal buffer for state/results.
- Use **JSONL events** to drive Neovim UI updates.
- Must respect Prompt Maker constraints from `docs/neovim-plugin-integration.md`:
  - Prefer `--quiet --stream jsonl` for editor integrations.
  - Set token budgets (`--max-input-tokens`/`--max-context-tokens`) and handle `context.overflow` for predictable behavior with large contexts.
  - `--json` **cannot** be used with interactive (`--interactive` or `--interactive-transport`).
  - `--interactive-transport` accepts newline-delimited JSON commands `{ "type": "refine", "instruction": "..." }` and `{ "type": "finish" }`.
  - When extracting the final prompt, use fallback: `renderedPrompt` → `polishedPrompt` → `prompt`.
- Cross-platform support:
  - Unix socket path on macOS/Linux
  - Windows named pipes (`\\.\pipe\...`)
  - Cleanup of stale socket files before launch (non-Windows only).

### Non-Goals

- Reimplement Prompt Maker’s internal TUI UX in Lua.
- Scrape the Ink terminal UI for prompts/state.
- Make network calls from tests (use fixtures/stubs).
- Store provider API keys in plugin state; rely on environment variables or Prompt Maker’s config file (`~/.config/prompt-maker-cli/config.json`).

---

## 4. UX Overview (user stories + keymaps)

### Core user stories

1. **Open the TUI**
   - As a user, I run `:Sidekick` / keymap and Prompt Maker’s TUI opens in a Sidekick terminal split.

2. **See progress and telemetry outside the terminal**
   - As a user, I see current phase (URL fetch / smart context / generate / polish), token totals, and upload progress in Neovim notifications / statusline.

3. **Refine from Neovim**
   - As a user, I can send a refine instruction from a Neovim prompt (floating input), without focusing the terminal.

4. **Finish and insert**
   - As a user, I can press a key to send `finish`, then the plugin opens a scratch buffer containing the final prompt and optionally inserts it into my current buffer.

5. **Close cleanly**
   - As a user, closing the terminal detaches the transport client, stops timers, and deletes any transport socket file.

### Suggested keymaps (defaults)

These are **suggestions**; integrate with your repo’s conventions.

- `:Sidekick cli toggle name=pmc` (or a custom command) → open/focus the TUI terminal
- `<leader>pr` → “Refine…” (`vim.ui.input`), send transport `refine`
- `<leader>pf` → “Finish” (`finish`), focus the “Prompt Result” buffer
- `<leader>po` → open a “Prompt Maker Status” floating window
- `<leader>pc` → copy final prompt to clipboard (Neovim-side; do not rely on `--copy`)

---

## 5. Architecture (diagram + components)

### High-level diagram

```text
+-------------------+         +------------------------------+
|     Neovim        |         |   sidekick.nvim terminal      |
|                   |         |   (term=true job)             |
|  - Commands       |  open   |                              |
|  - Status UI      +-------->+  prompt-maker-cli ui ...      |
|  - Transport      |         |  (Ink raw-mode TUI)           |
|    client         |         +------------------------------+
|        ^          |
|        | JSONL    |
|        | events   |
|        |          |
|  connect/send     |
|   over socket/pipe|
+--------+----------+
         |
         v
+------------------------------+
| prompt-maker-cli transport   |
| server (--interactive-transport)
|  - accepts NDJSON commands   |
|  - writes JSONL events back  |
+------------------------------+
```

### Components you will implement

1. **Session manager**
   - Owns per-session identifiers, transport path selection, and cleanup.

2. **Sidekick launcher**
   - Starts/attaches a Sidekick terminal session running `prompt-maker-cli ui --interactive-transport <path>`.

3. **Transport client**
   - Connects to the transport server (socket/pipe), reads JSONL events, and sends NDJSON commands.

4. **Event router**
   - Parses JSON lines into Lua tables and dispatches handlers.

5. **Neovim UI surfaces**
   - Statusline integration hooks, notifications, optional floating window, and a scratch buffer for final output.

---

## 6. CLI Invocation & Modes (exact commands)

### Mode A (Primary): Ink TUI in Sidekick terminal + transport tap for events

Run **Ink TUI** in a terminal, but drive out-of-band UX using the transport:

```bash
prompt-maker-cli ui --interactive-transport <transport_path>
```

Notes:

- `ui` only parses `--interactive-transport` (see `docs/prompt-maker-cli-tui-encyclopedia.md`). Do not depend on `ui --help`.
- Token budgets (`--max-input-tokens`, `--max-context-tokens`, `--context-overflow`) are **generate-mode flags** and are not parsed by `ui`.
  - To use budgets with the TUI, set defaults via config under `promptGenerator` (or set them interactively with the TUI `/budgets` popup, which persists back to config):

    ```json
    {
      "promptGenerator": {
        "maxInputTokens": 12000,
        "maxContextTokens": 8000,
        "contextOverflowStrategy": "drop-smart"
      }
    }
    ```

- PR #15 TUI parity workflows (popup-only; no inline args parsing):
  - Help overlay includes a “Workflows” section (`src/tui/help-config.ts`).
  - Command palette includes:
    - `/budgets` (persists `promptGenerator.maxInputTokens`, `promptGenerator.maxContextTokens`, `promptGenerator.contextOverflowStrategy`)
    - `/resume` (persists `resumeMode`, `resumeSourceKind`)
    - `/export` (persists `exportFormat`, `exportOutDir`)
  - These defaults are written back into CLI config (`src/config.ts`), so Sidekick sessions can “learn” settings from prior TUI usage.

- The TUI requires a real TTY; sidekick.nvim’s terminal uses `term=true` (`folke/sidekick.nvim/lua/sidekick/cli/terminal.lua`).

### Mode B (Optional): Headless generate loop (no Ink), still inside Sidekick terminal

If you want a non-TUI fallback that still streams progress deterministically:

```bash
prompt-maker-cli "<intent>" \
  --quiet \
  --stream jsonl \
  --interactive-transport <transport_path> \
  --context-template nvim \
  --max-context-tokens 8000 \
  --context-overflow drop-smart
```

Important constraints from Prompt Maker:

- `--json` cannot be combined with interactive mode (`--interactive` or `--interactive-transport`) (see `docs/neovim-plugin-integration.md` and `docs/prompt-maker-cli-tui-encyclopedia.md`).
- Therefore, your plugin must treat `generation.final` (JSONL) as the “final payload” source.

### Environment and binary resolution

- Binary is `prompt-maker-cli` (see `README.md`).
- Credentials are loaded from env vars or config file; your plugin should not try to pass keys directly.

---

## 7. Streaming (JSONL) Event Handling

### Wire format (exact)

Prompt Maker stream events are JSON objects **one per line**:

- Always include:
  - `event` (string)
  - `timestamp` (ISO string)
- Plus event-specific payload fields.

Schema source: `src/generate/types.ts` and serialization in `src/generate/stream.ts`.

Example line:

```json
{
  "event": "progress.update",
  "timestamp": "2026-01-03T12:00:00.000Z",
  "label": "Generating...",
  "state": "start",
  "scope": "generate"
}
```

### Relevant event types and expected plugin behavior

Below is the minimal mapping you should implement; expand as needed.

#### `transport.listening`

Payload: `{ path: string }`

- Mark session transport server as ready.
- If the client isn’t connected yet, attempt `connect()` with retry/backoff.

#### `transport.client.connected` / `transport.client.disconnected`

Payload: `{ status: 'connected'|'disconnected' }`

- Update statusline state.
- On disconnect: disable refine/finish UI and stop reading loop.

#### `transport.error`

Not part of the typed stream union, but written by the transport server as:

```json
{ "event": "transport.error", "message": "..." }
```

- Surface with `vim.notify(..., vim.log.levels.ERROR)`.
- Consider showing a “last transport error” in your status window.

#### `progress.update`

Payload:

- `label: string`
- `state: 'start'|'update'|'stop'`
- `scope?: 'url'|'smart'|'generate'|'polish'|'generic'`

Plugin actions:

- Maintain a simple “current phase” state machine:
  - `start`: show spinner/indicator
  - `update`: update label
  - `stop`: clear spinner
- UI options:
  - Notifications on `start`/`stop` only
  - Statusline/virtual text updates on every `update`

#### `context.telemetry`

Payload: `{ telemetry: { totalTokens, intentTokens, fileTokens, systemTokens, files:[{path,tokens}...] } }`

Plugin actions:

- Persist latest telemetry in session state.
- If token counts exceed a threshold, show warning (see “Security & Safety”).
- Optional: render top-N file token contributors in a floating window.

#### `context.overflow`

Emitted when token budgets are enabled and the CLI drops one or more **text** context entries to satisfy the budget.

Payload includes:

- `strategy`: `fail | drop-smart | drop-url | drop-largest | drop-oldest`
- `before`: token telemetry before trimming
- `after`: token telemetry after trimming
- `droppedPaths`: `[{ path, source }]` describing removed context entries

Plugin actions:

- Notify the user that some context was dropped (this can materially change results).
- Reconcile any “attached context” UI with `droppedPaths`.
- Consider offering a one-click re-run with a larger budget or a different overflow strategy.

#### `resume.loaded`

Emitted when resuming from history or a payload file and the CLI attempts to reuse prior `contextPaths`.

Payload: `{ source:'history'|'file', reusedContextPaths:[{path,source}], missingContextPaths:[{path,source}] }`

Plugin actions:

- Surface missing file paths clearly.
- If you run with strict mode, treat missing `source:"file"` paths as fatal (the CLI will throw).

#### `upload.state`

Payload: `{ state: 'start'|'finish', detail: { kind: 'image'|'video', filePath: string } }`

Plugin actions:

- Indicate uploads in statusline; discourage closing session while uploads active.

#### `generation.iteration.start`

Payload includes: `{ iteration, intent, model, interactive, inputTokens, refinements, latestRefinement? }`

Plugin actions:

- Update “iteration N” status.
- If `latestRefinement` exists, append it to a “refinement log” buffer.

#### `generation.iteration.complete`

Payload includes: `{ iteration, prompt, tokens, reasoningTokens? }`

Plugin actions:

- Optionally show “prompt preview” (not the whole prompt by default; it can be huge).

#### `interactive.state`

Payload: `{ phase:'start'|'prompt'|'refine'|'complete', iteration:number }`

Plugin actions:

- Use `phase` to enable/disable refine/finish controls:
  - `start`/`prompt`: allow refine + finish
  - `refine`: disable input until next `prompt`
  - `complete`: finalization UI (open prompt buffer)

#### `interactive.awaiting`

Payload: `{ mode:'transport'|'tty'|'none' }`

Plugin actions:

- If mode is `transport`, show “awaiting refine/finish” indicator.

#### `generation.final`

Payload: `{ result: GenerateJsonPayload }`

Plugin actions:

- Extract the final prompt text using:
  1. `result.renderedPrompt` (preferred)
  2. else `result.polishedPrompt`
  3. else `result.prompt`
- Open a scratch buffer and populate it with the extracted text.
- Optionally:
  - set filetype `markdown`
  - set buffer name like `Prompt Maker: <timestamp>`
  - provide actions: copy, insert into current buffer, save to file

---

## 8. Interactive Transport Protocol (client/server lifecycle)

### Server behavior (Prompt Maker)

Prompt Maker creates a local `net.Server` on the given `--interactive-transport` path (see `src/generate/interactive-transport.ts`).

- On Unix: PMC deletes any existing socket file at that path before binding.
- On Windows: the path must start with `\\.\pipe\`.

Once a client connects:

- PMC reads newline-delimited JSON commands.
- PMC writes JSONL events back to the same socket.

### Client responsibilities (your plugin)

- Connect to the transport server and keep reading lines until disconnect.
- Provide a `send_command(cmd)` API that serializes JSON + appends `\n`.
- Reconnect strategy:
  - If you connect before the server is listening, retry for up to `N` seconds.
  - If you disconnect mid-session, stop retries unless the terminal process is still running.

### Command schema (exact)

Send **one JSON object per line**:

- Refine:

```json
{ "type": "refine", "instruction": "Make it shorter" }
```

- Finish:

```json
{ "type": "finish" }
```

Rules from server implementation:

- `refine.instruction` must be a non-empty string after trimming.
- Unknown payloads yield `transport.error` event.

### Lifecycle sequencing (recommended)

1. Compute transport path.
2. Launch Sidekick terminal running `prompt-maker-cli ui --interactive-transport <path>`.
3. Wait for `transport.listening` OR retry-connect until it succeeds.
4. When connected, start JSONL reader loop.
5. On Neovim exit, terminal close, or user “stop session”:
   - Close client socket.
   - Delete stale Unix socket file (best-effort).

---

## 9. Integration with sidekick.nvim (what to reuse, what to extend)

### What to reuse

- **Terminal lifecycle + window management**
  - Sidekick’s terminal wrapper (`folke/sidekick.nvim/lua/sidekick/cli/terminal.lua`) already:
    - creates a scratch terminal buffer/window
    - starts jobs with `term=true` (needed for Ink)
    - manages focus/hide/toggle
    - has a send queue (`Terminal:send`) and keymap wiring

- **Session model**
  - Sidekick’s session layer (`folke/sidekick.nvim/lua/sidekick/cli/session/init.lua`) gives you:
    - stable session IDs per tool+cwd
    - attach/detach semantics
    - mux backends (tmux/zellij) if users enable them

- **File watching (optional)**
  - If the TUI writes files (e.g., `/series` output folders), you can enable sidekick watcher (`folke/sidekick.nvim/lua/sidekick/cli/watch.lua`) to refresh buffers automatically.

### What to extend (recommended)

Sidekick’s terminal is not designed to parse job stdout when `term=true`. For this integration:

- Treat the terminal as **display + interactive input only**.
- Treat the transport socket as your **machine-readable event stream**.

Recommended extension points:

1. **Add a dedicated tool entry**
   - In the user config: `cli.tools.pmc = { cmd = { "prompt-maker-cli", "ui" } }`.

2. **Add a “PMC adapter” module** (in your repo)
   - Wrap `sidekick.cli` to start/attach a session with a **modified cmd** that injects `--interactive-transport <path>`.
   - Keep the rest of Sidekick untouched.

3. **Emit User events**
   - Sidekick uses `Util.emit(...)` internally (see session attach/detach); your integration can emit `User` autocmd events like:
     - `User SidekickPmcEvent` (with payload in `vim.g`/in module state)
     - or direct `vim.api.nvim_exec_autocmds("User", { pattern = "SidekickPmcEvent", data = ... })`

---

## 10. Neovim UI Plan (buffers, windows, statusline, notifications)

### Recommended UI surfaces

1. **Statusline component**
   - Expose `require("pmc").status()` returning a short string like:
     - `PMC: idle`
     - `PMC: generating (2)`
     - `PMC: awaiting refine`
     - `PMC: uploads 1`

2. **Notifications**
   - Use `vim.notify` sparingly:
     - on `transport.error`
     - on `generation.final`
     - on fatal disconnects

3. **Prompt result buffer**
   - Create a scratch buffer (unlisted or listed depending on your preference) containing the final prompt.
   - Provide buffer-local mappings:
     - `y` copy to clipboard
     - `p` paste/insert into original buffer
     - `q` close

4. **Telemetry window (optional)**
   - A floating window rendering token totals and top-N files.

### Interaction design rules

- Never block Neovim UI while waiting for transport events.
  - Use `vim.uv` + `vim.schedule` to apply UI updates.
- Avoid flooding with progress updates.
  - Debounce `progress.update` rendering.

---

## 11. Configuration Spec (Lua table schema + defaults)

Define a dedicated config table (separate from sidekick’s own config) so the integration can be used without forking sidekick.nvim.

```lua
---@class PmcSidekickConfig
---@field enabled boolean
---@field tool_name string           -- sidekick cli tool name (default: "pmc")
---@field bin string|string[]?       -- override cmd[1] (binary) or full cmd
---@field cwd fun():string?          -- resolve cwd for session
---@field env table<string,string|false>? -- extra env vars; false clears
---@field transport
---@field transport.dir fun():string -- unix socket dir selection
---@field transport.name fun(ctx:{pid:number,cwd:string}):string -- filename/pipe name
---@field transport.connect_timeout_ms number
---@field transport.retry_interval_ms number
---@field ui
---@field ui.open_result_on_final boolean
---@field ui.result_filetype string
---@field ui.notify_level integer
---@field ui.token_warn_threshold number
---@field ui.token_error_threshold number
---@field ui.progress_debounce_ms number
local defaults = {
  enabled = true,
  tool_name = "pmc",
  bin = nil,
  cwd = function() return vim.fn.getcwd(0) end,
  env = {},
  transport = {
    dir = function()
      -- Prefer a per-user runtime dir if available.
      -- stdpath('run') exists on newer Neovim; otherwise fall back to tempname().
      return (vim.fn.has("win32") == 1) and "" or (vim.fn.stdpath("run") or vim.loop.os_tmpdir())
    end,
    name = function(ctx)
      return ("pmc-nvim-%d-%s.sock"):format(ctx.pid, vim.fn.sha256(ctx.cwd):sub(1, 8))
    end,
    connect_timeout_ms = 8000,
    retry_interval_ms = 100,
  },
  ui = {
    open_result_on_final = true,
    result_filetype = "markdown",
    notify_level = vim.log.levels.INFO,
    token_warn_threshold = 30000,
    token_error_threshold = 60000,
    progress_debounce_ms = 50,
  },
}
```

Notes:

- For Windows, use a named pipe path:
  - `\\.\pipe\pmc-nvim-<pid>-<hash>`
- For Unix, compute a socket path under `stdpath('run')` or `os_tmpdir()`.
- Always avoid collisions by including `vim.fn.getpid()` and a cwd hash.

---

## 12. Implementation Plan (files/modules + pseudocode)

This plan assumes you implement this in your Neovim repo as a small plugin/module that depends on sidekick.nvim.

### Suggested module layout

- `lua/pmc_sidekick/init.lua`
  - `setup(opts)`
  - user commands + keymaps

- `lua/pmc_sidekick/session.lua`
  - create/start/stop a session
  - store state: terminal session id, transport path, telemetry, etc.

- `lua/pmc_sidekick/transport.lua`
  - connect/read/write
  - jsonl decoder

- `lua/pmc_sidekick/events.lua`
  - event dispatch table: `handlers[event](session, payload)`

- `lua/pmc_sidekick/ui.lua`
  - open result buffer
  - status window
  - notifications

- `lua/pmc_sidekick/util.lua`
  - path helpers
  - debounce
  - safe json decode

### Session state machine

Keep a single Lua table per running session:

```lua
---@class PmcSession
---@field id string
---@field cwd string
---@field transport_path string
---@field terminal any?              -- sidekick terminal/session handle
---@field client any?                -- uv pipe/tcp handle
---@field status {phase?:string, label?:string, iteration?:number, connected?:boolean}
---@field telemetry table?           -- from context.telemetry
---@field final_payload table?       -- from generation.final
---@field closed boolean
```

### Starting the TUI session

Key behavior: start Sidekick terminal with a cmd including `--interactive-transport`.

Pseudocode:

```lua
local Sidekick = require("sidekick.cli")
local Session = require("sidekick.cli.session")

function M.start()
  local cwd = config.cwd()
  local transport_path = Transport.make_path(cwd)

  -- Ensure the tool exists in sidekick config: cli.tools[tool_name]
  -- Then clone/override cmd to append args.
  local base_tool = require("sidekick.config").get_tool(config.tool_name)
  local tool = base_tool:clone({
    cmd = { "prompt-maker-cli", "ui", "--interactive-transport", transport_path },
    env = config.env,
  })

  local session = Session.new({ tool = tool, cwd = cwd, backend = "terminal" })
  require("sidekick.cli.session").attach(session)

  -- Now connect transport client.
  Transport.connect(session, transport_path)
end
```

Implementation detail:

- Sidekick’s `Session.new` normalizes cwd and manages IDs (`folke/sidekick.nvim/lua/sidekick/cli/session/init.lua`).
- Sidekick’s terminal backend uses `jobstart(..., { term = true })` to provide a real TTY (`folke/sidekick.nvim/lua/sidekick/cli/terminal.lua`).

### Transport client: reading JSONL

Use `vim.uv`:

- On Unix: `uv.new_pipe(false)` and `pipe:connect(path, cb)`
- On Windows named pipe: `uv.new_pipe(true)` is typically required; validate with Neovim’s libuv.

Pseudocode:

```lua
local function decode_lines(chunk, state)
  state.buf = state.buf .. chunk
  while true do
    local i = state.buf:find("\n", 1, true)
    if not i then break end
    local line = vim.trim(state.buf:sub(1, i - 1))
    state.buf = state.buf:sub(i + 1)
    if line ~= "" then
      local ok, obj = pcall(vim.json.decode, line)
      if ok and type(obj) == "table" then
        Events.dispatch(session, obj)
      else
        -- optionally log parse failure
      end
    end
  end
end

function Transport.connect(session, path)
  -- retry connect until timeout
  -- once connected:
  client:read_start(function(err, chunk)
    if err then return Transport.on_error(session, err) end
    if not chunk then return Transport.on_eof(session) end
    decode_lines(chunk, session._decoder)
  end)
end

function Transport.send(session, obj)
  session.client:write(vim.json.encode(obj) .. "\n")
end
```

### Sending refine/finish from Neovim UI

- `refine`: prompt user via `vim.ui.input`, send `{ type="refine", instruction=input }`
- `finish`: send `{ type="finish" }`

Guardrails:

- Only allow if `session.status.connected == true`.
- Debounce refine: if `interactive.state.phase == "refine"`, block additional refine sends until next `prompt`.

### Extracting the final prompt

From `generation.final` event payload (`src/generate/types.ts`):

```lua
local result = event.result
local final = result.renderedPrompt or result.polishedPrompt or result.prompt
```

Create a buffer and store the full `result` as session.final_payload.

---

## 13. Testing Plan

Align with sidekick.nvim conventions (mini.test, fixtures, no network):

### Unit tests

1. **JSONL decoder**
   - Given arbitrary chunk boundaries, ensure the line splitter:
     - emits full decoded objects
     - tolerates empty lines
     - tolerates trailing partial lines

2. **Event mapping**
   - Feed sample events for each type and assert:
     - status transitions
     - telemetry storage
     - final prompt extraction

3. **Transport command serialization**
   - Ensure `refine`/`finish` commands are encoded as NDJSON lines.

### Integration tests (headless Neovim)

- Use `nvim --headless` and mini.test style similar to sidekick.nvim.
- Avoid running `prompt-maker-cli` in CI.
  - Instead, spawn a tiny Lua “fake server” using `vim.uv` that:
    - accepts a connection
    - sends JSONL events
    - asserts it received expected `refine`/`finish`

Fixtures

- Provide JSONL fixture files for:
  - a full successful run including `generation.final`
  - transport error line
  - token telemetry overflow case

---

## 14. Edge Cases & Failure Modes

- **Transport path collision (Unix)**
  - If the socket file exists from a previous crash, the server may fail to bind.
  - Best practice: delete stale socket file before launching.

- **Client connects too early**
  - Implement retry loop up to `connect_timeout_ms`.

- **Terminal closed while transport connected**
  - Detect terminal `TermClose` via sidekick terminal lifecycle and stop transport client.

- **Huge `generation.iteration.complete.prompt`**
  - Do not render full prompt in notifications.
  - Keep it in memory only; render on demand.

- **Uploads in progress**
  - Use `upload.state` counters; warn before stopping session if uploads are active.

- **Interactive mode mismatch**
  - If the TUI is used normally, it may not be “awaiting transport” at all times.
  - Only enable refine/finish UI when you observe `interactive.awaiting.mode == "transport"`.

---

## 15. Security/Privacy Considerations

- Never persist API keys; do not ask users to put secrets in config committed to git.
- Do not auto-read unbounded files.
  - If your plugin provides “attach buffer/file” helpers, enforce size caps (Prompt Maker’s own caps are described in `docs/neovim-plugin-integration.md`).
- Treat prompt output as potentially sensitive:
  - default to scratch buffers, not file writes
  - provide explicit “save to file” command
- Token telemetry thresholds:
  - If `telemetry.totalTokens` exceeds `token_warn_threshold`, show a warning.
  - If it exceeds `token_error_threshold`, consider prompting user to trim context.
  - If you observe `context.overflow`, show which files were dropped (`droppedPaths`) and consider offering a re-run with a higher budget.

---

## 16. Roadmap (MVP vs Phase 2)

### MVP

- Launch `prompt-maker-cli ui --interactive-transport ...` in Sidekick terminal.
- Connect transport client and parse JSONL events.
- Implement refine/finish commands from Neovim.
- Extract `generation.final.result` and open a result buffer.
- Basic statusline string + notifications.

### Phase 2

- Telemetry window with top-N token files and quickfix population.
- Rich progress UI (floating progress, virtual text in current buffer).
- History picker (tail `~/.config/prompt-maker-cli/history.jsonl`) using Sidekick pickers.
- Multi-session support + session picker.
- “Insert prompt at cursor” with preview + confirm.

---

## 17. Open Questions

1. In Prompt Maker’s **Ink TUI** flow, is `interactive.awaiting.mode` reliably `transport` when `--interactive-transport` is passed, or only during refinement loops?
2. Does the TUI always emit `generation.final` over the transport tap, or only in generate-mode interactive sessions?
3. Should the integration prefer launching **generate mode** (non-Ink) for deterministic transport-driven refinement, with Ink TUI offered as an optional “monitor UI”?
4. How should the plugin resolve a “default context template” when using the TUI (since `ui` doesn’t accept `--context-template`)? Rely on Prompt Maker config, or post-wrap in Neovim?
5. For Windows: what is the most reliable libuv pipe connect mode in Neovim (named pipe quirks vary)?
6. Should we expose “attach context from selection/buffer” via Sidekick’s context system, or keep the first version transport-only?
</file>

<file path="docs/tui-styling-guide.md">
# TUI Styling Guide (Ink) — prompt-maker

This guide teaches **Ink styling fundamentals** and then explains **exactly how styling and theming work in this repo**, with **screen-by-screen** and **component-by-component** pointers to the files you should change.

It is grounded in these repo docs and code:

- UX goals + input routing invariants: `docs/tui-design.md`
- Architecture + performance notes: `src/tui/DEVELOPER_NOTE.md`
- Boot sequence + module map: `docs/prompt-maker-cli-tui-encyclopedia.md`
- Worked example popup: `src/tui/components/popups/InstructionsPopup.tsx`

> Note on paths: some older docs may reference `apps/prompt-maker-cli/...`, but this repo’s authoritative TUI implementation is under `src/tui/**`.

---

## 1) Overview

You’ll learn:

1. **Ink basics**: how `<Box>` and `<Text>` render in a terminal, and which props matter.
2. **This repo’s theme system**: where theme tokens live, how a theme JSON becomes Ink props, and how theme selection is persisted.
3. **A styling map** of:
   - Screens (`Generate` and `Test Runner`)
   - Shared layout components (`core/*`)
   - Popup components (`popups/*`)
4. Practical recipes:
   - Change popup background/border
   - Change accent vs muted text
   - Make a layout responsive to terminal width
   - Add a new theme token (slot)
   - Add a new theme JSON

Constraints / philosophy (mirrors repo intent):

- Prefer **theme tokens** over hard-coded colors.
- Keep **input routing invariants** intact (help overlay and popups must “own” keyboard input).
- Avoid costly work in render paths.

---

## 2) Ink Basics (Beginner Tutorial)

### 2.1 What Ink is

Ink is **React for command-line interfaces**:

- You write React components (`function Component() { return <Box>...</Box> }`).
- Ink renders them to the terminal using a layout engine.
- Keyboard input is handled via Ink hooks (not browser events).

In this repo, the TUI mounts from `src/tui/index.tsx` via Ink’s `render()`.

### 2.2 The two primitives you’ll style 90% of the time

#### `<Text>`

`<Text>` prints text to the terminal.

Common styling props:

- `color`: foreground color (e.g. `"#ff0000"` or `"cyan"` depending on terminal)
- `backgroundColor`: background fill behind the text
- `bold`, `italic`, `underline`
- `wrap`: how to handle long lines (`"wrap"`, `"truncate"`, etc.)
- `inverse`: swaps foreground/background for that cell (used for cursors and selections)

Example:

```tsx
import { Text } from 'ink'

export const Example = () => (
  <Text color="#58a6ff" bold>
    Accent headline
  </Text>
)
```

#### `<Box>`

`<Box>` is Ink’s layout container.

Key layout props:

- `flexDirection`: `"row"` or `"column"` (most screens use column)
- `width`, `height`: numbers or strings like `"100%"`
- `flexGrow`, `flexShrink`
- `justifyContent`, `alignItems`
- `paddingX`, `paddingY`, `marginTop`, etc.
- `borderStyle`: `"round"`, `"single"`, etc.
- `borderColor`: border color
- `backgroundColor`: background fill
- `position="absolute"`: overlay layers (popups and toasts use this)

Example:

```tsx
import { Box, Text } from 'ink'

export const Card = () => (
  <Box borderStyle="round" paddingX={1} flexDirection="column">
    <Text>Title</Text>
    <Text>Body</Text>
  </Box>
)
```

### 2.3 Terminals impose constraints you must design around

#### Monospace + “columns”

Terminals measure width in **columns**. When you want a boxed UI that looks solid, you usually must:

- Compute the available width (via `useStdout().stdout.columns`).
- Clamp it to reasonable bounds.
- Pad each rendered line with spaces so background colors fill the entire box.

This “pad to full width” pattern is used throughout popups (see `src/tui/components/popups/InstructionsPopup.tsx` and many others).

#### Wrapping vs truncation

If your content may exceed the available width, you must decide:

- `wrap="wrap"`: can reflow but may make the UI jumpy.
- `wrap="truncate"`: stable layout but hides overflow.

The repo often prefers stable layouts in constrained panes (e.g. popups) by computing `contentWidth` and padding/truncating manually.

#### Unicode width and borders

Box borders (like `borderStyle="round"`) use Unicode line-drawing characters. Some terminals/fonts render these differently. When changing border styles, sanity-check in multiple terminals.

---

## 3) How Styling Works in This Repo

### 3.1 The ThemeProvider and `useTheme()`

The TUI is wrapped by `ThemeProvider` in `src/tui/AppContainer.tsx`.

- Provider: `src/tui/theme/theme-provider.tsx`
- Consumer hook: `useTheme()` exported from the same file

`useTheme()` gives you:

- `theme`: a **ResolvedTheme** (a map of named slots → Ink-compatible colors)
- `mode`: current mode (`'light' | 'dark' | 'system'`)
- `themes`: loaded theme descriptors (built-in + custom)
- `activeThemeName`: the current selected theme name
- `setTheme(name)`, `previewTheme(name)`
- `setMode(mode)`
- `error` and `warnings`

### 3.2 Theme tokens (slots)

Theme slot definitions live in `src/tui/theme/theme-types.ts`.

Required slots (current list):

- `background`
- `text`
- `mutedText`
- `border`
- `accent`
- `accentText`
- `warning`
- `error`
- `success`
- `panelBackground`
- `popupBackground`
- `selectionBackground`
- `selectionText`
- `chipBackground`
- `chipText`
- `chipMutedText`

These are enforced by:

- `REQUIRED_THEME_SLOTS` in `src/tui/theme/theme-types.ts`
- Theme JSON validation in `src/tui/theme/theme-loader.ts`

### 3.3 Turning theme tokens into Ink props

The repo uses tiny helpers in `src/tui/theme/theme-types.ts`:

- `inkColorProps(theme.text)` → `{ color?: string }`
- `inkBackgroundColorProps(theme.panelBackground)` → `{ backgroundColor?: string }`
- `inkBorderColorProps(theme.border)` → `{ borderColor?: string }`

They deliberately return `{}` when the value is `undefined`, which keeps JSX clean:

```tsx
<Text {...inkColorProps(theme.mutedText)}>Muted text</Text>
```

This pattern is everywhere (screens, core components, popups).

### 3.4 Theme JSON → ResolvedTheme

Theme JSON is **not** “Ink props”. It’s a higher-level schema that supports:

- Hex colors (`"#RRGGBB"`)
- 8-digit hex (`"#RRGGBBAA"`), where `AA == "00"` resolves to “transparent” (treated as `undefined`)
- ANSI color numbers (`0..255`) (converted to a hex value)
- References (strings that point to `defs.<name>` or `theme.<slot>` keys)
- Variants: `{ "dark": ..., "light": ... }`

Resolution is implemented in `src/tui/theme/theme-resolver.ts`.

### 3.5 Built-in themes

Built-in themes live in:

- `src/tui/theme/builtins/pm-dark.ts` (default)
- `src/tui/theme/builtins/pm-light.ts`

They are registered in `src/tui/theme/theme-registry.ts`.

### 3.6 Theme loading locations (custom themes)

Theme discovery and validation are implemented in `src/tui/theme/theme-loader.ts`.

It loads theme JSON files from:

1. **Built-in themes** (from `theme-registry.ts`)
2. **Global user themes**:
   - `~/.config/prompt-maker-cli/themes/*.json`
3. **Project themes** (walks upward from `cwd`):
   - `<project or parent>/.prompt-maker-cli/themes/*.json`

Project themes are discovered by walking up directories until an optional `stopAt` boundary.

The file name (minus `.json`) becomes the theme name.

### 3.7 Persistence: where theme selection is stored

Theme selection persists into the CLI config JSON via `src/config.ts`:

- Config fields: `theme?: string`, `themeMode?: ThemeMode`
- Update function: `updateCliThemeSettings(...)`

The bridge between the TUI and CLI config is in `src/tui/theme/theme-settings-service.ts`:

- `loadThemeSelection()` loads config + themes and resolves fallbacks.
- `saveThemeSelection()` writes `theme` and/or `themeMode`.

### 3.8 “System” mode and terminal appearance detection

`ThemeMode = 'light' | 'dark' | 'system'` in `src/tui/theme/theme-types.ts`.

When mode is `system`, `ThemeProvider` resolves it using `detectTerminalAppearanceMode()` from `src/tui/theme/terminal-appearance.ts`:

- Uses `TERM_BACKGROUND` if set (`light`/`dark`)
- Else uses `COLORFGBG` heuristics
- Else falls back to `dark`

---

## 4) Where Styles Live (Path Map)

### 4.1 Entry points and wrappers

| Concern                                                   | File(s)                                                     |
| --------------------------------------------------------- | ----------------------------------------------------------- |
| CLI → TUI mount                                           | `src/tui/index.tsx`                                         |
| Root shell (layout + help + global keys + theme provider) | `src/tui/AppContainer.tsx`                                  |
| Screen re-export stubs                                    | `src/tui/CommandScreen.tsx`, `src/tui/TestRunnerScreen.tsx` |

### 4.2 Theme system

| Concern                               | File(s)                                                                   |
| ------------------------------------- | ------------------------------------------------------------------------- |
| Theme provider + `useTheme()`         | `src/tui/theme/theme-provider.tsx`                                        |
| Theme token types + Ink prop helpers  | `src/tui/theme/theme-types.ts`                                            |
| Theme JSON resolution                 | `src/tui/theme/theme-resolver.ts`                                         |
| Theme discovery + validation          | `src/tui/theme/theme-loader.ts`                                           |
| Theme persistence bridge (CLI config) | `src/tui/theme/theme-settings-service.ts`                                 |
| Built-in theme registry               | `src/tui/theme/theme-registry.ts`                                         |
| Built-in themes                       | `src/tui/theme/builtins/pm-dark.ts`, `src/tui/theme/builtins/pm-light.ts` |
| Example theme JSON                    | `src/tui/theme/examples/ocean-example.json`                               |

### 4.3 Screens

| Screen               | Entry                          | Implementation directory        |
| -------------------- | ------------------------------ | ------------------------------- |
| Generate (“Command”) | `src/tui/CommandScreen.tsx`    | `src/tui/screens/command/*`     |
| Test Runner          | `src/tui/TestRunnerScreen.tsx` | `src/tui/screens/test-runner/*` |

### 4.4 Shared components

| Category                              | Directory                                |
| ------------------------------------- | ---------------------------------------- |
| Core presentational components        | `src/tui/components/core/*`              |
| Popup components                      | `src/tui/components/popups/*`            |
| Spinner (currently hard-coded colors) | `src/tui/components/OpencodeSpinner.tsx` |

---

## 5) Screen-by-Screen Styling Guide

This section is intentionally practical: _“I want to change X — where do I go?”_

### 5.1 App shell: `AppContainer`

**Primary file:** `src/tui/AppContainer.tsx`

What it styles:

- Global background via `inkBackgroundColorProps(theme.background)`.
- Header text colors (`theme.accent`, `theme.mutedText`).
- Help overlay and toast overlay positioning.

Safe styling changes:

- Change overall app background using theme token `background`.
- Change the header look by adjusting `accent`/`mutedText` tokens.
- Be careful changing `paddingX`/`paddingY`: multiple components assume `AppContainer` has `paddingX={2}` when computing widths.

Related invariants:

- Input routing order from `docs/tui-design.md` and `docs/prompt-maker-cli-tui-encyclopedia.md`:
  1. Help overlay
  2. Popup input
  3. Screen input
  4. Global keys

### 5.2 Generate screen (“CommandScreen”)

**Entry:** `src/tui/CommandScreen.tsx`

**Main implementation:** `src/tui/screens/command/CommandScreenImpl.tsx`

(There is also a local re-export at `src/tui/screens/command/CommandScreen.tsx`; the TUI shell imports the screen via `src/tui/CommandScreen.tsx`.)

Styled regions:

1. **Main background**: `theme.background`.
2. **History area**: `HistoryPane` paints `theme.background` and passes `backgroundColor={theme.background}` into `ScrollableOutput`.
3. **Command palette**: `CommandMenuPane` → `CommandMenu` uses `theme.panelBackground` + `theme.border`, with selection via `selectionBackground`/`selectionText`.
4. **Input bar**: `CommandInput` → `InputBar` uses `theme.panelBackground`; the left gutter uses `theme.border` or `theme.warning` depending on mode.
5. **Popup overlay**:
   - A full-screen background paint using `BackgroundFill` with `theme.background` (prevents “holes”/bleed-through in absolute overlays).
   - A centered popup (one of `src/tui/components/popups/*`), typically built on `PopupSheet` with `theme.popupBackground`.

Key files (styling focus):

- History area wrapper: `src/tui/screens/command/components/HistoryPane.tsx`
- Command palette wrapper: `src/tui/screens/command/components/CommandMenuPane.tsx`
- Input wrapper: `src/tui/screens/command/components/CommandInput.tsx`
- Popup selection switch: `src/tui/screens/command/components/PopupArea.tsx`

Safe modifications:

- To change the history “panel” look: there isn’t a dedicated token; it currently uses `background` (adjust tokens or edit `src/tui/components/core/ScrollableOutput.tsx`).
- To change selection styling in command palette/popups: use `selectionBackground`/`selectionText`.
- To change the popup overlay backdrop: today it uses `background`; for a dim overlay, add a dedicated theme slot and use it in `src/tui/screens/command/CommandScreenImpl.tsx`.

Terminal-width pitfalls (from `CommandScreenImpl.tsx`):

- `AppContainer` uses `paddingX={2}`; the command screen compensates when filling backgrounds to avoid Ink truncating with `...`.

### 5.3 Test Runner screen

**Entry:** `src/tui/TestRunnerScreen.tsx`

**Implementation:** `src/tui/screens/test-runner/TestRunnerScreen.tsx`

Styled regions:

- Headings (`Tests`, `Summary`, `Recent Logs`) use `theme.accent`.
- Summary uses `theme.success` and `theme.error`.
- Logs use `theme.mutedText`/`warning`/`error`.
- File input uses a bordered card with `theme.panelBackground` and a focus border using `theme.accent`.

Key styled files:

- Screen orchestration: `src/tui/screens/test-runner/TestRunnerScreen.tsx` (re-exported by `src/tui/TestRunnerScreen.tsx`)
- File input: `src/tui/screens/test-runner/components/TestRunnerFileInput.tsx`
- Summary: `src/tui/screens/test-runner/components/TestRunnerSummary.tsx`
- Logs: `src/tui/screens/test-runner/components/TestRunnerLogs.tsx`
- Test list: `src/tui/screens/test-runner/components/TestList.tsx`
- Errors: `src/tui/screens/test-runner/components/TestRunnerError.tsx`

Safe modifications:

- Change card background via `panelBackground`.
- Change focus border via `accent`.
- Change fail/pass colors via `error`/`success`.

---

## 6) Shared Components Styling Guide

### 6.1 Core components (`src/tui/components/core/*`)

These are used across screens and popups.

#### `BackgroundFill`

- File: `src/tui/components/core/BackgroundFill.tsx`
- Purpose: paint a solid background for absolute overlays (popups).
- Styling inputs: `background` prop (passed from theme in callers).

Tip: this component uses NBSP padding to force Ink to paint background cells.

#### `ScrollableOutput`

- File: `src/tui/components/core/ScrollableOutput.tsx`
- Purpose: windowed rendering of history/log lines.
- Styling:
  - Colors are chosen by entry kind (`user` → `accent`, `progress` → `warning`, default → `text`).
  - Uses `backgroundColor` prop (often `background`, `panelBackground`, or `popupBackground`).

Safe changes:

- Adjust kind → token mapping here if you want different “speaker” coloring.

#### `InputBar`

- File: `src/tui/components/core/InputBar.tsx`
- Purpose: the main multi-line input with status summary.
- Styling:
  - Card background: `panelBackground`
  - Border: `border` or `warning` depending on mode
  - Label and hint text: `mutedText` or `warning`
  - Spinner and accent text: `accent`

Note: the input bar tries to keep render work predictable via memoization.

#### `MultilineTextInput`

- File: `src/tui/components/core/MultilineTextInput.tsx`
- Purpose: editable multiline input with a custom cursor.
- Styling:
  - Prompt prefix uses `theme.accent`.
  - Placeholder uses `theme.mutedText`.
  - Cursor is rendered using `<Text inverse>`.

Pitfall: because the cursor is cell-based, extra styling (padding, emoji, wide unicode) can easily shift perceived cursor position.

#### `SingleLineTextInput`

- File: `src/tui/components/core/SingleLineTextInput.tsx`
- Purpose: single-line input used in popups and panels.
- Styling:
  - Placeholder uses `theme.mutedText`.
  - Cursor uses `<Text inverse>`.
  - Supports `width` and `backgroundColor` props so popups can paint full-width.

#### `CommandMenu`

- File: `src/tui/components/core/CommandMenu.tsx`
- Purpose: command palette list.
- Styling:
  - Background: `panelBackground`
  - Border: `border`
  - Header: `accent`
  - Selection: `selectionText` + `selectionBackground`

#### `HelpOverlay`

- File: `src/tui/components/core/HelpOverlay.tsx`
- Purpose: the highest-priority overlay that suppresses other input.
- Styling:
  - Background: `popupBackground`
  - Border: `border`
  - Titles: `accent`
  - Content: `mutedText`

Because it’s a key input layer, be careful making it too transparent or visually subtle.

#### `Toast` + `ToastOverlay`

- Files: `src/tui/components/core/Toast.tsx`, `src/tui/components/core/ToastOverlay.tsx`
- Purpose: transient messages.
- Styling:
  - Background: `popupBackground` (opaque)
  - Border: `border` / `warning` / `error` depending on toast kind
  - Title: `mutedText` / `warning` / `error`

Important pattern: `Toast` pads each line to `contentWidth` so Ink paints background cells in absolute overlays.

#### Status indicator segments

- File: `src/tui/components/core/status-indicators-layout.ts`
- Purpose: parse `statusChips` into display segments (used by `InputBar` and `SettingsPopup`).
- Styling:
  - Segments are mapped to theme tokens (`success`, `warning`, `danger` → `error`, etc.).

Note: the old `StatusIndicators.tsx` and `PastedSnippetCard.tsx` components were removed during cleanup; status/paste UI now lives in the screen + popup components that need it.

### 6.2 Top-level entrypoints (`src/tui/*.tsx`)

After the screen/component cleanup, `src/tui/*.tsx` is mostly entrypoints and glue (`AppContainer`, `CommandScreen`, `TestRunnerScreen`, `index`, `context`). Most styling work now lives under:

- `src/tui/screens/*` (screen layouts)
- `src/tui/components/*` (reusable building blocks)

### 6.3 Popups (`src/tui/components/popups/*`)

Popups share a consistent “sheet” style:

- Most popups render inside `PopupSheet` (`src/tui/components/popups/PopupSheet.tsx`).
- `PopupSheet` uses `BackgroundFill` to paint a solid `popupBackground` rectangle (no bleed-through).
- `width` is derived from terminal columns using `clamp(terminalColumns - 10, 40, 72)`.
- `contentWidth` is computed by subtracting **padding only** (there is no popup border anymore).
- `backgroundProps = inkBackgroundColorProps(theme.popupBackground)` is applied to each `<Text>` line.
- Header uses `theme.accent`, footer uses `theme.mutedText`.
- Selection uses `selectionText` + `selectionBackground`.
- Suggestion/unfocused selection styling uses `chipBackground` + `chipText`.

Note: popups are intentionally borderless after the cleanup; the `border` token is still used by bordered UI like `HelpOverlay`, `CommandMenu`, and `Toast`.

Popups are selected/rendered in `src/tui/screens/command/components/PopupArea.tsx`.

Below is a quick inventory with styling notes.

| Popup                      | File                                              | Main tokens                                                                                        |
| -------------------------- | ------------------------------------------------- | -------------------------------------------------------------------------------------------------- |
| Model selection            | `src/tui/components/popups/ModelPopup.tsx`        | `popupBackground`, `accent`, `mutedText`, `selection*`, plus `warning`/`error` for provider status |
| Generic list + suggestions | `src/tui/components/popups/ListPopup.tsx`         | `popupBackground`, `mutedText`, `text`, `selection*`, `chip*` for “unfocused selection”            |
| Intent file chooser        | `src/tui/components/popups/IntentFilePopup.tsx`   | `popupBackground`, `mutedText`, `selection*`, `chip*`                                              |
| Instructions               | `src/tui/components/popups/InstructionsPopup.tsx` | `popupBackground`, `accent`, `mutedText`                                                           |
| Toggle on/off              | `src/tui/components/popups/TogglePopup.tsx`       | `popupBackground`, `accent`, `mutedText`, `selection*`                                             |
| Series intent              | `src/tui/components/popups/SeriesIntentPopup.tsx` | `popupBackground`, `accent`, `mutedText`                                                           |
| Prompt test run            | `src/tui/components/popups/TestPopup.tsx`         | `popupBackground`, `accent`, `mutedText`                                                           |
| Token usage                | `src/tui/components/popups/TokenUsagePopup.tsx`   | `popupBackground`, `accent`, `text`, `mutedText`                                                   |
| Settings display           | `src/tui/components/popups/SettingsPopup.tsx`     | `popupBackground`, `accent`, `mutedText`, `success`/`warning`/`error`                              |
| Theme picker               | `src/tui/components/popups/ThemePickerPopup.tsx`  | `popupBackground`, `accent`, `mutedText`, `selection*`, `error`                                    |
| Theme mode picker          | `src/tui/components/popups/ThemeModePopup.tsx`    | `popupBackground`, `accent`, `mutedText`, `selection*`, `error`                                    |
| Reasoning view             | `src/tui/components/popups/ReasoningPopup.tsx`    | `popupBackground`, `accent`, `mutedText`, and `ScrollableOutput`                                   |
| Smart root chooser         | `src/tui/components/popups/SmartPopup.tsx`        | `popupBackground`, `accent`, `mutedText`, `selection*`, `chip*`                                    |

---

## 7) Worked Example: `InstructionsPopup.tsx`

**File:** `src/tui/components/popups/InstructionsPopup.tsx`

This popup is a great “microcosm” of how the repo styles popups.

### 7.1 Responsive sizing with `useStdout()` + clamp

```ts
const { stdout } = useStdout()
const terminalColumns = stdout?.columns ?? 80
const popupWidth = clamp(terminalColumns - 10, 40, 72)
```

Why:

- `stdout.columns` is the runtime terminal width.
- `-10` leaves a margin so the popup is not edge-to-edge.
- `clamp(..., 40, 72)` keeps the popup readable and stable.

### 7.2 Correct content width accounting

```ts
const paddingColumns = 2 * POPUP_PADDING_X
const contentWidth = Math.max(0, popupWidth - paddingColumns)
```

Why:

- These popups are borderless; padding is the main width cost.
- Without subtracting padding, your padded lines will overflow and Ink may truncate.

### 7.3 “Opaque background” pattern

Most popups render inside `PopupSheet` (`src/tui/components/popups/PopupSheet.tsx`), which uses `BackgroundFill` to paint an opaque rectangle behind the content.

Inside the popup, we still compute:

```ts
const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
```

…and apply `backgroundProps` to each `<Text>` line, padding to `contentWidth`:

```ts
{
  padRight('Meta Instructions', contentWidth)
}
```

This keeps the popup fully opaque even when rendered in an absolute overlay.

### 7.4 Theme helpers vs direct props

The popup uses:

- `inkBackgroundColorProps(theme.popupBackground)`
- `inkColorProps(theme.accent)`
- `inkColorProps(theme.mutedText)`

This is the preferred approach.

### 7.5 Example changes (documentation-only)

#### Change popup background

You usually do this by changing theme tokens, not the component.

Example: create a custom theme JSON that changes popup background (and the shared `border` token used by bordered UI):

```json
{
  "defs": {
    "bg": "#0b0f14",
    "popup": "#1a2029",
    "border": "#6cb6ff",
    "text": "#e6edf3",
    "muted": "#9aa4b2",
    "accent": "#6cb6ff",
    "warning": "#f2cc60",
    "error": "#ff6b6b",
    "success": "#3fb950",
    "panel": "#111820",
    "selectionBg": 60
  },
  "theme": {
    "background": "bg",
    "text": "text",
    "mutedText": "muted",
    "border": "border",
    "accent": "accent",
    "accentText": "bg",
    "warning": "warning",
    "error": "error",
    "success": "success",
    "panelBackground": "panel",
    "popupBackground": "popup",
    "selectionBackground": "selectionBg",
    "selectionText": "text",
    "chipBackground": "panelBackground",
    "chipText": "text",
    "chipMutedText": "mutedText"
  }
}
```

Put it in either:

- `~/.config/prompt-maker-cli/themes/<name>.json`
- `<repo>/.prompt-maker-cli/themes/<name>.json`

Then select it via the theme popup (see `ThemePickerPopup`).

---

## 8) Common Customizations (Recipes)

### 8.1 Change popup background

Goal: all popups use a new background.

Best practice: change tokens, not components.

- Background: `popupBackground`

Where to do it:

- Built-ins: `src/tui/theme/builtins/pm-dark.ts` / `src/tui/theme/builtins/pm-light.ts`
- Or create a custom theme JSON in the user/project theme directories.

Note: popups are borderless after the cleanup. The `border` token still controls bordered UI like `src/tui/components/core/HelpOverlay.tsx`, `src/tui/components/core/Toast.tsx`, and `src/tui/components/core/CommandMenu.tsx`.

### 8.2 Change “accent” and “muted” text styling

Tokens:

- “Accent”: `accent`
- “Muted” text: `mutedText`
- Default text: `text`

Common places:

- Headings across screens/panels: `theme.accent`
  - `src/tui/AppContainer.tsx`
  - `src/tui/screens/test-runner/TestRunnerScreen.tsx` (re-exported by `src/tui/TestRunnerScreen.tsx`)
- Instructional/help text: `theme.mutedText`
  - `src/tui/components/core/HelpOverlay.tsx`
  - most popup footers

### 8.3 Adjust widths responsively based on terminal columns

This repo now has two common width-math patterns.

**Borderless sheets (most popups via `PopupSheet`)**

```ts
const { stdout } = useStdout()
const terminalColumns = stdout?.columns ?? 80

const popupWidth = clamp(terminalColumns - 10, 40, 72)

const paddingColumns = 2 * POPUP_PADDING_X
const contentWidth = Math.max(0, popupWidth - paddingColumns)
```

**Bordered boxes (help overlay, command menu, toasts)**

```ts
const borderColumns = 2
const paddingColumns = 2
const contentWidth = Math.max(0, boxWidth - borderColumns - paddingColumns)
```

Guidelines:

- Subtract padding for `PopupSheet` popups; subtract **border + padding** for bordered boxes.
- Clamp widths to keep UX stable.
- Prefer manual padding/truncation (stable layout) over uncontrolled wrapping in overlays.

### 8.4 Add a new theme token (slot)

You only need this when you want a new semantic role that cannot be expressed with existing tokens.

Example goal: introduce a dedicated `dimOverlayBackground` token (instead of reusing `background`).

Steps (repo-accurate locations):

1. Add the slot to the union in `src/tui/theme/theme-types.ts`:

```ts
export type ThemeSlot =
  | 'background'
  // ...
  | 'chipMutedText'
  | 'dimOverlayBackground'
```

2. Add it to `REQUIRED_THEME_SLOTS` in `src/tui/theme/theme-types.ts`.

3. Update built-in themes to include it:

- `src/tui/theme/builtins/pm-dark.ts`
- `src/tui/theme/builtins/pm-light.ts`

4. Update any theme examples you ship (optional but recommended):

- `src/tui/theme/examples/ocean-example.json`

5. Use it in UI code (example, popup overlay in `src/tui/screens/command/CommandScreenImpl.tsx`).

Why this matters:

- Theme JSON validation in `src/tui/theme/theme-loader.ts` enforces required slots based on `REQUIRED_THEME_SLOTS`.
- Theme resolution in `src/tui/theme/theme-resolver.ts` resolves required slots.

### 8.5 Create a new theme JSON

Fast path:

1. Copy `src/tui/theme/examples/ocean-example.json`.
2. Save as:
   - `~/.config/prompt-maker-cli/themes/my-theme.json`
   - or `<repo>/.prompt-maker-cli/themes/my-theme.json`
3. Restart the TUI and select it in the theme picker popup.

Tips:

- Use `defs` for reusable named colors.
- Use references (`"popupBackground": "panelBackground"`) to keep things consistent.
- Use variants to support `system` mode:

```json
"bg": { "dark": "#10141a", "light": "#fafcff" }
```

---

## 9) Troubleshooting & Pitfalls

### 9.1 Input routing: styling can hide focus cues

From `docs/tui-design.md` and `docs/prompt-maker-cli-tui-encyclopedia.md`:

- Help overlay should visually read as “top layer”.
- Popups should clearly indicate they own input.

If you make backgrounds too similar (e.g., `popupBackground` == `background` and borders muted), users may not understand where focus is.

### 9.2 Overlays need explicit background painting

Ink won’t always repaint “empty cells” in absolute overlays.

Symptoms:

- Toasts or popups appear with “holes” or underlying text bleeding through.

Fix pattern:

- Apply `backgroundColor` to each `<Text>`.
- Pad each line to full width.

Examples:

- `src/tui/components/core/Toast.tsx`
- `src/tui/components/popups/InstructionsPopup.tsx`

### 9.3 Performance: avoid heavy work in render

From `src/tui/DEVELOPER_NOTE.md`:

- Avoid recreating arrays/objects as props.
- Memoize derived arrays when it prevents meaningful work.
- Use reducers for state transitions.

Styling-specific advice:

- Don’t build large padded string arrays in render unless memoized.
- Prefer `useMemo` for expensive formatting of lines (see `ScrollableOutput`, `CommandMenu`, `ListPopup`).

### 9.4 Cross-terminal differences

- Not all terminals support true color the same way.
- Unicode border glyphs may vary.
- Width calculations can break with wide characters.

When changing themes:

- Test on a light and dark terminal.
- Test with different font/renderers if possible.

### 9.5 Hard-coded colors exist in `OpencodeSpinner`

Most of the TUI uses `useTheme()` and theme tokens. One exception:

- `src/tui/components/OpencodeSpinner.tsx` uses hard-coded hex colors.

If you want fully themeable visuals, consider migrating this to theme tokens.

---

## 10) Safe Styling Change Checklist

Before you submit a styling change:

- Confirm you’re changing **tokens** (theme JSON / built-ins) rather than sprinkling hard-coded colors.
- If you touched popups or overlays:
  - Verify full opacity: backgrounds paint correctly (no “holes”).
  - Verify width math: `contentWidth` subtracts padding (and border columns when the component draws a border).
- If you touched layout/padding:
  - Re-check any components that assume `AppContainer` padding when computing widths.
- Verify input routing cues still make sense:
  - Help overlay obviously looks like a modal.
  - Popups look like the active layer.
- Keep render paths light:
  - Use `useMemo` for expensive padded line arrays.

---

### Appendix: Quick “where do I change…” index

- Global app background: `background` token (themes) → used in `src/tui/AppContainer.tsx` and `src/tui/screens/command/CommandScreenImpl.tsx`
- Panel background (command menu + input bar): `panelBackground` token (history uses `background`)
- Popup background: `popupBackground` token
- Border colors: `border` token (or `warning`/`error` for state)
- Selected row styling: `selectionBackground` + `selectionText`
- Unfocused selection / chip styling: `chipBackground` + `chipText` (+ `chipMutedText`)
</file>

<file path="packages/core/src/__tests__/llm.test.ts">
/// <reference types="jest" />
import type { Message } from '../lib/llm'
jest.mock('node:fs/promises', () => ({
  readFile: jest.fn(),
}))
declare const global: typeof globalThis & { fetch: jest.Mock }
describe('prompt-maker-core llm wrapper', () => {
  const fetchMock = jest.fn()
  let callLLM: typeof import('../lib/llm').callLLM
  let getEmbedding: typeof import('../lib/llm').getEmbedding
  const fsPromises = jest.requireMock('node:fs/promises') as { readFile: jest.Mock }
  beforeAll(async () => {
    const llm = await import('../lib/llm')
    callLLM = llm.callLLM
    getEmbedding = llm.getEmbedding
  })
  beforeEach(() => {
    fetchMock.mockReset()
    global.fetch = fetchMock
    fsPromises.readFile.mockReset()
    process.env.OPENAI_API_KEY = 'openai-key'
    process.env.GEMINI_API_KEY = 'gemini-key'
    process.env.OPENAI_BASE_URL = 'https://api.openai.com/v1'
    process.env.GEMINI_BASE_URL = 'https://generativelanguage.googleapis.com'
  })
  it('routes callLLM through OpenAI by default', async () => {
    fetchMock.mockResolvedValue({
      ok: true,
      json: async () => ({ choices: [{ message: { content: 'result text' } }] }),
    })
    const result = await callLLM([{ role: 'user', content: 'Hello' }])
    expect(result).toBe('result text')
    expect(fetchMock).toHaveBeenCalledWith(
      expect.stringContaining('/chat/completions'),
      expect.objectContaining({ method: 'POST' }),
    )
  })
  it('routes GPT-5 reasoning models to OpenAI Responses API', async () => {
    fetchMock.mockResolvedValue({
      ok: true,
      json: async () => ({ output_text: 'responses text' }),
    })
    const result = await callLLM(
      [
        { role: 'system', content: 'rules' },
        { role: 'user', content: 'Hi' },
      ],
      'gpt-5.2-pro',
    )
    expect(result).toBe('responses text')
    expect(fetchMock).toHaveBeenCalledWith(
      expect.stringContaining('/responses'),
      expect.objectContaining({ method: 'POST' }),
    )
    const [, options] = fetchMock.mock.calls[0]
    const body = JSON.parse((options as { body: string }).body)
    expect(body).toMatchObject({
      model: 'gpt-5.2-pro',
      input: [
        { role: 'developer', content: 'rules' },
        { role: 'user', content: 'Hi' },
      ],
    })
    expect(body.temperature).toBeUndefined()
  })
  it('retries via Responses API when Chat endpoint rejects model', async () => {
    fetchMock
      .mockResolvedValueOnce({
        ok: false,
        status: 400,
        text: async () =>
          'The model `gpt-5.2-pro-chat` does not support the /v1/chat/completions endpoint. Please use /v1/responses.',
      })
      .mockResolvedValueOnce({
        ok: true,
        json: async () => ({ output_text: 'retried text' }),
      })
    const result = await callLLM([{ role: 'user', content: 'Hello' }], 'gpt-5.2-pro-chat')
    expect(result).toBe('retried text')
    expect(fetchMock).toHaveBeenCalledTimes(2)
    expect(fetchMock.mock.calls[0][0]).toEqual(expect.stringContaining('/chat/completions'))
    expect(fetchMock.mock.calls[1][0]).toEqual(expect.stringContaining('/responses'))
    const [, secondOptions] = fetchMock.mock.calls[1]
    const secondBody = JSON.parse((secondOptions as { body: string }).body)
    expect(secondBody).toMatchObject({
      model: 'gpt-5.2-pro-chat',
      input: [{ role: 'user', content: 'Hello' }],
    })
  })
  it('throws when OpenAI API key is missing', async () => {
    delete process.env.OPENAI_API_KEY
    await expect(callLLM([{ role: 'user', content: 'Hi' }], 'gpt-4o')).rejects.toThrow(
      'OPENAI_API_KEY env var is not set.',
    )
  })
  it('supports OpenAI array content responses', async () => {
    fetchMock.mockResolvedValue({
      ok: true,
      json: async () => ({
        choices: [
          {
            message: {
              content: [
                { type: 'text', text: 'first' },
                { type: 'text', text: 'second' },
              ],
            },
          },
        ],
      }),
    })
    const result = await callLLM([{ role: 'user', content: 'Hello' }], 'gpt-4o')
    expect(result).toBe('firstsecond')
    expect(fetchMock).toHaveBeenCalledWith(
      expect.stringContaining('/chat/completions'),
      expect.objectContaining({ method: 'POST' }),
    )
  })
  it('extracts PDF text for OpenAI requests', async () => {
    fsPromises.readFile.mockResolvedValue(
      Buffer.from(
        '%PDF-1.4\n1 0 obj\n<<>>\nstream\nBT\n(Hello PDF) Tj\nET\nendstream\n%%EOF',
        'latin1',
      ),
    )
    fetchMock.mockResolvedValue({
      ok: true,
      json: async () => ({ choices: [{ message: { content: 'result text' } }] }),
    })
    const result = await callLLM(
      [
        {
          role: 'user',
          content: [{ type: 'pdf', mimeType: 'application/pdf', filePath: 'doc.pdf' }],
        } as Message,
      ],
      'gpt-4o',
    )
    expect(result).toBe('result text')
    const [, options] = fetchMock.mock.calls[0]
    const body = JSON.parse((options as { body: string }).body)
    expect(JSON.stringify(body.messages[0].content)).toContain('Hello PDF')
  })
  it('routes Gemini models to Gemini endpoint', async () => {
    fetchMock.mockResolvedValue({
      ok: true,
      json: async () => ({ candidates: [{ content: { parts: [{ text: 'gemini result' }] } }] }),
    })
    const result = await callLLM([{ role: 'user', content: 'Hi' }], 'gemini-1.5-pro')
    expect(result).toBe('gemini result')
    expect(fetchMock).toHaveBeenCalledWith(
      expect.stringContaining('/v1beta/models/gemini-1.5-pro:generateContent'),
      expect.objectContaining({ method: 'POST' }),
    )
  })
  it('throws when Gemini API key missing', async () => {
    delete process.env.GEMINI_API_KEY
    await expect(callLLM([{ role: 'user', content: 'Hi' }], 'gemini-1.5-pro')).rejects.toThrow(
      'GEMINI_API_KEY env var is not set.',
    )
  })
  it('includes systemInstruction for Gemini payloads', async () => {
    fetchMock.mockResolvedValue({
      ok: true,
      json: async () => ({ candidates: [{ content: { parts: [{ text: 'gemini result' }] } }] }),
    })
    await callLLM(
      [
        { role: 'system', content: 'rules' },
        { role: 'user', content: 'Do work' },
      ],
      'gemini-1.5-pro',
    )
    const [, options] = fetchMock.mock.calls[0]
    const body = JSON.parse((options as { body: string }).body)
    expect(body.systemInstruction.parts[0]).toEqual({ text: 'rules' })
  })
  it('callLLM rejects OpenAI video inputs', async () => {
    await expect(
      callLLM(
        [
          {
            role: 'user',
            content: [{ type: 'video_uri', mimeType: 'video/mp4', fileUri: 'gs://video' }],
          } as Message,
        ],
        'gpt-4o',
      ),
    ).rejects.toThrow('Video inputs are only supported when using Gemini models.')
  })
  it('getEmbedding uses OpenAI by default', async () => {
    fetchMock.mockResolvedValue({
      ok: true,
      json: async () => ({ data: [{ embedding: [0.1, 0.2] }] }),
    })
    const vector = await getEmbedding('text to embed')
    expect(vector).toEqual([0.1, 0.2])
    expect(fetchMock).toHaveBeenCalledWith(
      expect.stringContaining('/embeddings'),
      expect.objectContaining({ method: 'POST' }),
    )
  })
  it('getEmbedding routes to Gemini models when requested', async () => {
    fetchMock.mockResolvedValue({
      ok: true,
      json: async () => ({ embedding: { value: [0.9, 0.8] } }),
    })
    const vector = await getEmbedding('embed me', 'text-embedding-004')
    expect(vector).toEqual([0.9, 0.8])
    expect(fetchMock).toHaveBeenCalledWith(
      expect.stringContaining(':embedContent'),
      expect.objectContaining({ method: 'POST' }),
    )
  })
  it('getEmbedding rejects empty input', async () => {
    await expect(getEmbedding('  ')).rejects.toThrow('Text to embed must not be empty.')
  })
})
</file>

<file path="packages/core/src/lib/llm.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import { inflateSync } from 'node:zlib'
export type TextPart = { type: 'text'; text: string }
export type ImagePart = { type: 'image'; mimeType: string; data: string }
export type VideoPart = { type: 'video_uri'; mimeType: string; fileUri: string }
export type PdfPart = {
  type: 'pdf'
  mimeType: 'application/pdf'
  filePath: string
  fileUri?: string
}
export type MessageContent = string | (TextPart | ImagePart | VideoPart | PdfPart)[]
export type Message = {
  role: 'system' | 'user' | 'assistant'
  content: MessageContent
}
type OpenAIChatMessageContent =
  | string
  | Array<{ type: 'text'; text: string } | { type: 'image_url'; image_url: { url: string } }>
type OpenAIChatCompletionMessage = {
  role: 'system' | 'user' | 'assistant'
  content: OpenAIChatMessageContent
}
type OpenAIResponseContentPart = { type: 'text'; text: string }
type ChatCompletionChoice = {
  index: number
  message: { role: 'assistant'; content: string | OpenAIResponseContentPart[] }
}
type ChatCompletionResponse = {
  choices: ChatCompletionChoice[]
}
type GeminiApiVersion = 'v1' | 'v1beta'
type GeminiContentPart =
  | { text: string }
  | { inlineData: { mimeType: string; data: string } }
  | { fileData: { mimeType: string; fileUri: string } }
type GeminiContent = {
  role: 'user' | 'model' | 'system'
  parts: GeminiContentPart[]
}
type GeminiResponse = {
  candidates?: Array<{ content?: { parts?: GeminiContentPart[] } }>
}
type GeminiRequestBody = {
  contents: GeminiContent[]
  systemInstruction?: GeminiContent
  generationConfig: { temperature: number }
}
type OpenAIEmbeddingResponse = {
  data?: Array<{ embedding: number[] }>
}
type GeminiEmbeddingResponse = {
  embedding?: { value?: number[] }
}
/**
 * OpenAI Responses API types (partial, just what we need).
 * Docs: https://platform.openai.com/docs/api-reference/responses
 */
type OpenAIResponsesInputText = { type: 'input_text'; text: string }
type OpenAIResponsesInputImage = { type: 'input_image'; image_url: string }
type OpenAIResponsesInputContent =
  | string
  | Array<OpenAIResponsesInputText | OpenAIResponsesInputImage>
type OpenAIResponsesInputMessage = {
  role: 'developer' | 'user' | 'assistant'
  content: OpenAIResponsesInputContent
}
type OpenAIResponsesOutputText = { type: 'output_text'; text?: string }
type OpenAIResponsesOutputMessage = {
  type: 'message'
  role?: 'assistant' | 'user' | 'developer'
  content?: OpenAIResponsesOutputText[]
}
type OpenAIResponsesOutputOther = { type?: string; role?: unknown } & Record<string, unknown>
type OpenAIResponsesOutputItem = OpenAIResponsesOutputMessage | OpenAIResponsesOutputOther
type OpenAIResponsesResponse = {
  output_text?: string
  output?: OpenAIResponsesOutputItem[]
}
const DEFAULT_MODEL = process.env.OPENAI_MODEL ?? process.env.GEMINI_MODEL ?? 'gpt-5.1-codex'
const DEFAULT_OPENAI_EMBEDDING_MODEL = 'text-embedding-3-small'
const DEFAULT_GEMINI_EMBEDDING_MODEL = 'text-embedding-004'
const rawOpenAiBase = process.env.OPENAI_BASE_URL ?? 'https://api.openai.com/v1'
const OPENAI_BASE_URL = rawOpenAiBase.replace(/\/$/, '')
// Keep backwards-compatible behavior if OPENAI_BASE_URL was set to a nested path.
const OPENAI_CHAT_ENDPOINT = `${OPENAI_BASE_URL.replace(/\/chat\/completions$/, '')}/chat/completions`
const OPENAI_RESPONSES_ENDPOINT = `${OPENAI_BASE_URL.replace(/\/chat\/completions$/, '')}/responses`
const OPENAI_EMBEDDING_ENDPOINT = `${OPENAI_BASE_URL.replace(/\/chat\/completions$/, '')}/embeddings`
const DEFAULT_GEMINI_BASE_URL = 'https://generativelanguage.googleapis.com'
const normalizeGeminiBaseUrl = (value: string | undefined): string => {
  const trimmed = value?.trim()
  const candidate = trimmed && trimmed.length > 0 ? trimmed : DEFAULT_GEMINI_BASE_URL
  const withoutTrailingSlash = candidate.replace(/\/$/, '')
  const suffixes = ['/v1beta/models', '/v1/models', '/v1beta', '/v1']
  const stripped = suffixes.reduce((current, suffix) => {
    return current.endsWith(suffix) ? current.slice(0, -suffix.length) : current
  }, withoutTrailingSlash)
  return stripped || DEFAULT_GEMINI_BASE_URL
}
const GEMINI_BASE_URL = normalizeGeminiBaseUrl(process.env.GEMINI_BASE_URL)
const GEMINI_API_VERSION = process.env.GEMINI_API_VERSION?.trim() || 'v1beta'
const normalizeGeminiApiVersion = (value: string): GeminiApiVersion => {
  const trimmed = value.trim().toLowerCase()
  return trimmed === 'v1beta' ? 'v1beta' : 'v1'
}
export const callLLM = async (
  messages: Message[],
  model: string = DEFAULT_MODEL,
): Promise<string> => {
  const provider = resolveProvider(model)
  if (provider === 'gemini') {
    return callGemini(messages, model)
  }
  return callOpenAI(messages, model)
}
export const getEmbedding = async (text: string, model?: string): Promise<number[]> => {
  if (!text || !text.trim()) {
    throw new Error('Text to embed must not be empty.')
  }
  const requestedModel = model?.trim()
  const targetModel =
    requestedModel && requestedModel.length > 0 ? requestedModel : DEFAULT_OPENAI_EMBEDDING_MODEL
  const provider = resolveProvider(targetModel)
  if (provider === 'gemini') {
    const geminiModel =
      requestedModel && requestedModel.length > 0 ? requestedModel : DEFAULT_GEMINI_EMBEDDING_MODEL
    return callGeminiEmbedding(text, geminiModel)
  }
  return callOpenAIEmbedding(text, targetModel)
}
const resolveProvider = (model: string): 'openai' | 'gemini' => {
  const normalized = model.trim().toLowerCase()
  if (
    normalized.startsWith('gemini') ||
    normalized.startsWith('gemma') ||
    normalized === 'text-embedding-004'
  ) {
    return 'gemini'
  }
  return 'openai'
}
/**
 * Routing rule:
 * - Chat Completions endpoint is for chat-tuned models.
 * - GPT-5.x reasoning models (e.g., gpt-5.2-pro) should go to Responses API.
 */
const shouldUseChatCompletions = (model: string): boolean => {
  const m = model.trim().toLowerCase()
  // GPT-5.x reasoning models (and other "o" series reasoning models) are not supported on chat.
  if (m.startsWith('gpt-5') && !m.includes('chat') && !m.includes('codex')) {
    return false
  }
  if (m.startsWith('o1') || m.startsWith('o3') || m.startsWith('o4')) {
    return false
  }
  return true
}
const toErrorMessage = (error: unknown): string => {
  if (typeof error === 'string') return error
  if (error && typeof error === 'object' && 'message' in error) {
    const maybeMessage = (error as { message?: unknown }).message
    if (typeof maybeMessage === 'string') return maybeMessage
  }
  return ''
}
const isOpenAIEndpointMismatchError = (error: unknown): boolean => {
  const text = toErrorMessage(error).toLowerCase()
  if (!text) return false
  const mentionsChat = text.includes('/chat/completions') || text.includes('chat/completions')
  const mentionsResponses =
    text.includes('/responses') || text.includes('v1/responses') || text.includes('responses api')
  // Typical error: "does not support the /v1/chat/completions endpoint. Please use /v1/responses"
  return mentionsChat && mentionsResponses
}
const callOpenAI = async (messages: Message[], model: string): Promise<string> => {
  const apiKey = process.env.OPENAI_API_KEY
  if (!apiKey) {
    throw new Error('OPENAI_API_KEY env var is not set.')
  }
  const preferChat = shouldUseChatCompletions(model)
  try {
    return preferChat
      ? await callOpenAIChatCompletions(messages, model, apiKey)
      : await callOpenAIResponses(messages, model, apiKey)
  } catch (error: unknown) {
    if (!isOpenAIEndpointMismatchError(error)) {
      throw error
    }
    return preferChat
      ? await callOpenAIResponses(messages, model, apiKey)
      : await callOpenAIChatCompletions(messages, model, apiKey)
  }
}
const callOpenAIChatCompletions = async (
  messages: Message[],
  model: string,
  apiKey: string,
): Promise<string> => {
  const payloadMessages = await Promise.all(messages.map(toOpenAIMessageAsync))
  const response = await fetch(OPENAI_CHAT_ENDPOINT, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model,
      temperature: 0.2,
      messages: payloadMessages,
    }),
  })
  if (!response.ok) {
    const details = await response.text()
    throw new Error(`OpenAI request failed with status ${response.status}: ${details}`)
  }
  const data = (await response.json()) as ChatCompletionResponse
  const rawContent = data.choices?.[0]?.message?.content
  const content =
    typeof rawContent === 'string'
      ? rawContent.trim()
      : rawContent
        ? rawContent
            .map((part) => part.text ?? '')
            .join('')
            .trim()
        : ''
  if (!content) {
    throw new Error('OpenAI response did not include assistant content.')
  }
  return content
}
const callOpenAIResponses = async (
  messages: Message[],
  model: string,
  apiKey: string,
): Promise<string> => {
  const input = await Promise.all(messages.map(toOpenAIResponsesInputMessageAsync))
  // Note: reasoning models can have parameter compatibility constraints.
  // Keep this payload minimal to avoid invalid requests.
  const response = await fetch(OPENAI_RESPONSES_ENDPOINT, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model,
      input,
    }),
  })
  if (!response.ok) {
    const details = await response.text()
    throw new Error(`OpenAI request failed with status ${response.status}: ${details}`)
  }
  const data = (await response.json()) as OpenAIResponsesResponse
  const content = extractOpenAIResponsesText(data)
  if (!content) {
    throw new Error('OpenAI response did not include assistant content.')
  }
  return content
}
const extractOpenAIResponsesText = (response: OpenAIResponsesResponse): string | null => {
  const direct = response.output_text?.trim()
  if (direct && direct.length > 0) return direct
  const output = response.output ?? []
  const assistantMessages = output.filter(
    (item): item is OpenAIResponsesOutputMessage =>
      item.type === 'message' && item.role === 'assistant',
  )
  const text = assistantMessages
    .flatMap((msg) => msg.content ?? [])
    .filter((part): part is OpenAIResponsesOutputText => part.type === 'output_text')
    .map((part) => (part.text ?? '').toString())
    .join('')
    .trim()
  return text.length > 0 ? text : null
}
type GeminiCallFailure = {
  ok: false
  status: number
  details: string
  apiVersion: GeminiApiVersion
}
type GeminiCallResult = { ok: true; content: string } | GeminiCallFailure
const callGeminiOnce = async (
  messages: Message[],
  model: string,
  apiKey: string,
  apiVersion: GeminiApiVersion,
): Promise<GeminiCallResult> => {
  const endpointBase = GEMINI_BASE_URL
  const normalizedVersion = normalizeGeminiApiVersion(apiVersion)
  const url = `${endpointBase}/${normalizedVersion}/models/${model}:generateContent?key=${apiKey}`
  const body = buildGeminiRequestBody(messages)
  const response = await fetch(url, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify(body),
  })
  if (!response.ok) {
    const details = await response.text()
    return { ok: false, status: response.status, details, apiVersion: normalizedVersion }
  }
  const data = (await response.json()) as GeminiResponse
  const content = extractGeminiText(data)
  if (!content) {
    throw new Error('Gemini response did not include text content.')
  }
  return { ok: true, content }
}
const messageHasGeminiFileParts = (content: MessageContent): boolean => {
  return typeof content !== 'string' && content.some((part) => 'fileUri' in part)
}
const requestHasGeminiFileParts = (messages: Message[]): boolean => {
  return messages.some((message) => messageHasGeminiFileParts(message.content))
}
const shouldRetryGeminiApiVersion = (status: number): boolean => {
  // Gemini models sometimes move between API versions.
  return status === 404
}
const callGemini = async (messages: Message[], model: string): Promise<string> => {
  const apiKey = process.env.GEMINI_API_KEY
  if (!apiKey) {
    throw new Error('GEMINI_API_KEY env var is not set.')
  }
  const wantsFileParts = requestHasGeminiFileParts(messages)
  const envVersion = normalizeGeminiApiVersion(GEMINI_API_VERSION)
  // `fileData`/`file_data` parts are only supported on some Gemini API versions.
  const primaryVersion: GeminiApiVersion = wantsFileParts ? 'v1beta' : envVersion
  const primary = await callGeminiOnce(messages, model, apiKey, primaryVersion)
  if (primary.ok) {
    return primary.content
  }
  if (shouldRetryGeminiApiVersion(primary.status)) {
    const fallbackVersion: GeminiApiVersion = primaryVersion === 'v1beta' ? 'v1' : 'v1beta'
    const fallback = await callGeminiOnce(messages, model, apiKey, fallbackVersion)
    if (fallback.ok) {
      return fallback.content
    }
    throw new Error(
      `Gemini (${fallback.apiVersion}) request failed with status ${fallback.status}: ${fallback.details}\n` +
        `Tried ${primary.apiVersion} first: ${primary.details}`,
    )
  }
  throw new Error(
    `Gemini (${primary.apiVersion}) request failed with status ${primary.status}: ${primary.details}`,
  )
}
const buildGeminiRequestBody = (messages: Message[]): GeminiRequestBody => {
  const systemMessages = messages.filter((message) => message.role === 'system')
  const contents: GeminiContent[] = messages
    .filter((message) => message.role !== 'system')
    .map((message) => {
      const role = message.role === 'user' ? 'user' : 'model'
      const parts = toGeminiParts(message.content)
      if (parts.length === 0) {
        parts.push({ text: '' })
      }
      return {
        role,
        parts,
      }
    })
  if (contents.length === 0) {
    throw new Error('Gemini requests require at least one user message.')
  }
  const payload: GeminiRequestBody = {
    contents,
    generationConfig: { temperature: 0.2 },
  }
  const systemParts = systemMessages.flatMap((message) => toGeminiParts(message.content))
  if (systemParts.length > 0) {
    payload.systemInstruction = {
      role: 'system',
      parts: systemParts,
    }
  }
  return payload
}
const extractGeminiText = (response: GeminiResponse): string | null => {
  const firstCandidate = response.candidates?.[0]
  const parts = firstCandidate?.content?.parts ?? []
  const text = parts
    .map((part) => ('text' in part ? (part.text ?? '') : ''))
    .join('')
    .trim()
  return text || null
}
const callOpenAIEmbedding = async (text: string, model: string): Promise<number[]> => {
  const apiKey = process.env.OPENAI_API_KEY
  if (!apiKey) {
    throw new Error('OPENAI_API_KEY env var is not set.')
  }
  const response = await fetch(OPENAI_EMBEDDING_ENDPOINT, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${apiKey}`,
    },
    body: JSON.stringify({ model, input: text }),
  })
  if (!response.ok) {
    const details = await response.text()
    throw new Error(`OpenAI embedding request failed with status ${response.status}: ${details}`)
  }
  const data = (await response.json()) as OpenAIEmbeddingResponse
  const embedding = data.data?.[0]?.embedding
  if (!embedding) {
    throw new Error('OpenAI embedding response did not include embedding values.')
  }
  return embedding
}
const callGeminiEmbedding = async (text: string, model: string): Promise<number[]> => {
  const apiKey = process.env.GEMINI_API_KEY
  if (!apiKey) {
    throw new Error('GEMINI_API_KEY env var is not set.')
  }
  const endpointBase = GEMINI_BASE_URL
  const url = `${endpointBase}/${GEMINI_API_VERSION}/models/${model}:embedContent?key=${apiKey}`
  const body = {
    content: {
      parts: [{ text }],
    },
  }
  const response = await fetch(url, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify(body),
  })
  if (!response.ok) {
    const details = await response.text()
    throw new Error(`Gemini embedding request failed with status ${response.status}: ${details}`)
  }
  const data = (await response.json()) as GeminiEmbeddingResponse
  const embedding = data.embedding?.value
  if (!embedding) {
    throw new Error('Gemini embedding response did not include embedding values.')
  }
  return embedding
}
const PDF_MAX_PAGES_ENV = 'PROMPT_MAKER_PDF_MAX_PAGES'
const PDF_MAX_TEXT_CHARS_ENV = 'PROMPT_MAKER_PDF_MAX_TEXT_CHARS'
const PDF_MAX_STREAMS_ENV = 'PROMPT_MAKER_PDF_MAX_STREAMS'
const DEFAULT_PDF_MAX_PAGES = 30
const DEFAULT_PDF_MAX_TEXT_CHARS = 200_000
const DEFAULT_PDF_MAX_STREAMS = 200
const parsePositiveIntegerEnv = (key: string, fallback: number): number => {
  const raw = process.env[key]
  if (!raw) return fallback
  const parsed = Number.parseInt(raw, 10)
  return Number.isFinite(parsed) && parsed > 0 ? parsed : fallback
}
type PdfExtractResult = { ok: true; text: string } | { ok: false; message: string }
type ParsedPdfString = { bytes: Uint8Array; nextIndex: number }
type PdfJsLoadingTask = { promise: Promise<unknown> }
type PdfJsModule = {
  getDocument: (options: unknown) => PdfJsLoadingTask
}
type PdfJsDocument = {
  numPages: number
  getPage: (pageNumber: number) => Promise<unknown>
}
type PdfJsPage = {
  getTextContent: () => Promise<unknown>
}
type PdfJsTextItem = { str?: unknown } & Record<string, unknown>
type PdfJsTextContent = {
  items?: unknown
}
const isPdfJsModule = (value: unknown): value is PdfJsModule => {
  if (!value || typeof value !== 'object') {
    return false
  }
  const maybe = value as { getDocument?: unknown }
  return typeof maybe.getDocument === 'function'
}
const isPdfJsDocument = (value: unknown): value is PdfJsDocument => {
  if (!value || typeof value !== 'object') return false
  const maybe = value as { numPages?: unknown; getPage?: unknown }
  return typeof maybe.numPages === 'number' && typeof maybe.getPage === 'function'
}
const isPdfJsPage = (value: unknown): value is PdfJsPage => {
  if (!value || typeof value !== 'object') return false
  return typeof (value as { getTextContent?: unknown }).getTextContent === 'function'
}
const isPdfJsTextContent = (value: unknown): value is PdfJsTextContent => {
  return Boolean(value) && typeof value === 'object'
}
const loadPdfJs = async (): Promise<PdfJsModule | null> => {
  try {
    const mod = (await import('pdfjs-dist/legacy/build/pdf.mjs')) as unknown
    return isPdfJsModule(mod) ? mod : null
  } catch {
    return null
  }
}
const normalizeExtractedText = (value: string): string => {
  return value
    .replace(/[\u0000-\u0008\u000b\u000c\u000e-\u001f]/g, ' ')
    .replace(/\s+/g, ' ')
    .trim()
}
const extractPdfTextWithPdfJs = async (filePath: string): Promise<PdfExtractResult> => {
  const pdfjs = await loadPdfJs()
  if (!pdfjs) {
    return { ok: false, message: 'PDF text extraction library is unavailable.' }
  }
  const maxPages = parsePositiveIntegerEnv(PDF_MAX_PAGES_ENV, DEFAULT_PDF_MAX_PAGES)
  const maxChars = parsePositiveIntegerEnv(PDF_MAX_TEXT_CHARS_ENV, DEFAULT_PDF_MAX_TEXT_CHARS)
  let buffer: Buffer
  try {
    buffer = await fs.readFile(filePath)
  } catch (error) {
    const message = error instanceof Error ? error.message : String(error)
    return { ok: false, message: `Unable to read PDF ${filePath}: ${message}` }
  }
  try {
    const bytes = new Uint8Array(buffer)
    const task = pdfjs.getDocument({ data: bytes, disableWorker: true })
    const doc = await task.promise
    if (!isPdfJsDocument(doc)) {
      return { ok: false, message: `PDF parser returned an unexpected document for ${filePath}.` }
    }
    const pageCount = Math.max(0, Math.min(doc.numPages, maxPages))
    const chunks: string[] = []
    let charsSoFar = 0
    for (let pageNumber = 1; pageNumber <= pageCount; pageNumber += 1) {
      const page = await doc.getPage(pageNumber)
      if (!isPdfJsPage(page)) {
        continue
      }
      const textContent = await page.getTextContent()
      if (!isPdfJsTextContent(textContent)) {
        continue
      }
      const items = (textContent as PdfJsTextContent).items
      if (!Array.isArray(items)) {
        continue
      }
      const pageStrings = items
        .map((item) => {
          const str = (item as PdfJsTextItem).str
          return typeof str === 'string' ? str : ''
        })
        .filter((value) => value.length > 0)
      const pageText = normalizeExtractedText(pageStrings.join(' '))
      if (!pageText) {
        continue
      }
      chunks.push(pageText)
      charsSoFar += pageText.length + 1
      if (charsSoFar >= maxChars) {
        break
      }
    }
    const merged = normalizeExtractedText(chunks.join('\n'))
    if (!merged) {
      return {
        ok: false,
        message:
          'Unable to extract text from PDF. If this is a scanned PDF, convert it to searchable text (OCR) or use a Gemini model for native PDF input.',
      }
    }
    return { ok: true, text: merged.slice(0, maxChars) }
  } catch (error) {
    const message = error instanceof Error ? error.message : String(error)
    return { ok: false, message: `Failed to parse PDF ${filePath}: ${message}` }
  }
}
const isWhitespaceByte = (byte: number): boolean => {
  return byte === 0x09 || byte === 0x0a || byte === 0x0d || byte === 0x20
}
const decodeUtf16Be = (bytes: Uint8Array): string => {
  const length = bytes.length
  const start = length >= 2 && bytes[0] === 0xfe && bytes[1] === 0xff ? 2 : 0
  const codeUnits: number[] = []
  for (let i = start; i + 1 < length; i += 2) {
    const high = bytes[i] ?? 0
    const low = bytes[i + 1] ?? 0
    codeUnits.push((high << 8) | low)
  }
  return String.fromCharCode(...codeUnits)
}
const decodePdfStringBytes = (bytes: Uint8Array): string => {
  if (bytes.length >= 2 && bytes[0] === 0xfe && bytes[1] === 0xff) {
    return decodeUtf16Be(bytes)
  }
  // PDFDocEncoding is not implemented here; latin1 is a pragmatic fallback.
  return Buffer.from(bytes).toString('latin1')
}
const readPdfLiteralString = (data: string, startIndex: number): ParsedPdfString | null => {
  if (data[startIndex] !== '(') return null
  const bytes: number[] = []
  let depth = 1
  let i = startIndex + 1
  while (i < data.length) {
    const ch = data[i]
    if (ch === undefined) break
    if (ch === '\\') {
      const next = data[i + 1]
      if (next === undefined) {
        i += 1
        continue
      }
      // Line continuation
      if (next === '\n') {
        i += 2
        continue
      }
      if (next === '\r') {
        if (data[i + 2] === '\n') {
          i += 3
        } else {
          i += 2
        }
        continue
      }
      const octalMatch = data.slice(i + 1, i + 4).match(/^[0-7]{1,3}/)
      if (octalMatch) {
        bytes.push(Number.parseInt(octalMatch[0], 8))
        i += 1 + octalMatch[0].length
        continue
      }
      const mapped =
        next === 'n'
          ? 0x0a
          : next === 'r'
            ? 0x0d
            : next === 't'
              ? 0x09
              : next === 'b'
                ? 0x08
                : next === 'f'
                  ? 0x0c
                  : next.charCodeAt(0)
      bytes.push(mapped)
      i += 2
      continue
    }
    if (ch === '(') {
      depth += 1
      bytes.push(ch.charCodeAt(0))
      i += 1
      continue
    }
    if (ch === ')') {
      depth -= 1
      if (depth === 0) {
        return { bytes: Uint8Array.from(bytes), nextIndex: i + 1 }
      }
      bytes.push(ch.charCodeAt(0))
      i += 1
      continue
    }
    bytes.push(ch.charCodeAt(0))
    i += 1
  }
  return null
}
const readPdfHexString = (data: string, startIndex: number): ParsedPdfString | null => {
  if (data[startIndex] !== '<') return null
  if (data[startIndex + 1] === '<') return null
  let i = startIndex + 1
  const hexChars: string[] = []
  while (i < data.length) {
    const ch = data[i]
    if (ch === undefined) break
    if (ch === '>') {
      i += 1
      break
    }
    if (ch === ' ' || ch === '\n' || ch === '\r' || ch === '\t' || ch === '\f') {
      i += 1
      continue
    }
    hexChars.push(ch)
    i += 1
  }
  const hex = hexChars.join('')
  if (hex.length === 0) return null
  const normalized = hex.length % 2 === 1 ? `${hex}0` : hex
  const bytes: number[] = []
  for (let idx = 0; idx + 1 < normalized.length; idx += 2) {
    const byte = Number.parseInt(normalized.slice(idx, idx + 2), 16)
    if (!Number.isFinite(byte)) {
      return null
    }
    bytes.push(byte)
  }
  return { bytes: Uint8Array.from(bytes), nextIndex: i }
}
const readOperator = (data: string, startIndex: number): { op: string; nextIndex: number } => {
  let i = startIndex
  while (i < data.length) {
    const byte = data.charCodeAt(i)
    if (!isWhitespaceByte(byte)) break
    i += 1
  }
  const ch = data[i]
  if (ch === undefined) {
    return { op: '', nextIndex: i }
  }
  if (ch === "'" || ch === '"') {
    return { op: ch, nextIndex: i + 1 }
  }
  let end = i
  while (end < data.length) {
    const b = data.charCodeAt(end)
    const isAlpha = (b >= 0x41 && b <= 0x5a) || (b >= 0x61 && b <= 0x7a)
    if (!isAlpha) break
    end += 1
  }
  return { op: data.slice(i, end), nextIndex: end }
}
const extractTextFromContentStream = (data: string): string[] => {
  const out: string[] = []
  let i = 0
  while (i < data.length) {
    const ch = data[i]
    if (ch === '(') {
      const parsed = readPdfLiteralString(data, i)
      if (!parsed) {
        i += 1
        continue
      }
      const { op, nextIndex } = readOperator(data, parsed.nextIndex)
      if (op === 'Tj' || op === "'" || op === '"') {
        const text = normalizeExtractedText(decodePdfStringBytes(parsed.bytes))
        if (text) out.push(text)
        i = nextIndex
        continue
      }
      i = parsed.nextIndex
      continue
    }
    if (ch === '[') {
      const pieces: string[] = []
      i += 1
      while (i < data.length && data[i] !== ']') {
        const inner = data[i]
        if (inner === '(') {
          const parsed = readPdfLiteralString(data, i)
          if (!parsed) {
            i += 1
            continue
          }
          const text = normalizeExtractedText(decodePdfStringBytes(parsed.bytes))
          if (text) pieces.push(text)
          i = parsed.nextIndex
          continue
        }
        if (inner === '<') {
          const parsed = readPdfHexString(data, i)
          if (!parsed) {
            i += 1
            continue
          }
          const text = normalizeExtractedText(decodePdfStringBytes(parsed.bytes))
          if (text) pieces.push(text)
          i = parsed.nextIndex
          continue
        }
        i += 1
      }
      if (data[i] === ']') {
        const { op, nextIndex } = readOperator(data, i + 1)
        if (op === 'TJ') {
          const combined = normalizeExtractedText(pieces.join(' '))
          if (combined) out.push(combined)
          i = nextIndex
          continue
        }
      }
      i += 1
      continue
    }
    i += 1
  }
  return out
}
const extractPdfTextFromBuffer = (buffer: Buffer): PdfExtractResult => {
  const maxStreams = parsePositiveIntegerEnv(PDF_MAX_STREAMS_ENV, DEFAULT_PDF_MAX_STREAMS)
  const maxChars = parsePositiveIntegerEnv(PDF_MAX_TEXT_CHARS_ENV, DEFAULT_PDF_MAX_TEXT_CHARS)
  const pieces: string[] = []
  let searchIndex = 0
  let streamsSeen = 0
  while (streamsSeen < maxStreams) {
    const streamIndex = buffer.indexOf('stream', searchIndex)
    if (streamIndex < 0) break
    streamsSeen += 1
    const dictStart = Math.max(0, streamIndex - 2048)
    const dictText = buffer.subarray(dictStart, streamIndex).toString('latin1')
    const isFlate = /\/FlateDecode\b/.test(dictText)
    let dataStart = streamIndex + 'stream'.length
    while (dataStart < buffer.length) {
      const byte = buffer[dataStart]
      if (byte === 0x0a) {
        dataStart += 1
        break
      }
      if (byte === 0x0d) {
        if (buffer[dataStart + 1] === 0x0a) {
          dataStart += 2
        } else {
          dataStart += 1
        }
        break
      }
      if (byte === 0x20 || byte === 0x09) {
        dataStart += 1
        continue
      }
      // Unexpected; bail out of this stream.
      break
    }
    const endIndex = buffer.indexOf('endstream', dataStart)
    if (endIndex < 0) break
    const rawStream = buffer.subarray(dataStart, endIndex)
    let contentBytes: Buffer = rawStream
    if (isFlate) {
      try {
        contentBytes = inflateSync(rawStream)
      } catch {
        contentBytes = rawStream
      }
    }
    const streamText = contentBytes.toString('latin1')
    let lengthSoFar = pieces.reduce((sum, part) => sum + part.length + 1, 0)
    for (const extracted of extractTextFromContentStream(streamText)) {
      if (!extracted) {
        continue
      }
      pieces.push(extracted)
      lengthSoFar += extracted.length + 1
      if (lengthSoFar >= maxChars) {
        break
      }
    }
    if (lengthSoFar >= maxChars) {
      break
    }
    searchIndex = endIndex + 'endstream'.length
  }
  const merged = normalizeExtractedText(pieces.join('\n'))
  if (!merged) {
    return {
      ok: false,
      message:
        'Unable to extract text from PDF. If this is a scanned PDF, convert it to searchable text (OCR) or use a Gemini model for native PDF input.',
    }
  }
  return { ok: true, text: merged.slice(0, maxChars) }
}
const extractPdfTextFromFile = async (filePath: string): Promise<PdfExtractResult> => {
  const pdfJsResult = await extractPdfTextWithPdfJs(filePath)
  if (pdfJsResult.ok) {
    return pdfJsResult
  }
  try {
    const buffer = await fs.readFile(filePath)
    return extractPdfTextFromBuffer(buffer)
  } catch (error) {
    const message = error instanceof Error ? error.message : String(error)
    return { ok: false, message: `Unable to read PDF ${filePath}: ${message}` }
  }
}
const isVideoPart = (part: TextPart | ImagePart | VideoPart | PdfPart): part is VideoPart => {
  return part.type === 'video_uri'
}
const isPdfPart = (part: TextPart | ImagePart | VideoPart | PdfPart): part is PdfPart => {
  return part.type === 'pdf'
}
const toOpenAIMessageAsync = async (message: Message): Promise<OpenAIChatCompletionMessage> => ({
  role: message.role,
  content: await toOpenAIContentAsync(message.content),
})
const toOpenAIContentAsync = async (content: MessageContent): Promise<OpenAIChatMessageContent> => {
  if (typeof content === 'string') {
    return content
  }
  const hasVideo = content.some((part) => isVideoPart(part))
  if (hasVideo) {
    throw new Error(
      'Video inputs are only supported when using Gemini models. Remove --video or switch to a Gemini model.',
    )
  }
  const isAllText = content.every((part) => part.type === 'text')
  if (isAllText) {
    return content.map((part) => ('text' in part ? part.text : '')).join('')
  }
  const parts: Array<
    { type: 'text'; text: string } | { type: 'image_url'; image_url: { url: string } }
  > = []
  for (const part of content) {
    if (part.type === 'text') {
      parts.push({ type: 'text', text: part.text })
      continue
    }
    if (part.type === 'image') {
      parts.push({
        type: 'image_url',
        image_url: { url: `data:${part.mimeType};base64,${part.data}` },
      })
      continue
    }
    if (isPdfPart(part)) {
      const extracted = await extractPdfTextFromFile(part.filePath)
      if (extracted.ok) {
        const label = path.basename(part.filePath)
        parts.push({ type: 'text', text: `PDF (${label}):\n${extracted.text}` })
      } else {
        parts.push({ type: 'text', text: extracted.message })
      }
      continue
    }
    parts.push({ type: 'text', text: '' })
  }
  return parts
}
const toOpenAIResponsesInputMessageAsync = async (
  message: Message,
): Promise<OpenAIResponsesInputMessage> => ({
  role: message.role === 'system' ? 'developer' : message.role,
  content: await toOpenAIResponsesContentAsync(message.content),
})
const toOpenAIResponsesContentAsync = async (
  content: MessageContent,
): Promise<OpenAIResponsesInputContent> => {
  if (typeof content === 'string') {
    return content
  }
  const hasVideo = content.some((part) => isVideoPart(part))
  if (hasVideo) {
    throw new Error(
      'Video inputs are only supported when using Gemini models. Remove --video or switch to a Gemini model.',
    )
  }
  const isAllText = content.every((part) => part.type === 'text')
  if (isAllText) {
    return content.map((part) => ('text' in part ? part.text : '')).join('')
  }
  const parts: Array<OpenAIResponsesInputText | OpenAIResponsesInputImage> = []
  for (const part of content) {
    if (part.type === 'text') {
      parts.push({ type: 'input_text', text: part.text })
      continue
    }
    if (part.type === 'image') {
      parts.push({
        type: 'input_image',
        image_url: `data:${part.mimeType};base64,${part.data}`,
      })
      continue
    }
    if (isPdfPart(part)) {
      const extracted = await extractPdfTextFromFile(part.filePath)
      if (extracted.ok) {
        const label = path.basename(part.filePath)
        parts.push({ type: 'input_text', text: `PDF (${label}):\n${extracted.text}` })
      } else {
        parts.push({ type: 'input_text', text: extracted.message })
      }
      continue
    }
    parts.push({ type: 'input_text', text: '' })
  }
  return parts
}
const toGeminiParts = (content: MessageContent): GeminiContentPart[] => {
  if (typeof content === 'string') {
    return content ? [{ text: content }] : []
  }
  return content.map((part) => {
    if (part.type === 'text') {
      return { text: part.text }
    }
    if (part.type === 'image') {
      return { inlineData: { mimeType: part.mimeType, data: part.data } }
    }
    if (part.type === 'video_uri') {
      return { fileData: { mimeType: part.mimeType, fileUri: part.fileUri } }
    }
    if (part.type === 'pdf') {
      if (!part.fileUri) {
        throw new Error(
          `PDF attachment ${part.filePath} is missing a Gemini fileUri. ` +
            'Upload the PDF via the Gemini Files API before calling Gemini models.',
        )
      }
      return { fileData: { mimeType: part.mimeType, fileUri: part.fileUri } }
    }
    return { text: '' }
  })
}
</file>

<file path="src/__tests__/popup-reducer.test.ts">
import type { PopupState } from '../tui/types'
import { popupReducer, type PopupAction, type PopupManagerState } from '../tui/popup-reducer'
describe('popupReducer', () => {
  const reduce = (state: PopupManagerState, action: PopupAction): PopupManagerState =>
    popupReducer(state, action)
  const initialState = (): PopupManagerState => ({ popupState: null, activeScan: null })
  it('opens and closes popups explicitly', () => {
    const opened = reduce(initialState(), { type: 'open-tokens' })
    expect(opened.popupState).toEqual({ type: 'tokens' })
    const closed = reduce(opened, { type: 'close' })
    expect(closed).toEqual({ popupState: null, activeScan: null })
  })
  it('opens theme popup with initial name', () => {
    const opened = reduce(initialState(), {
      type: 'open-theme',
      selectionIndex: 2,
      initialThemeName: 'pm-dark',
    })
    expect(opened.popupState).toEqual({
      type: 'theme',
      selectionIndex: 2,
      initialThemeName: 'pm-dark',
    })
    expect(opened.activeScan).toBeNull()
  })
  it('opens theme mode popup with initial mode', () => {
    const opened = reduce(initialState(), {
      type: 'open-theme-mode',
      selectionIndex: 1,
      initialMode: 'dark',
    })
    expect(opened.popupState).toEqual({
      type: 'themeMode',
      selectionIndex: 1,
      initialMode: 'dark',
    })
    expect(opened.activeScan).toBeNull()
  })
  it('opens URL popup', () => {
    const opened = reduce(initialState(), { type: 'open-url' })
    expect(opened.popupState).toEqual({
      type: 'url',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      editingIndex: null,
    })
    expect(opened.activeScan).toBeNull()
  })
  it('opens image/video popups with scan state', () => {
    const afterImage = reduce(initialState(), { type: 'open-image', scanId: 1 })
    expect(afterImage.popupState).toEqual({
      type: 'image',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(afterImage.activeScan).toEqual({ kind: 'image', id: 1 })
    const afterVideo = reduce(afterImage, { type: 'open-video', scanId: 2 })
    expect(afterVideo.popupState).toEqual({
      type: 'video',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(afterVideo.activeScan).toEqual({ kind: 'video', id: 2 })
  })
  it('switching popups clears scan state', () => {
    const afterImage = reduce(initialState(), { type: 'open-image', scanId: 1 })
    expect(afterImage.popupState).toEqual({
      type: 'image',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(afterImage.activeScan).toEqual({ kind: 'image', id: 1 })
    const afterVideo = reduce(afterImage, { type: 'open-video', scanId: 2 })
    expect(afterVideo.popupState).toEqual({
      type: 'video',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(afterVideo.activeScan).toEqual({ kind: 'video', id: 2 })
  })
  it('switching popups clears scan state', () => {
    const afterFile = reduce(initialState(), { type: 'open-file', scanId: 1 })
    expect(afterFile.popupState).toEqual({
      type: 'file',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(afterFile.activeScan).toEqual({ kind: 'file', id: 1 })
    const afterSmart = reduce(afterFile, { type: 'open-smart', scanId: 2, draft: 'src' })
    expect(afterSmart.popupState).toEqual({
      type: 'smart',
      draft: 'src',
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(afterSmart.activeScan).toEqual({ kind: 'smart', id: 2 })
  })
  it('ignores stale async scan results', () => {
    const afterFile = reduce(initialState(), { type: 'open-file', scanId: 1 })
    const afterSmart = reduce(afterFile, { type: 'open-smart', scanId: 2, draft: '' })
    // A file scan resolving after the user switched popups must be ignored.
    const staleApplied = reduce(afterSmart, {
      type: 'scan-suggestions-success',
      kind: 'file',
      scanId: 1,
      suggestions: ['README.md'],
    })
    expect(staleApplied.popupState).toEqual(afterSmart.popupState)
    expect(staleApplied.activeScan).toEqual(afterSmart.activeScan)
  })
  it('applies scan results when popup and scanId match', () => {
    const afterFile = reduce(initialState(), { type: 'open-file', scanId: 1 })
    const applied = reduce(afterFile, {
      type: 'scan-suggestions-success',
      kind: 'file',
      scanId: 1,
      suggestions: ['src/index.ts', 'README.md'],
    })
    expect(applied.popupState).toEqual({
      type: 'file',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: ['src/index.ts', 'README.md'],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(applied.activeScan).toBeNull()
  })
  it('preserves active scan across same-type set updates', () => {
    const afterFile = reduce(initialState(), { type: 'open-file', scanId: 1 })
    const updated = reduce(afterFile, {
      type: 'set',
      next: (prev: PopupState) => (prev?.type === 'file' ? { ...prev, draft: 'x' } : prev),
    })
    expect(updated.popupState).toEqual({
      type: 'file',
      draft: 'x',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(updated.activeScan).toEqual({ kind: 'file', id: 1 })
  })
})
</file>

<file path="src/generate/payload-io.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import yaml from 'js-yaml'
import { GENERATE_JSON_PAYLOAD_SCHEMA_VERSION, type GenerateJsonPayload } from './types'
export type PayloadFormat = 'json' | 'yaml'
type JsonRecord = Record<string, unknown>
const isRecord = (value: unknown): value is JsonRecord =>
  typeof value === 'object' && value !== null && !Array.isArray(value)
const isStringArray = (value: unknown): value is string[] =>
  Array.isArray(value) && value.every((entry) => typeof entry === 'string')
const isContextPaths = (value: unknown): value is GenerateJsonPayload['contextPaths'] =>
  Array.isArray(value) &&
  value.every(
    (entry) =>
      isRecord(entry) && typeof entry.path === 'string' && typeof entry.source === 'string',
  )
const isGenerateJsonPayload = (value: unknown): value is GenerateJsonPayload => {
  if (!isRecord(value)) {
    return false
  }
  const hasMediaLists =
    (value.images === undefined || isStringArray(value.images)) &&
    (value.videos === undefined || isStringArray(value.videos)) &&
    (value.pdfs === undefined || isStringArray(value.pdfs))
  return (
    value.schemaVersion === GENERATE_JSON_PAYLOAD_SCHEMA_VERSION &&
    typeof value.intent === 'string' &&
    typeof value.model === 'string' &&
    typeof value.targetModel === 'string' &&
    typeof value.prompt === 'string' &&
    isStringArray(value.refinements) &&
    typeof value.iterations === 'number' &&
    Number.isFinite(value.iterations) &&
    typeof value.interactive === 'boolean' &&
    typeof value.timestamp === 'string' &&
    isContextPaths(value.contextPaths) &&
    hasMediaLists
  )
}
const getPayloadFormatForPath = (filePath: string): PayloadFormat => {
  const ext = path.extname(filePath).toLowerCase()
  if (ext === '.json') {
    return 'json'
  }
  if (ext === '.yaml' || ext === '.yml') {
    return 'yaml'
  }
  throw new Error(
    `Unsupported payload file extension for ${formatDisplayPath(filePath)}. Expected .json, .yaml, or .yml.`,
  )
}
const isMissingFileError = (error: unknown): error is { code: string } =>
  typeof error === 'object' &&
  error !== null &&
  'code' in error &&
  (error as { code?: unknown }).code === 'ENOENT'
const formatDisplayPath = (filePath: string): string => {
  const relative = path.relative(process.cwd(), filePath)
  return relative && !relative.startsWith('..') ? relative : filePath
}
const parseGeneratePayload = (raw: string, format: PayloadFormat, filePath: string): unknown => {
  if (format === 'json') {
    try {
      return JSON.parse(raw) as unknown
    } catch (error) {
      const message = error instanceof Error ? error.message : 'Unknown JSON error.'
      throw new Error(`Failed to parse JSON in ${formatDisplayPath(filePath)}: ${message}`)
    }
  }
  try {
    return yaml.load(raw)
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown YAML error.'
    throw new Error(`Failed to parse YAML in ${formatDisplayPath(filePath)}: ${message}`)
  }
}
export const validateGeneratePayloadObject = (
  value: unknown,
  sourceLabel: string,
): GenerateJsonPayload => {
  if (!isGenerateJsonPayload(value)) {
    throw new Error(
      `Invalid generate payload in ${sourceLabel}. Expected schemaVersion=${GENERATE_JSON_PAYLOAD_SCHEMA_VERSION} and required fields (intent, model, targetModel, prompt, refinements, iterations, interactive, timestamp, contextPaths).`,
    )
  }
  return value
}
export const loadGeneratePayloadFromFile = async (
  filePath: string,
): Promise<GenerateJsonPayload> => {
  let raw: string
  try {
    raw = await fs.readFile(filePath, 'utf8')
  } catch (error) {
    if (isMissingFileError(error)) {
      throw new Error(`Payload file not found: ${formatDisplayPath(filePath)}`)
    }
    const message = error instanceof Error ? error.message : 'Unknown file read error.'
    throw new Error(`Failed to read payload file ${formatDisplayPath(filePath)}: ${message}`)
  }
  const format = getPayloadFormatForPath(filePath)
  const parsed = parseGeneratePayload(raw, format, filePath)
  return validateGeneratePayloadObject(parsed, formatDisplayPath(filePath))
}
export const serializeGeneratePayload = (
  payload: GenerateJsonPayload,
  format: PayloadFormat,
): string => {
  if (format === 'json') {
    return `${JSON.stringify(payload, null, 2)}\n`
  }
  const dumped = yaml.dump(payload, {
    sortKeys: true,
    noRefs: true,
  })
  return dumped.endsWith('\n') ? dumped : `${dumped}\n`
}
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenContextPopupBindings.ts">
import type { PopupState } from '../../../types'
import { useContextPopupGlue } from './useContextPopupGlue'
import { useDroppedFileDetection } from './useDroppedFileDetection'
export type UseCommandScreenContextPopupBindingsOptions = {
  inputValue: string
  popupState: PopupState
  helpOpen: boolean
  isPopupOpen: boolean
  isCommandMode: boolean
  isCommandMenuActive: boolean
  isGenerating: boolean
  droppedFilePath: string | null
  files: string[]
  urls: string[]
  images: string[]
  videos: string[]
  pdfs: string[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  addFile: (value: string) => void
  removeFile: (index: number) => void
  addUrl: (value: string) => void
  removeUrl: (index: number) => void
  updateUrl: (index: number, value: string) => void
  addImage: (value: string) => void
  removeImage: (index: number) => void
  addVideo: (value: string) => void
  removeVideo: (index: number) => void
  addPdf: (value: string) => void
  removePdf: (index: number) => void
  toggleSmartContext: () => void
  setSmartRoot: (value: string) => void
  setInputValue: (value: string | ((prev: string) => string)) => void
  setPopupState: import('react').Dispatch<import('react').SetStateAction<PopupState>>
  suppressNextInput: () => void
  notify: (message: string) => void
  pushHistory: (content: string, kind?: 'system' | 'user' | 'progress') => void
  addCommandHistoryEntry: (value: string) => void
  handleCommandSelection: (
    commandId: import('../../../types').CommandDescriptor['id'],
    argsRaw?: string,
  ) => void
  consumeSuppressedTextInputChange: () => boolean
}
export type UseCommandScreenContextPopupBindingsResult = ReturnType<typeof useContextPopupGlue>
export const useCommandScreenContextPopupBindings = ({
  inputValue,
  popupState,
  helpOpen,
  isPopupOpen,
  isCommandMode,
  isCommandMenuActive,
  isGenerating,
  droppedFilePath,
  files,
  urls,
  images,
  videos,
  pdfs,
  smartContextEnabled,
  smartContextRoot,
  addFile,
  removeFile,
  addUrl,
  removeUrl,
  updateUrl,
  addImage,
  removeImage,
  addVideo,
  removeVideo,
  addPdf,
  removePdf,
  toggleSmartContext,
  setSmartRoot,
  setInputValue,
  setPopupState,
  suppressNextInput,
  notify,
  pushHistory,
  addCommandHistoryEntry,
  handleCommandSelection,
  consumeSuppressedTextInputChange,
}: UseCommandScreenContextPopupBindingsOptions): UseCommandScreenContextPopupBindingsResult => {
  const popupDraftInput =
    popupState?.type === 'file' ||
    popupState?.type === 'image' ||
    popupState?.type === 'video' ||
    popupState?.type === 'pdf'
      ? popupState.draft
      : ''
  const { isFilePath } = useDroppedFileDetection(popupDraftInput)
  return useContextPopupGlue({
    inputValue,
    popupState,
    helpOpen,
    isPopupOpen,
    isCommandMode,
    isCommandMenuActive,
    isGenerating,
    droppedFilePath,
    files,
    urls,
    images,
    videos,
    pdfs,
    smartContextEnabled,
    smartContextRoot,
    addFile,
    removeFile,
    addUrl,
    removeUrl,
    updateUrl,
    addImage,
    removeImage,
    addVideo,
    removeVideo,
    addPdf,
    removePdf,
    toggleSmartContext,
    setSmartRoot,
    setInputValue,
    setPopupState,
    suppressNextInput,
    notify,
    pushHistory,
    addCommandHistoryEntry,
    handleCommandSelection,
    consumeSuppressedTextInputChange,
    isFilePath,
  })
}
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenInputState.ts">
import type { MutableRefObject } from 'react'
import { useCallback, useMemo, useRef } from 'react'
import type { DebugKeyEvent } from '../../../components/core/MultilineTextInput'
import { isCommandInput } from '../../../drag-drop-path'
import type { HistoryEntry } from '../../../types'
import { useDroppedFileDetection } from './useDroppedFileDetection'
import { useCommandScreen } from '../useCommandScreen'
import { formatDebugKeyEvent } from '../utils/debug-keys'
export type UseCommandScreenInputStateOptions = {
  pushHistoryProxy: (
    content: string,
    kind?: HistoryEntry['kind'],
    format?: HistoryEntry['format'],
  ) => void
}
export type UseCommandScreenInputStateResult = {
  // Screen state
  terminalRows: number
  terminalColumns: number
  inputValue: string
  isPasteActive: boolean
  commandSelectionIndex: number
  debugKeyLine: string | null
  // Setters
  setTerminalSize: (rows: number, columns: number) => void
  setInputValue: (value: string | ((prev: string) => string)) => void
  setPasteActive: (active: boolean) => void
  setCommandSelectionIndex: (next: number | ((prev: number) => number)) => void
  setDebugKeyLine: (value: string) => void
  // Local toggles/state
  intentFilePath: string
  setIntentFilePath: (value: string) => void
  copyEnabled: boolean
  setCopyEnabled: (value: boolean) => void
  chatGptEnabled: boolean
  setChatGptEnabled: (value: boolean) => void
  jsonOutputEnabled: boolean
  setJsonOutputEnabled: (value: boolean) => void
  // Intent refs
  lastUserIntentRef: MutableRefObject<string | null>
  lastTypedIntentRef: MutableRefObject<string>
  // Text input suppression
  consumeSuppressedTextInputChange: () => boolean
  suppressNextInput: () => void
  // Debug
  debugKeysEnabled: boolean
  onDebugKeyEvent: (event: DebugKeyEvent) => void
  // Helpers
  updateLastTypedIntent: (next: string) => void
}
export const useCommandScreenInputState = ({
  pushHistoryProxy: _pushHistoryProxy,
}: UseCommandScreenInputStateOptions): UseCommandScreenInputStateResult => {
  const {
    state: screenState,
    setTerminalSize,
    setInputValue,
    setPasteActive,
    setCommandSelectionIndex,
    setDebugKeyLine,
    setIntentFilePath: setIntentFilePathState,
    setCopyEnabled: setCopyEnabledState,
    setChatGptEnabled: setChatGptEnabledState,
    setJsonOutputEnabled: setJsonOutputEnabledState,
  } = useCommandScreen()
  const terminalRows = screenState.terminalRows
  const terminalColumns = screenState.terminalColumns
  const debugKeyLine = screenState.debugKeyLine
  const inputValue = screenState.inputValue
  const isPasteActive = screenState.isPasteActive
  const commandSelectionIndex = screenState.commandSelectionIndex
  const lastUserIntentRef = useRef<string | null>(null)
  const lastTypedIntentRef = useRef<string>('')
  const debugKeysEnabled = useMemo(() => {
    const value = process.env.PROMPT_MAKER_DEBUG_KEYS
    if (!value) {
      return false
    }
    const normalized = value.trim().toLowerCase()
    return normalized !== '0' && normalized !== 'false'
  }, [])
  const onDebugKeyEvent = useCallback(
    (event: DebugKeyEvent): void => {
      if (!debugKeysEnabled) {
        return
      }
      setDebugKeyLine(formatDebugKeyEvent(event))
    },
    [debugKeysEnabled, setDebugKeyLine],
  )
  const intentFilePath = screenState.intentFilePath
  const copyEnabled = screenState.copyEnabled
  const chatGptEnabled = screenState.chatGptEnabled
  const jsonOutputEnabled = screenState.jsonOutputEnabled
  const setIntentFilePath = useCallback(
    (value: string) => {
      setIntentFilePathState(value)
    },
    [setIntentFilePathState],
  )
  const setCopyEnabled = useCallback(
    (value: boolean) => {
      setCopyEnabledState(value)
    },
    [setCopyEnabledState],
  )
  const setChatGptEnabled = useCallback(
    (value: boolean) => {
      setChatGptEnabledState(value)
    },
    [setChatGptEnabledState],
  )
  const setJsonOutputEnabled = useCallback(
    (value: boolean) => {
      setJsonOutputEnabledState(value)
    },
    [setJsonOutputEnabledState],
  )
  const suppressNextInputRef = useRef(false)
  const consumeSuppressedTextInputChange = useCallback((): boolean => {
    if (!suppressNextInputRef.current) {
      return false
    }
    suppressNextInputRef.current = false
    return true
  }, [])
  const suppressNextInput = useCallback(() => {
    suppressNextInputRef.current = true
  }, [])
  const { existsSync } = useDroppedFileDetection(inputValue)
  const updateLastTypedIntent = useCallback(
    (next: string): void => {
      if (isCommandInput(next, existsSync)) {
        return
      }
      lastTypedIntentRef.current = next
    },
    [existsSync],
  )
  return {
    terminalRows,
    terminalColumns,
    inputValue,
    isPasteActive,
    commandSelectionIndex,
    debugKeyLine,
    setTerminalSize,
    setInputValue,
    setPasteActive,
    setCommandSelectionIndex,
    setDebugKeyLine,
    intentFilePath,
    setIntentFilePath,
    copyEnabled,
    setCopyEnabled,
    chatGptEnabled,
    setChatGptEnabled,
    jsonOutputEnabled,
    setJsonOutputEnabled,
    lastUserIntentRef,
    lastTypedIntentRef,
    consumeSuppressedTextInputChange,
    suppressNextInput,
    debugKeysEnabled,
    onDebugKeyEvent,
    updateLastTypedIntent,
  }
}
</file>

<file path="src/tui/screens/command/hooks/useMiscPopupDraftHandlers.ts">
import { useCallback } from 'react'
import type { PopupState } from '../../../types'
type SetPopupState = (next: PopupState | ((prev: PopupState) => PopupState)) => void
type ConsumeSuppressedTextInputChange = () => boolean
export type UseMiscPopupDraftHandlersOptions = {
  setPopupState: SetPopupState
  consumeSuppressedTextInputChange: ConsumeSuppressedTextInputChange
}
export type UseMiscPopupDraftHandlersResult = {
  onModelPopupQueryChange: (next: string) => void
  onSeriesDraftChange: (next: string) => void
  onInstructionsDraftChange: (next: string) => void
  onTestDraftChange: (next: string) => void
  onBudgetsMaxContextTokensDraftChange: (next: string) => void
  onBudgetsMaxInputTokensDraftChange: (next: string) => void
  onResumePayloadPathDraftChange: (next: string) => void
  onExportOutPathDraftChange: (next: string) => void
}
export const useMiscPopupDraftHandlers = ({
  setPopupState,
  consumeSuppressedTextInputChange,
}: UseMiscPopupDraftHandlersOptions): UseMiscPopupDraftHandlersResult => {
  const onModelPopupQueryChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) =>
        prev?.type === 'model' ? { ...prev, query: next, selectionIndex: 0 } : prev,
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  const onSeriesDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) => (prev?.type === 'series' ? { ...prev, draft: next } : prev))
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  const onInstructionsDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) => (prev?.type === 'instructions' ? { ...prev, draft: next } : prev))
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  const onTestDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) => (prev?.type === 'test' ? { ...prev, draft: next } : prev))
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  const onBudgetsMaxContextTokensDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) =>
        prev?.type === 'budgets'
          ? { ...prev, maxContextTokensDraft: next, errorMessage: null }
          : prev,
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  const onBudgetsMaxInputTokensDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) =>
        prev?.type === 'budgets'
          ? { ...prev, maxInputTokensDraft: next, errorMessage: null }
          : prev,
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  const onResumePayloadPathDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) =>
        prev?.type === 'resume'
          ? {
              ...prev,
              payloadPathDraft: next,
              suggestedFocused: false,
              suggestedSelectionIndex: 0,
            }
          : prev,
      )
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  const onExportOutPathDraftChange = useCallback(
    (next: string) => {
      if (consumeSuppressedTextInputChange()) {
        return
      }
      setPopupState((prev) => (prev?.type === 'export' ? { ...prev, outPathDraft: next } : prev))
    },
    [consumeSuppressedTextInputChange, setPopupState],
  )
  return {
    onModelPopupQueryChange,
    onSeriesDraftChange,
    onInstructionsDraftChange,
    onTestDraftChange,
    onBudgetsMaxContextTokensDraftChange,
    onBudgetsMaxInputTokensDraftChange,
    onResumePayloadPathDraftChange,
    onExportOutPathDraftChange,
  }
}
</file>

<file path="src/tui/context-store.ts">
import { createContext, useContext } from 'react'
import type { ContextOverflowStrategy } from '../config'
export type ContextSourceState = {
  files: string[]
  urls: string[]
  images: string[]
  videos: string[]
  pdfs: string[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  metaInstructions: string
  maxContextTokens: number | null
  maxInputTokens: number | null
  contextOverflowStrategy: ContextOverflowStrategy | null
  lastReasoning: string | null
  lastGeneratedPrompt: string | null
}
export type ContextDispatch = {
  addFile: (value: string) => void
  removeFile: (index: number) => void
  addUrl: (value: string) => void
  removeUrl: (index: number) => void
  updateUrl: (index: number, value: string) => void
  addImage: (value: string) => void
  removeImage: (index: number) => void
  addVideo: (value: string) => void
  removeVideo: (index: number) => void
  addPdf: (value: string) => void
  removePdf: (index: number) => void
  toggleSmartContext: () => void
  setSmartRoot: (value: string) => void
  setMetaInstructions: (value: string) => void
  setBudgets: (value: {
    maxContextTokens: number | null
    maxInputTokens: number | null
    contextOverflowStrategy: ContextOverflowStrategy | null
  }) => void
  setLastReasoning: (value: string | null) => void
  setLastGeneratedPrompt: (value: string | null) => void
  resetContext: () => void
}
export const ContextStateContext = createContext<ContextSourceState | null>(null)
export const ContextDispatchContext = createContext<ContextDispatch | null>(null)
export const useContextState = (): ContextSourceState => {
  const context = useContext(ContextStateContext)
  if (!context) {
    throw new Error('useContextState must be used within ContextProvider')
  }
  return context
}
export const useContextDispatch = (): ContextDispatch => {
  const context = useContext(ContextDispatchContext)
  if (!context) {
    throw new Error('useContextDispatch must be used within ContextProvider')
  }
  return context
}
</file>

<file path="src/tui/context.tsx">
import React, { useCallback, useEffect, useState } from 'react'
import { loadCliConfig } from '../config'
import { ContextDispatchContext, ContextStateContext } from './context-store'
export const ContextProvider: React.FC<{ children: React.ReactNode }> = ({ children }) => {
  const [files, setFiles] = useState<string[]>([])
  const [urls, setUrls] = useState<string[]>([])
  const [images, setImages] = useState<string[]>([])
  const [videos, setVideos] = useState<string[]>([])
  const [pdfs, setPdfs] = useState<string[]>([])
  const [smartContextEnabled, setSmartContextEnabled] = useState(false)
  const [smartContextRoot, setSmartContextRoot] = useState<string | null>(null)
  const [metaInstructions, setMetaInstructions] = useState('')
  const [maxContextTokens, setMaxContextTokens] = useState<number | null>(null)
  const [maxInputTokens, setMaxInputTokens] = useState<number | null>(null)
  const [contextOverflowStrategy, setContextOverflowStrategy] = useState<
    import('../config').ContextOverflowStrategy | null
  >(null)
  const [lastReasoning, setLastReasoning] = useState<string | null>(null)
  const [lastGeneratedPrompt, setLastGeneratedPrompt] = useState<string | null>(null)
  const addEntry = useCallback(
    (value: string, setter: React.Dispatch<React.SetStateAction<string[]>>) => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      setter((prev) => [...prev, trimmed])
    },
    [],
  )
  const removeEntry = useCallback(
    (index: number, setter: React.Dispatch<React.SetStateAction<string[]>>) => {
      setter((prev) => prev.filter((_, idx) => idx !== index))
    },
    [],
  )
  const updateEntry = useCallback(
    (index: number, value: string, setter: React.Dispatch<React.SetStateAction<string[]>>) => {
      const trimmed = value.trim()
      if (!trimmed) {
        return
      }
      setter((prev) => {
        if (index < 0 || index >= prev.length) {
          return prev
        }
        const next = [...prev]
        next[index] = trimmed
        return next
      })
    },
    [],
  )
  const addFile = useCallback((value: string) => addEntry(value, setFiles), [addEntry])
  const removeFile = useCallback((index: number) => removeEntry(index, setFiles), [removeEntry])
  const addUrl = useCallback((value: string) => addEntry(value, setUrls), [addEntry])
  const removeUrl = useCallback((index: number) => removeEntry(index, setUrls), [removeEntry])
  const updateUrl = useCallback(
    (index: number, value: string) => updateEntry(index, value, setUrls),
    [updateEntry],
  )
  const addImage = useCallback((value: string) => addEntry(value, setImages), [addEntry])
  const removeImage = useCallback((index: number) => removeEntry(index, setImages), [removeEntry])
  const addVideo = useCallback((value: string) => addEntry(value, setVideos), [addEntry])
  const removeVideo = useCallback((index: number) => removeEntry(index, setVideos), [removeEntry])
  const addPdf = useCallback((value: string) => addEntry(value, setPdfs), [addEntry])
  const removePdf = useCallback((index: number) => removeEntry(index, setPdfs), [removeEntry])
  const toggleSmartContext = useCallback(() => {
    setSmartContextEnabled((prev) => !prev)
  }, [])
  const setSmartRoot = useCallback((value: string) => {
    const trimmed = value.trim()
    setSmartContextRoot(trimmed.length > 0 ? trimmed : null)
  }, [])
  const resetContext = useCallback(() => {
    setFiles([])
    setUrls([])
    setImages([])
    setVideos([])
    setPdfs([])
    setSmartContextEnabled(false)
    setSmartContextRoot(null)
    setMetaInstructions('')
    setLastReasoning(null)
  }, [])
  const setBudgets = useCallback(
    (value: {
      maxContextTokens: number | null
      maxInputTokens: number | null
      contextOverflowStrategy: import('../config').ContextOverflowStrategy | null
    }) => {
      setMaxContextTokens(value.maxContextTokens)
      setMaxInputTokens(value.maxInputTokens)
      setContextOverflowStrategy(value.contextOverflowStrategy)
    },
    [],
  )
  useEffect(() => {
    let cancelled = false
    const loadBudgetsFromConfig = async (): Promise<void> => {
      const config = await loadCliConfig().catch(() => null)
      if (cancelled) {
        return
      }
      const promptGenerator = config?.promptGenerator
      setMaxInputTokens(promptGenerator?.maxInputTokens ?? null)
      setMaxContextTokens(promptGenerator?.maxContextTokens ?? null)
      setContextOverflowStrategy(promptGenerator?.contextOverflowStrategy ?? null)
    }
    void loadBudgetsFromConfig()
    return () => {
      cancelled = true
    }
  }, [])
  return (
    <ContextStateContext.Provider
      value={{
        files,
        urls,
        images,
        videos,
        pdfs,
        smartContextEnabled,
        smartContextRoot,
        metaInstructions,
        maxContextTokens,
        maxInputTokens,
        contextOverflowStrategy,
        lastReasoning,
        lastGeneratedPrompt,
      }}
    >
      <ContextDispatchContext.Provider
        value={{
          addFile,
          removeFile,
          addUrl,
          removeUrl,
          updateUrl,
          addImage,
          removeImage,
          addVideo,
          removeVideo,
          addPdf,
          removePdf,
          toggleSmartContext,
          setSmartRoot,
          setMetaInstructions,
          setBudgets,
          setLastReasoning,
          setLastGeneratedPrompt,
          resetContext,
        }}
      >
        {children}
      </ContextDispatchContext.Provider>
    </ContextStateContext.Provider>
  )
}
</file>

<file path="src/tui/help-config.ts">
import type { CommandDescriptor } from './types'
export type HelpSection = {
  title: string
  lines: string[]
}
const chunk = <T>(items: readonly T[], size: number): T[][] => {
  if (size <= 0) {
    return [Array.from(items)]
  }
  const result: T[][] = []
  for (let index = 0; index < items.length; index += size) {
    result.push(items.slice(index, index + size))
  }
  return result
}
const formatCommandLines = (commandDescriptors: readonly CommandDescriptor[]): string[] => {
  const commands = commandDescriptors.map((descriptor) => `/${descriptor.id}`)
  const chunks = chunk(commands, 7)
  return chunks.map((group, index) => {
    const prefix = index === 0 ? 'Commands: ' : '         '
    return `${prefix}${group.join(' ')}`
  })
}
export type HelpConfigOptions = {
  commandDescriptors: readonly CommandDescriptor[]
}
export const estimateHelpOverlayHeight = (sections: readonly HelpSection[]): number => {
  const titleRows = 1
  const sectionRows = sections.reduce(
    (accumulator, section) => accumulator + 1 + 1 + section.lines.length,
    0,
  )
  const borderRows = 2
  return titleRows + sectionRows + borderRows
}
export const createHelpSections = ({ commandDescriptors }: HelpConfigOptions): HelpSection[] => {
  return [
    {
      title: 'Global',
      lines: [
        'Ctrl+G: Generate + open command palette',
        'Ctrl+T: Switch to Test Runner',
        '/help: Show this help overlay',
        'Ctrl+C: Exit (or /exit)',
        'Esc: Dismiss UI (never exits)',
      ],
    },
    {
      title: 'Quick Start',
      lines: [
        'Type natural language requests or start a command with /.',
        'Press Enter to log input; arrow keys scroll history.',
        'Type /help anytime to view keyboard shortcuts.',
        'Series: /series generates standalone atomic prompts (no cross-references); it prefills from typed/last intent (or /intent file).',
        'Tests: /test prompt-tests.yaml runs the prompt test suite.',
        'Tokens: /tokens shows token usage breakdown.',
        'Reasoning: /reasoning (or /why) shows last model reasoning.',
        'JSON: /json on|off toggles prompt payload in history.',
        'Tip: Drag & drop a file path, then press Tab to add it to context.',
        'Tip: Press Tab to open the Series intent popup.',
      ],
    },
    {
      title: 'Workflows',
      lines: [
        'Budgets: /budgets sets token limits + overflow strategy.',
        'Resume: /resume picks history or an exported payload file.',
        'Resume modes: strict (refuse missing files) · best-effort (skip missing).',
        'Export: /export writes a selected history payload to JSON/YAML.',
        'CLI-only: compose subcommand exists (shell/editor integration; not in TUI).',
      ],
    },
    {
      title: 'Generate',
      lines: ['History: ↑/↓ scroll · PgUp/PgDn page', ...formatCommandLines(commandDescriptors)],
    },
    {
      title: 'Test Runner',
      lines: [
        'Tab / Shift+Tab: Move focus',
        'Enter (File): Move to Actions',
        'Enter (Actions): Run tests',
      ],
    },
    {
      title: 'Popups',
      lines: ['Esc: Close · ↑/↓: Navigate · Enter: Confirm', 'Del/Backspace: Remove selected item'],
    },
  ]
}
</file>

<file path="docs/sidekick.nvim-prompt-maker-jsonl-fixtures.md">
# Prompt Maker JSONL + Transport Fixture Samples (for Neovim/Sidekick tests)

This file contains **copy/pasteable JSONL** streams for testing a Neovim-side client that parses Prompt Maker stream events and the interactive transport tap.

Sources of truth for schema:

- Stream event union: `src/generate/types.ts`
- Serialization: `src/generate/stream.ts`
- Transport server + `transport.error`: `src/generate/interactive-transport.ts`

Notes:

- Stream events emitted by `createStreamDispatcher(...).emit(...)` always include `timestamp`.
- The transport server’s `transport.error` is written as:
  - `{"event":"transport.error","message":"..."}\n`
  - It currently **does not include** `timestamp`.
  - Clients should tolerate this and treat it as a valid event line.

---

## Fixture A: “Happy Path” interactive transport run

This simulates:

- transport lifecycle (listening → connected)
- progress updates
- telemetry
- 2 iterations (initial + 1 refine)
- interactive loop completion
- final payload with `renderedPrompt`

```jsonl
{"event":"transport.listening","timestamp":"2026-01-03T12:00:00.000Z","path":"/tmp/pmc-nvim-12345-acde1234.sock"}
{"event":"transport.client.connected","timestamp":"2026-01-03T12:00:00.050Z","status":"connected"}

{"event":"progress.update","timestamp":"2026-01-03T12:00:00.100Z","label":"Resolving URL context","state":"start","scope":"url"}
{"event":"progress.update","timestamp":"2026-01-03T12:00:00.800Z","label":"Resolved 2 URLs","state":"stop","scope":"url"}

{"event":"progress.update","timestamp":"2026-01-03T12:00:00.900Z","label":"Scanning smart context","state":"start","scope":"smart"}
{"event":"progress.update","timestamp":"2026-01-03T12:00:01.400Z","label":"Smart context ready","state":"stop","scope":"smart"}

{"event":"context.telemetry","timestamp":"2026-01-03T12:00:01.500Z","telemetry":{"files":[{"path":"src/main.ts","tokens":820},{"path":"docs/spec.md","tokens":1410}],"intentTokens":140,"fileTokens":2230,"systemTokens":650,"totalTokens":3020}}

{"event":"generation.iteration.start","timestamp":"2026-01-03T12:00:01.600Z","iteration":1,"intent":"Draft a prompt for a code review agent","model":"gpt-4o-mini","interactive":true,"inputTokens":3020,"refinements":[]}
{"event":"generation.iteration.complete","timestamp":"2026-01-03T12:00:02.200Z","iteration":1,"prompt":"# Prompt\n\nYou are a code review agent...","tokens":620}

{"event":"interactive.state","timestamp":"2026-01-03T12:00:02.210Z","phase":"start","iteration":1}
{"event":"interactive.state","timestamp":"2026-01-03T12:00:02.220Z","phase":"prompt","iteration":1}
{"event":"interactive.awaiting","timestamp":"2026-01-03T12:00:02.230Z","mode":"transport"}

{"event":"interactive.state","timestamp":"2026-01-03T12:00:05.000Z","phase":"refine","iteration":1}

{"event":"generation.iteration.start","timestamp":"2026-01-03T12:00:05.010Z","iteration":2,"intent":"Draft a prompt for a code review agent","model":"gpt-4o-mini","interactive":true,"inputTokens":3020,"refinements":["Make it shorter and add a checklist"],"latestRefinement":"Make it shorter and add a checklist"}
{"event":"generation.iteration.complete","timestamp":"2026-01-03T12:00:05.600Z","iteration":2,"prompt":"# Prompt\n\nYou are a code review agent. Follow this checklist...","tokens":540,"reasoningTokens":120}

{"event":"interactive.state","timestamp":"2026-01-03T12:00:05.610Z","phase":"prompt","iteration":2}
{"event":"interactive.awaiting","timestamp":"2026-01-03T12:00:05.620Z","mode":"transport"}
{"event":"interactive.state","timestamp":"2026-01-03T12:00:06.200Z","phase":"complete","iteration":2}

{"event":"generation.final","timestamp":"2026-01-03T12:00:06.250Z","result":{"schemaVersion":"1","intent":"Draft a prompt for a code review agent","model":"gpt-4o-mini","targetModel":"gpt-4o-mini","prompt":"# Prompt\n\nYou are a code review agent...","refinements":["Make it shorter and add a checklist"],"iterations":2,"interactive":true,"timestamp":"2026-01-03T12:00:06.250Z","contextPaths":[{"path":"src/main.ts","source":"file"},{"path":"docs/spec.md","source":"file"}],"contextTemplate":"nvim","renderedPrompt":"## NeoVim Prompt Buffer\n\n# Prompt\n\nYou are a code review agent. Follow this checklist..."}}

{"event":"transport.client.disconnected","timestamp":"2026-01-03T12:00:06.300Z","status":"disconnected"}
```

---

## Fixture B: Transport error + recover

This simulates the server sending `transport.error` due to a bad refine instruction.

```jsonl
{"event":"transport.listening","timestamp":"2026-01-03T12:10:00.000Z","path":"/tmp/pmc-nvim-12345-acde9999.sock"}
{"event":"transport.client.connected","timestamp":"2026-01-03T12:10:00.050Z","status":"connected"}

{"event":"interactive.awaiting","timestamp":"2026-01-03T12:10:00.100Z","mode":"transport"}

{"event":"transport.error","message":"Refinement instruction must be non-empty."}

{"event":"interactive.awaiting","timestamp":"2026-01-03T12:10:02.000Z","mode":"transport"}

{"event":"generation.iteration.start","timestamp":"2026-01-03T12:10:03.000Z","iteration":2,"intent":"Draft a prompt","model":"gpt-4o-mini","interactive":true,"inputTokens":1200,"refinements":["Add explicit output format"],"latestRefinement":"Add explicit output format"}
{"event":"generation.iteration.complete","timestamp":"2026-01-03T12:10:03.500Z","iteration":2,"prompt":"# Prompt\n\nOutput format: ...","tokens":400}

{"event":"generation.final","timestamp":"2026-01-03T12:10:04.000Z","result":{"schemaVersion":"1","intent":"Draft a prompt","model":"gpt-4o-mini","targetModel":"gpt-4o-mini","prompt":"# Prompt\n\nOutput format: ...","refinements":["Add explicit output format"],"iterations":2,"interactive":true,"timestamp":"2026-01-03T12:10:04.000Z","contextPaths":[],"polishedPrompt":"# Prompt (Polished)\n\nOutput format: ..."}}
```

---

## Fixture C: Token telemetry warning/overflow scenario

This is for testing your Neovim-side guardrails when total tokens exceed thresholds.

```jsonl
{"event":"context.telemetry","timestamp":"2026-01-03T12:20:00.000Z","telemetry":{"files":[{"path":"src/bigfile.ts","tokens":41000},{"path":"docs/huge.md","tokens":22000}],"intentTokens":600,"fileTokens":63000,"systemTokens":1800,"totalTokens":65400}}
{"event":"progress.update","timestamp":"2026-01-03T12:20:00.100Z","label":"Generating...","state":"start","scope":"generate"}
```

---

## Fixture D: Upload progress (image + video)

Use this to test counters and UI gating while uploads are active.

```jsonl
{"event":"upload.state","timestamp":"2026-01-03T12:30:00.000Z","state":"start","detail":{"kind":"image","filePath":"assets/mock.png"}}
{"event":"upload.state","timestamp":"2026-01-03T12:30:00.800Z","state":"finish","detail":{"kind":"image","filePath":"assets/mock.png"}}

{"event":"upload.state","timestamp":"2026-01-03T12:30:01.000Z","state":"start","detail":{"kind":"video","filePath":"media/demo.mp4"}}
{"event":"progress.update","timestamp":"2026-01-03T12:30:01.100Z","label":"Uploading video...","state":"update","scope":"generic"}
{"event":"upload.state","timestamp":"2026-01-03T12:30:03.500Z","state":"finish","detail":{"kind":"video","filePath":"media/demo.mp4"}}
```

---

## Fixture E: Context budget trimming (`context.overflow`)

This simulates budgets being enabled (via CLI flags `--max-input-tokens`/`--max-context-tokens` or config defaults) and the CLI dropping one or more **text** context entries to satisfy the budget.

Important ordering note:

- When trimming occurs, the CLI emits `context.overflow` and then emits `context.telemetry` for the post-trim telemetry.
- `context.overflow` happens **before** any `generation.iteration.*` events.

```jsonl
{"event":"progress.update","timestamp":"2026-01-03T12:40:00.000Z","label":"Resolving context","state":"start","scope":"generic"}
{"event":"progress.update","timestamp":"2026-01-03T12:40:00.450Z","label":"Resolving context","state":"stop","scope":"generic"}

{"event":"context.overflow","timestamp":"2026-01-03T12:40:00.500Z","strategy":"drop-largest","before":{"files":[{"path":"src/core.ts","tokens":1200},{"path":"docs/huge.md","tokens":4800},{"path":"url:https://example.com","tokens":2600}],"intentTokens":220,"fileTokens":8600,"systemTokens":800,"totalTokens":9620},"after":{"files":[{"path":"src/core.ts","tokens":1200},{"path":"docs/brief.md","tokens":900}],"intentTokens":220,"fileTokens":2100,"systemTokens":800,"totalTokens":3120},"droppedPaths":[{"path":"docs/huge.md","source":"file"},{"path":"url:https://example.com","source":"url"}]}
{"event":"context.telemetry","timestamp":"2026-01-03T12:40:00.510Z","telemetry":{"files":[{"path":"src/core.ts","tokens":1200},{"path":"docs/brief.md","tokens":900}],"intentTokens":220,"fileTokens":2100,"systemTokens":800,"totalTokens":3120}}

{"event":"progress.update","timestamp":"2026-01-03T12:40:00.600Z","label":"Generating prompt","state":"start","scope":"generate"}
{"event":"generation.iteration.start","timestamp":"2026-01-03T12:40:00.610Z","iteration":1,"intent":"Draft a prompt with strict context budget","model":"gpt-4o-mini","interactive":false,"inputTokens":3120,"refinements":[]}
{"event":"generation.iteration.complete","timestamp":"2026-01-03T12:40:01.100Z","iteration":1,"prompt":"# Prompt\n\nUse only the provided context.","tokens":220}
{"event":"progress.update","timestamp":"2026-01-03T12:40:01.120Z","label":"Generating prompt","state":"stop","scope":"generate"}

{"event":"generation.final","timestamp":"2026-01-03T12:40:01.150Z","result":{"schemaVersion":"1","intent":"Draft a prompt with strict context budget","model":"gpt-4o-mini","targetModel":"gpt-4o-mini","prompt":"# Prompt\n\nUse only the provided context.","refinements":[],"iterations":1,"interactive":false,"timestamp":"2026-01-03T12:40:01.150Z","contextPaths":[{"path":"inline-intent","source":"intent"},{"path":"src/core.ts","source":"file"},{"path":"docs/brief.md","source":"file"}]}}
```

---

## Fixture F: Resume early in run (`resume.loaded`)

This simulates a run that loads a previous payload from history, reuses one context path successfully, and reports a missing context file. The `resume.loaded` event is emitted before context resolution telemetry and generation.

```jsonl
{"event":"resume.loaded","timestamp":"2026-01-03T12:50:00.000Z","source":"history","reusedContextPaths":[{"path":"notes/existing.md","source":"file"}],"missingContextPaths":[{"path":"notes/missing.md","source":"file"}]}

{"event":"progress.update","timestamp":"2026-01-03T12:50:00.050Z","label":"Resolving context","state":"start","scope":"generic"}
{"event":"progress.update","timestamp":"2026-01-03T12:50:00.250Z","label":"Resolving context","state":"stop","scope":"generic"}

{"event":"context.telemetry","timestamp":"2026-01-03T12:50:00.300Z","telemetry":{"files":[{"path":"notes/existing.md","tokens":640}],"intentTokens":160,"fileTokens":640,"systemTokens":780,"totalTokens":1580}}

{"event":"generation.iteration.start","timestamp":"2026-01-03T12:50:00.350Z","iteration":3,"intent":"Resumed intent","model":"gpt-4o-mini","interactive":false,"inputTokens":1580,"refinements":["prior refinement"]}
{"event":"generation.iteration.complete","timestamp":"2026-01-03T12:50:00.900Z","iteration":3,"prompt":"# Prompt\n\nContinue from the previous run.","tokens":260}

{"event":"generation.final","timestamp":"2026-01-03T12:50:00.950Z","result":{"schemaVersion":"1","intent":"Resumed intent","model":"gpt-4o-mini","targetModel":"gpt-4o-mini","prompt":"# Prompt\n\nContinue from the previous run.","refinements":["prior refinement"],"iterations":3,"interactive":false,"timestamp":"2026-01-03T12:50:00.950Z","contextPaths":[{"path":"inline-intent","source":"intent"},{"path":"notes/existing.md","source":"file"}]}}
```
</file>

<file path="src/__tests__/tui/popup-reducer.test.ts">
import type { PopupState } from '../../tui/types'
import { INITIAL_POPUP_MANAGER_STATE, popupReducer } from '../../tui/popup-reducer'
describe('popupReducer', () => {
  it('opens budgets popup with seeded drafts', () => {
    const next = popupReducer(INITIAL_POPUP_MANAGER_STATE, {
      type: 'open-budgets',
      maxContextTokens: 120,
      maxInputTokens: null,
      contextOverflowStrategy: 'drop-oldest',
    })
    expect(next.popupState).toEqual({
      type: 'budgets',
      selectionIndex: 0,
      maxContextTokensDraft: '120',
      maxInputTokensDraft: '',
      contextOverflowStrategyDraft: 'drop-oldest',
      errorMessage: null,
    })
    expect(next.activeScan).toBeNull()
  })
  it('opens resume popup with seeded defaults', () => {
    const next = popupReducer(INITIAL_POPUP_MANAGER_STATE, {
      type: 'open-resume',
      scanId: 123,
      sourceKind: 'history',
      mode: 'best-effort',
      payloadPathDraft: '',
      historyItems: [{ selector: 'last', title: 't', detail: 'd' }],
      historySelectionIndex: 0,
      historyErrorMessage: null,
    })
    expect(next.popupState).toEqual({
      type: 'resume',
      selectionIndex: 0,
      sourceKind: 'history',
      mode: 'best-effort',
      historyItems: [{ selector: 'last', title: 't', detail: 'd' }],
      historySelectionIndex: 0,
      historyErrorMessage: null,
      payloadPathDraft: '',
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
    expect(next.activeScan).toEqual({ kind: 'resume', id: 123 })
  })
  it('opens export popup with seeded defaults', () => {
    const next = popupReducer(INITIAL_POPUP_MANAGER_STATE, {
      type: 'open-export',
      format: 'json',
      outPathDraft: 'prompt-export.json',
      historyItems: [
        {
          selector: 'last',
          title: 't',
          detail: 'd',
          schemaVersion: 'v1',
          supported: false,
        },
      ],
      historySelectionIndex: 0,
      historyErrorMessage: null,
    })
    expect(next.popupState).toEqual({
      type: 'export',
      selectionIndex: 0,
      historyItems: [
        {
          selector: 'last',
          title: 't',
          detail: 'd',
          schemaVersion: 'v1',
          supported: false,
        },
      ],
      historySelectionIndex: 0,
      historyErrorMessage: null,
      format: 'json',
      outPathDraft: 'prompt-export.json',
    })
    expect(next.activeScan).toBeNull()
  })
  describe('scan-suggestions-success staleness gating', () => {
    it('does not apply suggestions when kind mismatches', () => {
      const state = popupReducer(INITIAL_POPUP_MANAGER_STATE, { type: 'open-file', scanId: 123 })
      const next = popupReducer(state, {
        type: 'scan-suggestions-success',
        kind: 'image',
        scanId: 123,
        suggestions: ['a'],
      })
      expect(next).toBe(state)
    })
    it('does not apply suggestions when scanId mismatches', () => {
      const state = popupReducer(INITIAL_POPUP_MANAGER_STATE, { type: 'open-file', scanId: 123 })
      const next = popupReducer(state, {
        type: 'scan-suggestions-success',
        kind: 'file',
        scanId: 999,
        suggestions: ['a'],
      })
      expect(next).toBe(state)
    })
    it('does not apply suggestions when activeScan is null', () => {
      const popupState = {
        type: 'file',
        draft: '',
        selectionIndex: 0,
        selectedFocused: false,
        suggestedItems: [],
        suggestedSelectionIndex: 0,
        suggestedFocused: false,
      } satisfies PopupState
      const state = { popupState, activeScan: null }
      const next = popupReducer(state, {
        type: 'scan-suggestions-success',
        kind: 'file',
        scanId: 123,
        suggestions: ['a'],
      })
      expect(next).toBe(state)
    })
  })
  describe('scan-suggestions-success applies suggestions and clears activeScan', () => {
    it('applies file suggestions, resets suggestion selection/focus, and clears activeScan', () => {
      const opened = popupReducer(INITIAL_POPUP_MANAGER_STATE, { type: 'open-file', scanId: 123 })
      const prepared = popupReducer(opened, {
        type: 'set',
        next: (prev) => {
          if (prev?.type !== 'file') {
            throw new Error('Expected file popup')
          }
          return {
            ...prev,
            selectionIndex: 1,
            selectedFocused: true,
            suggestedItems: ['old'],
            suggestedSelectionIndex: 2,
            suggestedFocused: true,
          }
        },
      })
      const next = popupReducer(prepared, {
        type: 'scan-suggestions-success',
        kind: 'file',
        scanId: 123,
        suggestions: ['a', 'b'],
      })
      expect(next.activeScan).toBeNull()
      expect(next.popupState?.type).toBe('file')
      if (next.popupState?.type !== 'file') {
        throw new Error('Expected file popup')
      }
      expect(next.popupState.suggestedItems).toEqual(['a', 'b'])
      expect(next.popupState.suggestedSelectionIndex).toBe(0)
      expect(next.popupState.suggestedFocused).toBe(false)
      // Ensure unrelated fields are preserved.
      expect(next.popupState.selectionIndex).toBe(1)
      expect(next.popupState.selectedFocused).toBe(true)
    })
  })
  describe("'set' preserves scan only when popup type does not change", () => {
    it('preserves activeScan when popup type stays the same', () => {
      const opened = popupReducer(INITIAL_POPUP_MANAGER_STATE, { type: 'open-file', scanId: 123 })
      const next = popupReducer(opened, {
        type: 'set',
        next: (prev) => {
          if (prev?.type !== 'file') {
            throw new Error('Expected file popup')
          }
          return { ...prev, draft: 'abc' }
        },
      })
      expect(next.activeScan).toEqual({ kind: 'file', id: 123 })
      expect(next.popupState?.type).toBe('file')
      if (next.popupState?.type !== 'file') {
        throw new Error('Expected file popup')
      }
      expect(next.popupState.draft).toBe('abc')
    })
    it('clears activeScan when popup type changes', () => {
      const opened = popupReducer(INITIAL_POPUP_MANAGER_STATE, { type: 'open-file', scanId: 123 })
      const next = popupReducer(opened, {
        type: 'set',
        next: {
          type: 'smart',
          draft: 'hi',
          suggestedItems: [],
          suggestedSelectionIndex: 0,
          suggestedFocused: false,
        },
      })
      expect(next.activeScan).toBeNull()
      expect(next.popupState?.type).toBe('smart')
    })
  })
})
</file>

<file path="src/__tests__/file-suggestions.test.ts">
import fg from 'fast-glob'
import {
  discoverDirectorySuggestions,
  discoverFileSuggestions,
  discoverIntentFileSuggestions,
  FILE_SUGGESTION_IGNORE_PATTERNS,
  filterDirectorySuggestions,
  filterFileSuggestions,
  filterIntentFileSuggestions,
} from '../tui/file-suggestions'
jest.mock('fast-glob')
const globMock = fg as unknown as jest.MockedFunction<typeof fg>
describe('file-suggestions', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    globMock.mockResolvedValue([])
  })
  it('discovers workspace files, normalizes, sorts, and limits', async () => {
    globMock.mockResolvedValue(['/repo/z.ts', '/repo/a.ts', '/other/outside.md'])
    const results = await discoverFileSuggestions({ cwd: '/repo', limit: 2 })
    expect(globMock).toHaveBeenCalledWith(
      expect.any(Array),
      expect.objectContaining({
        cwd: '/repo',
        ignore: FILE_SUGGESTION_IGNORE_PATTERNS,
        onlyFiles: true,
      }),
    )
    expect(results).toEqual(['a.ts', 'z.ts'])
  })
  it('discovers workspace directories, normalizes, sorts, and limits', async () => {
    globMock.mockResolvedValue(['/repo/z', '/repo/a', '/other/outside'])
    const results = await discoverDirectorySuggestions({ cwd: '/repo', limit: 2 })
    expect(globMock).toHaveBeenCalledWith(
      expect.any(Array),
      expect.objectContaining({
        cwd: '/repo',
        ignore: FILE_SUGGESTION_IGNORE_PATTERNS,
        onlyDirectories: true,
      }),
    )
    expect(results).toEqual(['a', 'z'])
  })
  it('filters directory suggestions using fuzzy query and exclusion', () => {
    const results = filterDirectorySuggestions({
      suggestions: ['apps/prompt-maker-cli', 'src/app', 'docs'],
      // "spm" is not a substring of "prompt-maker", but should match fuzzily.
      query: 'spm',
      exclude: ['docs'],
    })
    expect(results).toEqual(['apps/prompt-maker-cli'])
  })
  it('filters suggestions by query and excludes existing entries', () => {
    const results = filterFileSuggestions({
      suggestions: ['src/app.ts', 'docs/Guide.md', 'README.md', 'src/utils/helpers.ts'],
      query: 'src',
      exclude: ['README.md'],
    })
    expect(results).toEqual(['src/app.ts', 'src/utils/helpers.ts'])
  })
  it('supports non-contiguous fuzzy matches', () => {
    const results = filterFileSuggestions({
      suggestions: ['src/app.ts', 'docs/Guide.md', 'src/utils/helpers.ts'],
      // "suh" is not a substring; it matches "src/utils/helpers.ts" fuzzily.
      query: 'suh',
    })
    expect(results).toEqual(['src/utils/helpers.ts'])
  })
  it('normalizes absolute-path queries to workspace-relative matches', () => {
    const cwdSpy = jest.spyOn(process, 'cwd').mockReturnValue('/repo')
    try {
      const results = filterFileSuggestions({
        suggestions: ['src/tui/theme/terminal-appearance.ts'],
        query: '/repo/src/tui/theme/terminal-appearance.ts',
      })
      expect(results).toEqual(['src/tui/theme/terminal-appearance.ts'])
    } finally {
      cwdSpy.mockRestore()
    }
  })
  it('uses smart-case matching like fzf', () => {
    const lowerResults = filterFileSuggestions({
      suggestions: ['README.md', 'readme.md'],
      query: 'read',
    })
    expect(lowerResults).toEqual(['README.md', 'readme.md'])
    const upperResults = filterFileSuggestions({
      suggestions: ['README.md', 'readme.md'],
      query: 'README',
    })
    expect(upperResults).toEqual(['README.md'])
  })
  it('prefers forward matching for path-shaped queries', () => {
    const results = filterFileSuggestions({
      suggestions: ['src/theme/terminal-appearance.ts', 'src/tui/theme/terminal-appearance.ts'],
      query: 'src/tui',
    })
    expect(results[0]).toBe('src/tui/theme/terminal-appearance.ts')
  })
  it('supports fzf prefix/suffix operators', () => {
    const results = filterFileSuggestions({
      suggestions: [
        'src/tui/theme/terminal-appearance.ts',
        'xsrc/tui/theme/terminal-appearance.ts',
      ],
      query: '^src/tui/theme/terminal-appearance.ts$',
    })
    expect(results).toEqual(['src/tui/theme/terminal-appearance.ts'])
  })
  it('supports fzf exact match operator with absolute paths', () => {
    const cwdSpy = jest.spyOn(process, 'cwd').mockReturnValue('/repo')
    try {
      const results = filterFileSuggestions({
        suggestions: ['src/tui/theme/terminal-appearance.ts'],
        query: "'/repo/src/tui/theme/terminal-appearance.ts",
      })
      expect(results).toEqual(['src/tui/theme/terminal-appearance.ts'])
    } finally {
      cwdSpy.mockRestore()
    }
  })
  it('discovers intent file suggestions by scanning markdown/text files', async () => {
    globMock.mockResolvedValue(['/repo/intents/a.md', '/repo/intents/b.txt'])
    const results = await discoverIntentFileSuggestions({ cwd: '/repo', limit: 5 })
    expect(globMock).toHaveBeenCalledWith(
      expect.any(Array),
      expect.objectContaining({
        cwd: '/repo',
        ignore: FILE_SUGGESTION_IGNORE_PATTERNS,
        onlyFiles: true,
      }),
    )
    expect(results).toEqual(['intents/a.md', 'intents/b.txt'])
  })
  it('filters intent files using fuzzy token matching', () => {
    const results = filterIntentFileSuggestions({
      suggestions: [
        'tmp-uat/intent-basic.md',
        'intents/travel-app-notes.md',
        'intents/onboarding-bot.md',
      ],
      query: 'itb',
    })
    expect(results[0]).toBe('tmp-uat/intent-basic.md')
  })
  it('supports multi-token intent fuzzy search', () => {
    const results = filterIntentFileSuggestions({
      suggestions: ['intents/travel-app-notes.md', 'intents/onboarding-bot.md'],
      query: 'trav app',
    })
    expect(results).toEqual(['intents/travel-app-notes.md'])
  })
})
</file>

<file path="src/generate/args.ts">
import yargs from 'yargs'
import type { ArgumentsCamelCase } from 'yargs'
import type { ContextOverflowStrategy, GenerateArgs, StreamMode } from './types'
const VALUE_FLAGS = new Set([
  '--intent-file',
  '-f',
  '--model',
  '--target',
  '--polish-model',
  '--context',
  '-c',
  '--image',
  '--video',
  '--pdf',
  '--url',
  '--context-file',
  '--context-format',
  '--context-template',
  '--smart-context-root',
  '--max-input-tokens',
  '--max-context-tokens',
  '--context-overflow',
  '--interactive-transport',
  '--resume',
  '--resume-from',
  '--resume-mode',
  '--stream',
])
const HELP_FLAGS = new Set(['--help', '-h'])
const CONTEXT_OVERFLOW_STRATEGIES = [
  'fail',
  'drop-smart',
  'drop-url',
  'drop-largest',
  'drop-oldest',
] as const satisfies ReadonlyArray<ContextOverflowStrategy>
const RESUME_MODES = ['strict', 'best-effort'] as const
type ParsedArgs = {
  args: GenerateArgs
  showHelp: () => void
}
export const parseGenerateArgs = (argv: string[]): ParsedArgs => {
  const {
    optionArgs: rawOptionArgs,
    positionalIntent,
    positionalIntentAfterInteractive,
  } = extractIntentArg(argv)
  const { optionArgs, helpRequested } = stripHelpFlags(rawOptionArgs)
  const parser = yargs(optionArgs)
    .scriptName('prompt-maker-cli')
    .usage('Prompt Maker CLI (generate-only)\n\nUsage:\n  prompt-maker-cli [intent] [options]')
    .option('intent-file', {
      alias: 'f',
      type: 'string',
      describe: 'Read intent from file',
    })
    .option('model', {
      type: 'string',
      describe: 'Override model for generation',
    })
    .option('target', {
      type: 'string',
      describe:
        'Target/runtime model used for optimization (not included in the generated prompt text)',
    })
    .option('polish-model', {
      type: 'string',
      describe: 'Override the model used for polishing',
    })
    .option('interactive', {
      alias: 'i',
      type: 'boolean',
      default: false,
      describe: 'Enable interactive refinement loop',
    })
    .option('copy', {
      type: 'boolean',
      default: false,
      describe: 'Copy the final prompt to the clipboard',
    })
    .option('open-chatgpt', {
      type: 'boolean',
      default: false,
      describe: 'Open ChatGPT with the final prompt',
    })
    .option('polish', {
      type: 'boolean',
      default: false,
      describe: 'Run the polish pass after generation',
    })
    .option('json', {
      type: 'boolean',
      default: false,
      describe: 'Emit machine-readable JSON (non-interactive only)',
    })
    .option('quiet', {
      type: 'boolean',
      default: false,
      describe: 'Suppress interactive UI output (telemetry, banners)',
    })
    .option('progress', {
      type: 'boolean',
      default: true,
      describe: 'Show progress indicator',
    })
    .option('stream', {
      type: 'string',
      choices: ['none', 'jsonl'] as const,
      default: 'none',
      describe: 'Emit structured events via stdout',
    })
    .option('resume-last', {
      type: 'boolean',
      default: false,
      describe: 'Resume from the last history entry',
    })
    .option('resume', {
      type: 'string',
      describe: 'Resume from history selector (last, last:N, or N-th from end)',
    })
    .option('resume-from', {
      type: 'string',
      describe: 'Resume from a JSONL payload file path',
    })
    .option('resume-mode', {
      type: 'string',
      choices: RESUME_MODES,
      default: 'best-effort',
      describe: 'Resume validation mode for history/file inputs',
    })
    .option('context-template', {
      type: 'string',
      describe: 'Wrap the final prompt using a named template',
    })
    .option('interactive-transport', {
      type: 'string',
      describe: 'Listen on a local socket/pipe for interactive commands',
    })
    .option('show-context', {
      type: 'boolean',
      default: false,
      describe: 'Print resolved context files before generation',
    })
    .option('context', {
      alias: 'c',
      type: 'string',
      array: true,
      default: [],
      describe: 'Add file context via glob (repeatable)',
    })
    .option('url', {
      type: 'string',
      array: true,
      default: [],
      describe: 'Add URL context (repeatable)',
    })
    .option('image', {
      type: 'string',
      array: true,
      default: [],
      describe: 'Attach an image (repeatable)',
    })
    .option('video', {
      type: 'string',
      array: true,
      default: [],
      describe: 'Attach a video file (repeatable)',
    })
    .option('pdf', {
      type: 'string',
      array: true,
      default: [],
      describe: 'Attach a PDF file (repeatable)',
    })
    .option('context-file', {
      type: 'string',
      describe: 'Write resolved context to the specified file',
    })
    .option('context-format', {
      type: 'string',
      choices: ['text', 'json'] as const,
      default: 'text',
      describe: 'Format for --show-context or --context-file output',
    })
    .option('smart-context', {
      type: 'boolean',
      default: false,
      describe: 'Automatically attach relevant files via local embeddings',
    })
    .option('smart-context-root', {
      type: 'string',
      describe: 'Override the base directory scanned when --smart-context is enabled',
    })
    .option('max-input-tokens', {
      type: 'number',
      describe: 'Maximum allowed input tokens (intent + system + context)',
      coerce: (value: unknown) => parsePositiveIntegerFlag('--max-input-tokens', value),
    })
    .option('max-context-tokens', {
      type: 'number',
      describe: 'Maximum allowed tokens reserved for context attachments',
      coerce: (value: unknown) => parsePositiveIntegerFlag('--max-context-tokens', value),
    })
    .option('context-overflow', {
      type: 'string',
      choices: CONTEXT_OVERFLOW_STRATEGIES,
      describe: 'Strategy for resolving context token overflows',
    })
    .check((argv) => {
      if (argv.maxInputTokens !== undefined) {
        argv.maxInputTokens = parsePositiveIntegerFlag('--max-input-tokens', argv.maxInputTokens)
      }
      if (argv.maxContextTokens !== undefined) {
        argv.maxContextTokens = parsePositiveIntegerFlag(
          '--max-context-tokens',
          argv.maxContextTokens,
        )
      }
      if (argv.contextOverflow !== undefined && !isContextOverflowStrategy(argv.contextOverflow)) {
        throw new Error(
          `--context-overflow must be one of: ${CONTEXT_OVERFLOW_STRATEGIES.join(', ')}.`,
        )
      }
      if (argv.resume !== undefined) {
        argv.resume = normalizeHistorySelector('--resume', argv.resume)
      }
      if (argv.resumeFrom !== undefined) {
        argv.resumeFrom = normalizeNonEmptyStringFlag('--resume-from', argv.resumeFrom)
      }
      if (argv.resume !== undefined && argv.resumeLast) {
        throw new Error('--resume and --resume-last cannot be combined.')
      }
      if (argv.resumeFrom !== undefined && (argv.resume !== undefined || argv.resumeLast)) {
        throw new Error('--resume-from cannot be combined with --resume or --resume-last.')
      }
      if (argv.resumeMode !== undefined && !isResumeMode(argv.resumeMode)) {
        throw new Error(`--resume-mode must be one of: ${RESUME_MODES.join(', ')}.`)
      }
      return true
    })
    .help('help')
    .alias('help', 'h')
    .exitProcess(false)
    .showHelpOnFail(false)
    .parserConfiguration({ 'halt-at-non-option': true })
    .strict(false)
    .fail((msg, err) => {
      throw err ?? new Error(msg ?? 'Invalid CLI arguments.')
    })
  const parsed = parser.parseSync() as ArgumentsCamelCase<{
    intentFile?: string
    model?: string
    target?: string
    polishModel?: string
    interactive: boolean
    copy: boolean
    openChatgpt: boolean
    polish: boolean
    json: boolean
    quiet: boolean
    progress: boolean
    help?: boolean
    context: string | string[]
    contextFile?: string
    contextFormat?: 'text' | 'json'
    url: string | string[]
    image: string | string[]
    video: string | string[]
    smartContext: boolean
    smartContextRoot?: string
    showContext: boolean
    contextTemplate?: string
    interactiveTransport?: string
    stream?: StreamMode
    resumeLast: boolean
    resume?: string
    resumeFrom?: string
    resumeMode?: (typeof RESUME_MODES)[number]
    maxInputTokens?: number
    maxContextTokens?: number
    contextOverflow?: ContextOverflowStrategy
    _?: (string | number)[]
  }>
  const intent = positionalIntent ?? (typeof parsed._?.[0] === 'string' ? parsed._?.[0] : undefined)
  const resumeModeExplicit = optionArgs.some(
    (token) => token === '--resume-mode' || token.startsWith('--resume-mode='),
  )
  const resumeRequested =
    Boolean(parsed.resumeLast) ||
    Boolean(parsed.resume) ||
    Boolean(parsed.resumeFrom) ||
    resumeModeExplicit
  const args: GenerateArgs = {
    interactive: parsed.interactive ?? false,
    copy: parsed.copy ?? false,
    openChatGpt: parsed.openChatgpt ?? false,
    polish: parsed.polish ?? false,
    json: parsed.json ?? false,
    quiet: parsed.quiet ?? false,
    progress: parsed.progress ?? true,
    stream: parsed.stream ?? 'none',
    showContext: parsed.showContext ?? false,
    contextFormat: parsed.contextFormat ?? 'text',
    help: helpRequested || Boolean(parsed.help),
    ...(resumeRequested ? { resumeMode: parsed.resumeMode ?? 'best-effort' } : {}),
    ...(parsed.resume ? { resume: parsed.resume } : {}),
    ...(parsed.resumeFrom ? { resumeFrom: parsed.resumeFrom } : {}),
    ...(parsed.resumeLast ? { resumeLast: true } : {}),
    ...(parsed.maxInputTokens !== undefined ? { maxInputTokens: parsed.maxInputTokens } : {}),
    ...(parsed.maxContextTokens !== undefined ? { maxContextTokens: parsed.maxContextTokens } : {}),
    ...(parsed.contextOverflow !== undefined ? { contextOverflow: parsed.contextOverflow } : {}),
    context: normalizeListArg(parsed.context),
    urls: normalizeListArg(parsed.url),
    images: normalizeListArg(parsed.image),
    video: normalizeListArg(parsed.video),
    pdf: normalizeListArg(parsed.pdf),
    smartContext: parsed.smartContext ?? false,
    ...(parsed.contextTemplate ? { contextTemplate: parsed.contextTemplate } : {}),
    ...(parsed.interactiveTransport ? { interactiveTransport: parsed.interactiveTransport } : {}),
    ...(parsed.contextFile ? { contextFile: parsed.contextFile } : {}),
    ...(parsed.smartContextRoot ? { smartContextRoot: parsed.smartContextRoot } : {}),
  }
  if (intent) {
    args.intent = intent
    if (positionalIntentAfterInteractive) {
      args.inlineIntentAfterInteractive = true
    }
  }
  if (parsed.intentFile) {
    args.intentFile = parsed.intentFile
  }
  if (parsed.model) {
    args.model = parsed.model
  }
  if (parsed.target) {
    args.target = parsed.target
  }
  if (parsed.polishModel) {
    args.polishModel = parsed.polishModel
  }
  return {
    args,
    showHelp: () => parser.showHelp(),
  }
}
export const extractIntentArg = (
  argv: string[],
): {
  optionArgs: string[]
  positionalIntent?: string
  positionalIntentAfterInteractive?: boolean
} => {
  const optionArgs: string[] = []
  let positionalIntent: string | undefined
  let positionalIntentAfterInteractive = false
  let awaitingInteractiveIntent = false
  for (let i = 0; i < argv.length; i += 1) {
    const token = argv[i]
    if (token === undefined) {
      continue
    }
    if (token === '--') {
      optionArgs.push(...argv.slice(i))
      break
    }
    if (token.startsWith('-')) {
      optionArgs.push(token)
      awaitingInteractiveIntent = token === '-i' || token === '--interactive'
      if (VALUE_FLAGS.has(token)) {
        const next = argv[i + 1]
        if (next !== undefined) {
          optionArgs.push(next)
          i += 1
        }
        awaitingInteractiveIntent = false
      }
      continue
    }
    if (positionalIntent === undefined) {
      positionalIntent = token
      positionalIntentAfterInteractive = awaitingInteractiveIntent
      awaitingInteractiveIntent = false
      continue
    }
    awaitingInteractiveIntent = false
    optionArgs.push(token)
  }
  return positionalIntent !== undefined
    ? { optionArgs, positionalIntent, positionalIntentAfterInteractive }
    : { optionArgs }
}
export const stripHelpFlags = (
  tokens: string[],
): { optionArgs: string[]; helpRequested: boolean } => {
  if (tokens.length === 0) {
    return { optionArgs: tokens, helpRequested: false }
  }
  const sanitized: string[] = []
  let helpRequested = false
  let passthrough = false
  tokens.forEach((token) => {
    if (passthrough) {
      sanitized.push(token)
      return
    }
    if (token === '--') {
      sanitized.push(token)
      passthrough = true
      return
    }
    if (HELP_FLAGS.has(token)) {
      helpRequested = true
      return
    }
    sanitized.push(token)
  })
  return { optionArgs: sanitized, helpRequested }
}
const normalizeListArg = (value: unknown): string[] => {
  if (Array.isArray(value)) {
    return value.map((entry) => entry.toString())
  }
  if (value === undefined || value === null) {
    return []
  }
  return [value.toString()]
}
const isContextOverflowStrategy = (value: unknown): value is ContextOverflowStrategy =>
  typeof value === 'string' &&
  CONTEXT_OVERFLOW_STRATEGIES.includes(value as ContextOverflowStrategy)
const parsePositiveIntegerFlag = (flagName: string, value: unknown): number => {
  const numeric =
    typeof value === 'number'
      ? value
      : typeof value === 'string' && value.trim() !== ''
        ? Number(value)
        : Number.NaN
  if (!Number.isFinite(numeric) || !Number.isInteger(numeric) || numeric <= 0) {
    throw new Error(`${flagName} must be a positive integer.`)
  }
  return numeric
}
const normalizeNonEmptyStringFlag = (flagName: string, value: unknown): string => {
  const raw = typeof value === 'number' ? String(value) : value
  if (typeof raw !== 'string') {
    throw new Error(`${flagName} requires a string value.`)
  }
  const trimmed = raw.trim()
  if (!trimmed) {
    throw new Error(`${flagName} requires a non-empty value.`)
  }
  return trimmed
}
const normalizeHistorySelector = (flagName: string, value: unknown): string => {
  const selector = normalizeNonEmptyStringFlag(flagName, value)
  const parseOffset = (rawOffset: string | undefined): number => {
    if (!rawOffset) {
      throw new Error(
        `Invalid ${flagName} selector "${selector}". Offset must be a positive integer.`,
      )
    }
    const numeric = Number(rawOffset)
    if (!Number.isInteger(numeric) || numeric <= 0) {
      throw new Error(
        `Invalid ${flagName} selector "${selector}". Offset must be a positive integer.`,
      )
    }
    return numeric
  }
  if (selector === 'last') {
    return selector
  }
  const lastMatch = selector.match(/^last:(\d+)$/)
  if (lastMatch) {
    parseOffset(lastMatch[1])
    return selector
  }
  const numericMatch = selector.match(/^(\d+)$/)
  if (numericMatch) {
    parseOffset(numericMatch[1])
    return selector
  }
  throw new Error(
    `Invalid ${flagName} selector "${selector}". Use "last", "last:N", or "N" (N-th from end).`,
  )
}
const isResumeMode = (value: unknown): value is (typeof RESUME_MODES)[number] =>
  typeof value === 'string' && RESUME_MODES.includes(value as (typeof RESUME_MODES)[number])
</file>

<file path="src/tui/components/popups/ListPopup.tsx">
import { useMemo } from 'react'
import { Box, Text, useStdout } from 'ink'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps, type InkColorValue } from '../../theme/theme-types'
import { PopupSheet } from './PopupSheet'
import {
  buildListPopupModel,
  type ListPopupBlockModel,
  type ListPopupRowModel,
  type ListPopupSectionModel,
} from './list-popup-model'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const POPUP_MIN_HEIGHT = 10
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
export type ListPopupProps = {
  title: string
  placeholder: string
  draft: string
  items: readonly string[]
  selectedIndex: number
  emptyLabel: string
  instructions: string
  suggestedItems?: readonly string[]
  suggestedSelectionIndex?: number
  suggestedFocused?: boolean
  selectedFocused?: boolean
  layout?: 'input-first' | 'selected-first'
  maxHeight?: number
  onDraftChange: (value: string) => void
  onSubmitDraft: (value: string) => void
}
const resolveRowTextProps = (
  row: ListPopupRowModel,
  backgroundProps: ReturnType<typeof inkBackgroundColorProps>,
  theme: {
    text: InkColorValue
    mutedText: InkColorValue
  },
  focusedSelectionProps: ReturnType<typeof inkColorProps> &
    ReturnType<typeof inkBackgroundColorProps>,
  unfocusedSelectionProps: ReturnType<typeof inkColorProps> &
    ReturnType<typeof inkBackgroundColorProps>,
): ReturnType<typeof inkColorProps> & ReturnType<typeof inkBackgroundColorProps> => {
  if (row.selection === 'focused') {
    return focusedSelectionProps
  }
  if (row.selection === 'unfocused') {
    return unfocusedSelectionProps
  }
  if (row.tone === 'muted') {
    return { ...backgroundProps, ...inkColorProps(theme.mutedText) }
  }
  return { ...backgroundProps, ...inkColorProps(theme.text) }
}
export const ListPopup = ({
  title,
  placeholder,
  draft,
  items,
  selectedIndex,
  emptyLabel,
  instructions,
  suggestedItems,
  suggestedSelectionIndex,
  suggestedFocused,
  selectedFocused,
  layout = 'input-first',
  maxHeight,
  onDraftChange,
  onSubmitDraft,
}: ListPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const fallbackHeight = 16
  const popupHeight = Math.max(POPUP_MIN_HEIGHT, Math.floor(maxHeight ?? fallbackHeight))
  const focusedSelectionProps = {
    ...inkColorProps(theme.selectionText),
    ...inkBackgroundColorProps(theme.selectionBackground),
  }
  const unfocusedSelectionProps = {
    ...inkColorProps(theme.chipText),
    ...inkBackgroundColorProps(theme.chipBackground),
  }
  // Hooks must run consistently across renders (suggestions can arrive async).
  const model = useMemo(() => {
    const modelOptions: Parameters<typeof buildListPopupModel>[0] = {
      items,
      selectedIndex,
      emptyLabel,
      instructions,
      layout,
      popupHeight,
      ...(suggestedItems === undefined ? {} : { suggestedItems }),
      ...(suggestedSelectionIndex === undefined ? {} : { suggestedSelectionIndex }),
      ...(suggestedFocused === undefined ? {} : { suggestedFocused }),
      ...(selectedFocused === undefined ? {} : { selectedFocused }),
    }
    return buildListPopupModel(modelOptions)
  }, [
    emptyLabel,
    instructions,
    items,
    layout,
    popupHeight,
    selectedFocused,
    selectedIndex,
    suggestedFocused,
    suggestedItems,
    suggestedSelectionIndex,
  ])
  const renderSpacer = (key: string) => (
    <Text key={key} {...backgroundProps}>
      {padRight('', contentWidth)}
    </Text>
  )
  const renderSection = (section: ListPopupSectionModel) => {
    const containerProps =
      section.fixedRowCount === undefined
        ? { flexDirection: 'column' as const }
        : {
            flexDirection: 'column' as const,
            height: 1 + section.fixedRowCount,
            flexShrink: 0,
            overflow: 'hidden' as const,
          }
    return (
      <Box key={`section-${section.id}`} {...containerProps}>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight(section.header, contentWidth)}
        </Text>
        {section.rows.map((row) => (
          <Text
            key={row.key}
            {...resolveRowTextProps(
              row,
              backgroundProps,
              { text: theme.text, mutedText: theme.mutedText },
              focusedSelectionProps,
              unfocusedSelectionProps,
            )}
          >
            {padRight(row.label, contentWidth)}
          </Text>
        ))}
      </Box>
    )
  }
  const renderInput = (block: Extract<ListPopupBlockModel, { type: 'input' }>) => {
    if (block.input.variant === 'inline') {
      return (
        <Box key="input" flexDirection="row">
          <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
            {block.input.label}
          </Text>
          <SingleLineTextInput
            value={draft}
            onChange={onDraftChange}
            placeholder={placeholder}
            onSubmit={() => onSubmitDraft(draft)}
            focus={block.input.focus}
            width={Math.max(1, contentWidth - 'Add: '.length)}
            backgroundColor={theme.popupBackground}
          />
        </Box>
      )
    }
    return (
      <Box key="input" flexDirection="column">
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {padRight(block.input.title, contentWidth)}
        </Text>
        <SingleLineTextInput
          value={draft}
          onChange={onDraftChange}
          placeholder={placeholder}
          onSubmit={() => onSubmitDraft(draft)}
          focus={block.input.focus}
          width={contentWidth}
          backgroundColor={theme.popupBackground}
        />
      </Box>
    )
  }
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
        {padRight(title, contentWidth)}
      </Text>
      {model.blocks.map((block, index) => {
        switch (block.type) {
          case 'spacer':
            return renderSpacer(block.key)
          case 'input':
            return renderInput(block)
          case 'section':
            return renderSection(block.section)
          case 'instructions':
            return (
              <Box key={`instructions-${index}`} flexShrink={0} flexDirection="column">
                {block.lines.map((line, lineIndex) => (
                  <Text
                    key={`${lineIndex}-${line}`}
                    {...backgroundProps}
                    {...inkColorProps(theme.mutedText)}
                  >
                    {padRight(line, contentWidth)}
                  </Text>
                ))}
              </Box>
            )
        }
      })}
    </PopupSheet>
  )
}
</file>

<file path="src/tui/screens/command/hooks/useCommandGenerationPipeline.ts">
import { useRef } from 'react'
import type { BudgetSettings } from '../../../budget-settings'
import { useGenerationPipeline } from '../../../hooks/useGenerationPipeline'
import type { NotifyOptions } from '../../../notifier'
import { createTokenUsageStore } from '../../../token-usage-store'
import type { HistoryEntry, ProviderStatus } from '../../../types'
import type { TokenUsageBreakdown, TokenUsageRun } from '../../../token-usage-store'
export type UseCommandGenerationPipelineOptions = {
  pushHistory: (
    content: string,
    kind?: HistoryEntry['kind'],
    format?: HistoryEntry['format'],
  ) => void
  notify?: (message: string, options?: NotifyOptions) => void
  files: string[]
  urls: string[]
  images: string[]
  videos: string[]
  pdfs: string[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  metaInstructions: string
  budgets: BudgetSettings
  currentModel: string
  targetModel: string
  interactiveTransportPath?: string | undefined
  terminalColumns: number
  polishModelId: string | null
  jsonOutputEnabled: boolean
  copyEnabled: boolean
  chatGptEnabled: boolean
  isTestCommandRunning: boolean
  onProviderStatusUpdate: (status: ProviderStatus) => void
  onReasoningUpdate: (reasoning: string | null) => void
  onLastGeneratedPromptUpdate: (prompt: string) => void
}
export type UseCommandGenerationPipelineResult = {
  isGenerating: boolean
  runGeneration: ReturnType<typeof useGenerationPipeline>['runGeneration']
  runSeriesGeneration: ReturnType<typeof useGenerationPipeline>['runSeriesGeneration']
  statusChips: string[]
  latestContextOverflow: ReturnType<typeof useGenerationPipeline>['latestContextOverflow']
  isAwaitingRefinement: boolean
  submitRefinement: ReturnType<typeof useGenerationPipeline>['submitRefinement']
  awaitingInteractiveMode: ReturnType<typeof useGenerationPipeline>['awaitingInteractiveMode']
  tokenUsageRun: TokenUsageRun | null
  tokenUsageBreakdown: TokenUsageBreakdown | null
}
export const useCommandGenerationPipeline = ({
  pushHistory,
  notify,
  files,
  urls,
  images,
  videos,
  pdfs,
  smartContextEnabled,
  smartContextRoot,
  metaInstructions,
  budgets,
  currentModel,
  targetModel,
  interactiveTransportPath,
  terminalColumns,
  polishModelId,
  jsonOutputEnabled,
  copyEnabled,
  chatGptEnabled,
  isTestCommandRunning,
  onProviderStatusUpdate,
  onReasoningUpdate,
  onLastGeneratedPromptUpdate,
}: UseCommandGenerationPipelineOptions): UseCommandGenerationPipelineResult => {
  const tokenUsageStoreRef = useRef<ReturnType<typeof createTokenUsageStore> | null>(null)
  if (!tokenUsageStoreRef.current) {
    tokenUsageStoreRef.current = createTokenUsageStore()
  }
  const trimmedMetaInstructions = metaInstructions.trim()
  const pipeline = useGenerationPipeline({
    pushHistory,
    ...(notify ? { notify } : {}),
    files,
    urls,
    images,
    videos,
    pdfs,
    smartContextEnabled,
    smartContextRoot,
    currentModel,
    targetModel,
    ...(interactiveTransportPath ? { interactiveTransportPath } : {}),
    terminalColumns,
    metaInstructions: trimmedMetaInstructions,
    budgets,
    polishModelId,
    jsonOutputEnabled,
    copyEnabled,
    chatGptEnabled,
    isTestCommandRunning,
    tokenUsageStore: tokenUsageStoreRef.current,
    onProviderStatusUpdate,
    onReasoningUpdate,
    onLastGeneratedPromptUpdate,
  })
  const tokenUsageRun = tokenUsageStoreRef.current?.getLatestRun() ?? null
  const tokenUsageBreakdown = tokenUsageStoreRef.current?.getLatestBreakdown() ?? null
  return {
    isGenerating: pipeline.isGenerating,
    runGeneration: pipeline.runGeneration,
    runSeriesGeneration: pipeline.runSeriesGeneration,
    statusChips: pipeline.statusChips,
    latestContextOverflow: pipeline.latestContextOverflow,
    isAwaitingRefinement: pipeline.isAwaitingRefinement,
    submitRefinement: pipeline.submitRefinement,
    awaitingInteractiveMode: pipeline.awaitingInteractiveMode,
    tokenUsageRun,
    tokenUsageBreakdown,
  }
}
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenModelGeneration.ts">
import type { NotifyOptions } from '../../../notifier'
import { useCallback, useEffect, useRef, useState } from 'react'
import type { BudgetSettings } from '../../../budget-settings'
import type { HistoryEntry, ModelOption } from '../../../types'
import { DEFAULT_MODEL_ID, getPreferredModelId } from '../../../model-options'
import { resolveDefaultGenerateModel } from '../../../../prompt-generator-service'
import { useModelProviderState } from './useModelProviderState'
import { useCommandGenerationPipeline } from './useCommandGenerationPipeline'
export type UseCommandScreenModelGenerationOptions = {
  pushHistoryProxy: (
    content: string,
    kind?: HistoryEntry['kind'],
    format?: HistoryEntry['format'],
  ) => void
  notify: (message: string, options?: NotifyOptions) => void
  files: string[]
  urls: string[]
  images: string[]
  videos: string[]
  pdfs: string[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  metaInstructions: string
  budgets: BudgetSettings
  interactiveTransportPath?: string | undefined
  terminalColumns: number
  copyEnabled: boolean
  chatGptEnabled: boolean
  jsonOutputEnabled: boolean
  isTestCommandRunning: boolean
  setLastReasoning: (value: string | null) => void
  setLastGeneratedPrompt: (value: string | null) => void
}
export type UseCommandScreenModelGenerationResult = {
  modelOptions: ReturnType<typeof useModelProviderState>['modelOptions']
  currentModel: ReturnType<typeof useModelProviderState>['currentModel']
  selectModel: ReturnType<typeof useModelProviderState>['selectModel']
  polishModelId: ModelOption['id'] | null
  selectPolishModel: (nextId: ModelOption['id'] | null) => void
  currentTargetModel: ModelOption['id']
  selectTargetModel: (nextId: ModelOption['id']) => void
  providerStatuses: ReturnType<typeof useModelProviderState>['providerStatuses']
  updateProviderStatus: ReturnType<typeof useModelProviderState>['updateProviderStatus']
  pipeline: ReturnType<typeof useCommandGenerationPipeline>
}
export const useCommandScreenModelGeneration = ({
  pushHistoryProxy,
  notify,
  files,
  urls,
  images,
  videos,
  pdfs,
  smartContextEnabled,
  smartContextRoot,
  metaInstructions,
  budgets,
  interactiveTransportPath,
  terminalColumns,
  copyEnabled,
  chatGptEnabled,
  jsonOutputEnabled,
  isTestCommandRunning,
  setLastReasoning,
  setLastGeneratedPrompt,
}: UseCommandScreenModelGenerationOptions): UseCommandScreenModelGenerationResult => {
  const { modelOptions, currentModel, selectModel, providerStatuses, updateProviderStatus } =
    useModelProviderState({ pushHistory: pushHistoryProxy })
  const [polishModelId, setPolishModelIdState] = useState<ModelOption['id'] | null>(null)
  const selectPolishModel = useCallback((nextId: ModelOption['id'] | null) => {
    setPolishModelIdState((prev) => (prev === nextId ? prev : nextId))
  }, [])
  const [currentTargetModel, setCurrentTargetModelState] =
    useState<ModelOption['id']>(DEFAULT_MODEL_ID)
  const userSelectedTargetModelRef = useRef(false)
  const selectTargetModel = useCallback((nextId: ModelOption['id']) => {
    userSelectedTargetModelRef.current = true
    setCurrentTargetModelState((prev) => (prev === nextId ? prev : nextId))
  }, [])
  useEffect(() => {
    let cancelled = false
    const syncDefaultTargetModel = async (): Promise<void> => {
      if (userSelectedTargetModelRef.current) {
        return
      }
      const resolvedDefault = await resolveDefaultGenerateModel().catch(() => null)
      if (cancelled || userSelectedTargetModelRef.current) {
        return
      }
      const preferred = getPreferredModelId(modelOptions, resolvedDefault)
      setCurrentTargetModelState((prev) => (prev === preferred ? prev : preferred))
    }
    void syncDefaultTargetModel()
    return () => {
      cancelled = true
    }
  }, [modelOptions])
  const pipeline = useCommandGenerationPipeline({
    pushHistory: pushHistoryProxy,
    notify,
    files,
    urls,
    images,
    videos,
    pdfs,
    smartContextEnabled,
    smartContextRoot,
    metaInstructions,
    budgets,
    currentModel,
    targetModel: currentTargetModel,
    interactiveTransportPath,
    terminalColumns,
    polishModelId,
    jsonOutputEnabled,
    copyEnabled,
    chatGptEnabled,
    isTestCommandRunning,
    onProviderStatusUpdate: updateProviderStatus,
    onReasoningUpdate: setLastReasoning,
    onLastGeneratedPromptUpdate: setLastGeneratedPrompt,
  })
  return {
    modelOptions,
    currentModel,
    selectModel,
    polishModelId,
    selectPolishModel,
    currentTargetModel,
    selectTargetModel,
    providerStatuses,
    updateProviderStatus,
    pipeline,
  }
}
</file>

<file path="src/config.ts">
import fs from 'node:fs/promises'
import os from 'node:os'
import path from 'node:path'
import type { ModelDefinition, ModelProvider } from './model-providers'
import type { ThemeMode } from './tui/theme/theme-types'
export type ContextOverflowStrategy =
  | 'fail'
  | 'drop-smart'
  | 'drop-url'
  | 'drop-largest'
  | 'drop-oldest'
export type PromptGeneratorConfig = {
  defaultModel?: string
  defaultGeminiModel?: string
  models?: ModelDefinition[]
  maxInputTokens?: number
  maxContextTokens?: number
  contextOverflowStrategy?: ContextOverflowStrategy
}
export type TuiResumeMode = 'best-effort' | 'strict'
export type TuiResumeSourceKind = 'history' | 'file'
export type PromptMakerCliConfig = {
  openaiApiKey?: string
  openaiBaseUrl?: string
  geminiApiKey?: string
  geminiBaseUrl?: string
  promptGenerator?: PromptGeneratorConfig
  contextTemplates?: Record<string, string>
  // TUI theme settings (persisted).
  theme?: string
  themeMode?: ThemeMode
  // TUI resume defaults (persisted).
  resumeMode?: TuiResumeMode
  resumeSourceKind?: TuiResumeSourceKind
  // TUI export defaults (persisted).
  exportFormat?: 'json' | 'yaml'
  exportOutDir?: string
}
let cachedConfig: PromptMakerCliConfig | null | undefined
let cachedConfigPath: string | null | undefined
const getCandidateConfigPaths = (): string[] => {
  const explicit = process.env.PROMPT_MAKER_CLI_CONFIG?.trim()
  const home = os.homedir()
  const defaults = [
    path.join(home, '.config', 'prompt-maker-cli', 'config.json'),
    path.join(home, '.prompt-maker-cli.json'),
  ]
  return [explicit, ...defaults].filter((value): value is string => Boolean(value))
}
const getDefaultConfigPath = (): string => {
  const home = os.homedir()
  return path.join(home, '.config', 'prompt-maker-cli', 'config.json')
}
const resolveConfigPathForWrite = async (): Promise<string> => {
  const explicit = process.env.PROMPT_MAKER_CLI_CONFIG?.trim()
  if (explicit) {
    return explicit
  }
  if (cachedConfigPath) {
    return cachedConfigPath
  }
  for (const candidate of getCandidateConfigPaths()) {
    try {
      await fs.stat(candidate)
      return candidate
    } catch (error) {
      if (isFileMissingError(error)) {
        continue
      }
      const message = error instanceof Error ? error.message : 'Unknown config error.'
      throw new Error(`Failed to access config at ${candidate}: ${message}`)
    }
  }
  return getDefaultConfigPath()
}
export const loadCliConfig = async (): Promise<PromptMakerCliConfig | null> => {
  if (cachedConfig !== undefined) {
    return cachedConfig
  }
  for (const filePath of getCandidateConfigPaths()) {
    try {
      const contents = await fs.readFile(filePath, 'utf8')
      const parsed = JSON.parse(contents) as unknown
      const config = parseConfig(parsed)
      cachedConfig = config
      cachedConfigPath = filePath
      return config
    } catch (error) {
      if (isFileMissingError(error)) {
        continue
      }
      const message = error instanceof Error ? error.message : 'Unknown config error.'
      throw new Error(`Failed to load config at ${filePath}: ${message}`)
    }
  }
  cachedConfig = null
  cachedConfigPath = null
  return null
}
export const resolveOpenAiCredentials = async (): Promise<{
  apiKey: string
  baseUrl?: string
}> => {
  const envKey = process.env.OPENAI_API_KEY?.trim()
  const envBaseUrl = process.env.OPENAI_BASE_URL?.trim()
  if (envKey) {
    const credentials: { apiKey: string; baseUrl?: string } = { apiKey: envKey }
    if (envBaseUrl) {
      credentials.baseUrl = envBaseUrl
    }
    return credentials
  }
  const config = await loadCliConfig()
  const apiKey = config?.openaiApiKey?.trim()
  if (apiKey) {
    const baseUrl = config?.openaiBaseUrl?.trim()
    const credentials: { apiKey: string; baseUrl?: string } = { apiKey }
    if (baseUrl) {
      credentials.baseUrl = baseUrl
    }
    return credentials
  }
  throw new Error(
    'Missing OpenAI credentials. Set OPENAI_API_KEY or add "openaiApiKey" to ~/.config/prompt-maker-cli/config.json.',
  )
}
export const resolveGeminiCredentials = async (): Promise<{
  apiKey: string
  baseUrl?: string
}> => {
  const envKey = process.env.GEMINI_API_KEY?.trim()
  const envBaseUrl = process.env.GEMINI_BASE_URL?.trim()
  if (envKey) {
    const credentials: { apiKey: string; baseUrl?: string } = { apiKey: envKey }
    if (envBaseUrl) {
      credentials.baseUrl = envBaseUrl
    }
    return credentials
  }
  const config = await loadCliConfig()
  const apiKey = config?.geminiApiKey?.trim()
  if (apiKey) {
    const baseUrl = config?.geminiBaseUrl?.trim()
    const credentials: { apiKey: string; baseUrl?: string } = { apiKey }
    if (baseUrl) {
      credentials.baseUrl = baseUrl
    }
    return credentials
  }
  throw new Error(
    'Missing Gemini credentials. Set GEMINI_API_KEY or add "geminiApiKey" to ~/.config/prompt-maker-cli/config.json.',
  )
}
export type ThemeSettingsPatch = {
  theme?: string | null
  themeMode?: ThemeMode | null
}
export type ResumeSettingsPatch = {
  resumeMode?: TuiResumeMode | null
  resumeSourceKind?: TuiResumeSourceKind | null
}
export type ExportSettingsPatch = {
  exportFormat?: 'json' | 'yaml' | null
  exportOutDir?: string | null
}
export const updateCliThemeSettings = async (
  patch: ThemeSettingsPatch,
  options?: { configPath?: string },
): Promise<void> => {
  const configPath = options?.configPath ?? (await resolveConfigPathForWrite())
  const directory = path.dirname(configPath)
  await fs.mkdir(directory, { recursive: true })
  let raw: unknown = {}
  try {
    const contents = await fs.readFile(configPath, 'utf8')
    raw = JSON.parse(contents) as unknown
  } catch (error) {
    if (!isFileMissingError(error)) {
      const message = error instanceof Error ? error.message : 'Unknown config error.'
      throw new Error(`Failed to read config at ${configPath}: ${message}`)
    }
  }
  if (!isRecord(raw)) {
    throw new Error(`Failed to update config at ${configPath}: root must be a JSON object.`)
  }
  const next: Record<string, unknown> = { ...raw }
  if ('theme' in patch) {
    if (patch.theme === null || patch.theme === undefined || patch.theme.trim() === '') {
      delete next.theme
    } else {
      next.theme = patch.theme.trim()
    }
  }
  if ('themeMode' in patch) {
    if (patch.themeMode === null || patch.themeMode === undefined) {
      delete next.themeMode
    } else {
      next.themeMode = patch.themeMode
    }
  }
  const contents = JSON.stringify(next, null, 2)
  const tempFile = `${configPath}.${process.pid}.tmp`
  await fs.writeFile(tempFile, `${contents}\n`, 'utf8')
  try {
    await fs.rename(tempFile, configPath)
  } catch {
    await fs.writeFile(configPath, `${contents}\n`, 'utf8')
  }
  cachedConfig = parseConfig(next)
  cachedConfigPath = configPath
}
export const updateCliResumeSettings = async (
  patch: ResumeSettingsPatch,
  options?: { configPath?: string },
): Promise<void> => {
  const configPath = options?.configPath ?? (await resolveConfigPathForWrite())
  const directory = path.dirname(configPath)
  await fs.mkdir(directory, { recursive: true })
  let raw: unknown = {}
  try {
    const contents = await fs.readFile(configPath, 'utf8')
    raw = JSON.parse(contents) as unknown
  } catch (error) {
    if (!isFileMissingError(error)) {
      const message = error instanceof Error ? error.message : 'Unknown config error.'
      throw new Error(`Failed to read config at ${configPath}: ${message}`)
    }
  }
  if (!isRecord(raw)) {
    throw new Error(`Failed to update config at ${configPath}: root must be a JSON object.`)
  }
  const next: Record<string, unknown> = { ...raw }
  if ('resumeMode' in patch) {
    if (patch.resumeMode === null || patch.resumeMode === undefined) {
      delete next.resumeMode
    } else {
      next.resumeMode = expectResumeMode(patch.resumeMode, 'resumeMode')
    }
  }
  if ('resumeSourceKind' in patch) {
    if (patch.resumeSourceKind === null || patch.resumeSourceKind === undefined) {
      delete next.resumeSourceKind
    } else {
      next.resumeSourceKind = expectResumeSourceKind(patch.resumeSourceKind, 'resumeSourceKind')
    }
  }
  const contents = JSON.stringify(next, null, 2)
  const tempFile = `${configPath}.${process.pid}.tmp`
  await fs.writeFile(tempFile, `${contents}\n`, 'utf8')
  try {
    await fs.rename(tempFile, configPath)
  } catch {
    await fs.writeFile(configPath, `${contents}\n`, 'utf8')
  }
  cachedConfig = parseConfig(next)
  cachedConfigPath = configPath
}
export const updateCliExportSettings = async (
  patch: ExportSettingsPatch,
  options?: { configPath?: string },
): Promise<void> => {
  const configPath = options?.configPath ?? (await resolveConfigPathForWrite())
  const directory = path.dirname(configPath)
  await fs.mkdir(directory, { recursive: true })
  let raw: unknown = {}
  try {
    const contents = await fs.readFile(configPath, 'utf8')
    raw = JSON.parse(contents) as unknown
  } catch (error) {
    if (!isFileMissingError(error)) {
      const message = error instanceof Error ? error.message : 'Unknown config error.'
      throw new Error(`Failed to read config at ${configPath}: ${message}`)
    }
  }
  if (!isRecord(raw)) {
    throw new Error(`Failed to update config at ${configPath}: root must be a JSON object.`)
  }
  const next: Record<string, unknown> = { ...raw }
  if ('exportFormat' in patch) {
    if (patch.exportFormat === null || patch.exportFormat === undefined) {
      delete next.exportFormat
    } else {
      next.exportFormat = expectExportFormat(patch.exportFormat, 'exportFormat')
    }
  }
  if ('exportOutDir' in patch) {
    const outDir = patch.exportOutDir
    if (outDir === null || outDir === undefined || outDir.trim() === '') {
      delete next.exportOutDir
    } else {
      next.exportOutDir = outDir.trim()
    }
  }
  const contents = JSON.stringify(next, null, 2)
  const tempFile = `${configPath}.${process.pid}.tmp`
  await fs.writeFile(tempFile, `${contents}\n`, 'utf8')
  try {
    await fs.rename(tempFile, configPath)
  } catch {
    await fs.writeFile(configPath, `${contents}\n`, 'utf8')
  }
  cachedConfig = parseConfig(next)
  cachedConfigPath = configPath
}
export type PromptGeneratorSettingsPatch = {
  maxInputTokens?: number | null
  maxContextTokens?: number | null
  contextOverflowStrategy?: ContextOverflowStrategy | null
}
export const updateCliPromptGeneratorSettings = async (
  patch: PromptGeneratorSettingsPatch,
  options?: { configPath?: string },
): Promise<void> => {
  const configPath = options?.configPath ?? (await resolveConfigPathForWrite())
  const directory = path.dirname(configPath)
  await fs.mkdir(directory, { recursive: true })
  let raw: unknown = {}
  try {
    const contents = await fs.readFile(configPath, 'utf8')
    raw = JSON.parse(contents) as unknown
  } catch (error) {
    if (!isFileMissingError(error)) {
      const message = error instanceof Error ? error.message : 'Unknown config error.'
      throw new Error(`Failed to read config at ${configPath}: ${message}`)
    }
  }
  if (!isRecord(raw)) {
    throw new Error(`Failed to update config at ${configPath}: root must be a JSON object.`)
  }
  const next: Record<string, unknown> = { ...raw }
  const existingPromptGenerator = next.promptGenerator
  const promptGenerator = isRecord(existingPromptGenerator)
    ? { ...existingPromptGenerator }
    : ({} satisfies Record<string, unknown>)
  if ('maxInputTokens' in patch) {
    if (patch.maxInputTokens === null || patch.maxInputTokens === undefined) {
      delete promptGenerator.maxInputTokens
    } else {
      promptGenerator.maxInputTokens = expectPositiveInteger(
        patch.maxInputTokens,
        'promptGenerator.maxInputTokens',
      )
    }
  }
  if ('maxContextTokens' in patch) {
    if (patch.maxContextTokens === null || patch.maxContextTokens === undefined) {
      delete promptGenerator.maxContextTokens
    } else {
      promptGenerator.maxContextTokens = expectPositiveInteger(
        patch.maxContextTokens,
        'promptGenerator.maxContextTokens',
      )
    }
  }
  if ('contextOverflowStrategy' in patch) {
    if (patch.contextOverflowStrategy === null || patch.contextOverflowStrategy === undefined) {
      delete promptGenerator.contextOverflowStrategy
    } else {
      promptGenerator.contextOverflowStrategy = expectContextOverflowStrategy(
        patch.contextOverflowStrategy,
        'promptGenerator.contextOverflowStrategy',
      )
    }
  }
  if (Object.keys(promptGenerator).length === 0) {
    delete next.promptGenerator
  } else {
    next.promptGenerator = promptGenerator
  }
  const contents = JSON.stringify(next, null, 2)
  const tempFile = `${configPath}.${process.pid}.tmp`
  await fs.writeFile(tempFile, `${contents}\n`, 'utf8')
  try {
    await fs.rename(tempFile, configPath)
  } catch {
    await fs.writeFile(configPath, `${contents}\n`, 'utf8')
  }
  cachedConfig = parseConfig(next)
  cachedConfigPath = configPath
}
const parseConfig = (raw: unknown): PromptMakerCliConfig => {
  if (!isRecord(raw)) {
    throw new Error('CLI config must be a JSON object.')
  }
  const config: PromptMakerCliConfig = {}
  if (raw.openaiApiKey !== undefined) {
    config.openaiApiKey = expectString(raw.openaiApiKey, 'openaiApiKey')
  }
  if (raw.openaiBaseUrl !== undefined) {
    config.openaiBaseUrl = expectString(raw.openaiBaseUrl, 'openaiBaseUrl')
  }
  if (raw.geminiApiKey !== undefined) {
    config.geminiApiKey = expectString(raw.geminiApiKey, 'geminiApiKey')
  }
  if (raw.geminiBaseUrl !== undefined) {
    config.geminiBaseUrl = expectString(raw.geminiBaseUrl, 'geminiBaseUrl')
  }
  if (raw.promptGenerator !== undefined) {
    if (!isRecord(raw.promptGenerator)) {
      throw new Error('"promptGenerator" must be an object if provided.')
    }
    const promptGenerator: PromptGeneratorConfig = {}
    if (raw.promptGenerator.defaultModel !== undefined) {
      promptGenerator.defaultModel = expectString(
        raw.promptGenerator.defaultModel,
        'promptGenerator.defaultModel',
      )
    }
    if (raw.promptGenerator.defaultGeminiModel !== undefined) {
      promptGenerator.defaultGeminiModel = expectString(
        raw.promptGenerator.defaultGeminiModel,
        'promptGenerator.defaultGeminiModel',
      )
    }
    if (raw.promptGenerator.models !== undefined) {
      promptGenerator.models = parsePromptGeneratorModels(raw.promptGenerator.models)
    }
    if (raw.promptGenerator.maxInputTokens !== undefined) {
      promptGenerator.maxInputTokens = expectPositiveInteger(
        raw.promptGenerator.maxInputTokens,
        'promptGenerator.maxInputTokens',
      )
    }
    if (raw.promptGenerator.maxContextTokens !== undefined) {
      promptGenerator.maxContextTokens = expectPositiveInteger(
        raw.promptGenerator.maxContextTokens,
        'promptGenerator.maxContextTokens',
      )
    }
    if (raw.promptGenerator.contextOverflowStrategy !== undefined) {
      promptGenerator.contextOverflowStrategy = expectContextOverflowStrategy(
        raw.promptGenerator.contextOverflowStrategy,
        'promptGenerator.contextOverflowStrategy',
      )
    }
    config.promptGenerator = promptGenerator
  }
  if (raw.contextTemplates !== undefined) {
    if (!isRecord(raw.contextTemplates)) {
      throw new Error('"contextTemplates" must be an object if provided.')
    }
    const templates: Record<string, string> = {}
    for (const [key, value] of Object.entries(raw.contextTemplates)) {
      templates[key] = expectString(value, `contextTemplates.${key}`)
    }
    config.contextTemplates = templates
  }
  if (raw.theme !== undefined) {
    const theme = expectString(raw.theme, 'theme').trim()
    if (theme) {
      config.theme = theme
    }
  }
  if (raw.themeMode !== undefined) {
    config.themeMode = expectThemeMode(raw.themeMode, 'themeMode')
  }
  if (raw.resumeMode !== undefined) {
    config.resumeMode = expectResumeMode(raw.resumeMode, 'resumeMode')
  }
  if (raw.resumeSourceKind !== undefined) {
    config.resumeSourceKind = expectResumeSourceKind(raw.resumeSourceKind, 'resumeSourceKind')
  }
  if (raw.exportFormat !== undefined) {
    config.exportFormat = expectExportFormat(raw.exportFormat, 'exportFormat')
  }
  if (raw.exportOutDir !== undefined) {
    const exportOutDir = expectString(raw.exportOutDir, 'exportOutDir').trim()
    if (exportOutDir) {
      config.exportOutDir = exportOutDir
    }
  }
  return config
}
const parsePromptGeneratorModels = (value: unknown): ModelDefinition[] => {
  if (!Array.isArray(value)) {
    throw new Error('"promptGenerator.models" must be an array when provided.')
  }
  return value.map((entry, index) => parsePromptGeneratorModel(entry, index))
}
const parsePromptGeneratorModel = (value: unknown, index: number): ModelDefinition => {
  if (!isRecord(value)) {
    throw new Error(`promptGenerator.models[${index}] must be an object.`)
  }
  const id = expectString(value.id, `promptGenerator.models[${index}].id`).trim()
  if (!id) {
    throw new Error(`promptGenerator.models[${index}].id must not be empty.`)
  }
  const model: ModelDefinition = { id }
  if (value.label !== undefined) {
    const label = expectString(value.label, `promptGenerator.models[${index}].label`).trim()
    if (label) {
      model.label = label
    }
  }
  if (value.provider !== undefined) {
    model.provider = expectProvider(value.provider, `promptGenerator.models[${index}].provider`)
  }
  if (value.description !== undefined) {
    const description = expectString(
      value.description,
      `promptGenerator.models[${index}].description`,
    ).trim()
    if (description) {
      model.description = description
    }
  }
  if (value.notes !== undefined) {
    const notes = expectString(value.notes, `promptGenerator.models[${index}].notes`).trim()
    if (notes) {
      model.notes = notes
    }
  }
  if (value.capabilities !== undefined) {
    const capabilities = parseCapabilitiesField(
      value.capabilities,
      `promptGenerator.models[${index}].capabilities`,
    )
    if (capabilities.length > 0) {
      model.capabilities = capabilities
    }
  }
  if (value.default !== undefined) {
    model.default = expectBoolean(value.default, `promptGenerator.models[${index}].default`)
  }
  return model
}
const parseCapabilitiesField = (value: unknown, label: string): string[] => {
  if (typeof value === 'string') {
    const normalized = value.trim()
    return normalized ? [normalized] : []
  }
  if (Array.isArray(value)) {
    return value
      .map((entry, idx) => expectString(entry, `${label}[${idx}]`).trim())
      .filter((entry) => entry.length > 0)
  }
  throw new Error(`${label} must be a string or array of strings.`)
}
const CONTEXT_OVERFLOW_STRATEGIES = [
  'fail',
  'drop-smart',
  'drop-url',
  'drop-largest',
  'drop-oldest',
] as const satisfies ReadonlyArray<ContextOverflowStrategy>
function expectPositiveInteger(value: unknown, label: string): number {
  if (
    typeof value !== 'number' ||
    !Number.isFinite(value) ||
    !Number.isInteger(value) ||
    value <= 0
  ) {
    throw new Error(`${label} must be a positive integer.`)
  }
  return value
}
function expectContextOverflowStrategy(value: unknown, label: string): ContextOverflowStrategy {
  if (typeof value !== 'string') {
    throw new Error(`${label} must be one of: ${CONTEXT_OVERFLOW_STRATEGIES.join(', ')}.`)
  }
  const normalized = value.trim().toLowerCase()
  if (isContextOverflowStrategy(normalized)) {
    return normalized
  }
  throw new Error(`${label} must be one of: ${CONTEXT_OVERFLOW_STRATEGIES.join(', ')}.`)
}
function isContextOverflowStrategy(value: string): value is ContextOverflowStrategy {
  return CONTEXT_OVERFLOW_STRATEGIES.includes(value as ContextOverflowStrategy)
}
const expectBoolean = (value: unknown, label: string): boolean => {
  if (typeof value !== 'boolean') {
    throw new Error(`${label} must be a boolean.`)
  }
  return value
}
const RESUME_MODES = ['best-effort', 'strict'] as const satisfies ReadonlyArray<TuiResumeMode>
const expectResumeMode = (value: unknown, label: string): TuiResumeMode => {
  if (typeof value !== 'string') {
    throw new Error(`${label} must be one of: ${RESUME_MODES.join(', ')}.`)
  }
  const normalized = value.trim().toLowerCase()
  if ((RESUME_MODES as readonly string[]).includes(normalized)) {
    return normalized as TuiResumeMode
  }
  throw new Error(`${label} must be one of: ${RESUME_MODES.join(', ')}.`)
}
const RESUME_SOURCE_KINDS = [
  'history',
  'file',
] as const satisfies ReadonlyArray<TuiResumeSourceKind>
const expectResumeSourceKind = (value: unknown, label: string): TuiResumeSourceKind => {
  if (typeof value !== 'string') {
    throw new Error(`${label} must be one of: ${RESUME_SOURCE_KINDS.join(', ')}.`)
  }
  const normalized = value.trim().toLowerCase()
  if ((RESUME_SOURCE_KINDS as readonly string[]).includes(normalized)) {
    return normalized as TuiResumeSourceKind
  }
  throw new Error(`${label} must be one of: ${RESUME_SOURCE_KINDS.join(', ')}.`)
}
const EXPORT_FORMATS = ['json', 'yaml'] as const satisfies ReadonlyArray<'json' | 'yaml'>
const expectExportFormat = (value: unknown, label: string): 'json' | 'yaml' => {
  if (typeof value !== 'string') {
    throw new Error(`${label} must be one of: ${EXPORT_FORMATS.join(', ')}.`)
  }
  const normalized = value.trim().toLowerCase()
  if ((EXPORT_FORMATS as readonly string[]).includes(normalized)) {
    return normalized as 'json' | 'yaml'
  }
  throw new Error(`${label} must be one of: ${EXPORT_FORMATS.join(', ')}.`)
}
const expectProvider = (value: unknown, label: string): ModelProvider => {
  if (typeof value !== 'string') {
    throw new Error(`${label} must be one of openai, gemini, or other.`)
  }
  const normalized = value.trim().toLowerCase()
  if (normalized === 'openai' || normalized === 'gemini' || normalized === 'other') {
    return normalized as ModelProvider
  }
  throw new Error(`${label} must be one of openai, gemini, or other.`)
}
const expectString = (value: unknown, label: string): string => {
  if (typeof value !== 'string') {
    throw new Error(`${label} must be a string.`)
  }
  return value
}
const expectThemeMode = (value: unknown, label: string): ThemeMode => {
  if (typeof value !== 'string') {
    throw new Error(`${label} must be one of light, dark, system, or auto.`)
  }
  const normalized = value.trim().toLowerCase()
  if (normalized === 'auto') {
    return 'system'
  }
  if (normalized === 'light' || normalized === 'dark' || normalized === 'system') {
    return normalized as ThemeMode
  }
  throw new Error(`${label} must be one of light, dark, system, or auto.`)
}
function isRecord(value: unknown): value is Record<string, unknown> {
  return typeof value === 'object' && value !== null && !Array.isArray(value)
}
function hasErrnoCode(value: unknown): value is { code: string } {
  return (
    typeof value === 'object' &&
    value !== null &&
    'code' in value &&
    typeof (value as { code: unknown }).code === 'string'
  )
}
function isFileMissingError(error: unknown): boolean {
  return hasErrnoCode(error) && error.code === 'ENOENT'
}
</file>

<file path="src/__tests__/tui/command-screen-popup-bindings-contract.test.ts">
import { renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import {
  useCommandScreenPopupBindings,
  type UseCommandScreenPopupBindingsOptions,
} from '../../tui/screens/command/hooks/useCommandScreenPopupBindings'
const dom = new JSDOM('<!doctype html><html><body></body></html>')
type GlobalDom = { window: Window; document: Document }
beforeAll(() => {
  const target = globalThis as unknown as GlobalDom
  target.window = dom.window as unknown as Window
  target.document = dom.window.document
})
afterAll(() => {
  const target = globalThis as unknown as Partial<GlobalDom>
  delete target.window
  delete target.document
})
jest.mock('../../tui/screens/command/hooks/useCommandScreenPasteBindings', () => {
  const tokenLabel = jest.fn(() => null)
  const handleInputChange = jest.fn(() => undefined)
  const expandInputForSubmit = jest.fn((value: string) => value)
  return {
    useCommandScreenPasteBindings: jest.fn(() => ({
      tokenLabel,
      handleInputChange,
      expandInputForSubmit,
    })),
  }
})
jest.mock('../../tui/screens/command/hooks/useCommandScreenContextPopupBindings', () => {
  const onFilePopupDraftChange = jest.fn(() => undefined)
  const onAddFile = jest.fn(() => undefined)
  const onRemoveFile = jest.fn(() => undefined)
  const onUrlPopupDraftChange = jest.fn(() => undefined)
  const onAddUrl = jest.fn(() => undefined)
  const onRemoveUrl = jest.fn(() => undefined)
  const onImagePopupDraftChange = jest.fn(() => undefined)
  const onAddImage = jest.fn(() => undefined)
  const onRemoveImage = jest.fn(() => undefined)
  const onVideoPopupDraftChange = jest.fn(() => undefined)
  const onAddVideo = jest.fn(() => undefined)
  const onRemoveVideo = jest.fn(() => undefined)
  const onSmartPopupDraftChange = jest.fn(() => undefined)
  const onSmartRootSubmit = jest.fn(() => undefined)
  return {
    useCommandScreenContextPopupBindings: jest.fn(() => ({
      filePopupSuggestions: [],
      filePopupSuggestionSelectionIndex: 0,
      filePopupSuggestionsFocused: false,
      onFilePopupDraftChange,
      onAddFile,
      onRemoveFile,
      onUrlPopupDraftChange,
      onAddUrl,
      onRemoveUrl,
      imagePopupSuggestions: [],
      imagePopupSuggestionSelectionIndex: 0,
      imagePopupSuggestionsFocused: false,
      onImagePopupDraftChange,
      onAddImage,
      onRemoveImage,
      videoPopupSuggestions: [],
      videoPopupSuggestionSelectionIndex: 0,
      videoPopupSuggestionsFocused: false,
      onVideoPopupDraftChange,
      onAddVideo,
      onRemoveVideo,
      smartPopupSuggestions: [],
      smartPopupSuggestionSelectionIndex: 0,
      smartPopupSuggestionsFocused: false,
      onSmartPopupDraftChange,
      onSmartRootSubmit,
    })),
  }
})
jest.mock('../../tui/screens/command/hooks/useCommandScreenHistoryIntentPopupBindings', () => {
  const onHistoryPopupDraftChange = jest.fn(() => undefined)
  const onHistoryPopupSubmit = jest.fn(() => undefined)
  const onIntentPopupDraftChange = jest.fn(() => undefined)
  return {
    useCommandScreenHistoryIntentPopupBindings: jest.fn(() => ({
      history: {
        historyPopupItems: [],
        onHistoryPopupDraftChange,
        onHistoryPopupSubmit,
      },
      intent: {
        intentPopupSuggestions: [],
        intentPopupSuggestionSelectionIndex: 0,
        intentPopupSuggestionsFocused: false,
        onIntentPopupDraftChange,
      },
    })),
  }
})
jest.mock('../../tui/screens/command/hooks/useModelPopupData', () => ({
  useModelPopupData: jest.fn(() => ({
    modelPopupOptions: [],
    modelPopupRecentCount: 0,
    modelPopupSelection: 0,
  })),
}))
jest.mock('../../tui/screens/command/hooks/useReasoningPopup', () => ({
  useReasoningPopup: jest.fn(() => ({
    reasoningPopupVisibleRows: 0,
    reasoningPopupLines: [],
  })),
}))
jest.mock('../../tui/screens/command/hooks/useThemePopupGlue', () => ({
  useThemePopupGlue: jest.fn(() => ({
    themeCount: 0,
    onThemeConfirm: jest.fn(),
    onThemeCancel: jest.fn(),
  })),
}))
jest.mock('../../tui/screens/command/hooks/useThemeModePopupGlue', () => ({
  useThemeModePopupGlue: jest.fn(() => ({
    optionCount: 0,
    onConfirm: jest.fn(),
    onCancel: jest.fn(),
  })),
}))
jest.mock('../../tui/screens/command/hooks/usePopupKeyboardShortcuts', () => ({
  usePopupKeyboardShortcuts: jest.fn(),
}))
jest.mock('../../tui/screens/command/hooks/useCommandScreenSubmitBindings', () => {
  const handleSubmit = jest.fn(() => undefined)
  const onSeriesSubmit = jest.fn(() => undefined)
  return {
    useCommandScreenSubmitBindings: jest.fn(() => ({
      handleSubmit,
      onSeriesSubmit,
    })),
  }
})
jest.mock('../../tui/screens/command/hooks/useMiscPopupDraftHandlers', () => {
  const onModelPopupQueryChange = jest.fn(() => undefined)
  const onSeriesDraftChange = jest.fn(() => undefined)
  const onInstructionsDraftChange = jest.fn(() => undefined)
  const onTestDraftChange = jest.fn(() => undefined)
  const onBudgetsMaxContextTokensDraftChange = jest.fn(() => undefined)
  const onBudgetsMaxInputTokensDraftChange = jest.fn(() => undefined)
  const onResumePayloadPathDraftChange = jest.fn(() => undefined)
  const onExportOutPathDraftChange = jest.fn(() => undefined)
  return {
    useMiscPopupDraftHandlers: jest.fn(() => ({
      onModelPopupQueryChange,
      onSeriesDraftChange,
      onInstructionsDraftChange,
      onTestDraftChange,
      onBudgetsMaxContextTokensDraftChange,
      onBudgetsMaxInputTokensDraftChange,
      onResumePayloadPathDraftChange,
      onExportOutPathDraftChange,
    })),
  }
})
describe('useCommandScreenPopupBindings (shape contract)', () => {
  it('returns grouped handlers that are safe to call', () => {
    const setState =
      jest.fn() as unknown as UseCommandScreenPopupBindingsOptions['popup']['setState']
    const options: UseCommandScreenPopupBindingsOptions = {
      input: {
        value: '',
        setValue: jest.fn(),
        setPasteActive: jest.fn(),
        consumeSuppressedTextInputChange: jest.fn(() => false),
        suppressNextInput: jest.fn(),
        updateLastTypedIntent: jest.fn(),
        intentFilePath: '',
        lastUserIntentRef: { current: null },
      },
      popup: {
        state: null,
        setState,
        isOpen: false,
        helpOpen: false,
        close: jest.fn(),
        actions: {
          handleCommandSelection: jest.fn(),
          handleModelPopupSubmit: jest.fn(),
          applyToggleSelection: jest.fn(),
          handleIntentFileSubmit: jest.fn(),
          handleResumeSubmit: jest.fn(),
          handleExportSubmit: jest.fn(),
          handleBudgetsSubmit: jest.fn(),
          handleSeriesIntentSubmit: jest.fn(),
        },
      },
      menu: {
        isActive: false,
        selectedCommandId: null,
        argsRaw: '',
        isCommandMode: false,
        actions: {
          handleNewCommand: jest.fn(),
          handleReuseCommand: jest.fn(),
        },
      },
      generation: {
        isGenerating: false,
        isAwaitingRefinement: false,
        submitRefinement: jest.fn(),
        runGeneration: jest.fn(async () => undefined),
      },
      history: {
        pushHistory: jest.fn(),
        addCommandHistoryEntry: jest.fn(),
        commandHistoryValues: [],
      },
      context: {
        droppedFilePath: null,
        files: [],
        urls: [],
        images: [],
        videos: [],
        pdfs: [],
        smartContextEnabled: false,
        smartContextRoot: null,
        addFile: jest.fn(),
        removeFile: jest.fn(),
        addUrl: jest.fn(),
        removeUrl: jest.fn(),
        updateUrl: jest.fn(),
        addImage: jest.fn(),
        removeImage: jest.fn(),
        addVideo: jest.fn(),
        removeVideo: jest.fn(),
        addPdf: jest.fn(),
        removePdf: jest.fn(),
        toggleSmartContext: jest.fn(),
        setSmartRoot: jest.fn(),
        notify: jest.fn(),
        modelOptions: [],
        lastReasoning: null,
        terminalColumns: 80,
        reasoningPopupHeight: 10,
      },
    }
    const { result, rerender } = renderHook((props) => useCommandScreenPopupBindings(props), {
      initialProps: options,
    })
    expect(result.current).toEqual(
      expect.objectContaining({
        input: expect.objectContaining({
          tokenLabel: expect.any(Function),
          onChange: expect.any(Function),
        }),
        submit: expect.objectContaining({
          onSubmit: expect.any(Function),
          onSeriesSubmit: expect.any(Function),
        }),
        popup: expect.any(Object),
      }),
    )
    expect(() => result.current.input.onChange('next')).not.toThrow()
    expect(() => result.current.submit.onSubmit('run')).not.toThrow()
    expect(() => result.current.popup.misc.onInstructionsDraftChange('draft')).not.toThrow()
    expect(() => result.current.popup.context.file.onAdd('README.md')).not.toThrow()
    const firstInputChange = result.current.input.onChange
    const firstSubmit = result.current.submit.onSubmit
    const firstInstructionsDraftChange = result.current.popup.misc.onInstructionsDraftChange
    rerender(options)
    expect(result.current.input.onChange).toBe(firstInputChange)
    expect(result.current.submit.onSubmit).toBe(firstSubmit)
    expect(result.current.popup.misc.onInstructionsDraftChange).toBe(firstInstructionsDraftChange)
  })
})
</file>

<file path="src/__tests__/tui/popup-manager-command-mapping.test.ts">
import {
  JSON_INTERACTIVE_ERROR,
  mapPopupCommandSelection,
  type CommandMappingContext,
} from '../../tui/hooks/popup-manager/command-mapping'
describe('popup manager command mapping', () => {
  const baseContext: CommandMappingContext = {
    copyEnabled: false,
    chatGptEnabled: false,
    jsonOutputEnabled: false,
    interactiveTransportPath: undefined,
    urls: [],
    images: [],
    videos: [],
    pdfs: [],
    smartContextEnabled: false,
    smartContextRoot: null,
  }
  it('blocks /json when interactive transport is active', () => {
    const result = mapPopupCommandSelection({
      commandId: 'json',
      argsRaw: undefined,
      context: {
        ...baseContext,
        interactiveTransportPath: '/tmp/socket',
      },
    })
    expect(result).toEqual({
      kind: 'steps',
      steps: [
        { type: 'push-history', message: JSON_INTERACTIVE_ERROR, kind: 'system' },
        { type: 'set-input', value: '' },
      ],
    })
  })
  it('maps /url args into add/warn steps with dedupe', () => {
    const result = mapPopupCommandSelection({
      commandId: 'url',
      argsRaw: 'http://a.example, example.com http://a.example',
      context: baseContext,
    })
    if (result.kind !== 'steps') {
      throw new Error('Expected steps result')
    }
    expect(result.steps).toEqual([
      { type: 'add-url', value: 'http://a.example' },
      { type: 'push-history', message: 'Context URL added: http://a.example', kind: 'system' },
      { type: 'push-history', message: 'Warning: Invalid URL: example.com', kind: 'system' },
      { type: 'set-input', value: '' },
      { type: 'close-popup' },
    ])
  })
  it('opens budgets popup from /budgets', () => {
    const result = mapPopupCommandSelection({
      commandId: 'budgets',
      argsRaw: undefined,
      context: baseContext,
    })
    expect(result).toEqual({
      kind: 'steps',
      steps: [
        { type: 'open-popup', popup: 'budgets' },
        { type: 'set-input', value: '' },
      ],
    })
  })
  it('opens resume popup from /resume', () => {
    const result = mapPopupCommandSelection({
      commandId: 'resume',
      argsRaw: undefined,
      context: baseContext,
    })
    expect(result).toEqual({
      kind: 'steps',
      steps: [
        { type: 'open-popup', popup: 'resume' },
        { type: 'set-input', value: '' },
      ],
    })
  })
  it('opens export popup from /export', () => {
    const result = mapPopupCommandSelection({
      commandId: 'export',
      argsRaw: undefined,
      context: baseContext,
    })
    expect(result).toEqual({
      kind: 'steps',
      steps: [
        { type: 'open-popup', popup: 'export' },
        { type: 'set-input', value: '' },
      ],
    })
  })
  it('clears smart root when disabling smart context', () => {
    const result = mapPopupCommandSelection({
      commandId: 'smart',
      argsRaw: 'off',
      context: {
        ...baseContext,
        smartContextEnabled: true,
        smartContextRoot: 'src',
      },
    })
    if (result.kind !== 'steps') {
      throw new Error('Expected steps result')
    }
    expect(result.steps).toEqual([
      { type: 'set-smart-root', value: '' },
      { type: 'toggle-smart-context' },
      {
        type: 'notify',
        message: 'Smart context disabled; root cleared',
        kind: 'warning',
      },
      { type: 'set-input', value: '' },
      { type: 'close-popup' },
    ])
  })
})
</file>

<file path="src/__tests__/generate-command.test.ts">
// eslint-disable unnecessary-semicolon
import clipboard from 'clipboardy'
import open from 'open'
import { callLLM } from '@prompt-maker/core'
import { runGenerateCommand, InteractiveTransport } from '../generate-command'
import { appendToHistory } from '../history-logger'
import { readFromStdin } from '../io'
import { resolveFileContext } from '../file-context'
import { resolveSmartContextFiles } from '../smart-context-service'
import {
  createPromptGeneratorService,
  resolveDefaultGenerateModel,
  isGemini,
} from '../prompt-generator-service'
import { resolveUrlContext } from '../url-context'
import { countTokens } from '../token-counter'
jest.mock('enquirer', () => {
  const prompt = jest.fn()
  return {
    __esModule: true,
    default: { prompt },
    prompt,
  }
})
const promptMock = (jest.requireMock('enquirer') as { prompt: jest.Mock }).prompt
jest.mock('../config', () => ({
  loadCliConfig: jest.fn().mockResolvedValue({
    promptGenerator: { defaultGeminiModel: 'gemini-1.5-pro' },
  }),
}))
const mockLoadCliConfig = (jest.requireMock('../config') as { loadCliConfig: jest.Mock })
  .loadCliConfig
jest.mock('clipboardy', () => ({ write: jest.fn() }))
jest.mock('open', () => jest.fn())
jest.mock('@prompt-maker/core', () => ({ callLLM: jest.fn() }))
jest.mock('../prompt-generator-service', () => {
  const actual = jest.requireActual(
    '../prompt-generator-service',
  ) as typeof import('../prompt-generator-service')
  return {
    createPromptGeneratorService: jest.fn(),
    ensureModelCredentials: jest.fn(),
    isGemini: jest.fn((model: string) => model.startsWith('gemini')),
    resolveDefaultGenerateModel: jest.fn().mockResolvedValue('gpt-4o-mini'),
    sanitizePromptForTargetModelLeakage: actual.sanitizePromptForTargetModelLeakage,
  }
})
jest.mock('../file-context', () => ({
  resolveFileContext: jest.fn().mockResolvedValue([{ path: 'ctx.md', content: '# ctx' }]),
  formatContextForPrompt: jest.requireActual('../file-context').formatContextForPrompt,
}))
jest.mock('../smart-context-service', () => ({
  resolveSmartContextFiles: jest.fn().mockResolvedValue([]),
}))
jest.mock('../url-context', () => ({
  resolveUrlContext: jest.fn().mockResolvedValue([]),
}))
jest.mock('../history-logger', () => {
  const os = jest.requireActual('node:os') as typeof import('node:os')
  const path = jest.requireActual('node:path') as typeof import('node:path')
  return {
    appendToHistory: jest.fn().mockResolvedValue(undefined),
    resolveHistoryFilePath: jest.fn(() => {
      const envHome = process.env.HOME?.trim()
      const homeDir = envHome && envHome.length > 0 ? envHome : os.homedir()
      return path.join(homeDir, '.config', 'prompt-maker-cli', 'history.jsonl')
    }),
  }
})
jest.mock('../io', () => ({ readFromStdin: jest.fn().mockResolvedValue(null) }))
jest.mock('../image-loader', () => ({ resolveImageParts: jest.fn().mockResolvedValue([]) }))
jest.mock('../prompt-generator/video-parts', () => ({
  resolveVideoParts: jest.fn().mockResolvedValue([]),
}))
jest.mock('../prompt-generator/pdf-parts', () => ({
  resolvePdfParts: jest.fn().mockResolvedValue([]),
}))
jest.mock('../token-counter', () => ({
  countTokens: jest.fn().mockReturnValue(10),
  formatTokenCount: jest.fn((count: number) => `${count} tokens`),
}))
jest.mock('node:fs/promises', () => ({
  readFile: jest.fn(),
  stat: jest.fn(),
  writeFile: jest.fn(),
}))
const fs = jest.requireMock('node:fs/promises') as {
  readFile: jest.Mock
  stat: jest.Mock
  writeFile: jest.Mock
}
const promptService = { generatePrompt: jest.fn() }
const mockCreatePromptService = createPromptGeneratorService as jest.Mock
const mockResolveDefaultModel = resolveDefaultGenerateModel as jest.Mock
const mockResolveFileContext = resolveFileContext as jest.Mock
const mockResolveSmartContext = resolveSmartContextFiles as jest.Mock
const mockResolveUrlContext = resolveUrlContext as jest.Mock
const mockReadFromStdin = readFromStdin as jest.Mock
const mockCountTokens = countTokens as jest.Mock
const mockIsGemini = isGemini as jest.Mock
const mockCallLLM = callLLM as jest.Mock
mockCreatePromptService.mockResolvedValue(promptService)
mockResolveDefaultModel.mockResolvedValue('gpt-4o-mini')
const originalStdinIsTTY = process.stdin.isTTY
const originalStdoutIsTTY = process.stdout.isTTY
const setTtyState = (stdinTty: boolean, stdoutTty: boolean): void => {
  Object.defineProperty(process.stdin, 'isTTY', { value: stdinTty, configurable: true })
  Object.defineProperty(process.stdout, 'isTTY', { value: stdoutTty, configurable: true })
}
afterAll(() => {
  Object.defineProperty(process.stdin, 'isTTY', { value: originalStdinIsTTY })
  Object.defineProperty(process.stdout, 'isTTY', { value: originalStdoutIsTTY })
})
type TestInteractiveCommand = { type: 'refine'; instruction: string } | { type: 'finish' }
type LifecycleEmitter = Parameters<InteractiveTransport['setEventEmitter']>[0]
const setupTransportMock = (commands: TestInteractiveCommand[], events: string[]): (() => void) => {
  const commandQueue = [...commands]
  let lifecycleEmitter: LifecycleEmitter | null = null
  const startSpy = jest
    .spyOn(InteractiveTransport.prototype, 'start')
    .mockImplementation(async () => {
      lifecycleEmitter?.({ event: 'transport.listening', path: '/tmp/pmc.sock' })
      lifecycleEmitter?.({ event: 'transport.client.connected', status: 'connected' })
    })
  const stopSpy = jest
    .spyOn(InteractiveTransport.prototype, 'stop')
    .mockImplementation(async () => {
      lifecycleEmitter?.({ event: 'transport.client.disconnected', status: 'disconnected' })
    })
  const writerSpy = jest
    .spyOn(InteractiveTransport.prototype, 'getEventWriter')
    .mockReturnValue((chunk: string) => {
      events.push(chunk.trim())
    })
  const setEmitterSpy = jest
    .spyOn(InteractiveTransport.prototype, 'setEventEmitter')
    .mockImplementation(function (this: InteractiveTransport, emitter: LifecycleEmitter) {
      lifecycleEmitter = emitter
    })
  const nextCommandSpy = jest
    .spyOn(InteractiveTransport.prototype, 'nextCommand')
    .mockImplementation(async () => commandQueue.shift() ?? null)
  return () => {
    startSpy.mockRestore()
    stopSpy.mockRestore()
    writerSpy.mockRestore()
    setEmitterSpy.mockRestore()
    nextCommandSpy.mockRestore()
  }
}
describe('runGenerateCommand', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    mockCreatePromptService.mockResolvedValue(promptService)
    mockResolveDefaultModel.mockResolvedValue('gpt-4o-mini')
    mockLoadCliConfig.mockResolvedValue({
      promptGenerator: { defaultGeminiModel: 'gemini-1.5-pro' },
    })
    promptService.generatePrompt.mockResolvedValue('prompt v1')
    setTtyState(false, false)
    fs.readFile.mockReset()
    fs.stat.mockReset()
    fs.writeFile.mockReset()
    promptMock.mockReset()
    mockResolveFileContext.mockResolvedValue([{ path: 'ctx.md', content: '# ctx' }])
    mockResolveSmartContext.mockResolvedValue([])
    mockResolveUrlContext.mockResolvedValue([])
    mockReadFromStdin.mockResolvedValue(null)
    mockCountTokens.mockReturnValue(10)
  })
  it('generates a prompt with inline intent and logs output', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['Write something'])
    expect(promptService.generatePrompt).toHaveBeenCalledWith(
      expect.objectContaining({ intent: 'Write something' }),
    )
    expect(appendToHistory).toHaveBeenCalledWith(
      expect.objectContaining({ intent: 'Write something', prompt: 'prompt v1' }),
    )
    const sawPrompt = log.mock.calls.some(
      (args) => typeof args[0] === 'string' && args[0].includes('prompt v1'),
    )
    expect(sawPrompt).toBe(true)
    log.mockRestore()
  })
  it('reads intent from file when --intent-file is provided', async () => {
    fs.stat.mockResolvedValue({ size: 128 })
    fs.readFile.mockResolvedValue(Buffer.from(' file intent '))
    await runGenerateCommand(['--intent-file', 'intent.txt'])
    expect(fs.stat).toHaveBeenCalledWith('intent.txt')
    expect(promptService.generatePrompt).toHaveBeenCalledWith(
      expect.objectContaining({ intent: 'file intent' }),
    )
  })
  it('treats inline intent after -i as a file path when it exists', async () => {
    fs.stat.mockResolvedValue({ size: 256 })
    fs.readFile.mockResolvedValue(Buffer.from('interactive file intent'))
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    await runGenerateCommand(['-i', 'intent.md'])
    expect(promptService.generatePrompt).toHaveBeenCalledWith(
      expect.objectContaining({ intent: 'interactive file intent' }),
    )
    expect(warn).toHaveBeenCalledWith(expect.stringContaining('--intent-file'))
    warn.mockRestore()
  })
  it('falls back to stdin when no inline intent is provided', async () => {
    mockReadFromStdin.mockResolvedValue('stdin intent')
    await runGenerateCommand([])
    expect(promptService.generatePrompt).toHaveBeenCalledWith(
      expect.objectContaining({ intent: 'stdin intent' }),
    )
  })
  it('appends smart context files when enabled', async () => {
    mockResolveSmartContext.mockResolvedValue([{ path: 'smart.md', content: 'smart content' }])
    await runGenerateCommand(['intent', '--smart-context', '--context', 'ctx/**/*.md'])
    const call = promptService.generatePrompt.mock.calls[0][0]
    expect(mockResolveSmartContext).toHaveBeenCalled()
    expect(call.fileContext).toEqual([
      { path: 'ctx.md', content: '# ctx' },
      { path: 'smart.md', content: 'smart content' },
    ])
  })
  it('merges URL context before smart context resolution', async () => {
    mockResolveUrlContext.mockResolvedValue([
      { path: 'url:https://example.com', content: 'Example Domain' },
    ])
    mockResolveSmartContext.mockResolvedValue([{ path: 'smart.md', content: 'smart content' }])
    await runGenerateCommand(['intent text', '--url', 'https://example.com', '--smart-context'])
    const smartCallArgs = mockResolveSmartContext.mock.calls[0]
    expect(smartCallArgs[1]).toEqual([
      { path: 'ctx.md', content: '# ctx' },
      { path: 'url:https://example.com', content: 'Example Domain' },
    ])
    expect(smartCallArgs[3]).toBeUndefined()
    const call = promptService.generatePrompt.mock.calls[0][0]
    expect(call.fileContext).toEqual([
      { path: 'ctx.md', content: '# ctx' },
      { path: 'url:https://example.com', content: 'Example Domain' },
      { path: 'smart.md', content: 'smart content' },
    ])
  })
  it('switches to gemini model when video assets provided', async () => {
    mockIsGemini.mockImplementation((model: string) => model.startsWith('gemini'))
    mockLoadCliConfig.mockResolvedValueOnce(null)
    const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--video', 'clip.mp4'])
    const call = promptService.generatePrompt.mock.calls[0][0]
    expect(call.model).toBe('gemini-2.5-pro')
    expect(warn).toHaveBeenCalledWith('Switching to gemini-2.5-pro to support video input.')
    warn.mockRestore()
  })
  it('passes smart context root through when provided', async () => {
    mockResolveSmartContext.mockResolvedValue([{ path: 'smart.md', content: 'content' }])
    await runGenerateCommand(['intent text', '--smart-context', '--smart-context-root', 'apps'])
    const smartCallArgs = mockResolveSmartContext.mock.calls[0]
    expect(smartCallArgs[3]).toBe('apps')
  })
  it('prints context files when --show-context is provided', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--show-context'])
    const sawContextDump = log.mock.calls.some(
      (args) => typeof args[0] === 'string' && args[0]?.includes('<file path="ctx.md">'),
    )
    expect(sawContextDump).toBe(true)
    log.mockRestore()
  })
  it('prints json context when --show-context and --context-format json are provided', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--show-context', '--context-format', 'json'])
    const jsonCall = log.mock.calls.find(
      (args) => typeof args[0] === 'string' && args[0].trim().startsWith('['),
    )
    expect(jsonCall).toBeDefined()
    log.mockRestore()
  })
  it('writes resolved context to a file when --context-file is provided', async () => {
    fs.writeFile.mockResolvedValue(undefined)
    await runGenerateCommand(['intent text', '--context-file', 'ctx.out'])
    expect(fs.writeFile).toHaveBeenCalledWith(
      'ctx.out',
      expect.stringContaining('<file path="ctx.md">'),
      'utf8',
    )
  })
  it('writes json context when --context-file and --context-format json are used', async () => {
    fs.writeFile.mockResolvedValue(undefined)
    await runGenerateCommand([
      'intent text',
      '--context-file',
      'ctx.json',
      '--context-format',
      'json',
    ])
    const [, payload] = fs.writeFile.mock.calls[0]
    expect(typeof payload).toBe('string')
    expect(payload.trim().startsWith('[')).toBe(true)
  })
  it('runs interactive refinements when tty is present', async () => {
    setTtyState(true, true)
    promptService.generatePrompt
      .mockResolvedValueOnce('first prompt')
      .mockResolvedValueOnce('second prompt')
    promptMock
      .mockResolvedValueOnce({ refine: true })
      .mockResolvedValueOnce({ refinement: 'Refine tone' })
      .mockResolvedValueOnce({ refine: false })
    await runGenerateCommand(['intent text', '--interactive'])
    expect(promptService.generatePrompt).toHaveBeenCalledTimes(2)
    expect(promptService.generatePrompt).toHaveBeenLastCalledWith(
      expect.objectContaining({
        refinementInstruction: 'Refine tone',
        previousPrompt: 'first prompt',
      }),
    )
  })
  it('polishes prompt and copies/open as requested', async () => {
    mockCallLLM.mockResolvedValue('polished prompt')
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--polish', '--copy', '--open-chatgpt'])
    expect(callLLM).toHaveBeenCalledWith(expect.any(Array), 'gpt-4o-mini')
    expect(clipboard.write).toHaveBeenCalledWith('polished prompt')
    expect(open).toHaveBeenCalledWith(expect.stringContaining('https://chatgpt.com'))
    log.mockRestore()
  })
  it('sanitizes polished output to avoid leaking --target', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    mockCallLLM.mockResolvedValue(
      'Target runtime model: gpt-4o-mini\nPolished output for gpt-4o-mini.',
    )
    await runGenerateCommand(['intent text', '--polish', '--json', '--target', 'gpt-4o-mini'])
    const firstCall = log.mock.calls[0]
    if (!firstCall) {
      throw new Error('Expected JSON output')
    }
    const payload = JSON.parse(firstCall[0] as string) as {
      polishedPrompt?: string
      targetModel: string
    }
    expect(payload.targetModel).toBe('gpt-4o-mini')
    expect(payload.polishedPrompt).toBeDefined()
    expect(payload.polishedPrompt?.toLowerCase()).not.toContain('target runtime model')
    expect(payload.polishedPrompt?.toLowerCase()).not.toContain('gpt-4o-mini')
    log.mockRestore()
  })
  it('emits json payload when --json is provided', async () => {
    jest.useFakeTimers().setSystemTime(new Date('2024-01-01T00:00:00Z'))
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--json'])
    expect(log).toHaveBeenCalled()
    const firstCall = log.mock.calls[0]
    if (!firstCall) {
      throw new Error('Expected console.log to be called with JSON output')
    }
    const payload = JSON.parse(firstCall[0] as string) as {
      intent: string
      targetModel: string
      contextPaths: Array<{ path: string; source: string }>
    }
    expect(payload.intent).toBe('intent text')
    expect(payload.targetModel).toBe('gpt-4o-mini')
    expect(payload.contextPaths).toEqual(
      expect.arrayContaining([
        expect.objectContaining({ source: 'intent', path: 'inline-intent' }),
        expect.objectContaining({ source: 'file', path: 'ctx.md' }),
      ]),
    )
    expect(payload).not.toHaveProperty('outputPath')
    expect(appendToHistory).toHaveBeenCalledTimes(1)
    jest.useRealTimers()
    log.mockRestore()
  })
  it('uses explicit --target and defaults separately from --model', async () => {
    jest.useFakeTimers().setSystemTime(new Date('2024-01-01T00:00:00Z'))
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand([
      'intent text',
      '--model',
      'gpt-4o',
      '--target',
      'gpt-4o-mini',
      '--json',
    ])
    const firstCall = log.mock.calls[0]
    if (!firstCall) {
      throw new Error('Expected JSON output')
    }
    const payload = JSON.parse(firstCall[0] as string) as { model: string; targetModel: string }
    expect(payload.model).toBe('gpt-4o')
    expect(payload.targetModel).toBe('gpt-4o-mini')
    const generationCall = promptService.generatePrompt.mock.calls[0]?.[0]
    expect(generationCall).toEqual(
      expect.objectContaining({ model: 'gpt-4o', targetModel: 'gpt-4o-mini' }),
    )
    jest.useRealTimers()
    log.mockRestore()
  })
  it('streams jsonl events when enabled', async () => {
    const chunks: string[] = []
    const writeSpy = jest.spyOn(process.stdout, 'write').mockImplementation(((
      chunk: string | Uint8Array,
      encoding?: BufferEncoding,
      cb?: (err?: Error) => void,
    ) => {
      if (typeof chunk === 'string') {
        chunks.push(chunk)
      }
      if (typeof cb === 'function') {
        cb()
      }
      return true
    }) as unknown as typeof process.stdout.write)
    await runGenerateCommand(['intent text', '--stream', 'jsonl', '--progress=false'])
    writeSpy.mockRestore()
    const events = chunks
      .map((chunk) => chunk.trim())
      .filter((chunk) => chunk.startsWith('{') && chunk.endsWith('}'))
      .map((chunk) => JSON.parse(chunk) as { event: string } & Record<string, unknown>)
    const eventTypes = events.map((event) => event.event)
    expect(eventTypes).toContain('context.telemetry')
    expect(eventTypes).toContain('generation.iteration.start')
    expect(eventTypes).toContain('generation.iteration.complete')
    expect(eventTypes).toContain('generation.final')
    const generateStart = events.find((event) => {
      if (event.event !== 'progress.update') {
        return false
      }
      return (event as { label?: string }).label === 'Generating prompt'
    }) as { state?: string; scope?: string } | undefined
    expect(generateStart?.state).toBe('start')
    expect(generateStart?.scope).toBe('generate')
    const iterationStart = events.find((event) => event.event === 'generation.iteration.start') as
      | { inputTokens?: number }
      | undefined
    expect(iterationStart?.inputTokens).toBeGreaterThan(0)
    const finalEvent = events.find((event) => event.event === 'generation.final') as
      | { result?: { schemaVersion?: string; contextPaths?: Array<{ source: string }> } }
      | undefined
    expect(finalEvent?.result?.schemaVersion).toBe('1')
    expect(finalEvent?.result?.contextPaths).toEqual(
      expect.arrayContaining([expect.objectContaining({ source: 'intent' })]),
    )
  })
  it('emits context.overflow and prunes contextPaths when budget exceeded', async () => {
    mockResolveFileContext.mockResolvedValueOnce([
      { path: 'ctx1.md', content: 'c1' },
      { path: 'ctx2.md', content: 'c2' },
      { path: 'ctx3.md', content: 'c3' },
    ])
    mockCountTokens.mockImplementation((value: string) => {
      if (value === 'c1' || value === 'c2' || value === 'c3') {
        return 10
      }
      return 0
    })
    const chunks: string[] = []
    const writeSpy = jest.spyOn(process.stdout, 'write').mockImplementation(((
      chunk: string | Uint8Array,
      encoding?: BufferEncoding,
      cb?: (err?: Error) => void,
    ) => {
      if (typeof chunk === 'string') {
        chunks.push(chunk)
      }
      if (typeof cb === 'function') {
        cb()
      }
      return true
    }) as unknown as typeof process.stdout.write)
    await runGenerateCommand([
      'intent text',
      '--stream',
      'jsonl',
      '--progress=false',
      '--max-context-tokens',
      '10',
      '--context-overflow',
      'drop-oldest',
    ])
    writeSpy.mockRestore()
    const events = chunks
      .map((chunk) => chunk.trim())
      .filter((chunk) => chunk.startsWith('{') && chunk.endsWith('}'))
      .map((chunk) => JSON.parse(chunk) as { event: string } & Record<string, unknown>)
    const overflowEvent = events.find((event) => event.event === 'context.overflow') as
      | {
          strategy?: string
          before?: { fileTokens?: number }
          after?: { fileTokens?: number }
          droppedPaths?: Array<{ path: string; source: string }>
        }
      | undefined
    expect(overflowEvent?.strategy).toBe('drop-oldest')
    expect(overflowEvent?.before?.fileTokens).toBe(30)
    expect(overflowEvent?.after?.fileTokens).toBe(10)
    expect(overflowEvent?.droppedPaths).toEqual([
      { path: 'ctx1.md', source: 'file' },
      { path: 'ctx2.md', source: 'file' },
    ])
    const finalEvent = events.find((event) => event.event === 'generation.final') as
      | {
          result?: {
            schemaVersion?: string
            contextPaths?: Array<{ path: string; source: string }>
          }
        }
      | undefined
    expect(finalEvent?.result?.schemaVersion).toBe('1')
    expect(finalEvent?.result?.contextPaths).toEqual(
      expect.arrayContaining([
        { path: 'inline-intent', source: 'intent' },
        { path: 'ctx3.md', source: 'file' },
      ]),
    )
    const droppedInFinal = finalEvent?.result?.contextPaths?.filter((entry) =>
      ['ctx1.md', 'ctx2.md'].includes(entry.path),
    )
    expect(droppedInFinal).toEqual([])
  })
  it('emits only jsonl lines when quiet streaming is requested', async () => {
    const chunks: string[] = []
    const writeSpy = jest.spyOn(process.stdout, 'write').mockImplementation(((
      chunk: string | Uint8Array,
      encoding?: BufferEncoding,
      cb?: (err?: Error) => void,
    ) => {
      if (typeof chunk === 'string') {
        chunks.push(chunk)
      }
      if (typeof cb === 'function') {
        cb()
      }
      return true
    }) as unknown as typeof process.stdout.write)
    const logSpy = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--quiet', '--stream', 'jsonl', '--progress=false'])
    writeSpy.mockRestore()
    expect(logSpy).not.toHaveBeenCalled()
    logSpy.mockRestore()
    const jsonLines = chunks.map((chunk) => chunk.trim()).filter(Boolean)
    expect(jsonLines.length).toBeGreaterThan(0)
    jsonLines.forEach((line) => {
      expect(() => JSON.parse(line)).not.toThrow()
    })
  })
  it('suppresses UI banners when --quiet is provided', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--quiet'])
    expect(log).not.toHaveBeenCalled()
    log.mockRestore()
  })
  it('still prints JSON payload when --quiet and --json are combined', async () => {
    jest.useFakeTimers().setSystemTime(new Date('2024-01-01T00:00:00Z'))
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--quiet', '--json'])
    expect(log).toHaveBeenCalledTimes(1)
    expect(log).toHaveBeenCalledWith(expect.stringContaining('"schemaVersion": "1"'))
    expect(log).toHaveBeenCalledWith(expect.stringContaining('"intent": "intent text"'))
    jest.useRealTimers()
    log.mockRestore()
  })
  it('includes url and smart context metadata in json output', async () => {
    mockResolveUrlContext.mockResolvedValueOnce([
      { path: 'url:https://example.com', content: '# url' },
    ])
    mockResolveSmartContext.mockResolvedValueOnce([{ path: 'smart.md', content: '# smart' }])
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand([
      'intent text',
      '--json',
      '--url',
      'https://example.com',
      '--smart-context',
    ])
    const firstCall = log.mock.calls[0]
    if (!firstCall) {
      throw new Error('Expected JSON output to be logged')
    }
    const payload = JSON.parse(firstCall[0] as string) as {
      schemaVersion?: string
      contextPaths: Array<{ path: string; source: string }>
    }
    expect(payload.schemaVersion).toBe('1')
    expect(payload.contextPaths).toEqual(
      expect.arrayContaining([
        expect.objectContaining({ source: 'url', path: 'url:https://example.com' }),
        expect.objectContaining({ source: 'smart', path: 'smart.md' }),
      ]),
    )
    log.mockRestore()
  })
  it('records outputPath when writing context file in json mode', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--json', '--context-file', '/tmp/out.json'])
    const firstCall = log.mock.calls[0]
    if (!firstCall) {
      throw new Error('Expected JSON output to be logged')
    }
    const payload = JSON.parse(firstCall[0] as string) as {
      schemaVersion?: string
      outputPath?: string
    }
    expect(payload.schemaVersion).toBe('1')
    expect(payload.outputPath).toBe('/tmp/out.json')
    log.mockRestore()
  })
  it('copies to clipboard without emitting cosmetic logs when quiet', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--quiet', '--copy'])
    expect(clipboard.write).toHaveBeenCalledWith('prompt v1')
    const copiedMessages = log.mock.calls
      .map((args) => args[0])
      .filter((arg) => typeof arg === 'string' && arg.includes('Copied prompt'))
    expect(copiedMessages).toHaveLength(0)
    log.mockRestore()
  })
  it('opens ChatGPT silently when quiet mode suppresses success ticks', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--quiet', '--open-chatgpt'])
    expect(open).toHaveBeenCalled()
    const openMessages = log.mock.calls
      .map((args) => args[0])
      .filter((arg) => typeof arg === 'string' && arg.includes('Opened ChatGPT'))
    expect(openMessages).toHaveLength(0)
    log.mockRestore()
  })
  it('drives interactive refinements via transport commands without TTY', async () => {
    promptService.generatePrompt
      .mockResolvedValueOnce('first prompt')
      .mockResolvedValueOnce('second prompt')
    const transportEvents: string[] = []
    const restoreTransport = setupTransportMock(
      [{ type: 'refine', instruction: 'Tighten tone' }, { type: 'finish' }],
      transportEvents,
    )
    try {
      await runGenerateCommand(['intent text', '--interactive-transport', '/tmp/pmc.sock'])
    } finally {
      restoreTransport()
    }
    const parsedEvents = transportEvents
      .map((event) => event.trim())
      .filter((event) => event.startsWith('{') && event.endsWith('}'))
      .map((event) => JSON.parse(event) as { event: string })
    expect(promptMock).not.toHaveBeenCalled()
    expect(promptService.generatePrompt).toHaveBeenCalledTimes(2)
    expect(parsedEvents.some((event) => event.event === 'transport.listening')).toBe(true)
    expect(parsedEvents.some((event) => event.event === 'interactive.awaiting')).toBe(true)
    expect(parsedEvents.some((event) => event.event === 'interactive.state')).toBe(true)
  })
  it('ends interactive transport sessions when finish command arrives first', async () => {
    const transportEvents: string[] = []
    const restoreTransport = setupTransportMock([{ type: 'finish' }], transportEvents)
    try {
      await runGenerateCommand(['intent text', '--interactive-transport', '/tmp/pmc.sock'])
    } finally {
      restoreTransport()
    }
    const parsedEvents = transportEvents
      .map((event) => event.trim())
      .filter((event) => event.startsWith('{') && event.endsWith('}'))
      .map((event) => JSON.parse(event) as { event: string })
    expect(promptService.generatePrompt).toHaveBeenCalledTimes(1)
    expect(promptMock).not.toHaveBeenCalled()
    expect(parsedEvents.some((event) => event.event === 'transport.listening')).toBe(true)
  })
  it('throws when an unknown context template is provided', async () => {
    await expect(
      runGenerateCommand(['intent text', '--context-template', 'missing']),
    ).rejects.toThrow('Unknown context template')
  })
  it('applies the built-in nvim context template and surfaces metadata', async () => {
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--json', '--context-template', 'nvim'])
    expect(log).toHaveBeenCalled()
    const firstCall = log.mock.calls[0]
    if (!firstCall) {
      throw new Error('console.log was not called')
    }
    const [jsonOutput] = firstCall as [string]
    const payload = JSON.parse(jsonOutput) as {
      schemaVersion?: string
      contextTemplate?: string
      renderedPrompt?: string
    }
    expect(payload.schemaVersion).toBe('1')
    expect(payload.contextTemplate).toBe('nvim')
    expect(payload.renderedPrompt).toContain('NeoVim Prompt Buffer')
    expect(payload.renderedPrompt).toContain('prompt v1')
    log.mockRestore()
  })
  it('uses user-defined templates from config and appends prompt when placeholder is missing', async () => {
    mockLoadCliConfig.mockResolvedValue({
      promptGenerator: { defaultGeminiModel: 'gemini-1.5-pro' },
      contextTemplates: { scratch: 'Paste into scratch buffer for review' },
    })
    const log = jest.spyOn(console, 'log').mockImplementation(() => undefined)
    await runGenerateCommand(['intent text', '--json', '--context-template', 'scratch'])
    expect(log).toHaveBeenCalled()
    const firstCall = log.mock.calls[0]
    if (!firstCall) {
      throw new Error('console.log was not called')
    }
    const [jsonOutput] = firstCall as [string]
    const payload = JSON.parse(jsonOutput) as { schemaVersion?: string; renderedPrompt?: string }
    expect(payload.schemaVersion).toBe('1')
    expect(payload.renderedPrompt).toContain('Paste into scratch buffer for review')
    expect(payload.renderedPrompt?.trim().endsWith('prompt v1')).toBe(true)
    log.mockRestore()
  })
  it('resumes from history in best-effort mode and emits resume.loaded', async () => {
    const originalHome = process.env.HOME
    process.env.HOME = '/tmp/pmc-resume-home'
    try {
      const historyPath = `${process.env.HOME}/.config/prompt-maker-cli/history.jsonl`
      const resumedPayload = {
        schemaVersion: '1',
        intent: 'resumed intent',
        model: 'gpt-4o-mini',
        targetModel: 'gpt-4o-mini',
        prompt: 'previous prompt',
        polishedPrompt: 'previous polished',
        polishModel: 'polish-model',
        metaInstructions: 'Be concise',
        refinements: ['prior refinement'],
        iterations: 2,
        interactive: true,
        timestamp: '2025-01-01T00:00:00.000Z',
        contextPaths: [
          { path: 'existing.md', source: 'file' },
          { path: 'missing.md', source: 'file' },
          { path: 'url:https://example.com', source: 'url' },
        ],
      }
      fs.readFile.mockImplementation(async (filePath: string) => {
        if (filePath === historyPath) {
          return `${JSON.stringify(resumedPayload)}\n`
        }
        if (filePath === 'existing.md') {
          return 'existing content'
        }
        const error = Object.assign(new Error('ENOENT'), { code: 'ENOENT' })
        throw error
      })
      mockReadFromStdin.mockResolvedValue(null)
      const chunks: string[] = []
      const writeSpy = jest.spyOn(process.stdout, 'write').mockImplementation(((
        chunk: string | Uint8Array,
        encoding?: BufferEncoding,
        cb?: (err?: Error) => void,
      ) => {
        if (typeof chunk === 'string') {
          chunks.push(chunk)
        }
        if (typeof cb === 'function') {
          cb()
        }
        return true
      }) as unknown as typeof process.stdout.write)
      const warn = jest.spyOn(console, 'warn').mockImplementation(() => undefined)
      await runGenerateCommand([
        '',
        '--resume-last',
        '--resume-mode',
        'best-effort',
        '--stream',
        'jsonl',
        '--quiet',
        '--progress=false',
      ])
      writeSpy.mockRestore()
      const events = chunks
        .join('')
        .split('\n')
        .map((line) => line.trim())
        .filter(Boolean)
        .map((line) => JSON.parse(line) as { event: string } & Record<string, unknown>)
      const resumeEvent = events.find((event) => event.event === 'resume.loaded') as
        | {
            reusedContextPaths?: Array<{ path: string; source: string }>
            missingContextPaths?: Array<{ path: string; source: string }>
          }
        | undefined
      expect(resumeEvent).toBeDefined()
      expect(resumeEvent?.reusedContextPaths).toEqual([{ path: 'existing.md', source: 'file' }])
      expect(resumeEvent?.missingContextPaths).toEqual(
        expect.arrayContaining([
          { path: 'missing.md', source: 'file' },
          { path: 'url:https://example.com', source: 'url' },
        ]),
      )
      expect(warn).toHaveBeenCalledWith(expect.stringContaining('Resume skipped missing context'))
      warn.mockRestore()
      expect(promptService.generatePrompt).toHaveBeenCalledTimes(0)
      expect(appendToHistory).toHaveBeenCalledWith(
        expect.objectContaining({ intent: 'resumed intent', prompt: 'previous polished' }),
      )
    } finally {
      if (originalHome === undefined) {
        delete process.env.HOME
      } else {
        process.env.HOME = originalHome
      }
    }
  })
  it('fails in strict resume mode when a context path is missing', async () => {
    const originalHome = process.env.HOME
    process.env.HOME = '/tmp/pmc-resume-home'
    try {
      const historyPath = `${process.env.HOME}/.config/prompt-maker-cli/history.jsonl`
      const resumedPayload = {
        schemaVersion: '1',
        intent: 'resumed intent',
        model: 'gpt-4o-mini',
        targetModel: 'gpt-4o-mini',
        prompt: 'previous prompt',
        refinements: [],
        iterations: 1,
        interactive: false,
        timestamp: '2025-01-01T00:00:00.000Z',
        contextPaths: [
          { path: 'existing.md', source: 'file' },
          { path: 'missing.md', source: 'file' },
        ],
      }
      fs.readFile.mockImplementation(async (filePath: string) => {
        if (filePath === historyPath) {
          return `${JSON.stringify(resumedPayload)}\n`
        }
        if (filePath === 'existing.md') {
          return 'existing content'
        }
        const error = Object.assign(new Error('ENOENT'), { code: 'ENOENT' })
        throw error
      })
      mockReadFromStdin.mockResolvedValue(null)
      await expect(
        runGenerateCommand(['', '--resume-last', '--resume-mode', 'strict', '--quiet']),
      ).rejects.toThrow(/Missing required resumed context file/i)
      expect(appendToHistory).not.toHaveBeenCalled()
    } finally {
      if (originalHome === undefined) {
        delete process.env.HOME
      } else {
        process.env.HOME = originalHome
      }
    }
  })
  it('throws when --json and --interactive are combined', async () => {
    await expect(runGenerateCommand(['intent text', '--json', '--interactive'])).rejects.toThrow(
      '--json cannot be combined with --interactive.',
    )
  })
})
</file>

<file path="src/tui/components/popups/ModelPopup.tsx">
import { useMemo } from 'react'
import { Box, Text, useStdout } from 'ink'
import { SingleLineTextInput } from '../core/SingleLineTextInput'
import { PopupSheet } from './PopupSheet'
import { MODEL_PROVIDER_LABELS } from '../../../model-providers'
import { useTheme } from '../../theme/theme-provider'
import { inkBackgroundColorProps, inkColorProps } from '../../theme/theme-types'
import type { InkColorValue } from '../../theme/theme-types'
import type { ModelOption, ProviderStatusMap } from '../../types'
import {
  buildModelPopupRows,
  resolveModelPopupListRows,
  resolveModelPopupVisibleRows,
} from './model-popup-model'
const clamp = (value: number, min: number, max: number): number =>
  Math.max(min, Math.min(value, max))
const POPUP_PADDING_X = 2
const POPUP_PADDING_Y = 2
const POPUP_MIN_HEIGHT = 10
const padRight = (value: string, width: number): string => {
  if (width <= 0) {
    return ''
  }
  const trimmed = value.length > width ? value.slice(0, width) : value
  return trimmed.length === width ? trimmed : trimmed.padEnd(width, ' ')
}
const joinColumns = (left: string, right: string, width: number): string => {
  const safeWidth = Math.max(0, width)
  if (safeWidth === 0) {
    return ''
  }
  const leftTrimmed = left.length > safeWidth ? left.slice(0, safeWidth) : left
  const remaining = Math.max(0, safeWidth - leftTrimmed.length)
  const rightTrimmed = right.length > remaining ? right.slice(0, remaining) : right
  const gap = Math.max(1, safeWidth - leftTrimmed.length - rightTrimmed.length)
  return `${leftTrimmed}${' '.repeat(gap)}${rightTrimmed}`
}
export type ModelPopupProps = {
  title?: string
  query: string
  options: readonly ModelOption[]
  selectedIndex: number
  recentCount: number
  maxHeight?: number
  providerStatuses: ProviderStatusMap
  onQueryChange: (value: string) => void
  onSubmit: (option: ModelOption | null | undefined) => void
}
const resolveListRows = (popupHeight: number): number =>
  resolveModelPopupListRows({ popupHeight, paddingY: POPUP_PADDING_Y, fixedRows: 6 })
export const ModelPopup = ({
  title,
  query,
  options,
  selectedIndex,
  recentCount,
  maxHeight,
  providerStatuses,
  onQueryChange,
  onSubmit,
}: ModelPopupProps) => {
  const { theme } = useTheme()
  const { stdout } = useStdout()
  const terminalColumns = stdout?.columns ?? 80
  const popupWidth = clamp(terminalColumns - 10, 40, 72)
  const paddingColumns = 2 * POPUP_PADDING_X
  const contentWidth = Math.max(0, popupWidth - paddingColumns)
  const backgroundProps = inkBackgroundColorProps(theme.popupBackground)
  const resolveOptionColor = (option: ModelOption): InkColorValue => {
    const status = providerStatuses[option.provider]?.status
    if (status === 'missing') {
      return theme.warning
    }
    if (status === 'error') {
      return theme.error
    }
    return theme.text
  }
  const selectedOption = options[selectedIndex]
  const fallbackHeight = 16
  const popupHeight = Math.max(POPUP_MIN_HEIGHT, Math.floor(maxHeight ?? fallbackHeight))
  const listRows = useMemo(() => resolveListRows(popupHeight), [popupHeight])
  const rows = useMemo(() => buildModelPopupRows(options, recentCount), [options, recentCount])
  const { slice, visibleRows } = useMemo(
    () =>
      resolveModelPopupVisibleRows({
        rows,
        selectedOptionIndex: selectedIndex,
        maxVisibleRows: listRows,
      }),
    [listRows, rows, selectedIndex],
  )
  const selectedTextProps = {
    ...inkColorProps(theme.selectionText),
    ...inkBackgroundColorProps(theme.selectionBackground),
  }
  const headerLeft = title ?? 'Select model'
  const headerRight = 'esc'
  const headerGap = Math.max(0, contentWidth - headerLeft.length - headerRight.length)
  return (
    <PopupSheet
      width={popupWidth}
      height={popupHeight}
      paddingX={POPUP_PADDING_X}
      paddingY={POPUP_PADDING_Y}
      background={theme.popupBackground}
    >
      <Box flexDirection="row">
        <Text {...backgroundProps} {...inkColorProps(theme.accent)}>
          {headerLeft}
        </Text>
        <Text {...backgroundProps}>{' '.repeat(headerGap)}</Text>
        <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
          {headerRight}
        </Text>
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box>
        <SingleLineTextInput
          value={query}
          onChange={onQueryChange}
          onSubmit={() => onSubmit(selectedOption)}
          placeholder="Search"
          focus
          width={contentWidth}
          backgroundColor={theme.popupBackground}
        />
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Box flexDirection="column" height={listRows} overflow="hidden">
        {rows.length === 0 ? (
          <>
            <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
              {padRight('No models match.', contentWidth)}
            </Text>
            {Array.from({ length: Math.max(0, listRows - 1) }).map((_, index) => (
              <Text key={`empty-${index}`} {...backgroundProps}>
                {padRight('', contentWidth)}
              </Text>
            ))}
          </>
        ) : (
          visibleRows.map((row, rowIndex) => {
            if (row.type === 'spacer') {
              return (
                <Text key={`spacer-${slice.start + rowIndex}`} {...backgroundProps}>
                  {padRight('', contentWidth)}
                </Text>
              )
            }
            if (row.type === 'header') {
              return (
                <Text
                  key={`header-${row.title}-${slice.start + rowIndex}`}
                  {...backgroundProps}
                  {...inkColorProps(theme.accent)}
                >
                  {padRight(row.title, contentWidth)}
                </Text>
              )
            }
            const isSelected = row.optionIndex === selectedIndex
            const baseProviderLabel = MODEL_PROVIDER_LABELS[row.option.provider]
            const providerLabel =
              row.option.provider === 'gemini' && row.option.capabilities.includes('video')
                ? `${baseProviderLabel} video`
                : baseProviderLabel
            const rowTextProps = isSelected
              ? selectedTextProps
              : { ...backgroundProps, ...inkColorProps(resolveOptionColor(row.option)) }
            const line = joinColumns(row.option.label, providerLabel, contentWidth)
            return (
              <Text key={`option-${row.option.id}`} {...rowTextProps}>
                {padRight(line, contentWidth)}
              </Text>
            )
          })
        )}
      </Box>
      <Text {...backgroundProps}>{padRight('', contentWidth)}</Text>
      <Text {...backgroundProps} {...inkColorProps(theme.mutedText)}>
        {padRight('Enter to select', contentWidth)}
      </Text>
    </PopupSheet>
  )
}
</file>

<file path="src/tui/hooks/popup-manager/command-mapping.ts">
import type { NotifyOptions } from '../../notifier'
import type { CommandDescriptor, HistoryEntry, ToggleField } from '../../types'
import { parseUrlArgs, validateHttpUrlCandidate } from '../../screens/command/utils/url-args'
export const JSON_INTERACTIVE_ERROR =
  'JSON output is unavailable while interactive transport is enabled.'
type NotificationKind = Exclude<NotifyOptions['kind'], undefined>
type PushHistoryStep = {
  type: 'push-history'
  message: string
  kind?: HistoryEntry['kind']
}
type NotifyStep = {
  type: 'notify'
  message: string
  kind: NotificationKind
}
type SetInputStep = {
  type: 'set-input'
  value: string
}
type ClosePopupStep = {
  type: 'close-popup'
}
type OpenPopupStep = {
  type: 'open-popup'
  popup:
    | 'model'
    | 'target'
    | 'polish'
    | 'toggle'
    | 'file'
    | 'url'
    | 'image'
    | 'video'
    | 'pdf'
    | 'history'
    | 'resume'
    | 'export'
    | 'smart-root'
    | 'tokens'
    | 'budgets'
    | 'settings'
    | 'theme'
    | 'theme-mode'
    | 'reasoning'
    | 'test'
    | 'intent'
    | 'instructions'
  field?: ToggleField
}
type ApplyToggleStep = {
  type: 'apply-toggle'
  field: ToggleField
  value: boolean
}
type ClearPolishStep = {
  type: 'clear-polish'
}
type AddUrlStep = {
  type: 'add-url'
  value: string
}
type AddImageStep = {
  type: 'add-image'
  value: string
}
type AddVideoStep = {
  type: 'add-video'
  value: string
}
type AddPdfStep = {
  type: 'add-pdf'
  value: string
}
type SmartContextToggleStep = {
  type: 'toggle-smart-context'
}
type SetSmartRootStep = {
  type: 'set-smart-root'
  value: string
}
type SetIntentFileStep = {
  type: 'set-intent-file'
  value: string
}
type SetMetaInstructionsStep = {
  type: 'set-meta-instructions'
  value: string
}
type ClearScreenStep = {
  type: 'clear-screen'
}
type ExitAppStep = {
  type: 'exit-app'
}
type RunTestsStep = {
  type: 'run-tests'
  value: string
}
export type PopupManagerCommandStep =
  | PushHistoryStep
  | NotifyStep
  | SetInputStep
  | ClosePopupStep
  | OpenPopupStep
  | ApplyToggleStep
  | ClearPolishStep
  | AddUrlStep
  | AddImageStep
  | AddVideoStep
  | AddPdfStep
  | SmartContextToggleStep
  | SetSmartRootStep
  | SetIntentFileStep
  | SetMetaInstructionsStep
  | ClearScreenStep
  | ExitAppStep
  | RunTestsStep
export type CommandMappingResult =
  | { kind: 'steps'; steps: readonly PopupManagerCommandStep[] }
  | { kind: 'series'; trimmedArgs: string }
export type CommandMappingContext = {
  copyEnabled: boolean
  chatGptEnabled: boolean
  jsonOutputEnabled: boolean
  interactiveTransportPath?: string | undefined
  urls: readonly string[]
  images: readonly string[]
  videos: readonly string[]
  pdfs: readonly string[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
}
const OPEN_TOGGLE_ARGS = new Set(['on', 'off'])
const CLEAR_ARGS = new Set(['off', 'clear', '--clear'])
export const mapPopupCommandSelection = ({
  commandId,
  argsRaw,
  context,
}: {
  commandId: CommandDescriptor['id']
  argsRaw?: string | undefined
  context: CommandMappingContext
}): CommandMappingResult => {
  const trimmedArgs = argsRaw?.trim() ?? ''
  const normalizedToggleArgs = trimmedArgs.toLowerCase()
  switch (commandId) {
    case 'model':
      return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'model' }] }
    case 'target':
      return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'target' }] }
    case 'polish': {
      if (trimmedArgs && CLEAR_ARGS.has(normalizedToggleArgs)) {
        return { kind: 'steps', steps: [{ type: 'clear-polish' }] }
      }
      return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'polish' }] }
    }
    case 'copy': {
      if (!trimmedArgs) {
        return {
          kind: 'steps',
          steps: [{ type: 'apply-toggle', field: 'copy', value: !context.copyEnabled }],
        }
      }
      if (OPEN_TOGGLE_ARGS.has(normalizedToggleArgs)) {
        return {
          kind: 'steps',
          steps: [{ type: 'apply-toggle', field: 'copy', value: normalizedToggleArgs === 'on' }],
        }
      }
      return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'toggle', field: 'copy' }] }
    }
    case 'chatgpt': {
      if (!trimmedArgs) {
        return {
          kind: 'steps',
          steps: [{ type: 'apply-toggle', field: 'chatgpt', value: !context.chatGptEnabled }],
        }
      }
      if (OPEN_TOGGLE_ARGS.has(normalizedToggleArgs)) {
        return {
          kind: 'steps',
          steps: [{ type: 'apply-toggle', field: 'chatgpt', value: normalizedToggleArgs === 'on' }],
        }
      }
      return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'toggle', field: 'chatgpt' }] }
    }
    case 'json': {
      if (context.interactiveTransportPath) {
        return {
          kind: 'steps',
          steps: [
            { type: 'push-history', message: JSON_INTERACTIVE_ERROR, kind: 'system' },
            { type: 'set-input', value: '' },
          ],
        }
      }
      if (!trimmedArgs) {
        return {
          kind: 'steps',
          steps: [{ type: 'apply-toggle', field: 'json', value: !context.jsonOutputEnabled }],
        }
      }
      if (OPEN_TOGGLE_ARGS.has(normalizedToggleArgs)) {
        return {
          kind: 'steps',
          steps: [{ type: 'apply-toggle', field: 'json', value: normalizedToggleArgs === 'on' }],
        }
      }
      return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'toggle', field: 'json' }] }
    }
    case 'file':
      return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'file' }] }
    case 'url': {
      if (!trimmedArgs) {
        return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'url' }] }
      }
      const candidates = parseUrlArgs(trimmedArgs)
      if (candidates.length === 0) {
        return {
          kind: 'steps',
          steps: [{ type: 'set-input', value: '' }, { type: 'close-popup' }],
        }
      }
      const steps: PopupManagerCommandStep[] = []
      const seen = new Set<string>()
      for (const candidate of candidates) {
        if (seen.has(candidate)) {
          continue
        }
        seen.add(candidate)
        const validation = validateHttpUrlCandidate(candidate)
        if (!validation.ok) {
          steps.push({
            type: 'push-history',
            message: `Warning: ${validation.message}`,
            kind: 'system',
          })
          continue
        }
        if (context.urls.includes(candidate)) {
          steps.push({
            type: 'push-history',
            message: `Context URL already added: ${candidate}`,
            kind: 'system',
          })
          continue
        }
        steps.push({ type: 'add-url', value: candidate })
        steps.push({
          type: 'push-history',
          message: `Context URL added: ${candidate}`,
          kind: 'system',
        })
      }
      steps.push({ type: 'set-input', value: '' })
      steps.push({ type: 'close-popup' })
      return { kind: 'steps', steps }
    }
    case 'image': {
      if (!trimmedArgs) {
        return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'image' }] }
      }
      if (context.images.includes(trimmedArgs)) {
        return {
          kind: 'steps',
          steps: [
            {
              type: 'push-history',
              message: `[image] Already attached: ${trimmedArgs}`,
              kind: 'system',
            },
            { type: 'set-input', value: '' },
            { type: 'close-popup' },
          ],
        }
      }
      return {
        kind: 'steps',
        steps: [
          { type: 'add-image', value: trimmedArgs },
          { type: 'push-history', message: `[image] Attached: ${trimmedArgs}`, kind: 'system' },
          { type: 'set-input', value: '' },
          { type: 'close-popup' },
        ],
      }
    }
    case 'video': {
      if (!trimmedArgs) {
        return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'video' }] }
      }
      if (context.videos.includes(trimmedArgs)) {
        return {
          kind: 'steps',
          steps: [
            {
              type: 'push-history',
              message: `[video] Already attached: ${trimmedArgs}`,
              kind: 'system',
            },
            { type: 'set-input', value: '' },
            { type: 'close-popup' },
          ],
        }
      }
      return {
        kind: 'steps',
        steps: [
          { type: 'add-video', value: trimmedArgs },
          { type: 'push-history', message: `[video] Attached: ${trimmedArgs}`, kind: 'system' },
          { type: 'set-input', value: '' },
          { type: 'close-popup' },
        ],
      }
    }
    case 'pdf': {
      if (!trimmedArgs) {
        return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'pdf' }] }
      }
      if (context.pdfs.includes(trimmedArgs)) {
        return {
          kind: 'steps',
          steps: [
            {
              type: 'push-history',
              message: `[pdf] Already attached: ${trimmedArgs}`,
              kind: 'system',
            },
            { type: 'set-input', value: '' },
            { type: 'close-popup' },
          ],
        }
      }
      return {
        kind: 'steps',
        steps: [
          { type: 'add-pdf', value: trimmedArgs },
          { type: 'push-history', message: `[pdf] Attached: ${trimmedArgs}`, kind: 'system' },
          { type: 'set-input', value: '' },
          { type: 'close-popup' },
        ],
      }
    }
    case 'smart': {
      const nextEnabled = !trimmedArgs
        ? !context.smartContextEnabled
        : normalizedToggleArgs === 'on'
          ? true
          : normalizedToggleArgs === 'off'
            ? false
            : null
      if (nextEnabled === null) {
        return {
          kind: 'steps',
          steps: [
            { type: 'notify', message: 'Smart context expects /smart on|off', kind: 'warning' },
            { type: 'set-input', value: '' },
            { type: 'close-popup' },
          ],
        }
      }
      const steps: PopupManagerCommandStep[] = []
      const isDisabling = nextEnabled === false
      const shouldClearRoot = isDisabling && Boolean(context.smartContextRoot)
      if (shouldClearRoot) {
        steps.push({ type: 'set-smart-root', value: '' })
      }
      if (context.smartContextEnabled !== nextEnabled) {
        steps.push({ type: 'toggle-smart-context' })
      }
      steps.push({
        type: 'notify',
        message: nextEnabled
          ? 'Smart context enabled'
          : shouldClearRoot
            ? 'Smart context disabled; root cleared'
            : 'Smart context disabled',
        kind: nextEnabled ? 'info' : 'warning',
      })
      steps.push({ type: 'set-input', value: '' })
      steps.push({ type: 'close-popup' })
      return { kind: 'steps', steps }
    }
    case 'smart-root': {
      if (!trimmedArgs) {
        return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'smart-root' }] }
      }
      const normalizedRootArgs = trimmedArgs.toLowerCase()
      const rootValue = CLEAR_ARGS.has(normalizedRootArgs) ? '' : trimmedArgs
      const shouldEnable = Boolean(rootValue) && !context.smartContextEnabled
      const steps: PopupManagerCommandStep[] = [{ type: 'set-smart-root', value: rootValue }]
      if (shouldEnable) {
        steps.push({ type: 'toggle-smart-context' })
      }
      steps.push({
        type: 'notify',
        message: rootValue
          ? shouldEnable
            ? `Smart context enabled; root set to ${rootValue}`
            : `Smart context root set to ${rootValue}`
          : 'Smart context root cleared',
        kind: rootValue ? 'info' : 'warning',
      })
      steps.push({ type: 'set-input', value: '' })
      steps.push({ type: 'close-popup' })
      return { kind: 'steps', steps }
    }
    case 'tokens':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'tokens' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'budgets':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'budgets' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'settings':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'settings' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'theme':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'theme' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'theme-mode':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'theme-mode' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'reasoning':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'reasoning' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'history':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'history' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'resume':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'resume' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'export':
      return {
        kind: 'steps',
        steps: [
          { type: 'open-popup', popup: 'export' },
          { type: 'set-input', value: '' },
        ],
      }
    case 'intent': {
      if (!trimmedArgs) {
        return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'intent' }] }
      }
      const normalizedValue = trimmedArgs.trim()
      return {
        kind: 'steps',
        steps: [
          { type: 'set-intent-file', value: normalizedValue },
          {
            type: 'push-history',
            message: normalizedValue
              ? `Intent file set to ${normalizedValue}`
              : 'Intent file cleared; using typed intent.',
          },
          { type: 'set-input', value: '' },
          { type: 'close-popup' },
        ],
      }
    }
    case 'instructions': {
      if (!trimmedArgs) {
        return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'instructions' }] }
      }
      const normalizedValue = trimmedArgs.trim()
      return {
        kind: 'steps',
        steps: [
          { type: 'set-meta-instructions', value: normalizedValue },
          {
            type: 'push-history',
            message: normalizedValue ? `[instr] ${normalizedValue}` : '[instr] cleared',
          },
          { type: 'set-input', value: '' },
          { type: 'close-popup' },
        ],
      }
    }
    case 'exit':
      return {
        kind: 'steps',
        steps: [
          { type: 'push-history', message: 'Exiting…', kind: 'system' },
          { type: 'set-input', value: '' },
          { type: 'clear-screen' },
          { type: 'exit-app' },
        ],
      }
    case 'series':
      return { kind: 'series', trimmedArgs }
    case 'test': {
      if (trimmedArgs) {
        return {
          kind: 'steps',
          steps: [
            {
              type: 'push-history',
              message: `[tests] Running /test ${trimmedArgs}`,
              kind: 'system',
            },
            { type: 'run-tests', value: trimmedArgs },
          ],
        }
      }
      return { kind: 'steps', steps: [{ type: 'open-popup', popup: 'test' }] }
    }
    default:
      return { kind: 'steps', steps: [{ type: 'push-history', message: `Selected ${commandId}` }] }
  }
}
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenPopupAndView.ts">
import { useCommandScreenBindings } from './useCommandScreenBindings'
import { useCommandScreenViewModel } from './useCommandScreenViewModel'
import type {
  UseCommandScreenPopupAndViewOptions,
  UseCommandScreenPopupAndViewResult,
} from './useCommandScreenPopupAndView.types'
export const useCommandScreenPopupAndView = (
  options: UseCommandScreenPopupAndViewOptions,
): UseCommandScreenPopupAndViewResult => {
  const { context, input, popup, history, generation } = options
  const { popupManager, shell, bindings, enhancedStatusChips } = useCommandScreenBindings(options)
  const viewModel = useCommandScreenViewModel({
    transport: { isAwaitingTransportInput: shell.isAwaitingTransportInput },
    panes: {
      history: {
        lines: shell.history,
        visibleRows: shell.historyRows,
        scrollOffset: shell.scrollOffset,
      },
      menu: {
        isActive: shell.isCommandMenuActive,
        height: shell.menuHeight,
        commands: shell.visibleCommands,
        selectedIndex: input.commandSelectionIndex,
      },
    },
    popup: {
      base: {
        popupState: popupManager.popupState,
        helpOpen: popup.helpOpen,
        overlayHeight: shell.overlayHeight,
      },
      model: {
        modelPopupOptions: bindings.popup.model.options,
        modelPopupSelection: bindings.popup.model.selection,
        modelPopupRecentCount: bindings.popup.model.recentCount,
        providerStatuses: generation.providerStatuses,
        onModelPopupQueryChange: bindings.popup.model.onQueryChange,
        onModelPopupSubmit: popupManager.actions.handleModelPopupSubmit,
      },
      context: {
        files: context.files,
        filePopupSuggestions: bindings.popup.context.file.suggestions,
        filePopupSuggestionSelectionIndex: bindings.popup.context.file.suggestionSelectionIndex,
        filePopupSuggestionsFocused: bindings.popup.context.file.suggestionsFocused,
        onFilePopupDraftChange: bindings.popup.context.file.onDraftChange,
        onAddFile: bindings.popup.context.file.onAdd,
        urls: context.urls,
        onUrlPopupDraftChange: bindings.popup.context.url.onDraftChange,
        onAddUrl: bindings.popup.context.url.onAdd,
        images: context.images,
        imagePopupSuggestions: bindings.popup.context.image.suggestions,
        imagePopupSuggestionSelectionIndex: bindings.popup.context.image.suggestionSelectionIndex,
        imagePopupSuggestionsFocused: bindings.popup.context.image.suggestionsFocused,
        onImagePopupDraftChange: bindings.popup.context.image.onDraftChange,
        onAddImage: bindings.popup.context.image.onAdd,
        videos: context.videos,
        videoPopupSuggestions: bindings.popup.context.video.suggestions,
        videoPopupSuggestionSelectionIndex: bindings.popup.context.video.suggestionSelectionIndex,
        videoPopupSuggestionsFocused: bindings.popup.context.video.suggestionsFocused,
        onVideoPopupDraftChange: bindings.popup.context.video.onDraftChange,
        onAddVideo: bindings.popup.context.video.onAdd,
        pdfs: context.pdfs,
        pdfPopupSuggestions: bindings.popup.context.pdf.suggestions,
        pdfPopupSuggestionSelectionIndex: bindings.popup.context.pdf.suggestionSelectionIndex,
        pdfPopupSuggestionsFocused: bindings.popup.context.pdf.suggestionsFocused,
        onPdfPopupDraftChange: bindings.popup.context.pdf.onDraftChange,
        onAddPdf: bindings.popup.context.pdf.onAdd,
        smartContextEnabled: context.smartContextEnabled,
        smartContextRoot: context.smartContextRoot,
        smartPopupSuggestions: bindings.popup.context.smart.suggestions,
        smartPopupSuggestionSelectionIndex: bindings.popup.context.smart.suggestionSelectionIndex,
        smartPopupSuggestionsFocused: bindings.popup.context.smart.suggestionsFocused,
        onSmartPopupDraftChange: bindings.popup.context.smart.onDraftChange,
        onSmartRootSubmit: bindings.popup.context.smart.onRootSubmit,
      },
      history: {
        historyPopupItems: bindings.popup.history.items,
        onHistoryPopupDraftChange: bindings.popup.history.onDraftChange,
        onHistoryPopupSubmit: bindings.popup.history.onSubmit,
      },
      resume: {
        onResumePayloadPathDraftChange: bindings.popup.misc.onResumePayloadPathDraftChange,
        onResumeSubmit: popupManager.actions.handleResumeSubmit,
      },
      export: {
        onExportOutPathDraftChange: bindings.popup.misc.onExportOutPathDraftChange,
        onExportSubmit: popupManager.actions.handleExportSubmit,
      },
      intent: {
        intentPopupSuggestions: bindings.popup.intent.suggestions,
        intentPopupSuggestionSelectionIndex: bindings.popup.intent.suggestionSelectionIndex,
        intentPopupSuggestionsFocused: bindings.popup.intent.suggestionsFocused,
        onIntentPopupDraftChange: bindings.popup.intent.onDraftChange,
        onIntentFileSubmit: popupManager.actions.handleIntentFileSubmit,
      },
      instructions: {
        onInstructionsDraftChange: bindings.popup.misc.onInstructionsDraftChange,
        onInstructionsSubmit: popupManager.actions.handleInstructionsSubmit,
      },
      series: {
        isGenerating: generation.isGenerating,
        onSeriesDraftChange: bindings.popup.misc.onSeriesDraftChange,
        onSeriesSubmit: bindings.submit.onSeriesSubmit,
      },
      test: {
        isTestCommandRunning: history.isTestCommandRunning,
        onTestDraftChange: bindings.popup.misc.onTestDraftChange,
        onTestSubmit: history.onTestPopupSubmit,
      },
      tokens: {
        tokenUsageRun: generation.tokenUsageRun,
        tokenUsageBreakdown: generation.tokenUsageBreakdown,
        maxContextTokens: context.maxContextTokens,
        maxInputTokens: context.maxInputTokens,
        contextOverflowStrategy: context.contextOverflowStrategy,
        latestContextOverflow: generation.latestContextOverflow,
      },
      budgets: {
        onBudgetsMaxContextTokensDraftChange:
          bindings.popup.misc.onBudgetsMaxContextTokensDraftChange,
        onBudgetsMaxInputTokensDraftChange: bindings.popup.misc.onBudgetsMaxInputTokensDraftChange,
        onBudgetsSubmit: popupManager.actions.handleBudgetsSubmit,
      },
      settings: { statusChips: enhancedStatusChips },
      reasoning: {
        reasoningPopupLines: bindings.popup.reasoning.lines,
        reasoningPopupVisibleRows: bindings.popup.reasoning.visibleRows,
      },
    },
    input: {
      base: {
        value: input.inputValue,
        onChange: bindings.input.onChange,
        onSubmit: bindings.submit.onSubmit,
        isPasteActive: input.isPasteActive,
        hint: shell.inputBarHint,
        debugLine: shell.inputBarDebugLine,
        tokenLabel: bindings.input.tokenLabel,
        debugKeysEnabled: input.debugKeysEnabled,
        onDebugKeyEvent: input.onDebugKeyEvent,
      },
      state: {
        isPopupOpen: popupManager.isPopupOpen,
        helpOpen: popup.helpOpen,
        isAwaitingRefinement: generation.isAwaitingRefinement,
        isBusy: generation.isGenerating || history.isTestCommandRunning,
      },
      statusChips: enhancedStatusChips,
    },
  })
  return {
    transportMessage: viewModel.transportMessage,
    historyPaneProps: viewModel.historyPaneProps,
    popupAreaProps: viewModel.popupAreaProps,
    commandMenuPaneProps: viewModel.commandMenuPaneProps,
    commandInputProps: viewModel.commandInputProps,
  }
}
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenViewModel.ts">
import { useMemo } from 'react'
import type { DebugKeyEvent } from '../../../components/core/MultilineTextInput'
import type { HistoryEntry, ModelOption, PopupState, ProviderStatusMap } from '../../../types'
import type { TokenUsageBreakdown, TokenUsageRun } from '../../../token-usage-store'
import type { CommandInputProps } from '../components/CommandInput'
import type { CommandMenuPaneProps } from '../components/CommandMenuPane'
import type { HistoryPaneProps } from '../components/HistoryPane'
import type { PopupAreaProps } from '../components/PopupArea'
export type UseCommandScreenViewModelOptions = {
  transport: {
    isAwaitingTransportInput: boolean
  }
  panes: {
    history: HistoryPaneProps
    menu: CommandMenuPaneProps
  }
  popup: {
    base: {
      popupState: PopupState
      helpOpen: boolean
      overlayHeight: number
    }
    model: {
      modelPopupOptions: ModelOption[]
      modelPopupSelection: number
      modelPopupRecentCount: number
      providerStatuses: ProviderStatusMap
      onModelPopupQueryChange: (next: string) => void
      onModelPopupSubmit: (option: ModelOption | null | undefined) => void
    }
    context: {
      files: string[]
      filePopupSuggestions: string[]
      filePopupSuggestionSelectionIndex: number
      filePopupSuggestionsFocused: boolean
      onFilePopupDraftChange: (next: string) => void
      onAddFile: (value: string) => void
      urls: string[]
      onUrlPopupDraftChange: (next: string) => void
      onAddUrl: (value: string) => void
      images: string[]
      imagePopupSuggestions: string[]
      imagePopupSuggestionSelectionIndex: number
      imagePopupSuggestionsFocused: boolean
      onImagePopupDraftChange: (next: string) => void
      onAddImage: (value: string) => void
      videos: string[]
      videoPopupSuggestions: string[]
      videoPopupSuggestionSelectionIndex: number
      videoPopupSuggestionsFocused: boolean
      onVideoPopupDraftChange: (next: string) => void
      onAddVideo: (value: string) => void
      pdfs: string[]
      pdfPopupSuggestions: string[]
      pdfPopupSuggestionSelectionIndex: number
      pdfPopupSuggestionsFocused: boolean
      onPdfPopupDraftChange: (next: string) => void
      onAddPdf: (value: string) => void
      smartContextEnabled: boolean
      smartContextRoot: string | null
      smartPopupSuggestions: string[]
      smartPopupSuggestionSelectionIndex: number
      smartPopupSuggestionsFocused: boolean
      onSmartPopupDraftChange: (next: string) => void
      onSmartRootSubmit: (value: string) => void
    }
    history: {
      historyPopupItems: string[]
      onHistoryPopupDraftChange: (next: string) => void
      onHistoryPopupSubmit: (value: string) => void
    }
    resume: {
      onResumePayloadPathDraftChange: (next: string) => void
      onResumeSubmit: () => void
    }
    export: {
      onExportOutPathDraftChange: (next: string) => void
      onExportSubmit: () => void
    }
    intent: {
      intentPopupSuggestions: string[]
      intentPopupSuggestionSelectionIndex: number
      intentPopupSuggestionsFocused: boolean
      onIntentPopupDraftChange: (next: string) => void
      onIntentFileSubmit: (value: string) => void
    }
    instructions: {
      onInstructionsDraftChange: (next: string) => void
      onInstructionsSubmit: (value: string) => void
    }
    series: {
      isGenerating: boolean
      onSeriesDraftChange: (next: string) => void
      onSeriesSubmit: (value: string) => void
    }
    test: {
      isTestCommandRunning: boolean
      onTestDraftChange: (next: string) => void
      onTestSubmit: (value: string) => void
    }
    tokens: {
      tokenUsageRun: TokenUsageRun | null
      tokenUsageBreakdown: TokenUsageBreakdown | null
      maxContextTokens: number | null
      maxInputTokens: number | null
      contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
      latestContextOverflow:
        | import('../../../generation-pipeline-reducer').ContextOverflowDetails
        | null
    }
    budgets: {
      onBudgetsMaxContextTokensDraftChange: (next: string) => void
      onBudgetsMaxInputTokensDraftChange: (next: string) => void
      onBudgetsSubmit: () => void
    }
    settings: {
      statusChips: string[]
    }
    reasoning: {
      reasoningPopupLines: HistoryEntry[]
      reasoningPopupVisibleRows: number
    }
  }
  input: {
    base: {
      value: string
      onChange: (next: string) => void
      onSubmit: (value: string) => void
      isPasteActive: boolean
      hint: string | undefined
      debugLine: string | undefined
      tokenLabel: (token: string) => string | null
      debugKeysEnabled: boolean
      onDebugKeyEvent: (event: DebugKeyEvent) => void
    }
    state: {
      isPopupOpen: boolean
      helpOpen: boolean
      isAwaitingRefinement: boolean
      isBusy: boolean
    }
    statusChips: string[]
  }
}
export type UseCommandScreenViewModelResult = {
  transportMessage: string | null
  historyPaneProps: HistoryPaneProps
  popupAreaProps: PopupAreaProps
  commandMenuPaneProps: CommandMenuPaneProps
  commandInputProps: CommandInputProps
}
export const useCommandScreenViewModel = ({
  transport,
  panes,
  popup,
  input,
}: UseCommandScreenViewModelOptions): UseCommandScreenViewModelResult => {
  const transportMessage = transport.isAwaitingTransportInput
    ? 'Waiting for interactive transport input (send refine/finish).'
    : null
  const historyPaneProps = panes.history
  const commandMenuPaneProps = panes.menu
  const popupAreaProps = useMemo<PopupAreaProps>(
    () => ({
      ...popup.base,
      ...popup.model,
      ...popup.context,
      ...popup.history,
      ...popup.resume,
      ...popup.export,
      ...popup.intent,
      ...popup.instructions,
      ...popup.series,
      ...popup.test,
      ...popup.tokens,
      ...popup.budgets,
      ...popup.settings,
      ...popup.reasoning,
    }),
    [
      popup.base,
      popup.context,
      popup.history,
      popup.resume,
      popup.export,
      popup.instructions,
      popup.intent,
      popup.model,
      popup.reasoning,
      popup.series,
      popup.settings,
      popup.test,
      popup.tokens,
      popup.budgets,
    ],
  )
  const commandInputProps = useMemo<CommandInputProps>(
    () => ({
      value: input.base.value,
      onChange: input.base.onChange,
      onSubmit: input.base.onSubmit,
      mode: input.state.isAwaitingRefinement ? 'refinement' : 'intent',
      isDisabled: input.state.isPopupOpen || input.state.helpOpen,
      isPasteActive: input.base.isPasteActive,
      isBusy: input.state.isBusy,
      statusChips: input.statusChips,
      hint: input.base.hint,
      debugLine: input.base.debugLine,
      tokenLabel: input.base.tokenLabel,
      onDebugKeyEvent: input.base.debugKeysEnabled ? input.base.onDebugKeyEvent : undefined,
      placeholder: input.state.isAwaitingRefinement
        ? 'Describe refinement (or empty to finish)...'
        : 'Describe your goal or type /command',
    }),
    [input],
  )
  return {
    transportMessage,
    historyPaneProps,
    popupAreaProps,
    commandMenuPaneProps,
    commandInputProps,
  }
}
</file>

<file path="src/tui/file-suggestions.ts">
/*
 * File/directory suggestion helpers used by the Ink TUI.
 *
 * This module contains two layers:
 * 1) Discovery (IO): scan the workspace using `fast-glob`.
 * 2) Filtering (pure): rank/limit suggestions based on a user query.
 *
 * Keeping filtering logic pure makes it easy to unit test and helps ensure
 * later UI refactors don’t accidentally change suggestion behavior.
 */
import path from 'node:path'
import fg from 'fast-glob'
import { byLengthAsc, byStartAsc, Fzf, extendedMatch } from 'fzf'
const FILE_SUGGESTION_PATTERNS = ['**/*']
export const FILE_SUGGESTION_IGNORE_PATTERNS = [
  '**/node_modules/**',
  '**/dist/**',
  '**/coverage/**',
  '**/.git/**',
  '**/.nx/**',
  '**/.next/**',
  '**/.turbo/**',
  '**/.cache/**',
  '**/build/**',
  '**/out/**',
  '**/package-lock.json',
  '**/pnpm-lock.yaml',
  '**/yarn.lock',
]
const DEFAULT_FILE_SUGGESTION_LIMIT = 200
const normalizeToPosix = (value: string): string => value.split(path.sep).join('/')
const toDisplayPath = (cwd: string, candidatePath: string): string | null => {
  const absolutePath = path.isAbsolute(candidatePath)
    ? candidatePath
    : path.resolve(cwd, candidatePath)
  const relative = path.relative(cwd, absolutePath)
  if (!relative || relative.startsWith('..')) {
    return null
  }
  return normalizeToPosix(relative)
}
export type DiscoverFileSuggestionsOptions = {
  cwd?: string
  limit?: number
}
export const discoverFileSuggestions = async (
  options: DiscoverFileSuggestionsOptions = {},
): Promise<string[]> => {
  const cwd = options.cwd ?? process.cwd()
  const limit = options.limit ?? DEFAULT_FILE_SUGGESTION_LIMIT
  const matches = await fg(FILE_SUGGESTION_PATTERNS, {
    cwd,
    dot: true,
    absolute: true,
    onlyFiles: true,
    unique: true,
    suppressErrors: true,
    followSymbolicLinks: false,
    ignore: FILE_SUGGESTION_IGNORE_PATTERNS,
  })
  const unique = new Set<string>()
  for (const match of matches) {
    const displayPath = toDisplayPath(cwd, match)
    if (!displayPath) {
      continue
    }
    unique.add(displayPath)
  }
  return [...unique].sort().slice(0, limit)
}
export type DiscoverDirectorySuggestionsOptions = {
  cwd?: string
  limit?: number
}
export const discoverDirectorySuggestions = async (
  options: DiscoverDirectorySuggestionsOptions = {},
): Promise<string[]> => {
  const cwd = options.cwd ?? process.cwd()
  const limit = options.limit ?? DEFAULT_FILE_SUGGESTION_LIMIT
  const matches = await fg(FILE_SUGGESTION_PATTERNS, {
    cwd,
    dot: true,
    absolute: true,
    onlyDirectories: true,
    unique: true,
    suppressErrors: true,
    followSymbolicLinks: false,
    ignore: FILE_SUGGESTION_IGNORE_PATTERNS,
  })
  const unique = new Set<string>()
  for (const match of matches) {
    const displayPath = toDisplayPath(cwd, match)
    if (!displayPath) {
      continue
    }
    unique.add(displayPath)
  }
  return [...unique].sort().slice(0, limit)
}
const INTENT_FILE_SUGGESTION_PATTERNS = ['**/*.{md,markdown,txt}']
export type DiscoverIntentFileSuggestionsOptions = {
  cwd?: string
  limit?: number
}
export const discoverIntentFileSuggestions = async (
  options: DiscoverIntentFileSuggestionsOptions = {},
): Promise<string[]> => {
  const cwd = options.cwd ?? process.cwd()
  const limit = options.limit ?? DEFAULT_FILE_SUGGESTION_LIMIT
  const matches = await fg(INTENT_FILE_SUGGESTION_PATTERNS, {
    cwd,
    dot: true,
    absolute: true,
    onlyFiles: true,
    unique: true,
    suppressErrors: true,
    followSymbolicLinks: false,
    ignore: FILE_SUGGESTION_IGNORE_PATTERNS,
  })
  const unique = new Set<string>()
  for (const match of matches) {
    const displayPath = toDisplayPath(cwd, match)
    if (!displayPath) {
      continue
    }
    unique.add(displayPath)
  }
  return [...unique].sort().slice(0, limit)
}
export type FilterIntentFileSuggestionsOptions = {
  suggestions: readonly string[]
  query: string
  exclude?: readonly string[]
  limit?: number
}
export const filterIntentFileSuggestions = ({
  suggestions,
  query,
  exclude = [],
  limit = DEFAULT_FILE_SUGGESTION_LIMIT,
}: FilterIntentFileSuggestionsOptions): string[] => {
  const excluded = new Set(exclude)
  const eligible = suggestions.filter((suggestion) => !excluded.has(suggestion))
  return fuzzyFilterStrings(eligible, query, limit)
}
export type FilterFileSuggestionsOptions = {
  suggestions: readonly string[]
  query: string
  exclude?: readonly string[]
  limit?: number
}
const normalizeFzfToken = (token: string): string => {
  if (!token) {
    return token
  }
  // Preserve fzf query operators while normalizing slashes / absolute paths.
  let rest = token
  let prefix = ''
  let suffix = ''
  while (
    rest.startsWith('!') ||
    rest.startsWith('^') ||
    rest.startsWith("'") ||
    rest.startsWith('"')
  ) {
    prefix += rest.slice(0, 1)
    rest = rest.slice(1)
  }
  if (rest.endsWith('$')) {
    suffix = '$'
    rest = rest.slice(0, -1)
  }
  if (!rest) {
    return `${prefix}${suffix}`
  }
  // Normalize absolute paths to a workspace-relative form so matching can work
  // against `discoverFileSuggestions()` results.
  if (!path.isAbsolute(rest)) {
    return `${prefix}${normalizeToPosix(rest)}${suffix}`
  }
  const relative = path.relative(process.cwd(), rest)
  if (relative && !relative.startsWith('..')) {
    return `${prefix}${normalizeToPosix(relative)}${suffix}`
  }
  return `${prefix}${normalizeToPosix(path.basename(rest))}${suffix}`
}
const normalizeQueryForFzf = (query: string): string => {
  const trimmed = query.trim()
  if (!trimmed) {
    return trimmed
  }
  return trimmed
    .split(/\s+/)
    .map((token) => normalizeFzfToken(token))
    .join(' ')
}
const fuzzyFilterStrings = (items: readonly string[], query: string, limit: number): string[] => {
  const trimmed = normalizeQueryForFzf(query)
  if (!trimmed) {
    return items.slice(0, limit)
  }
  // Heuristic: if the query contains path separators, treat it as a path search
  // and use forward matching (fzf's default). Otherwise prefer matching from the
  // end to bias toward filenames.
  const isPathQuery = trimmed.includes('/')
  const fzf = new Fzf(items, {
    casing: 'smart-case',
    normalize: true,
    fuzzy: 'v2',
    forward: isPathQuery,
    match: extendedMatch,
    tiebreakers: [byStartAsc, byLengthAsc],
    limit,
  })
  return fzf
    .find(trimmed)
    .map((result) => result.item)
    .slice(0, limit)
}
export const filterFileSuggestions = ({
  suggestions,
  query,
  exclude = [],
  limit = DEFAULT_FILE_SUGGESTION_LIMIT,
}: FilterFileSuggestionsOptions): string[] => {
  const excluded = new Set(exclude)
  const eligible = suggestions.filter((suggestion) => !excluded.has(suggestion))
  return fuzzyFilterStrings(eligible, query, limit)
}
export type FilterDirectorySuggestionsOptions = FilterFileSuggestionsOptions
export const filterDirectorySuggestions = (options: FilterDirectorySuggestionsOptions): string[] =>
  filterFileSuggestions(options)
</file>

<file path="docs/cookbook.md">
# The prompt-maker-cli Cookbook

**prompt-maker-cli** is a terminal-first prompt generator with two faces:

- A flag-driven CLI (`generate`, `test`, plus utility subcommands like `export` and `compose`)
- An Ink-based TUI (default when you run with no args)

Under the hood, the CLI routes commands via `src/index.ts` and the generate workflow runs through `src/generate/pipeline.ts` (context resolution → optional smart context → prompt generation → optional polishing/template → history logging). This cookbook focuses on practical “recipes” and flags you can combine to build repeatable prompt contracts.

A note on versions: this doc preserves all existing recipes. When something is likely outdated, it’s kept and labeled as **Legacy / may be outdated** with a pointer to the current replacement.

## Table of Contents

- [Quickstart (CLI + TUI)](#quickstart-cli--tui)
- [Prompting Masterclass](#prompting-masterclass)
- [Flag Strategy & Mechanics](#flag-strategy--mechanics)
- [Debugging Prompt Runs](#debugging-prompt-runs)
- [Template Playbook](#template-playbook)
- [AI Systems Recipes](#ai-systems-recipes)
- [Developer Recipes](#developer-recipes)
- [Self-Directed Learning Recipes](#self-directed-learning-recipes)
- [Git Commit Workflows](#git-commit-workflows)
- [Editor Workflow Recipes](#editor-workflow-recipes)
- [Jira Ticket Recipes](#jira-ticket-recipes)
- [CI Integration Recipes](#ci-integration-recipes)
- [NeoVim Plugin Integration Recipes](#neovim-plugin-integration-recipes)
- [Recipes](#recipes)

## Quickstart (CLI + TUI)

Authoritative references for the current TUI behavior and structure:

- `docs/tui-design.md` (user-facing behavior, keybinds, input routing invariants)
- `src/tui/DEVELOPER_NOTE.md` (architecture + reducer/hook structure)
- `AGENTS.md` (dev commands + project conventions)

### Recipe: Install + run from source

**Prereqs**

- Node.js `>=18` (repo commonly uses Node `22.x`; see `.nvmrc`)

**Solution**

```bash
npm ci
npm run build

# TUI (default when no args)
npm start

# Generate (one-shot) from source
npm start -- "Draft a confident onboarding-bot spec" --model gpt-4o-mini

# Prompt tests (from source)
npm start -- test prompt-tests.yaml
```

**Expected result**

- `npm start` launches the Ink TUI.
- `npm start -- <intent> ...` runs the generate pipeline and prints the final prompt (or JSON if requested).
- `npm start -- test ...` runs the YAML test suite and sets a non-zero exit code on failures.

**Troubleshooting**

- If you see “Missing OpenAI credentials” or “Missing Gemini credentials”, configure env vars or a config file (see recipe below).

### Recipe: Run the published CLI (after install)

**Prereqs**

- `prompt-maker-cli` installed globally (or available on your PATH)

**Solution**

```bash
# TUI
prompt-maker-cli

# Explicit TUI entry
prompt-maker-cli ui

# Generate
prompt-maker-cli "Summarize src/tui/ in 5 bullets" -c "src/tui/**/*.ts*" --polish

# Prompt tests
prompt-maker-cli test
```

**Expected result**

- `prompt-maker-cli` with no args starts the TUI.
- `prompt-maker-cli test` runs `prompt-tests.yaml` by default.

### Recipe: Configure credentials + defaults

**Problem**

You want `prompt-maker-cli` to run without re-exporting env vars every time, and you want the TUI’s theme settings to persist.

**Solution**

1. Create (or set) a config file:
   - `PROMPT_MAKER_CLI_CONFIG=/path/to/config.json` (highest precedence)
   - `~/.config/prompt-maker-cli/config.json`
   - `~/.prompt-maker-cli.json`
2. Add credentials and optional defaults:

```json
{
  "openaiApiKey": "sk-...",
  "geminiApiKey": "gk-...",
  "promptGenerator": {
    "defaultModel": "gpt-4o-mini",
    "defaultGeminiModel": "gemini-3-pro-preview",
    "maxInputTokens": 12000,
    "maxContextTokens": 8000,
    "contextOverflowStrategy": "drop-smart"
  },
  "contextTemplates": {
    "scratch": "# Scratch\n\n{{prompt}}"
  },
  "theme": "ocean",
  "themeMode": "system"
}
```

**Discussion**

- Env vars override config:
  - `OPENAI_API_KEY` (optional `OPENAI_BASE_URL`)
  - `GEMINI_API_KEY` (optional `GEMINI_BASE_URL`)
- Some TUI choices persist by writing back into the same config file (see `src/config.ts`):
  - Theme: `theme`, `themeMode`
  - Budgets (`/budgets`): `promptGenerator.maxInputTokens`, `promptGenerator.maxContextTokens`, `promptGenerator.contextOverflowStrategy`
  - Resume defaults (`/resume`): `resumeMode`, `resumeSourceKind`
  - Export defaults (`/export`): `exportFormat`, `exportOutDir`

### Recipe: Learn the TUI quickly

**Solution**

```bash
prompt-maker-cli
```

Then:

- Type `/help` to open the help overlay (it is the definitive keybind list).
- Press `Ctrl+G` to open the command palette.
- Press `Ctrl+T` to switch to the Test Runner view.

**Discussion**

The TUI is intentionally “keyboard-first” and follows strict input-routing invariants (help overlay > popup > screen > global keys). See `docs/tui-design.md`.

### Recipe: Use the command palette (/commands)

**Problem**

You want to discover what the TUI can do (models, context, tests, theming) without memorizing flags.

**Solution**

1. Press `Ctrl+G` (or type `/`) to open the command palette.
2. Type to filter commands.
3. Use arrow keys to select.
4. Press Enter to run the selected command (some open a popup).

**Expected result**

- The palette shows the current command list from `src/tui/config.ts` (`COMMAND_DESCRIPTORS`).
- If a command opens a popup, the popup owns input until you close it (usually with `Esc`).

**Troubleshooting**

- If keys “don’t work,” check whether the help overlay is open (`/help`) or a popup is active; those layers intentionally suppress screen/global keys.

### Recipe: Add context in the TUI (files, URLs, images, video, smart)

**Problem**

You want to attach context interactively without rebuilding a long CLI command.

**Solution**

- Open the palette, then use:
  - `/file` to add file globs (repeatable)
  - `/url` to add URL context
  - `/image` to attach image paths
  - `/video` to attach video paths (will force Gemini at generation time)
  - `/smart` to toggle smart context
  - `/smart-root` to set/clear the smart-context scan root

**Fast path: drag + drop**

- Drag an absolute file path into the terminal.
- When you see the hint “Press Tab to add … to context”, press `Tab`.

**Expected result**

- Context items are tracked as chips in the input bar and used for subsequent generations.
- Popups support list management (add/remove) and some provide suggestions (toggle with `Tab`).

### Recipe: Generate an atomic prompt series (TUI)

**Problem**

You want a set of standalone “atomic prompts” you can hand off step-by-step (no cross-references).

**Solution**

- Type an intent, then press `Tab`.
  - If an absolute file path is currently “dropped” in the input, `Tab` adds that file to context instead.
- Or run `/series` from the command palette.

**Expected result**

- The TUI generates a series and writes markdown files under `generated/series/<timestamped-folder>/`.
- The folder includes `00-overview.md` plus one file per atomic prompt.

**Troubleshooting**

- If writing fails (permissions/readonly FS), the TUI still generates but will report file-write errors in history.

### Recipe: Run prompt tests in the TUI

**Problem**

You want to run `prompt-tests.yaml` without leaving the UI.

**Solution**

1. Press `Ctrl+T` to switch to the Test Runner view.
2. Keep `prompt-tests.yaml` (default) or enter another path.
3. Press `Tab` to focus the Run button, then `Enter` to start.

**Expected result**

- The test runner loads the YAML suite, runs each test, and shows PASS/FAIL with reasons.

### Recipe: Theme + theme mode (TUI)

**Problem**

You want the TUI colors to match your terminal and persist across sessions.

**Solution**

- Run `/theme` to pick a theme.
- Run `/theme-mode dark|light|system` to switch appearance mode.

**Expected result**

- The selection persists in your CLI config (`theme` and `themeMode`; see `src/config.ts`).

### Recipe: Run the TUI with interactive transport

**Problem**

You want an external tool (editor plugin, automation) to push refine/finish commands during interactive runs.

**Solution**

```bash
prompt-maker-cli ui --interactive-transport /tmp/prompt-maker.sock
```

**Expected result**

- The TUI starts and the underlying generate pipeline listens on the provided socket/pipe.
- When the pipeline reaches an interactive step, the UI shows it is waiting for transport input.

**Troubleshooting**

- The transport protocol is implemented in `src/generate/interactive-transport.ts` and emits `transport.*` + `interactive.*` stream events.

### Recipe: Use prompt-maker-cli without a TTY (headless)

**Problem**

You’re running in CI or piping output, so the Ink TUI and interactive prompts won’t work.

**Solution**

```bash
echo "Draft a release risk summary" | prompt-maker-cli --quiet --progress=false --stream jsonl
```

**Expected result**

- The CLI runs non-interactively and emits JSONL progress events to stdout.

**Troubleshooting**

- Interactive mode (`--interactive`) is ignored without a TTY; use `--interactive-transport` for headless interactive workflows.

### Recipe: Machine-readable output (JSON + JSONL)

**Problem**

You want prompt-maker to integrate with other tooling (scripts, bots, editor plugins).

**Solution (JSON payload)**

```bash
prompt-maker-cli "Summarize the changes" -c /tmp/staged.patch --json > run.json
```

Notes:

- `--json` prints a pretty JSON payload to stdout and still appends a JSONL history entry.
- If you also use `--show-context` with `--json`, the context is printed to stderr to avoid corrupting the JSON payload.

**Solution (JSONL stream events)**

```bash
prompt-maker-cli "Summarize the changes" -c /tmp/staged.patch \
  --stream jsonl --quiet --progress=false > events.jsonl
```

Notes:

- `--stream jsonl` writes newline-delimited JSON events to stdout (see `src/generate/stream.ts`).
- Use `--quiet` if you want _only_ JSONL lines on stdout.

### Recipe: Token budgets + deterministic overflow (and `context.overflow`)

**When to use this**

You’re automating prompt generation and need deterministic behavior when your text context is too large (instead of “sometimes it fits”).

**Steps**

```bash
# Intentionally set a small context budget to force trimming.
# Use --quiet + --stream jsonl so stdout stays machine-readable.
prompt-maker-cli "Summarize the selected modules" \
  -c "src/**/*.ts" \
  --max-context-tokens 800 \
  --context-overflow drop-largest \
  --stream jsonl \
  --quiet \
  --progress=false > events.jsonl
```

**Expected output (snippet)**

`events.jsonl` includes a `context.overflow` line before generation begins:

```json
{
  "event": "context.overflow",
  "strategy": "drop-largest",
  "droppedPaths": [{ "path": "...", "source": "file" }]
}
```

**Notes**

- Budgets apply only to **text** context entries (`--context`, `--url`, `--smart-context`), not images/videos.
- With `--stream jsonl`, the CLI treats stdout as a machine channel; combine with `--quiet` to avoid mixing human output.

**Troubleshooting / pitfalls**

- If no `context.overflow` appears, your context likely fit within the budget.
- If you see a hard failure, you may have left `--context-overflow` unset (default is `fail` when budgets are enabled).

---

### Recipe: Export a generate payload (portable JSON/YAML)

**When to use this**

You want a portable artifact you can check into a repo, attach to a ticket, or diff in CI — without scraping `history.jsonl` manually.

**Steps**

```bash
# Export the most recent generate payload from history.
prompt-maker-cli export --format json --out runs/last-run.json

# Export an older entry ("third from last") as YAML.
prompt-maker-cli export --from-history last:3 --format yaml --out runs/third-from-last.yaml
```

**Expected output (snippet)**

- Files are written to `runs/*.json` or `runs/*.yaml`.
- The payload includes a `schemaVersion` field:

```json
{
  "schemaVersion": "1",
  "intent": "...",
  "prompt": "..."
}
```

**Notes (determinism + stdout/stderr)**

- The export command writes **only** to the file path you provide.
- Success messages are printed to **stderr** by default; pass `--quiet` to suppress them.

**Troubleshooting / pitfalls**

- “History file not found …”: run any generate command first; exports read from `~/.config/prompt-maker-cli/history.jsonl`.
- “No generate payload entries found …”: your history file may contain non-payload JSONL; run a normal generate and try again.
- “Unsupported history payload schemaVersion=…”: upgrade/downgrade prompt-maker-cli, or re-run generation to create a compatible history entry.

**TUI equivalent**

- In the TUI, run `/export` to pick a history entry and write JSON/YAML.
- The picker marks incompatible history entries as unsupported, and export will fail if you select one (`src/history/generate-history.ts`).

---

### Recipe: Resume from history or an exported payload (`resume.loaded`)

**When to use this**

You want to iterate on a previous run (including refinements) without retyping intent/model/meta settings — and you want explicit, testable handling of missing context files.

**Steps (resume from history)**

```bash
# Resume from the last history entry.
# Note the explicit empty intent string: resume supplies the intent.
prompt-maker-cli "" --resume-last --resume-mode best-effort \
  --stream jsonl --quiet --progress=false > resume-events.jsonl

# Resume from the 5th entry from the end.
prompt-maker-cli "" --resume 5 --resume-mode best-effort --quiet
```

**Steps (resume from an exported payload file)**

```bash
# Export, then resume from the exported file.
prompt-maker-cli export --format json --out runs/last-run.json

# Strict mode fails if any resumed file context paths are missing.
prompt-maker-cli "" --resume-from runs/last-run.json --resume-mode strict --quiet
```

**Expected output (snippet)**

With `--stream jsonl`, the run emits `resume.loaded` early:

```json
{
  "event": "resume.loaded",
  "source": "history",
  "reusedContextPaths": [{ "path": "notes/existing.md", "source": "file" }],
  "missingContextPaths": [{ "path": "notes/missing.md", "source": "file" }]
}
```

**Notes (precedence + machine-readable output)**

- Explicit CLI flags override resumed payload values:
  - `--model` overrides `payload.model`
  - `--target` overrides `payload.targetModel`
  - `metaInstructions` is reused from the payload when present (there is no generate-mode flag for it today)
- Explicit `--context ...` overrides resumed `contextPaths`.
- `--resume-mode best-effort` warns on missing files and continues.
- `--resume-mode strict` fails fast if any resumed `source:"file"` paths are missing.

**TUI equivalent**

- In the TUI, run `/resume` and select:
  - `Source: history` (pick `last`, `last:2`, …), or
  - `Source: file` (enter a payload path)
- The TUI persists the default source/mode (`resumeSourceKind`, `resumeMode`) back into config (`src/config.ts`).

**Troubleshooting / pitfalls**

- If you pass both `--resume` and `--resume-last`, argument parsing fails (`--resume and --resume-last cannot be combined.`).
- If you see interactive/JSON incompatibility errors, remember: `--json` cannot be combined with `--interactive` or `--interactive-transport`.

---

### Recipe: Compare two runs (export + diff)

**When to use this**

You want a simple, deterministic way to compare outputs across runs (prompt text, model choice, context paths) without adding a bespoke “compare” mode.

**Steps**

```bash
mkdir -p runs

# Export the last two runs.
prompt-maker-cli export --from-history last --format json --out runs/run-a.json
prompt-maker-cli export --from-history last:2 --format json --out runs/run-b.json

# Extract just the prompt text for diffing (portable; no jq required).
node --input-type=module -e "
  import fs from 'node:fs/promises'
  const a = JSON.parse(await fs.readFile('runs/run-a.json', 'utf8'))
  const b = JSON.parse(await fs.readFile('runs/run-b.json', 'utf8'))
  await fs.writeFile('runs/run-a.prompt.txt', a.renderedPrompt ?? a.polishedPrompt ?? a.prompt)
  await fs.writeFile('runs/run-b.prompt.txt', b.renderedPrompt ?? b.polishedPrompt ?? b.prompt)
"

diff -u runs/run-b.prompt.txt runs/run-a.prompt.txt || true
```

**Expected output (snippet)**

A unified diff showing prompt-level changes:

```diff
- Old wording...
+ New wording...
```

**Notes**

- Prefer `renderedPrompt` over `polishedPrompt` over `prompt` (matches how integrations choose the final artifact).
- This workflow is “comparative” by construction: the exported payloads are stable artifacts you can diff, review, or attach to tickets.

**Troubleshooting / pitfalls**

- If `diff` shows nothing, the final artifact text is identical.
- If your last two runs used different templates/polish settings, compare both `prompt` and `renderedPrompt` fields to understand why.

---

### Recipe: Deterministic compose scaffold (no LLM)

**When to use this**

You want a deterministic, offline way to produce a prompt-like artifact from a “recipe” and some input (useful for scaffolding integrations).

**Steps**

```bash
prompt-maker-cli compose --recipe docs/tui-design.md --input "Draft a summary" > composed.txt
```

**Expected output (snippet)**

`composed.txt` is:

```text
<recipe file contents>
---
Draft a summary
```

**Notes (stdout/stderr)**

- Compose writes successful output to **stdout** only; errors are on **stderr**.
- Current behavior is deliberately simple: it normalizes newlines and joins recipe + input with a `---` delimiter.

**Troubleshooting / pitfalls**

- “Failed to read recipe file …”: check the path and permissions.
- This command is scaffolding; it does not parse YAML/JSON recipes yet.

### Recipe: Where outputs are stored (history)

**Problem**

You want an audit trail of what was sent, with what context, and when.

**Solution**

- Every generate run appends a JSONL line to:
  - `~/.config/prompt-maker-cli/history.jsonl`

**Discussion**

- The TUI also exposes history browsing via `/history`.
- The saved JSON includes `contextPaths` and any `refinements`, which makes it possible to replay or diff runs.

### Recipe: Common troubleshooting checklist

**Problem**

A run fails or behaves unexpectedly.

**Checklist**

- **Credentials**: set `OPENAI_API_KEY` / `GEMINI_API_KEY` or configure `~/.config/prompt-maker-cli/config.json`.
- **No files matched**: your `-c/--context` glob matched nothing (see warning from `src/file-context.ts`).
- **Interactive seems ignored**: you’re in a non-TTY environment; use `--interactive-transport` or run in a real terminal.
- **Video forces Gemini**: `--video` switches to `resolveGeminiVideoModel()`; ensure Gemini is configured.
- **Stream output looks “mixed”**: combine `--stream jsonl` with `--quiet`.

## Prompting Masterclass

### Mental Models that Travel Across Models

- **Chain of Thought (CoT)**: Ask the model to reason step-by-step when tackling logic-heavy work (tracing bugs, drafting proofs). Combine CoT language in your intent with `--interactive` so you can append refinement instructions as fresh insights appear.
- **Few-Shot Priming**: Embed curated exemplars via `-c examples/*.md` or `--context-template` to bias style and structure. Works best when the examples are short and sharply relevant—watch token bloat via the CLI’s token telemetry.
- **Persona Adoption**: State the persona plus decision criteria directly in the intent (`"Adopt the voice of a staff engineer..."`) or maintain persona snippets in markdown files referenced through `-c personas/staff-engineer.md`. Personas pair well with the polish pass because the `POLISH_SYSTEM_PROMPT` preserves headings while tightening tone.
- **Constraint Stacking**: The CLI’s default format enforces Context → Intent → Output Format; use bullet lists, acceptance criteria, and schema-like checklists to corral powerful models. When you need absolute structure, emit `--json` so downstream tools can parse the run artifact.

### Model-Specific Tactics

- **OpenAI**: Favor detailed work orders with enumerated deliverables; use `--polish` to squeeze extra clarity after interactive refinement.
- **Gemini**: Lean into multimodal runs—`--image` and `--video` feed attachments through `prompt-generator-service`. When a video is present and the requested model is not Gemini, the CLI automatically switches to the configured Gemini video model (default: `gemini-3-pro-preview`) so you stay within supported modalities.
- **Smaller / Local Models**: Prune context aggressively. Combine targeted globs (`-c "src/core/**/*.ts"`) with `--smart-context` to fetch only the top-N embedding matches, keeping token counts within local limits.

### Automating Prompt Structure

1. **Context**: Mix and match `-c/--context` globs, `--url`, and `--smart-context` (which indexes files under `--smart-context-root` via `smart-context-service.ts`). Use `--show-context` or `--context-file` with `--context-format json` to inspect what will be sent upstream.
2. **Intent**: Pass a positional string, pipe stdin, or rely on `--intent-file/-f`. The parser prevents ambiguous mixes (inline + file) so you always know which source won.
3. **Output Format & Delivery**: Apply a template (`--context-template nvim`), request a polish pass (`--polish`/`--polish-model`), copy results to the clipboard (`--copy`), or jump straight into ChatGPT (`--open-chatgpt`). Interactive refinement and JSON streaming (`--stream jsonl`) round out the automation loop.

## Flag Strategy & Mechanics

### Core Run Modes

- `--interactive/-i`: Launches a refinement loop (TTY or `--interactive-transport`). You can add instructions between iterations; transport mode enables external tooling to push JSON commands.
- `--json`: Emits the final payload as structured JSON; **cannot** be combined with `--interactive` or `--interactive-transport` (enforced in `src/generate/pipeline.ts`), so pick one output path.
- `--stream jsonl`: Mirrors telemetry and lifecycle events to stdout—ideal for logging or UI bridges. Combine with `--quiet` to suppress boxed UI while still receiving machine-friendly events.

### Token Budgets & Context Overflow

When you attach lots of context, you may need deterministic trimming to stay within model limits.

Flags (generate mode):

- `--max-input-tokens <n>`: caps total input tokens (`intentTokens + systemTokens + fileTokens`).
- `--max-context-tokens <n>`: caps tokens reserved for **text context entries** (`fileTokens`).
- `--context-overflow <strategy>`: how to respond if budgets are exceeded.

Strategies:

- `fail`: throw and abort.
- `drop-smart`: drop smart-context entries first, then oldest-first.
- `drop-url`: drop URL context first, then oldest-first.
- `drop-largest`: drop largest entries first.
- `drop-oldest`: drop in original attachment order.

Notes:

- Budgets apply only to text context entries (`--context`, `--url`, `--smart-context`). Images/videos are not trimmed by these strategies.
- If budgets are enabled and trimming happens, the CLI emits a `context.overflow` stream event and prunes `contextPaths` in the final JSON payload.

Example (budgets + streaming):

```bash
prompt-maker-cli "Summarize the selected modules" \
  -c "src/**/*.ts" \
  --stream jsonl \
  --quiet \
  --progress=false \
  --max-context-tokens 8000 \
  --context-overflow drop-smart
```

### Context Assembly Flags

- `-c/--context <glob>`: Backed by `fast-glob`, supports includes/excludes (prefix with `!`), repeatable. Great for language- or folder-specific pulls.
- `--url <link>`: Fetches remote docs with progress callbacks.
- `--smart-context`: Runs the RAG pipeline (`smart-context-service.ts`) to index code/text under the current working tree or a custom `--smart-context-root`, automatically attaching the top 5 files under 25 KB.
- `--show-context`, `--context-file`, `--context-format text|json`: Inspect or persist the resolved context envelope for auditing.

### Media Inputs

- `--image <path>`: Attaches one or more images; they flow through to the prompt generator for multimodal models such as GPT-4o or Gemini.
- `--video <path>`: Triggers the Gemini pipeline. `src/generate/pipeline.ts` switches the model to `resolveGeminiVideoModel()` (default comes from config; commonly `gemini-3-pro-preview`) when the selected model is not Gemini, and prints a warning like `Switching to … to support video input.` Uploads run through `media-loader.ts`, which requires `GEMINI_API_KEY` and polls `GoogleAIFileManager` until the file becomes `ACTIVE`.

### Output Tailoring

- `--polish`, `--polish-model`: Runs a final LLM pass with the baked-in system prompt to tighten formatting while preserving structure. You can also set `PROMPT_MAKER_POLISH_MODEL` to choose a default polish model when `--polish-model` is omitted.
- `--context-template <name>`: Wraps the final prompt inside a named template (`nvim` is built-in; custom templates live in CLI config). The parser enforces non-empty template names.
- `--copy`, `--open-chatgpt`: Quality-of-life delivery flags.

### High-Value Combinations

- `--smart-context` + `--interactive`: Start with an embedding-ranked snapshot, then iteratively refine based on what you learn during the session—ideal for sprawling repos.
- `-c "<glob>"` + `--context-file prompt-context.md`: Capture exactly which files were read so teammates can replay the run.
- `--stream jsonl` + `--json`: Emit JSONL events during the run and print the final JSON payload at the end. (Note: this mixes JSONL + pretty JSON on stdout; for strictly machine-parseable stdout, prefer `--stream jsonl --quiet` and read the final payload from `~/.config/prompt-maker-cli/history.jsonl`.)
- `--context-template nvim` + `--copy`: Spits out an editor-ready buffer and places it on your clipboard for immediate paste.
- `--video` + `--polish`: Lean on Gemini for multimodal understanding, then run a polish pass (which reuses the Gemini credentials) for clean instructions.

### Conflicts and Guardrails

- `--json` vs `--interactive`: Mutually exclusive; the CLI throws early to prevent orphaned interactive sessions.
- Inline intent vs `--intent-file`: You must pick one; `resolveIntent()` enforces this and warns if you accidentally pass a file path immediately after `-i`.
- `--video` vs non-Gemini models: The CLI switches to the configured Gemini video model (see `resolveGeminiVideoModel()` in `src/generate/models.ts`) and prints a warning so you don’t accidentally run an unsupported combination. Ensure `GEMINI_API_KEY` is set.
- `--interactive` without a TTY: The CLI warns and downgrades to non-interactive mode; use `--interactive-transport` for headless setups.
- Empty `--context-template` or `--interactive-transport`: The parser trims values and rejects blank strings so you don’t end up with silent no-ops.

## Debugging Prompt Runs

- **Trace Context Inputs**: Pair `--show-context` with `--context-format json` during dry runs to print the exact `<file>` payloads gathered by `resolveFileContext` and `resolveSmartContextFiles`. When you need an audit trail, add `--context-file tmp/context-dump.md` to persist the snapshot that fed the LLM.
- **Watch Token Telemetry**: Every generation prints a Context Telemetry box sourced from `countTokens()`. Large spikes in `fileTokens` signal sloppy globs; tighten them or let `--smart-context` re-rank files automatically.
- **Budget overflows**: If you enable `--max-input-tokens`/`--max-context-tokens` and choose a non-`fail` overflow strategy, the CLI may emit `context.overflow` (streaming) and drop some context entries. Treat this as a meaningful warning and consider re-running with a higher budget or tighter globs.
- **Stream Everything**: `--stream jsonl` mirrors `progress.update`, `context.telemetry`, `context.overflow`, `generation.iteration.*`, and `upload.state` events to stdout. If you want to pipe directly into `jq`, combine with `--quiet` so non-JSON output doesn’t interleave with the stream.
- **Replay with History Artifacts**: `--json` writes the final payload (intent, context paths, iterations, polish metadata) and `appendToHistory()` stores it locally. Diff these blobs to understand how refinements changed the contract over time.
- **Interactive Diagnostics**: In TTY mode, each refinement is boxed via `displayPrompt()`. When headless, use `--interactive-transport /tmp/prompt.sock` and send JSON commands from another process; hook into the emitted `transport.*` events to orchestrate automated QA.
- **Clipboard & Flag Tracing**: Set `PROMPT_MAKER_DEBUG_FLAGS=1` to log a JSON snapshot of parsed flags (copy, polish, quiet, json, etc.) right after argument parsing. Add `PROMPT_MAKER_COPY_TRACE=1` (or rely on the debug flag) to emit `[pmc:copy …]` diagnostics showing when clipboard writes are attempted, skipped, or fail. Example: `PROMPT_MAKER_DEBUG_FLAGS=1 PROMPT_MAKER_COPY_TRACE=1 prompt-maker-cli --polish --copy …` instantly proves whether the CLI saw `--copy` and what happened inside `clipboardy`.
- **Media Upload Issues**: Stuck video uploads surface as repeated `upload.state` events. If they never flip from `start` to `stop`, confirm `GEMINI_API_KEY` and MIME support in `media-loader.ts` (e.g., `.mp4`, `.webm`).
- **Spinner Hygiene**: Disable spinners with `--progress=false` when your logs run in CI/CD; combine with `--quiet` to keep transcripts clean while still consuming JSONL telemetry.

## Template Playbook

- **Built-in templates**: Pass `--context-template nvim` to wrap the prompt inside the bundled buffer-friendly layout (resolved via `src/generate/context-templates.ts`). The template drops your artifact where `{{prompt}}` lives, so headings and shortcuts remain intact.
- **Custom templates**: Add entries to your CLI config (`contextTemplates` map). Reference them with `--context-template my-handoff`. The parser enforces non-empty names and throws if the template is missing, saving you from silent fallbacks.
- **Composable Delivery**: Templates stack with `--copy`, `--open-chatgpt`, and `--context-file`. Render a Neovim scratch buffer, copy it to the clipboard, and archive the text file in one run.
- **Previewing Output**: Pair `--context-template` with `--json` to capture both the raw prompt (in the JSON payload) and the rendered template (saved as `renderedPrompt`). This is handy when diffing changes across runs.

**Example – Sprint Handoff Template**

```bash
prompt-maker-cli "Summarize sprint 42 backend work" \
  -c "src/services/**/*.ts" \
  -c docs/notes/sprint-42.md \
  --context-template nvim \
  --copy
```

This command collects key files, wraps the result in the Neovim template, and drops it on your clipboard so you can open a scratch buffer and paste immediately.

## AI Systems Recipes

### Recipe: Agent Persona Contract

**Problem**
You’re designing a new autonomous agent and need a consistent system prompt that references existing SOPs, guardrails, and escalation rules.

**Solution**

```bash
prompt-maker-cli "Draft an agent persona spec for the Atlas migration agent, including goals, redlines, and escalation protocol." \
  -c agents/atlas/mission.md \
  -c agents/atlas/playbooks/*.md \
  -c security/guardrails.md \
  --context-template nvim \
  --polish
```

**Discussion**
Pulling SOPs and guardrails grounds the agent spec in real policy. The Neovim template yields a ready-to-paste contract, while the polish pass keeps tone tight—perfect for feeding into downstream orchestration frameworks.

### Recipe: MCP Server Capability Brief

**Problem**
You’re exposing a new Model Context Protocol (MCP) server and want a prompt that enumerates capabilities, auth model, and sample invocations for agent developers.

**Solution**

```bash
prompt-maker-cli "Summarize MCP server capabilities (tools, auth, rate limits) for documentation consumers." \
  -c mcp/servers/inventory/**/*.ts \
  -c mcp/docs/authentication.md \
  --smart-context-root mcp \
  --json \
  --context-file docs/mcp/inventory-brief.md
```

**Discussion**
Using both implementation files and docs ensures the LLM sees handler signatures plus narrative context. JSON output can be fed into build steps that publish docs or update MCP registries, while the context file anchors what inputs were used.

### Recipe: Multi-Agent Handoff Matrix

**Problem**
You have multiple specialized agents (research, implementation, QA) and need a structured prompt that defines handoff triggers and shared artifacts.

**Solution**

```bash
prompt-maker-cli "Create a multi-agent collaboration matrix (Research → Build → QA) with handoff triggers and shared artifacts." \
  -c agents/research/*.md \
  -c agents/build/*.md \
  -c agents/qa/*.md \
  -c docs/process/handoff-checklist.md \
  --interactive \
  --context-template nvim
```

**Discussion**
Stacking context from each agent’s playbook plus the handoff checklist produces a unified contract. Interactive mode lets stakeholders refine responsibilities live (e.g., add “QA can bounce back to Research if acceptance criteria missing”). The resulting template is ready to drop into orchestration configs or Confluence.

### Recipe: Auto Tool Discovery Guide

**Problem**
You’re wiring an agent runtime that needs to decide which internal tools to load per request; you want a prompt that inspects tool metadata and emits a selection strategy.

**Solution**

```bash
prompt-maker-cli "Given these tool manifests, decide when to load each tool and define fallback heuristics." \
  -c tools/manifests/**/*.json \
  -c docs/agents/tool-governance.md \
  --smart-context-root tools \
  --polish \
  --context-file agents/tool-discovery-plan.md
```

**Discussion**
Tool manifests plus governance rules allow the model to derive eligibility matrices (“Use `vector-search` when intents mention embeddings, fall back to `doc-search` otherwise”). Persisting the plan to `agents/tool-discovery-plan.md` lets your orchestrator ingest it or flag deviations during audits.

### Recipe: Embedding Pipeline Playbook

**Problem**
You’re rolling out a new embedding pipeline (chunking, filtering, indexing) and want a prompt that stitches together engineering docs, ETL scripts, and schema definitions into a deployment plan.

**Solution**

```bash
prompt-maker-cli "Produce an embedding ingestion playbook (chunking, filters, index rollout) for the Docs corpus." \
  -c pipelines/embeddings/docs/**/*.ts \
  -c pipelines/embeddings/docs/config/*.yaml \
  -c docs/data-quality/*.md \
  --stream jsonl \
  --context-template nvim
```

**Discussion**
Mixing code, config, and policy text yields actionable steps (pre-flight validation, TF-IDF filters, reindex cadence). JSONL streaming captures intermediate telemetry if you need to trace iterations. Pair with `--json` if you want to store the final prompt alongside the rollout ticket.

### Recipe: Evaluation Harness Generator

**Problem**
You need a consistent process for evaluating agent prompts against regression suites (fixtures, rubrics, scoring scripts).

**Solution**

```bash
prompt-maker-cli "Design an evaluation harness (fixtures, metrics, scoring scripts) for the Atlas agent." \
  -c eval/fixtures/atlas/**/*.json \
  -c eval/rubrics/**/*.md \
  -c scripts/metrics/atlas_eval.ts \
  --polish \
  --context-template nvim
```

**Discussion**
By combining fixtures, qualitative rubrics, and scoring code, the CLI outputs a contract describing how to run and interpret evaluations. Feed the result into CI (e.g., nightly evaluation jobs) or share with red teams.

### Recipe: Retriever Tuning Blueprint

**Problem**
Your RAG system needs per-domain retriever settings (chunk sizes, rerankers, max matches). You want a prompt that reviews current telemetry and proposes tuned parameters.

**Solution**

```bash
prompt-maker-cli "Recommend retriever tuning parameters for the Support knowledge base." \
  -c rag/support/telemetry/*.json \
  -c rag/support/config/*.yaml \
  -c docs/rag/retriever-guidelines.md \
  --smart-context-root rag \
  --json
```

**Discussion**
Telemetry plus configs plus guidelines help the model suggest evidence-backed changes. JSON output is perfect for feeding into dashboards or PR bots that annotate config diffs with rationale.

### Recipe: Safety Review Packet

**Problem**
Before launching a new agent, compliance requires a safety packet covering data handling, prompt safeguards, and escalation paths.

**Solution**

```bash
prompt-maker-cli "Assemble a safety review packet for the Commerce agent (data flow, prompt safeguards, escalation)." \
  -c agents/commerce/persona.md \
  -c agents/commerce/prompts/*.md \
  -c security/safety-checklist.md \
  -c docs/legal/data-retention.md \
  --polish \
  --context-file reviews/commerce-safety.md
```

**Discussion**
Mixing persona, prompt contracts, safety checklists, and legal guidance lets the model produce a thorough review doc. Writing to `reviews/commerce-safety.md` ensures auditors and approvers have a consistent artifact to sign off.

### Recipe: Launch Readiness Checklist

**Problem**
Product and Ops need a single document confirming an agent is launch-ready (docs, telemetry, on-call coverage, rollback plan).

**Solution**

```bash
prompt-maker-cli "Create a launch readiness checklist for the Commerce agent (docs, telemetry SLAs, on-call, rollback)." \
  -c agents/commerce/persona.md \
  -c docs/launch/commerce/*.md \
  -c ops/runbooks/commerce-oncall.md \
  -c metrics/commerce/uptime.json \
  --context-template nvim \
  --json
```

**Discussion**
Combining product docs, runbooks, and live metrics results in a comprehensive go/no-go sheet. JSON output can feed dashboards or Slack bots that notify stakeholders when all boxes are checked.

### Recipe: Hallucination Test Suite Plan

**Problem**
Before shipping an assistant, you want targeted hallucination tests covering sensitive topics, long contexts, and out-of-domain queries.

**Solution**

```bash
prompt-maker-cli "Draft a hallucination testing plan (datasets, prompts, scoring) for the Support assistant." \
  -c eval/hallucination/support-fixtures/**/*.json \
  -c docs/safety/hallucination-tests.md \
  -c scripts/eval/hallucination_runner.ts \
  --interactive \
  --context-file eval/support-hallucination-plan.md
```

**Discussion**
The CLI merges fixtures, policy docs, and runner scripts to build a plan across scenarios. Interactive refinement lets researchers iteratively add edge cases discovered mid-review. The saved plan becomes a living document tied to release gates.

### Recipe: Automated Post-Mortem Primer

**Problem**
After an incident, you want a prompt that synthesizes logs, user reports, and recovery steps into a blameless post-mortem draft.

**Solution**

```bash
prompt-maker-cli "Produce a blameless post-mortem draft for incident INC-423 (timeline, impact, action items)." \
  -c incidents/INC-423/logs/*.log \
  -c incidents/INC-423/notes.md \
  -c docs/postmortem/template.md \
  --polish \
  --context-file incidents/INC-423/postmortem-draft.md
```

**Discussion**
Feeding raw logs, investigator notes, and the official template ensures the output respects your post-mortem format. The polish pass tidies tone, while the saved draft accelerates follow-up reviews and action-item tracking.

## Developer Recipes

### Recipe: Crash Reproduction Capsule

**Problem**
QA reported an intermittent null-pointer crash—you need a prompt that guides the LLM through logs, stack traces, and reproduction steps.

**Solution**

```bash
prompt-maker-cli "Diagnose and propose fixes for the null-pointer crash when saving drafts." \
  -c "logs/crash/*.log" \
  -c "src/app/**/DraftService.ts" \
  --smart-context \
  --context-file crash-context.md \
  --json
```

**Discussion**
Combining explicit globs with `--smart-context` pulls in the most relevant nearby files. Writing `crash-context.md` preserves the exact evidence bundle, while `--json` records iterations/refinements for ticket attachments.

### Recipe: API Contract Snapshot

**Problem**
You must brief another team on the current REST/GraphQL surface, including payload shapes and validation rules.

**Solution**

```bash
prompt-maker-cli "Summarize public API endpoints with request/response schemas and validation rules." \
  -c "src/api/**/*.ts" \
  -c "docs/api/*.md" \
  --polish \
  --show-context
```

**Discussion**
The TypeScript + markdown mix gives the model both typed contracts and human notes. `--show-context` lets you validate that only the intended files were loaded, and `--polish` ensures the final document reads like a publishable API brief.

### Recipe: Framework Migration Coach

**Problem**
You’re migrating from Redux Toolkit to Zustand and need a structured plan referencing existing state slices.

**Solution**

```bash
prompt-maker-cli "Create a step-by-step plan to migrate Redux Toolkit slices to Zustand with shared selectors." \
  -c "src/state/**/*.ts" \
  -c "docs/architecture/state.md" \
  --smart-context-root src \
  --interactive
```

**Discussion**
Initial context sketches the architecture, while `--interactive` lets you add refinements after the first draft (e.g., “address SSR data hydration”). Restrict smart-context scanning to `src` to keep embeddings fast.

### Recipe: Dependency Upgrade Risk Brief

**Problem**
Before upgrading `nx` and `vite`, you want a prompt that enumerates risks, test plans, and rollback steps using release notes and local config.

**Solution**

```bash
prompt-maker-cli "Assess upgrading Nx and Vite to the next minor release, listing risky plugins and verification steps." \
  -c package.json \
  -c nx.json \
  -c "docs/releases/nx/*.md" \
  --polish-model gpt-4o-mini \
  --copy
```

**Discussion**
Pointing to config files plus curated release notes equips the model with both current state and vendor guidance. Overriding the polish model keeps consistency with other platform reviews, and `--copy` macros the result straight into your change request doc.

## Self-Directed Learning Recipes

### Recipe: Technical Textbook Navigator

**Problem**
You’re working through a dense systems textbook and want a prompt to summarize each chapter, surface prerequisites, and suggest practice problems.

**Solution**

```bash
prompt-maker-cli "Break down Chapter 6 of 'Distributed Systems' into prerequisites, key ideas, and practice drills." \
  -c notes/distributed-systems/ch06/*.md \
  -c "books/distributed-systems/ch06/**/*.pdf" \
  --context-template nvim \
  --polish
```

**Discussion**
Attach your reading notes plus exported chapter snippets (converted to markdown/pdf text). The template outputs a structured study card, while polishing keeps terminology precise. Repeat per chapter to build a learning map.

### Recipe: Deep Research Dossier

**Problem**
You need a self-study prompt that orchestrates multi-source research (papers, blogs, RFCs) and proposes a reading/experiment plan.

**Solution**

```bash
prompt-maker-cli "Assemble a deep-research plan for vector databases (questions, sources, experiments)." \
  -c research/vector-db/papers/*.md \
  -c research/vector-db/notes/**/*.md \
  -c bookmarks/vector-db/**/*.md \
  --smart-context \
  --json \
  --context-file research/vector-db/dossier.md
```

**Discussion**
Combining curated sources with smart-context ensures the prompt generator sees both high-signal references and ambient notes. JSON output lets you sync the research plan to Notion or Obsidian while the markdown dossier becomes your running log.

### Recipe: Language Learning Companion

**Problem**
You’re teaching yourself Japanese and want prompts that adapt grammar drills to your mistakes and native material.

**Solution**

```bash
prompt-maker-cli "Design a Japanese study session focusing on keigo and business email corrections." \
  -c language/japanese/mistake-log.md \
  -c language/japanese/reading/sales-emails/*.txt \
  --interactive \
  --context-template nvim
```

**Discussion**
Feed the CLI your mistake log plus authentic examples; interactive mode lets you refine instructions after each session (“add listening exercises”, “increase kanji coverage”). The template keeps sections organized (Warm-up, Drills, Reflection) for spaced repetition apps.

### Recipe: Software Engineering Mastery Sprint

**Problem**
You want a weekly mastery plan that targets architectural topics, code katas, and review prompts tailored to your repo.

**Solution**

```bash
prompt-maker-cli "Plan a weekly mastery sprint covering event-driven architecture patterns in this repo." \
  -c "src/**/*.ts" \
  -c docs/architecture/*.md \
  -c notes/learning-goals.md \
  --smart-context \
  --polish \
  --context-file learning/eda-week.md
```

**Discussion**
Mixing real code with architecture docs ensures the prompt references live examples. Smart context narrows the scope to relevant files, while the polish pass produces a schedule you can commit to version control and revisit in retros.

### Recipe: Spaced-Repetition Exporter

**Problem**
You want to convert textbook notes into spaced-repetition cards (Front/Back/Extra) compatible with tools like Anki or Mochi.

**Solution**

```bash
prompt-maker-cli "Turn these chapter notes into spaced repetition cards with cloze deletions." \
  -c notes/distributed-systems/ch06/*.md \
  -c notes/distributed-systems/glossary.md \
  --context-template nvim \
  --polish \
  --context-file learning/ch06-cards.md
```

**Discussion**
Feed the CLI your notes and glossary; the template organizes each card, while polishing enforces concise wording. The saved markdown can be imported or transformed into CSV for your spaced-repetition app.

### Recipe: Socratic Tutor Flow

**Problem**
You want prompts that guide you via questions instead of answers, forcing active recall for each concept.

**Solution**

```bash
prompt-maker-cli "Act as a Socratic tutor for Chapter 3 of 'Programming Languages', asking layered questions and hints." \
  -c notes/pl/ch03/*.md \
  -c exercises/pl/ch03/*.md \
  --interactive \
  --stream jsonl
```

**Discussion**
Context from notes plus exercises lets the model pose targeted questions. Interactive mode means you can answer, refine, or request hints between iterations; JSONL streaming logs each tutor exchange for later review.

### Recipe: Feynman Method Worksheet

**Problem**
You want to explain a dense topic in simple language, identify gaps, then loop back with targeted readings.

**Solution**

```bash
prompt-maker-cli "Apply the Feynman technique to Raft consensus (explain like I'm new, spot gaps, suggest drills)." \
  -c notes/distributed-systems/raft.md \
  -c "src/consensus/raft/**/*.ts" \
  -c research/raft/open-questions.md \
  --polish \
  --context-template nvim
```

**Discussion**
Combining conceptual notes, actual code, and open questions yields a worksheet with four sections: Teach, Identify Gaps, Review, Simplify. Re-run after each study session to track how explanations improve over time.

## Git Commit Workflows

### Recipe: Conventional Commit Forges

**Problem**
You want consistent conventional commit messages derived from the staged diff plus nearby docs.

**Solution**

```bash
git diff --cached > /tmp/staged.patch && \
prompt-maker-cli "Write a conventional commit message with summary + body + testing notes." \
  -c /tmp/staged.patch \
  -c docs/CONTRIBUTING.md \
  --polish \
  --copy
```

**Discussion**
Export the staged diff to a temp file so `-c` can ingest it alongside your contributing guide. The polish pass enforces tone guidelines, while `--copy` lets you paste the final result directly into `git commit`.

### Recipe: Multi-Commit Release Notes

**Problem**
You’re preparing a release branch and want a prompt that condenses the last N commits into user-facing notes plus internal TODOs.

**Solution**

```bash
git log -n 20 --pretty=medium > /tmp/release-log.txt && \
prompt-maker-cli "Summarize these commits into release highlights, breaking changes, and QA focus." \
  -c /tmp/release-log.txt \
  -c CHANGELOG.md \
  --context-template nvim \
  --json
```

**Discussion**
Feeding `git log` output plus the existing changelog ensures the model sees both history and format expectations. Capturing JSON output gives you a structured artifact you can commit or attach to release tickets.

## Editor Workflow Recipes

### Recipe: VS Code Task Runner

**Problem**
You want a one-click VS Code task that summarizes the currently open file plus related tests for rubber-ducking.

**Solution**

```bash
prompt-maker-cli "Explain the active module, its dependencies, and edge cases." \
  -c "${file}" \
  -c "${workspaceFolder}/src/**/*.spec.ts" \
  --smart-context-root ${workspaceFolder} \
  --context-template nvim
```

**Discussion**
Define this as a VS Code task with `type: shell` so `${file}` and `${workspaceFolder}` expand automatically. The smart-context scan pulls in nearby helpers while the template keeps the response readable inside VS Code’s terminal panel.

### Recipe: JetBrains External Tool for Code Reviews

**Problem**
You need an IDE command (WebStorm, IntelliJ, etc.) that packages currently selected files and generates code-review talking points.

**Solution**

```bash
prompt-maker-cli "Prepare code review notes for the selected files, focusing on risks and tests." \
  -c "$FilePath$" \
  -c "$ContentRoot$/tests/**/*.ts" \
  --show-context \
  --polish
```

**Discussion**
Configure an External Tool that sends `$FilePath$` and `$ContentRoot$` placeholders. JetBrains pipes output to the Run tool window, so `--show-context` doubles as a sanity check before you paste the generated review notes into your PR.

### Recipe: Zed Editor Tasks for Pairing Sessions

**Problem**
You’re hosting a remote pairing session in Zed and want quick, repeatable prompts capturing the current pane and design doc.

**Solution**

```bash
prompt-maker-cli "Act as a pairing partner; summarize this buffer and list open design questions." \
  -c "$ZED_FOCUSED_FILE" \
  -c docs/design/active/*.md \
  --progress=false \
  --stream jsonl
```

**Discussion**
Register a Zed Task that exports `ZED_FOCUSED_FILE`. Disabling the spinner keeps Zed’s task output tidy, while JSONL streaming lets you capture telemetry in a side panel or send it to collaborators via `websocat`.

## Jira Ticket Recipes

### Recipe: Convert Requirements to Gherkin

**Problem**
Product dropped a prose requirements blob into a Jira ticket; you need executable acceptance criteria in Cucumber/Gherkin format.

**Solution**

```bash
prompt-maker-cli "Rewrite these requirements as Gherkin acceptance criteria with Scenario/Scenario Outline blocks." \
  -c jira/REQ-582-description.md \
  -c jira/REQ-582-comments/*.md \
  --context-template nvim \
  --polish
```

**Discussion**
Export the Jira description/comments (many teams sync them via API into `jira/`). Feeding both files gives the model complete context. The template delivers a clean block ready for Jira Markdown, while the polish pass enforces consistent Given/When/Then phrasing.

### Recipe: Ticket Grooming Checklist

**Problem**
You want to confirm that high-priority tickets include personas, acceptance tests, and data considerations before sprint planning.

**Solution**

```bash
prompt-maker-cli "Audit this Jira ticket for grooming completeness (personas, data, test cases, open questions)." \
  -c jira/REQ-610-description.md \
  -c jira/REQ-610-attachments/*.md \
  --smart-context-root jira \
  --json
```

**Discussion**
Smart context pulls in related tickets or design notes under `jira/`. Emitting JSON lets you store the audit result back in Jira via automation (each run lists contextPaths so you can trace the evidence used for the checklist).

### Recipe: Regression Matrix for Linked Issues

**Problem**
Before closing a ticket with multiple linked bugs, you need a regression plan referencing the linked IDs and their components.

**Solution**

```bash
prompt-maker-cli "Create a regression checklist covering all linked Jira issues and their components." \
  -c jira/REQ-599-description.md \
  -c jira/links/REQ-599-linked-issues.md \
  -c docs/testing/regression-template.md \
  --copy \
  --context-file jira/REQ-599-regression.md
```

**Discussion**
Combining the ticket body, linked-issue export, and a regression template ensures the LLM maps each linked bug to concrete verification steps. Writing the output to `jira/REQ-599-regression.md` gives QA a canonical doc, while `--copy` makes it easy to paste into the Jira comment thread.

### Recipe: Automating via Jira Webhooks

**Problem**
You want Jira to trigger prompt-maker-cli automatically whenever a ticket changes state (e.g., when QA moves an issue to “Ready for Test”).

**Solution**

1. Configure a Jira webhook targeting an internal endpoint (e.g., a lightweight Node/Express or serverless function).
2. Inside the webhook handler, fetch the ticket body/attachments via Jira REST and write them to disk:
   ```bash
   curl -u "$JIRA_USER:$JIRA_TOKEN" \
     "$JIRA_BASE/rest/api/3/issue/$ISSUE_KEY?expand=renderedFields" \
     -o "/tmp/${ISSUE_KEY}.json"
   ```
3. Invoke prompt-maker-cli with the downloaded payload and any playbook files:
   ```bash
   prompt-maker-cli "Generate QA test matrix for ${ISSUE_KEY}" \
     -c "/tmp/${ISSUE_KEY}.json" \
     -c docs/testing/regression-template.md \
     --json \
     --context-file "jira/${ISSUE_KEY}-artifact.md"
   ```
4. Post the resulting artifact back to Jira (comment or attachment) using the JSON output.

**Discussion**
The webhook handler acts as glue: it hydrates the ticket data, runs the CLI headlessly, and records the artifact paths provided in the JSON payload. Because prompt-maker-cli streams progress, you can enable `--stream jsonl` for observability or push logs to your monitoring stack. Remember to guard concurrent runs with a queue so multiple Jira events don’t clobber `/tmp` assets.

### Sample Node.js Webhook Handler

```ts
import express from 'express'
import { execFile } from 'node:child_process'
import { writeFile } from 'node:fs/promises'
import fetch from 'node-fetch'

const app = express()
app.use(express.json({ limit: '1mb' }))

app.post('/jira-webhook', async (req, res) => {
  const issueKey = req.body.issue?.key
  if (!issueKey) {
    return res.status(400).send('Missing issue key')
  }

  const jiraResponse = await fetch(
    `${process.env.JIRA_BASE}/rest/api/3/issue/${issueKey}?expand=renderedFields`,
    {
      headers: {
        Authorization: `Basic ${Buffer.from(`${process.env.JIRA_USER}:${process.env.JIRA_TOKEN}`).toString('base64')}`,
        Accept: 'application/json',
      },
    },
  )

  const payload = await jiraResponse.text()
  const inputPath = `/tmp/${issueKey}.json`
  await writeFile(inputPath, payload, 'utf8')

  execFile(
    'prompt-maker-cli',
    [
      `Generate QA test matrix for ${issueKey}`,
      '-c',
      inputPath,
      '-c',
      'docs/testing/regression-template.md',
      '--json',
      '--context-file',
      `jira/${issueKey}-artifact.md`,
    ],
    (error, stdout, stderr) => {
      if (error) {
        console.error(stderr)
      } else {
        console.log(stdout)
      }
    },
  )

  res.status(202).send('Processing')
})

app.listen(process.env.PORT ?? 3000, () => {
  console.log('Webhook listener ready')
})
```

**Highlights**

- Uses the Jira REST API to pull fresh ticket data.
- Writes the payload to `/tmp` and shells out to prompt-maker-cli.
- Streams CLI output to server logs; connect this to your observability stack or ship the JSON artifact back to Jira asynchronously.

## CI Integration Recipes

### Recipe: Pull Request Prompt Validator (Generic CI)

**Problem**
You want CI to fail fast when prompt-maker-cli can’t assemble prompts for changed files (missing context, model issues, etc.).

**Solution**
Add a CI job that:

```bash
CHANGED=$(git diff --name-only origin/main...HEAD | tr '\n' ' ')
prompt-maker-cli "Summarize risk areas for this PR" \
  -c $CHANGED \
  --smart-context \
  --progress=false \
  --json
```

**Discussion**
Treat failures (non-zero exit codes) as CI blockers, surfacing actionable errors (missing files, credential issues) before reviewers even open the PR. Capturing JSON output allows downstream steps to upload artifacts or comment on the PR.

### Recipe: Nightly Requirements Drift Report

**Problem**
You run nightly jobs that compare Jira specs to actual code; when they drift, generate prompts instructing teams to reconcile differences.

**Solution**

```bash
prompt-maker-cli "Detect drift between Jira specs and implementation for ${SERVICE_NAME}" \
  -c "exports/jira/${SERVICE_NAME}-requirements.md" \
  -c "src/services/${SERVICE_NAME}/**/*.ts" \
  --context-template nvim \
  --stream jsonl \
  --context-file "reports/${SERVICE_NAME}-drift.md"
```

**Discussion**
Schedule this in your CI orchestrator (CircleCI, Buildkite, etc.). JSONL streaming feeds live telemetry into logs, while the context file serves as a paper trail for compliance/audit teams.

### Recipe: GitHub Actions Artifact Builder

**Problem**
You want a GitHub Actions workflow that generates prompts for every PR and attaches the result as an artifact/comment.

**Solution**

```yaml
name: prompt-maker

on:
  pull_request:
    types: [opened, synchronize]

jobs:
  build-prompt:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - uses: actions/setup-node@v4
        with:
          node-version: 20
      - run: npm ci
      - name: Generate PR prompt
        run: |
          CHANGED=$(git diff --name-only origin/${{ github.base_ref }}...HEAD)
          prompt-maker-cli "Review context for PR #${{ github.event.number }}" \
            -c "$CHANGED" \
            --smart-context \
            --json \
            --context-file pr-prompts/${{ github.event.number }}.md > prompt.json
      - uses: actions/upload-artifact@v4
        with:
          name: pr-${{ github.event.number }}-prompt
          path: |
            prompt.json
            pr-prompts/${{ github.event.number }}.md
```

**Discussion**
Actions runners install dependencies once (`npm ci`). The workflow diffs against the PR base branch, generates prompts, and uploads both the JSON payload and rendered markdown. Pair this with a follow-up step that comments on the PR using `gh api` if you want reviewers to see the artifact inline.

### Recipe: CircleCI Prompt Gate

**Problem**
You need CircleCI to enforce that prompt-maker-cli succeeds whenever a PR touches specific directories (e.g., `src/` or `packages/core/`).

**Solution**

```yaml
version: 2.1

orbs:
  node: circleci/node@6.2.0

jobs:
  prompt_gate:
    executor: node/default
    steps:
      - checkout
      - node/install-packages:
          pkg-manager: npm
      - run:
          name: Detect changed files
          command: |
            git fetch origin ${CIRCLE_BRANCH} --depth=1
            git diff --name-only origin/main...HEAD > changed.txt
      - run:
          name: Generate prompt artifact
          command: |
            CHANGED=$(tr '\n' ' ' < changed.txt)
            prompt-maker-cli "CircleCI prompt gate for ${CIRCLE_BRANCH}" \
              -c "$CHANGED" \
              --smart-context \
              --json \
              --context-file ci-artifacts/${CIRCLE_BUILD_NUM}.md > prompt.json
      - store_artifacts:
          path: ci-artifacts
      - store_artifacts:
          path: prompt.json

workflows:
  prompt-workflow:
    jobs:
      - prompt_gate
```

**Discussion**
This job runs on every pipeline. The stored artifacts (JSON + markdown) let reviewers inspect prompt contracts directly from the CircleCI UI. You can gate merges by requiring this workflow to pass.

### Recipe: CircleCI Nightly Prompt Publisher

**Problem**
You want a scheduled CircleCI workflow that enumerates services, runs prompt-maker-cli per service, and uploads the outputs for compliance.

**Solution**

```yaml
workflows:
  nightly-prompts:
    triggers:
      - schedule:
          cron: '0 3 * * *'
          filters:
            branches:
              only: main
    jobs:
      - prompt_gate
      - run_service_prompts

jobs:
  run_service_prompts:
    docker:
      - image: cimg/node:20.11
    steps:
      - checkout
      - run: npm ci
      - run:
          name: Generate prompts per service
          command: |
            for SERVICE in billing auth analytics; do
              prompt-maker-cli "Nightly drift check for $SERVICE" \
                -c "exports/jira/${SERVICE}-requirements.md" \
                -c "services/${SERVICE}/src/**/*.ts" \
                --context-file reports/${SERVICE}-drift.md \
                --json > reports/${SERVICE}-drift.json
            done
      - store_artifacts:
          path: reports
```

**Discussion**
Scheduled workflows keep documentation in sync. Each service run produces both markdown and JSON artifacts, giving compliance and platform engineers a searchable trail. Expand the `SERVICE` list or drive it from a manifest file as your platform grows.

## NeoVim Plugin Integration Recipes

### Recipe: Buffer Snapshot from a Plugin Command

**Problem**
You maintain a NeoVim plugin that exports the active buffer to a temp file and wants prompt-maker-cli to ingest it with surrounding context.

**Solution**

```bash
prompt-maker-cli "Review the attached buffer for race conditions and propose fixes." \
  -c "/tmp/nvim-buffer-*.md" \
  --context-template nvim \
  --copy
```

**Discussion**
Have your plugin write the current buffer to `/tmp/nvim-buffer-<id>.md`, then call the CLI via `vim.fn.jobstart`. Using the `nvim` template means the returned prompt is already formatted for a scratch buffer.

### Recipe: Interactive Refinement via Remote Transport

**Problem**
You want the plugin to send refinement commands without leaving NeoVim.

**Solution**

```bash
prompt-maker-cli "Draft a refactor plan for the active file." \
  -c "/tmp/nvim-buffer-current.ts" \
  --interactive-transport /tmp/prompt-maker.sock \
  --stream jsonl
```

**Discussion**
The plugin listens for `interactive.awaiting` events from the JSONL stream and surfaces prompts inside NeoVim. Users type refinements, and the plugin pushes `{"type":"refine","instruction":"..."}` messages through the Unix socket.

### Recipe: Project-Wide Summaries from Telescope Picks

**Problem**
You use Telescope to select files and want to pass all selections as context without manual globs.

**Solution**

```bash
prompt-maker-cli "Summarize the selected files for code review notes." \
  -c "/tmp/nvim-selected-files/*.md" \
  --smart-context-root $(pwd) \
  --context-template nvim \
  --progress=false
```

**Discussion**
The plugin writes each Telescope selection to `/tmp/nvim-selected-files/`. Adding `--smart-context-root` brings in nearby matches, while `--progress=false` keeps Neovim’s command output clean during background runs.

## Recipes

### Recipe: Advanced Context Selection

**Problem**
You need every TypeScript file under `src/`, but none of the tests or stories should pollute the prompt.

**Solution**

```bash
prompt-maker-cli "Document the shared data loader contract" \
  -c "src/**/*.ts" \
  -c "!src/**/*.test.ts" \
  -c "!src/**/*.spec.ts" \
  -c "!src/**/__tests__/**" \
  --show-context --context-format json
```

**Discussion**
`fast-glob` honors negated patterns, so you can stack `!` excludes to prune tests. `--show-context --context-format json` prints the resolved files (path + content) to stderr/stdout so you can verify exactly what the LLM sees before generating.

---

### Recipe: Image Enhancement with “Nano Banana”

**Problem**
You want to attach a marketing mockup and have your AI partner—code-named **Nano Banana**—describe improvements for accessibility and polish.

**Solution**

```bash
prompt-maker-cli "Nano Banana, critique and enhance the attached hero mockup for accessibility and contrast." \
  --image assets/hero-v2.png \
  --polish \
  --model gpt-4o-mini
```

**Discussion**
`--image` accepts repeatable paths, so drop in multiple angles if needed. Mention Nano Banana directly in the intent to anchor the persona. Adding `--polish` runs the meta-refinement pass, giving you a crisp, well-structured instruction set tailored to GPT-4o’s multimodal strengths.

---

### Recipe: Shopping Assistant Prompt

**Problem**
You need a prompt that tells an LLM to comb Amazon for a specific brand/price window and report recommended products.

**Solution**

```bash
prompt-maker-cli "Research Amazon listings for Breville espresso machines under $900 and surface top 3 matches with pros/cons, freshness check, and price volatility notes." \
  --context-template nvim \
  --copy
```

**Discussion**
Here the intent fully encodes the search criteria, and `--context-template nvim` wraps the response in a scratch-buffer-friendly format so you can paste it into Neovim and keep iterating. `--copy` places the final prompt on your clipboard for immediate use in your preferred chat client.

---

### Recipe: Engineering / CAD Generation

**Problem**
You must solicit OpenSCAD or Python (CadQuery) code that produces a printable enclosure, combining local design guidelines as context.

**Solution**

```bash
prompt-maker-cli "Produce OpenSCAD or CadQuery code for a snap-fit Raspberry Pi 5 enclosure with filleted edges and removable lid." \
  -c "docs/cad/clearance-table.md" \
  -c "docs/cad/materials/*.md" \
  --smart-context --smart-context-root ./hardware \
  --interactive
```

**Discussion**
Static globs inject canonical clearance/material tables, while `--smart-context` surfaces the five most relevant hardware notes under `./hardware`. Kick on `--interactive` to iterate: after each draft, feed refinements like “increase wall thickness to 2.2 mm” without rebuilding the command.

---

### Recipe: Genealogical Research Plan

**Problem**
You need a structured research strategy for a specific ancestor, weaving in source notes stored locally.

**Solution**

```bash
prompt-maker-cli "Draft a genealogical research plan for Mary O'Hara (b. 1884, County Mayo → Boston 1906)." \
  -c "research/mayo-family/*.md" \
  --smart-context-root research \
  --context-file mary-ohara-plan.md \
  --context-format text \
  --json
```

**Discussion**
The CLI resolves explicit notes plus smart-context matches from the broader `research` directory, then writes the merged context to `mary-ohara-plan.md` for archival. `--json` emits the final prompt payload (intent, context paths, iterations) so you can log runs programmatically—remember this disables interactive mode.

---

### Recipe: Martial Arts Video Analysis

**Problem**
You captured a sparring session and need a prompt that asks the model to analyze timing, guard discipline, and footwork.

**Solution**

```bash
prompt-maker-cli "Break down this kali sparring clip—focus on timing windows, guard recovery, and footwork corrections." \
  --video media/kali-round3.mp4 \
  --model gemini-3-pro-preview \
  --polish \
  --progress=false
```

**Discussion**
Passing `--video` causes `src/generate/pipeline.ts` to call `resolveGeminiVideoModel()`, overriding non-Gemini choices with your configured Gemini video model (commonly `gemini-3-pro-preview`) so the Files API can ingest your clip. The upload path (`media-loader.ts`) demands a readable file and `GEMINI_API_KEY`; the CLI shows upload progress via `upload.state` events. Gemini’s multimodal context pairs well with a polish pass to distill the final coaching checklist.

---
</file>

<file path="src/__tests__/useGenerationPipeline.test.ts">
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import { useGenerationPipeline } from '../tui/hooks/useGenerationPipeline'
jest.mock('wrap-ansi', () => jest.fn((text: string) => text))
jest.mock('../tui/provider-status', () => ({
  checkModelProviderStatus: jest.fn(),
}))
jest.mock('../generate-command', () => ({
  runGeneratePipeline: jest.fn().mockResolvedValue({
    finalPrompt: 'Prompt',
    model: 'gpt-4o-mini',
    iterations: 1,
    telemetry: null,
    payload: {},
  }),
  maybeCopyToClipboard: jest.fn(),
  maybeOpenChatGpt: jest.fn(),
}))
jest.mock('../prompt-generator-service', () => ({
  generatePromptSeries: jest.fn().mockResolvedValue({
    reasoning: 'r',
    overviewPrompt: '# Overview',
    atomicPrompts: [{ title: 'Step', content: 'Do a thing\n\nValidation: check' }],
  }),
  isGemini: jest.fn((model: string) => model.startsWith('gemini')),
}))
jest.mock('../generate/models', () => {
  const actual = jest.requireActual('../generate/models') as typeof import('../generate/models')
  return {
    ...actual,
    resolveGeminiVideoModel: jest.fn().mockResolvedValue('gemini-2.5-pro'),
  }
})
jest.mock('../file-context', () => ({ resolveFileContext: jest.fn().mockResolvedValue([]) }))
jest.mock('../url-context', () => ({ resolveUrlContext: jest.fn().mockResolvedValue([]) }))
jest.mock('../smart-context-service', () => ({
  resolveSmartContextFiles: jest.fn().mockResolvedValue([]),
}))
jest.mock('node:fs/promises', () => ({ mkdir: jest.fn(), writeFile: jest.fn() }))
const providerStatusModule = jest.requireMock('../tui/provider-status') as {
  checkModelProviderStatus: jest.Mock
}
const generateCommandModule = jest.requireMock('../generate-command') as {
  runGeneratePipeline: jest.Mock
}
const promptGeneratorModule = jest.requireMock('../prompt-generator-service') as {
  generatePromptSeries: jest.Mock
}
const fileContextModule = jest.requireMock('../file-context') as {
  resolveFileContext: jest.Mock
}
const urlContextModule = jest.requireMock('../url-context') as {
  resolveUrlContext: jest.Mock
}
const smartContextModule = jest.requireMock('../smart-context-service') as {
  resolveSmartContextFiles: jest.Mock
}
const fsPromisesModule = jest.requireMock('node:fs/promises') as {
  mkdir: jest.Mock
  writeFile: jest.Mock
}
const dom = new JSDOM('<!doctype html><html><body></body></html>')
const globalEnv = globalThis as typeof globalThis & {
  window: Window & typeof globalThis
  document: Document
  navigator: Navigator
}
globalEnv.window = dom.window as typeof globalEnv.window
globalEnv.document = dom.window.document as Document
globalEnv.navigator = dom.window.navigator
describe('useGenerationPipeline', () => {
  const baseOptions = {
    files: [] as string[],
    urls: [] as string[],
    images: [] as string[],
    videos: [] as string[],
    pdfs: [] as string[],
    smartContextEnabled: false,
    smartContextRoot: null,
    metaInstructions: '',
    budgets: {
      maxContextTokens: null,
      maxInputTokens: null,
      contextOverflowStrategy: null,
    },
    interactiveTransportPath: undefined as string | undefined,
    terminalColumns: 80,
    polishModelId: null,
    jsonOutputEnabled: false,
    copyEnabled: false,
    chatGptEnabled: false,
    isTestCommandRunning: false,
    notify: jest.fn(),
  }
  beforeEach(() => {
    jest.clearAllMocks()
    generateCommandModule.runGeneratePipeline.mockResolvedValue({
      finalPrompt: 'Prompt',
      model: 'gpt-4o-mini',
      iterations: 1,
      telemetry: null,
      payload: {},
    })
    promptGeneratorModule.generatePromptSeries.mockResolvedValue({
      reasoning: 'r',
      overviewPrompt: '# Overview',
      atomicPrompts: [{ title: 'Step', content: 'Do a thing\n\nValidation: check' }],
    })
    fileContextModule.resolveFileContext.mockResolvedValue([])
    urlContextModule.resolveUrlContext.mockResolvedValue([])
    smartContextModule.resolveSmartContextFiles.mockResolvedValue([])
    fsPromisesModule.mkdir.mockResolvedValue(undefined)
    fsPromisesModule.writeFile.mockResolvedValue(undefined)
  })
  it('aborts runGeneration when provider credentials are missing', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'missing',
      message: 'OPENAI_API_KEY missing',
    })
    const pushHistory = jest.fn()
    const onProviderStatusUpdate = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
        onProviderStatusUpdate,
      }),
    )
    await act(async () => {
      await result.current.runGeneration({ intent: 'Do a thing' })
    })
    expect(pushHistory).toHaveBeenCalledWith(
      'Generation aborted: OpenAI unavailable (OPENAI_API_KEY missing).',
      'system',
    )
    expect(generateCommandModule.runGeneratePipeline).not.toHaveBeenCalled()
    expect(onProviderStatusUpdate).toHaveBeenCalledWith(
      expect.objectContaining({ provider: 'openai', status: 'missing' }),
    )
  })
  it('blocks runGeneration when intent is empty and resume is missing', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
      }),
    )
    await act(async () => {
      await result.current.runGeneration({})
    })
    expect(generateCommandModule.runGeneratePipeline).not.toHaveBeenCalled()
    expect(pushHistory).toHaveBeenCalledWith(
      'No intent provided. Enter text or set an intent file.',
      'system',
    )
  })
  it('runs generation when provider check passes', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    const onLastGeneratedPromptUpdate = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
        onLastGeneratedPromptUpdate,
      }),
    )
    await act(async () => {
      await result.current.runGeneration({ intent: 'Ship it' })
    })
    expect(generateCommandModule.runGeneratePipeline).toHaveBeenCalled()
    expect(pushHistory).toHaveBeenCalledWith('Prompt', 'system', 'markdown')
    expect(onLastGeneratedPromptUpdate).toHaveBeenCalledWith('Prompt')
  })
  it('passes resume selector through and allows empty intent', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
        files: ['src/index.ts'],
        urls: ['https://example.com'],
        smartContextEnabled: true,
      }),
    )
    await act(async () => {
      await result.current.runGeneration({
        resume: { kind: 'history', selector: 'last', mode: 'best-effort' },
      })
    })
    const args = generateCommandModule.runGeneratePipeline.mock.calls[0]?.[0] as unknown
    expect(args).toEqual(
      expect.objectContaining({
        resume: 'last',
        resumeMode: 'best-effort',
        context: [],
        urls: [],
        smartContext: false,
      }),
    )
    if (typeof args === 'object' && args !== null && 'intent' in args) {
      expect((args as { intent?: unknown }).intent).toBeUndefined()
    }
  })
  it('passes selected polish model into generation args', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
        polishModelId: 'gpt-4o',
      }),
    )
    await act(async () => {
      await result.current.runGeneration({ intent: 'Ship it' })
    })
    const args = generateCommandModule.runGeneratePipeline.mock.calls[0]?.[0] as unknown
    expect(args).toEqual(expect.objectContaining({ polish: true, polishModel: 'gpt-4o' }))
    expect(result.current.statusChips).toEqual(expect.arrayContaining(['[polish:gpt-4o]']))
  })
  it('updates status chips with token telemetry', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
      }),
    )
    await act(async () => {
      await result.current.runGeneration({ intent: 'Ship it' })
    })
    const optionsArg = generateCommandModule.runGeneratePipeline.mock.calls[0]?.[1] as unknown
    const onStreamEvent =
      typeof optionsArg === 'object' &&
      optionsArg !== null &&
      'onStreamEvent' in optionsArg &&
      typeof (optionsArg as { onStreamEvent?: unknown }).onStreamEvent === 'function'
        ? ((optionsArg as { onStreamEvent?: unknown }).onStreamEvent as (event: unknown) => void)
        : null
    expect(onStreamEvent).not.toBeNull()
    await act(async () => {
      onStreamEvent?.({
        event: 'context.telemetry',
        telemetry: {
          files: [],
          intentTokens: 200,
          fileTokens: 300,
          systemTokens: 700,
          totalTokens: 1200,
        },
      })
    })
    expect(result.current.statusChips).toEqual(expect.arrayContaining(['[tokens:1.2k]']))
  })
  it('surfaces upload state via toasts instead of history', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    const notify = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        notify,
        currentModel: 'gpt-4o-mini',
      }),
    )
    await act(async () => {
      await result.current.runGeneration({ intent: 'Ship it' })
    })
    const optionsArg = generateCommandModule.runGeneratePipeline.mock.calls[0]?.[1] as unknown
    const onStreamEvent =
      typeof optionsArg === 'object' &&
      optionsArg !== null &&
      'onStreamEvent' in optionsArg &&
      typeof (optionsArg as { onStreamEvent?: unknown }).onStreamEvent === 'function'
        ? ((optionsArg as { onStreamEvent?: unknown }).onStreamEvent as (event: unknown) => void)
        : null
    expect(onStreamEvent).not.toBeNull()
    await act(async () => {
      onStreamEvent?.({
        event: 'upload.state',
        state: 'start',
        detail: { kind: 'image', filePath: '/tmp/image.png' },
      })
    })
    expect(notify).toHaveBeenCalledWith('Uploading image: /tmp/image.png', expect.any(Object))
    const uploadHistoryCalls = pushHistory.mock.calls.filter(
      (call) => typeof call[0] === 'string' && call[0].includes('Uploading image:'),
    )
    expect(uploadHistoryCalls).toHaveLength(0)
  })
  it('surfaces transport waiting state in status', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    let resolvePipeline: ((value: unknown) => void) | null = null
    generateCommandModule.runGeneratePipeline.mockImplementation(
      (_args: unknown, _options: unknown) => {
        return new Promise((resolve) => {
          resolvePipeline = resolve
        })
      },
    )
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
        interactiveTransportPath: '/tmp/socket',
      }),
    )
    let runPromise: Promise<void> | null = null
    act(() => {
      runPromise = result.current.runGeneration({ intent: 'Ship it' })
    })
    await act(async () => {
      await Promise.resolve()
    })
    const optionsArg = generateCommandModule.runGeneratePipeline.mock.calls[0]?.[1] as unknown
    const onStreamEvent =
      typeof optionsArg === 'object' &&
      optionsArg !== null &&
      'onStreamEvent' in optionsArg &&
      typeof (optionsArg as { onStreamEvent?: unknown }).onStreamEvent === 'function'
        ? ((optionsArg as { onStreamEvent?: unknown }).onStreamEvent as (event: unknown) => void)
        : null
    expect(onStreamEvent).not.toBeNull()
    await act(async () => {
      onStreamEvent?.({ event: 'interactive.awaiting', mode: 'transport' })
    })
    expect(result.current.awaitingInteractiveMode).toBe('transport')
    expect(result.current.statusMessage).toBe('Waiting for interactive transport input…')
    expect(pushHistory).toHaveBeenCalledWith(
      'Waiting for interactive transport input…',
      'progress',
      undefined,
    )
    expect(pushHistory).toHaveBeenCalledWith(
      'Tip: connect a client and send refine/finish to continue.',
      'system',
      undefined,
    )
    await act(async () => {
      resolvePipeline?.({
        finalPrompt: 'Prompt',
        model: 'gpt-4o-mini',
        iterations: 1,
        telemetry: null,
        payload: {},
      })
      await runPromise
    })
  })
  it('sets the status chip while tests run', () => {
    jest.useFakeTimers()
    const pushHistory = jest.fn()
    const { result, rerender } = renderHook(
      ({ isTestCommandRunning }: { isTestCommandRunning: boolean }) =>
        useGenerationPipeline({
          ...baseOptions,
          pushHistory,
          currentModel: 'gpt-4o-mini',
          isTestCommandRunning,
        }),
      { initialProps: { isTestCommandRunning: false } },
    )
    expect(result.current.statusChips[0]).toBe('[status:Idle]')
    rerender({ isTestCommandRunning: true })
    const firstFrame = result.current.statusChips[0]
    expect(firstFrame).toBe('[status:Running tests]')
    act(() => {
      jest.advanceTimersByTime(240)
    })
    const secondFrame = result.current.statusChips[0]
    expect(secondFrame).toBe(firstFrame)
    rerender({ isTestCommandRunning: false })
    expect(result.current.statusChips[0]).toBe('[status:Idle]')
    jest.useRealTimers()
  })
  it('passes meta instructions to the generation pipeline', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
        metaInstructions: 'Be concise',
      }),
    )
    await act(async () => {
      await result.current.runGeneration({ intent: 'Add feature' })
    })
    expect(generateCommandModule.runGeneratePipeline).toHaveBeenCalledWith(
      expect.objectContaining({ metaInstructions: 'Be concise' }),
      expect.any(Object),
    )
  })
  it('passes image/video paths to the generation pipeline', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    const pushHistory = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
        images: ['diagram.png'],
        videos: ['clip.mp4'],
      }),
    )
    await act(async () => {
      await result.current.runGeneration({ intent: 'Ship it' })
    })
    expect(generateCommandModule.runGeneratePipeline).toHaveBeenCalledWith(
      expect.objectContaining({ images: ['diagram.png'], video: ['clip.mp4'] }),
      expect.any(Object),
    )
  })
  it('aborts series generation when provider is unavailable', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'gemini',
      status: 'missing',
      message: 'GEMINI_API_KEY missing',
    })
    const pushHistory = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
        videos: ['clip.mp4'],
      }),
    )
    await act(async () => {
      await result.current.runSeriesGeneration('Plan work')
    })
    expect(promptGeneratorModule.generatePromptSeries).not.toHaveBeenCalled()
    expect(pushHistory).toHaveBeenCalledWith(
      'Generation aborted: Gemini unavailable (GEMINI_API_KEY missing).',
      'system',
    )
  })
  it('surfaces series validation failures in history output', async () => {
    providerStatusModule.checkModelProviderStatus.mockResolvedValue({
      provider: 'openai',
      status: 'ok',
      message: 'ready',
    })
    promptGeneratorModule.generatePromptSeries.mockRejectedValueOnce(
      new Error(
        'Atomic prompt 1 contains forbidden cross-reference phrase "from step N". Atomic prompts must be standalone.',
      ),
    )
    const pushHistory = jest.fn()
    const { result } = renderHook(() =>
      useGenerationPipeline({
        ...baseOptions,
        pushHistory,
        currentModel: 'gpt-4o-mini',
      }),
    )
    await act(async () => {
      await result.current.runSeriesGeneration('Plan work')
    })
    expect(pushHistory).toHaveBeenCalledWith('[series] Starting series generation…', 'progress')
    expect(pushHistory).toHaveBeenCalledWith(
      expect.stringContaining(
        '[series] Failed: Atomic prompt 1 contains forbidden cross-reference phrase',
      ),
      'progress',
    )
  })
})
</file>

<file path="src/generate/pipeline.ts">
import fs from 'node:fs/promises'
import { stdin as input, stdout as output } from 'node:process'
import chalk from 'chalk'
import { loadCliConfig } from '../config'
import { resolveFileContext, type FileContext } from '../file-context'
import { appendToHistory, resolveHistoryFilePath } from '../history-logger'
import { resolveSmartContextFiles } from '../smart-context-service'
import type { ResolveUrlContextOptions } from '../url-context'
import { resolveUrlContext } from '../url-context'
import {
  createPromptGeneratorService,
  isGemini,
  resolveDefaultGenerateModel,
} from '../prompt-generator-service'
import { maybeCopyToClipboard, maybeOpenChatGpt } from './actions'
import { resolveContextTemplate, renderContextTemplate } from './context-templates'
import { evaluateContextBudget, type ContextEntry, type ContextEntrySource } from './context-budget'
import { displayContextFiles, writeContextFile } from './context-output'
import { displayContextTemplatePrompt, displayPolishedPrompt } from './display'
import { shouldTraceFlags } from './debug'
import { runGenerationWorkflow } from './interactive'
import { InteractiveTransport } from './interactive-transport'
import { resolveIntent } from './intent'
import { resolveGeminiVideoModel, resolveTargetModel } from './models'
import { loadGeneratePayloadFromFile } from './payload-io'
import { polishPrompt } from './polish'
import { createUploadStateTracker, startProgress, type ProgressHandle } from './progress'
import { createStreamDispatcher, type StreamDispatcher } from './stream'
import { buildTokenTelemetry } from './token-telemetry'
import {
  GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
  type ContextPathMetadata,
  type GenerateArgs,
  type GenerateJsonPayload,
  type GeneratePipelineOptions,
  type GeneratePipelineResult,
  type InteractiveMode,
  type ProgressScope,
  type ResumeMode,
  type StreamEventInput,
} from './types'
type ResumePayload = GenerateJsonPayload & {
  metaInstructions?: string
}
type ResumeLoadResult = {
  payload: ResumePayload
  source: 'history' | 'file'
}
type HistorySelector = {
  fromEnd: number
  label: string
}
type ResumeContextResult = {
  fileContext: FileContext[]
  reusedContextPaths: GenerateJsonPayload['contextPaths']
  missingContextPaths: GenerateJsonPayload['contextPaths']
}
const parseHistorySelector = (selector: string): HistorySelector => {
  const trimmed = selector.trim()
  if (trimmed === 'last') {
    return { fromEnd: 1, label: 'last' }
  }
  const lastMatch = trimmed.match(/^last:(\d+)$/)
  if (lastMatch) {
    return { fromEnd: Number(lastMatch[1]), label: trimmed }
  }
  const numericMatch = trimmed.match(/^(\d+)$/)
  if (numericMatch) {
    return { fromEnd: Number(numericMatch[1]), label: trimmed }
  }
  throw new Error(
    `Invalid resume selector "${selector}". Use "last", "last:N", or "N" (N-th from end).`,
  )
}
const isRecord = (value: unknown): value is Record<string, unknown> =>
  typeof value === 'object' && value !== null && !Array.isArray(value)
const isStringArray = (value: unknown): value is string[] =>
  Array.isArray(value) && value.every((entry) => typeof entry === 'string')
const isContextPaths = (value: unknown): value is GenerateJsonPayload['contextPaths'] =>
  Array.isArray(value) &&
  value.every(
    (entry) =>
      isRecord(entry) && typeof entry.path === 'string' && typeof entry.source === 'string',
  )
const isGenerateJsonPayload = (value: unknown): value is ResumePayload => {
  if (!isRecord(value)) {
    return false
  }
  if (value.schemaVersion !== GENERATE_JSON_PAYLOAD_SCHEMA_VERSION) {
    return false
  }
  return (
    typeof value.intent === 'string' &&
    typeof value.model === 'string' &&
    typeof value.targetModel === 'string' &&
    typeof value.prompt === 'string' &&
    isStringArray(value.refinements) &&
    typeof value.iterations === 'number' &&
    Number.isFinite(value.iterations) &&
    typeof value.interactive === 'boolean' &&
    typeof value.timestamp === 'string' &&
    isContextPaths(value.contextPaths)
  )
}
const selectFromEnd = <T>(entries: T[], fromEnd: number): T => {
  const index = entries.length - fromEnd
  if (index < 0 || index >= entries.length) {
    const noun = entries.length === 1 ? 'entry' : 'entries'
    throw new Error(
      `History selector is out of range. Requested ${fromEnd} from end but only ${entries.length} ${noun} available.`,
    )
  }
  const selected = entries[index]
  if (!selected) {
    throw new Error('Invariant violation: selected history entry is missing.')
  }
  return selected
}
const readJsonlPayloads = async (filePath: string): Promise<ResumePayload[]> => {
  let raw: string
  try {
    raw = await fs.readFile(filePath, 'utf8')
  } catch (error) {
    const code =
      typeof error === 'object' &&
      error !== null &&
      'code' in error &&
      typeof (error as { code?: unknown }).code === 'string'
        ? (error as { code: string }).code
        : null
    if (code === 'ENOENT') {
      throw new Error(
        `History file not found at ${filePath}. Run a generate command first to create it.`,
      )
    }
    const message = error instanceof Error ? error.message : 'Unknown file error.'
    throw new Error(`Failed to read history file ${filePath}: ${message}`)
  }
  if (raw.trim().length === 0) {
    throw new Error(`History file ${filePath} is empty.`)
  }
  const entries: ResumePayload[] = []
  raw
    .split('\n')
    .map((line) => line.trim())
    .filter((line) => line.length > 0)
    .forEach((line) => {
      try {
        const parsed = JSON.parse(line) as unknown
        if (isGenerateJsonPayload(parsed)) {
          entries.push(parsed)
        }
      } catch {
        // ignore invalid json
      }
    })
  if (entries.length === 0) {
    throw new Error(`No valid generate payloads found in history file ${filePath}.`)
  }
  return entries
}
const loadResumePayload = async (args: GenerateArgs): Promise<ResumeLoadResult | null> => {
  if (!args.resume && !args.resumeLast && !args.resumeFrom) {
    return null
  }
  if (args.resumeFrom) {
    const filePath = args.resumeFrom
    const payload = await loadGeneratePayloadFromFile(filePath)
    return { payload, source: 'file' }
  }
  const selector = args.resumeLast ? 'last' : (args.resume ?? 'last')
  const parsedSelector = parseHistorySelector(selector)
  const historyPath = resolveHistoryFilePath()
  const entries = await readJsonlPayloads(historyPath)
  const payload = selectFromEnd(entries, parsedSelector.fromEnd)
  return { payload, source: 'history' }
}
const resolveResumeContext = async (
  payload: ResumePayload,
  mode: ResumeMode,
): Promise<ResumeContextResult> => {
  const reused: GenerateJsonPayload['contextPaths'] = []
  const missing: GenerateJsonPayload['contextPaths'] = []
  const fileContext: FileContext[] = []
  const contextCandidates = payload.contextPaths.filter((entry) => entry.source !== 'intent')
  for (const entry of contextCandidates) {
    if (entry.source !== 'file') {
      missing.push(entry)
      continue
    }
    try {
      const content = await fs.readFile(entry.path, 'utf8')
      fileContext.push({ path: entry.path, content })
      reused.push(entry)
    } catch {
      missing.push(entry)
    }
  }
  if (missing.length > 0 && mode === 'best-effort') {
    const fileMissing = missing.filter((entry) => entry.source === 'file')
    if (fileMissing.length > 0) {
      const paths = fileMissing.map((entry) => entry.path).join(', ')
      console.warn(chalk.yellow(`Resume skipped missing context file(s): ${paths}`))
    }
  }
  return { fileContext, reusedContextPaths: reused, missingContextPaths: missing }
}
const isMissingIntentError = (error: unknown): boolean =>
  error instanceof Error && error.message.startsWith('Intent text is required.')
const logFlagSnapshot = (args: GenerateArgs): void => {
  if (!shouldTraceFlags()) {
    return
  }
  const snapshot = {
    copy: args.copy,
    polish: args.polish,
    openChatGpt: args.openChatGpt,
    json: args.json,
    quiet: args.quiet,
    stream: args.stream,
    interactive: args.interactive,
    showContext: args.showContext,
    contextTemplate: args.contextTemplate ?? null,
    contextFile: args.contextFile ?? null,
    smartContext: args.smartContext,
  }
  console.error(chalk.dim('[pmc:flags]'), JSON.stringify(snapshot, null, 2))
}
type ContextPathSource = ContextPathMetadata['source']
type TransportCleanupHandler = {
  event: NodeJS.Signals | 'exit'
  handler: () => void
}
export const runGeneratePipeline = async (
  args: GenerateArgs,
  options: GeneratePipelineOptions = {},
): Promise<GeneratePipelineResult> => {
  logFlagSnapshot(args)
  const interactiveTransportPath = args.interactiveTransport?.trim()
  if (args.interactiveTransport && !interactiveTransportPath) {
    throw new Error('--interactive-transport requires a non-empty path.')
  }
  const wantsInteractiveSession = args.interactive || Boolean(interactiveTransportPath)
  if (args.json && wantsInteractiveSession) {
    throw new Error('--json cannot be combined with --interactive.')
  }
  const contextTemplateName = args.contextTemplate?.trim()
  if (args.contextTemplate && !contextTemplateName) {
    throw new Error('--context-template requires a non-empty template name.')
  }
  const transportCleanupHandlers: TransportCleanupHandler[] = []
  const interactiveTransport = interactiveTransportPath
    ? new InteractiveTransport(interactiveTransportPath)
    : null
  try {
    const resume = await loadResumePayload(args)
    let intent: string
    let intentMetadataPath: string
    if (resume) {
      // Precedence: if the user provides a new intent (inline, --intent-file, or stdin), it wins.
      // Otherwise, fall back to the resumed payload intent.
      try {
        intent = await resolveIntent(args)
        intentMetadataPath = args.intentFile
          ? args.intentFile
          : args.intent?.trim()
            ? 'inline-intent'
            : 'stdin-intent'
      } catch (error) {
        if (!isMissingIntentError(error)) {
          throw error
        }
        intent = resume.payload.intent
        intentMetadataPath = resume.source === 'history' ? 'history-intent' : 'resume-file-intent'
      }
    } else {
      intent = await resolveIntent(args)
      intentMetadataPath = args.intentFile
        ? args.intentFile
        : args.intent?.trim()
          ? 'inline-intent'
          : 'stdin-intent'
    }
    const cliConfig = await loadCliConfig()
    let contextPaths: ContextPathMetadata[] = []
    const recordContextPaths = (entries: FileContext[], source: ContextPathSource): void => {
      entries.forEach((entry) => {
        contextPaths.push({ path: entry.path, source })
      })
    }
    const pruneContextPaths = (
      paths: ContextPathMetadata[],
      dropped: ContextPathMetadata[],
    ): ContextPathMetadata[] => {
      if (dropped.length === 0) {
        return paths
      }
      const remainingDrops = new Map<string, number>()
      dropped.forEach((entry) => {
        const key = `${entry.source}:${entry.path}`
        remainingDrops.set(key, (remainingDrops.get(key) ?? 0) + 1)
      })
      const next: ContextPathMetadata[] = []
      paths.forEach((entry) => {
        const key = `${entry.source}:${entry.path}`
        const remaining = remainingDrops.get(key)
        if (remaining !== undefined) {
          if (remaining <= 1) {
            remainingDrops.delete(key)
          } else {
            remainingDrops.set(key, remaining - 1)
          }
          return
        }
        next.push(entry)
      })
      return next
    }
    contextPaths.push({ path: intentMetadataPath, source: 'intent' })
    const resumeMode = args.resumeMode ?? 'best-effort'
    let resumeContext: ResumeContextResult | null = null
    let fileContext: FileContext[]
    // Precedence: explicit `--context` file paths override resumed context paths.
    // When no explicit context is provided, we attempt to reuse contextPaths from the resume payload
    // (and enforce `--resume-mode` for missing files).
    if (args.context.length > 0 || !resume) {
      fileContext = await resolveFileContext(args.context)
      recordContextPaths(fileContext, 'file')
    } else {
      resumeContext = await resolveResumeContext(resume.payload, resumeMode)
      fileContext = resumeContext.fileContext
      recordContextPaths(fileContext, 'file')
    }
    let contextEntrySources: ContextEntrySource[] = fileContext.map(() => 'file')
    const service = await createPromptGeneratorService()
    const defaultGenerateModel = await resolveDefaultGenerateModel()
    // Precedence: explicit CLI flags override resumed payload values, which override defaults.
    let model = args.model ?? resume?.payload.model ?? defaultGenerateModel
    const targetModel = await resolveTargetModel({
      defaultTargetModel: resume?.payload.targetModel ?? defaultGenerateModel,
      ...(args.target !== undefined ? { explicitTarget: args.target } : {}),
    })
    if (args.video.length > 0 && !isGemini(model)) {
      const previousModel = model
      model = await resolveGeminiVideoModel()
      if (model !== previousModel) {
        console.warn(`Switching to ${model} to support video input.`)
      }
    }
    const contextTemplateDefinition = contextTemplateName
      ? await resolveContextTemplate(contextTemplateName)
      : null
    const refinements: string[] = resume ? [...resume.payload.refinements] : []
    // Precedence: explicit CLI meta instructions override resumed meta instructions.
    const trimmedMetaInstructions = (
      args.metaInstructions ??
      resume?.payload.metaInstructions ??
      ''
    ).trim()
    const streamDispatcher = createStreamDispatcher(args.stream, {
      ...(interactiveTransport ? { taps: [interactiveTransport.getEventWriter()] } : {}),
    })
    const emitEvent = (event: StreamEventInput): void => {
      if (options.onStreamEvent) {
        try {
          options.onStreamEvent(event)
        } catch {
          // ignore listener errors to avoid breaking pipeline
        }
      }
      streamDispatcher.emit(event)
    }
    const streamProxy: StreamDispatcher = {
      mode: streamDispatcher.mode,
      emit: emitEvent,
    }
    if (
      resumeContext &&
      (resumeContext.reusedContextPaths.length > 0 || resumeContext.missingContextPaths.length > 0)
    ) {
      emitEvent({
        event: 'resume.loaded',
        source: resume?.source ?? 'history',
        reusedContextPaths: resumeContext.reusedContextPaths,
        missingContextPaths: resumeContext.missingContextPaths,
      })
      if (
        resumeMode === 'strict' &&
        resumeContext.missingContextPaths.some((entry) => entry.source === 'file')
      ) {
        const missingFiles = resumeContext.missingContextPaths
          .filter((entry) => entry.source === 'file')
          .map((entry) => entry.path)
          .join(', ')
        throw new Error(`Missing required resumed context file(s): ${missingFiles}`)
      }
    }
    interactiveTransport?.setEventEmitter((event) => {
      emitEvent(event)
    })
    if (interactiveTransport) {
      await interactiveTransport.start()
      const signals: Array<NodeJS.Signals | 'exit'> = ['SIGINT', 'SIGTERM', 'exit']
      signals.forEach((signal) => {
        const handler = (): void => {
          void interactiveTransport.stop()
        }
        process.once(signal, handler)
        transportCleanupHandlers.push({ event: signal, handler })
      })
    }
    const interactiveMode: InteractiveMode = interactiveTransport
      ? 'transport'
      : args.interactive && input.isTTY && output.isTTY
        ? 'tty'
        : 'none'
    if (args.interactive && interactiveMode === 'none') {
      console.warn(
        'Interactive mode requested but no TTY detected; continuing non-interactive run.',
      )
    }
    const uiSuppressed = args.quiet || streamDispatcher.mode !== 'none'
    const shouldDisplay = !args.json && !args.quiet
    const progressSpinnersEnabled = args.progress && interactiveMode === 'none'
    const startSpinner = (label: string): ProgressHandle | null =>
      progressSpinnersEnabled ? startProgress(label, { showSpinner: !uiSuppressed }) : null
    const emitProgress = (
      label: string,
      state: 'start' | 'update' | 'stop',
      scope: ProgressScope = 'generic',
    ): void => {
      emitEvent({ event: 'progress.update', label, state, scope })
    }
    emitProgress('Resolving context', 'start', 'generic')
    if (args.urls.length > 0) {
      const label = 'Fetching URL context'
      emitProgress(label, 'start', 'url')
      const urlSpinner = startSpinner(label)
      const urlOptions: ResolveUrlContextOptions = {
        onProgress: (message: string) => {
          urlSpinner?.setLabel(message)
          emitProgress(message, 'update', 'url')
        },
      }
      try {
        const urlFiles = await resolveUrlContext(args.urls, urlOptions)
        if (urlFiles.length > 0) {
          fileContext = [...fileContext, ...urlFiles]
          contextEntrySources = [
            ...contextEntrySources,
            ...urlFiles.map((): ContextEntrySource => 'url'),
          ]
          recordContextPaths(urlFiles, 'url')
        }
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown URL fetch error.'
        console.warn(chalk.yellow(`Failed to fetch URL context: ${message}`))
        emitEvent({ event: 'progress.update', label: `URL error: ${message}`, state: 'update' })
      } finally {
        urlSpinner?.stop('URL context ready')
        emitProgress(label, 'stop', 'url')
      }
    }
    if (args.smartContext) {
      const label = 'Preparing smart context'
      emitProgress(label, 'start', 'smart')
      const smartSpinner = startSpinner(label)
      try {
        const smartFiles = await resolveSmartContextFiles(
          intent,
          fileContext,
          (message) => {
            smartSpinner?.setLabel(message)
            emitProgress(message, 'update', 'smart')
          },
          args.smartContextRoot,
        )
        if (smartFiles.length > 0) {
          fileContext = [...fileContext, ...smartFiles]
          contextEntrySources = [
            ...contextEntrySources,
            ...smartFiles.map((): ContextEntrySource => 'smart'),
          ]
          recordContextPaths(smartFiles, 'smart')
        }
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown smart context error.'
        console.warn(chalk.yellow(`Smart context failed: ${message}`))
        emitEvent({
          event: 'progress.update',
          label: `Smart context error: ${message}`,
          state: 'update',
        })
      } finally {
        smartSpinner?.stop('Smart context ready')
        emitProgress(label, 'stop', 'smart')
      }
    }
    emitProgress('Resolving context', 'stop', 'generic')
    const promptGeneratorConfig = cliConfig?.promptGenerator
    const maxInputTokens = args.maxInputTokens ?? promptGeneratorConfig?.maxInputTokens
    const maxContextTokens = args.maxContextTokens ?? promptGeneratorConfig?.maxContextTokens
    const overflowStrategy = args.contextOverflow ?? promptGeneratorConfig?.contextOverflowStrategy
    const contextEntries: ContextEntry[] = fileContext.map((entry, index) => {
      const source = contextEntrySources[index]
      if (!source) {
        throw new Error(`Invariant violation: missing source for context entry ${entry.path}.`)
      }
      return { ...entry, source }
    })
    const budgetEvaluation = evaluateContextBudget({
      intentText: intent,
      metaInstructions: trimmedMetaInstructions,
      contextEntries,
      ...(maxInputTokens !== undefined ? { maxInputTokens } : {}),
      ...(maxContextTokens !== undefined ? { maxContextTokens } : {}),
      ...(overflowStrategy ? { strategy: overflowStrategy } : {}),
      buildTelemetry: (intentText, files, metaInstructions) =>
        buildTokenTelemetry(intentText, files, metaInstructions),
    })
    if (budgetEvaluation.droppedEntries.length > 0) {
      fileContext = budgetEvaluation.keptEntries.map(({ path, content }) => ({ path, content }))
      contextEntrySources = budgetEvaluation.keptEntries.map((entry) => entry.source)
      contextPaths = pruneContextPaths(contextPaths, budgetEvaluation.droppedPaths)
      emitEvent({
        event: 'context.overflow',
        strategy: budgetEvaluation.strategy ?? 'fail',
        before: budgetEvaluation.before,
        after: budgetEvaluation.after,
        droppedPaths: budgetEvaluation.droppedPaths,
      })
    }
    const telemetry = budgetEvaluation.after
    emitEvent({ event: 'context.telemetry', telemetry })
    if (args.showContext) {
      const writeLine = args.json
        ? (value: string): void => {
            console.error(value)
          }
        : (value: string): void => {
            console.log(value)
          }
      displayContextFiles(fileContext, args.contextFormat, writeLine)
    }
    let outputPath: string | undefined
    if (args.contextFile) {
      await writeContextFile(args.contextFile, args.contextFormat, fileContext)
      outputPath = args.contextFile
    }
    emitProgress('Generating prompt', 'start', 'generate')
    const generationSpinner = startSpinner('Generating prompt')
    const handleUploadStateChange = createUploadStateTracker(
      generationSpinner,
      'Generating prompt',
      streamProxy,
    )
    const resumeState = resume
      ? {
          prompt: resume.payload.polishedPrompt ?? resume.payload.prompt,
          iterations: resume.payload.iterations,
        }
      : undefined
    const {
      prompt: generatedPrompt,
      reasoning: generationReasoning,
      iterations,
    } = await runGenerationWorkflow({
      service,
      context: {
        intent,
        refinements,
        model,
        targetModel,
        fileContext,
        images: args.images,
        videos: args.video,
        pdfs: args.pdf,
        metaInstructions: trimmedMetaInstructions,
      },
      telemetry,
      interactiveMode,
      interactiveTransport,
      interactiveDelegate: options.interactiveDelegate,
      display: shouldDisplay,
      stream: streamProxy,
      onUploadStateChange: handleUploadStateChange,
      ...(resumeState ? { resume: resumeState } : {}),
    })
    generationSpinner?.stop('Generated prompt ✓')
    emitProgress('Generating prompt', 'stop', 'generate')
    const polishModel =
      args.polishModel ??
      resume?.payload.polishModel ??
      process.env.PROMPT_MAKER_POLISH_MODEL ??
      model
    let polishedPrompt: string | undefined
    if (args.polish) {
      const label = 'Polishing prompt'
      emitProgress(label, 'start', 'polish')
      const polishSpinner = startSpinner(label)
      try {
        polishedPrompt = await polishPrompt(intent, generatedPrompt, polishModel, targetModel)
      } finally {
        polishSpinner?.stop('Polished prompt ✓')
        emitProgress(label, 'stop', 'polish')
      }
    }
    const artifact = polishedPrompt ?? generatedPrompt
    const renderedPrompt = contextTemplateDefinition
      ? renderContextTemplate(contextTemplateDefinition, artifact)
      : undefined
    const finalArtifact = renderedPrompt ?? artifact
    await maybeCopyToClipboard(args.copy, finalArtifact, shouldDisplay)
    await maybeOpenChatGpt(args.openChatGpt, finalArtifact, shouldDisplay)
    const payload: GenerateJsonPayload = {
      schemaVersion: GENERATE_JSON_PAYLOAD_SCHEMA_VERSION,
      intent,
      model,
      targetModel,
      prompt: generatedPrompt,
      ...(typeof generationReasoning === 'string' ? { reasoning: generationReasoning } : {}),
      ...(trimmedMetaInstructions ? { metaInstructions: trimmedMetaInstructions } : {}),
      refinements: [...refinements],
      iterations,
      interactive: interactiveMode !== 'none',
      timestamp: new Date().toISOString(),
      contextPaths,
      images: [...args.images],
      videos: [...args.video],
      pdfs: [...args.pdf],
      ...(outputPath ? { outputPath } : {}),
    }
    if (polishedPrompt) {
      payload.polishedPrompt = polishedPrompt
      payload.polishModel = polishModel
    } else if (resume?.payload.polishedPrompt && resume?.payload.polishModel && !args.polish) {
      payload.polishedPrompt = resume.payload.polishedPrompt
      payload.polishModel = resume.payload.polishModel
    }
    if (contextTemplateName && renderedPrompt) {
      payload.contextTemplate = contextTemplateName
      payload.renderedPrompt = renderedPrompt
    }
    emitEvent({ event: 'generation.final', result: payload })
    const pipelineResult: GeneratePipelineResult = {
      payload,
      telemetry,
      generatedPrompt,
      ...(typeof generationReasoning === 'string' ? { reasoning: generationReasoning } : {}),
      finalPrompt: finalArtifact,
      iterations,
      model,
      contextPaths,
      ...(polishedPrompt ? { polishedPrompt } : {}),
    }
    if (args.json) {
      console.log(JSON.stringify(payload, null, 2))
      await appendToHistory(payload)
      return pipelineResult
    }
    if (renderedPrompt && shouldDisplay && contextTemplateName) {
      displayContextTemplatePrompt(renderedPrompt, contextTemplateName)
    } else if (polishedPrompt && shouldDisplay) {
      displayPolishedPrompt(polishedPrompt, polishModel)
    }
    await appendToHistory(payload)
    return pipelineResult
  } finally {
    transportCleanupHandlers.forEach(({ event, handler }) => {
      process.off(event, handler)
    })
    await interactiveTransport?.stop()
  }
}
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenController.ts">
import { useMemo } from 'react'
import { useStdout } from 'ink'
import type { NotifyOptions } from '../../../notifier'
import { useContextDispatch, useContextState } from '../../../context-store'
import { useCommandScreenHistoryAndTests } from './useCommandScreenHistoryAndTests'
import { useCommandScreenInputState } from './useCommandScreenInputState'
import {
  useCommandScreenModelGeneration,
  type UseCommandScreenModelGenerationResult,
} from './useCommandScreenModelGeneration'
import { useCommandScreenPopupAndView } from './useCommandScreenPopupAndView'
import type {
  UseCommandScreenPopupAndViewOptions,
  UseCommandScreenPopupAndViewResult,
} from './useCommandScreenPopupAndView.types'
export type UseCommandScreenControllerOptions = {
  transport?: {
    interactiveTransportPath?: string | undefined
  }
  popup: {
    onPopupVisibilityChange?: (isOpen: boolean) => void
    commandMenuSignal?: number
    helpOpen: boolean
    reservedRows: number
    onOpenHelp?: () => void
  }
  notify: (message: string, options?: NotifyOptions) => void
}
export type UseCommandScreenControllerResult = {
  view: UseCommandScreenPopupAndViewResult
  actions: {
    suppressNextInput: () => void
  }
}
export const useCommandScreenController = ({
  transport,
  popup,
  notify,
}: UseCommandScreenControllerOptions): UseCommandScreenControllerResult => {
  const { stdout } = useStdout()
  const interactiveTransportPath = transport?.interactiveTransportPath
  const { onPopupVisibilityChange, commandMenuSignal, helpOpen, reservedRows, onOpenHelp } = popup
  const {
    files,
    urls,
    images,
    videos,
    pdfs,
    smartContextEnabled,
    smartContextRoot,
    metaInstructions,
    maxContextTokens,
    maxInputTokens,
    contextOverflowStrategy,
    lastReasoning,
    lastGeneratedPrompt,
  } = useContextState()
  const {
    addFile,
    removeFile,
    addUrl,
    removeUrl,
    updateUrl,
    addImage,
    removeImage,
    addVideo,
    removeVideo,
    addPdf,
    removePdf,
    toggleSmartContext,
    setSmartRoot,
    setMetaInstructions,
    setBudgets,
    setLastReasoning,
    setLastGeneratedPrompt,
    resetContext,
  } = useContextDispatch()
  const historyAndTests = useCommandScreenHistoryAndTests()
  const inputState = useCommandScreenInputState({
    pushHistoryProxy: historyAndTests.pushHistoryProxy,
  })
  const modelAndGeneration: UseCommandScreenModelGenerationResult = useCommandScreenModelGeneration(
    {
      pushHistoryProxy: historyAndTests.pushHistoryProxy,
      notify,
      files,
      urls,
      images,
      videos,
      pdfs,
      smartContextEnabled,
      smartContextRoot,
      metaInstructions,
      budgets: { maxContextTokens, maxInputTokens, contextOverflowStrategy },
      ...(interactiveTransportPath ? { interactiveTransportPath } : {}),
      terminalColumns: inputState.terminalColumns,
      copyEnabled: inputState.copyEnabled,
      chatGptEnabled: inputState.chatGptEnabled,
      jsonOutputEnabled: inputState.jsonOutputEnabled,
      isTestCommandRunning: historyAndTests.isTestCommandRunning,
      setLastReasoning,
      setLastGeneratedPrompt,
    },
  )
  const contextOptions: UseCommandScreenPopupAndViewOptions['context'] = useMemo(
    () => ({
      interactiveTransportPath,
      notify,
      stdout,
      files,
      urls,
      images,
      videos,
      pdfs,
      smartContextEnabled,
      smartContextRoot,
      metaInstructions,
      maxContextTokens,
      maxInputTokens,
      contextOverflowStrategy,
      lastReasoning,
      lastGeneratedPrompt,
      addFile,
      removeFile,
      addUrl,
      removeUrl,
      updateUrl,
      addImage,
      removeImage,
      addVideo,
      removeVideo,
      addPdf,
      removePdf,
      toggleSmartContext,
      setSmartRoot,
      setMetaInstructions,
      setBudgets,
      resetContext,
    }),
    [
      interactiveTransportPath,
      notify,
      stdout,
      files,
      urls,
      images,
      videos,
      smartContextEnabled,
      smartContextRoot,
      metaInstructions,
      maxContextTokens,
      maxInputTokens,
      contextOverflowStrategy,
      lastReasoning,
      lastGeneratedPrompt,
      addFile,
      removeFile,
      addUrl,
      removeUrl,
      updateUrl,
      addImage,
      removeImage,
      addVideo,
      removeVideo,
      addPdf,
      removePdf,
      toggleSmartContext,
      setSmartRoot,
      setMetaInstructions,
      setBudgets,
      resetContext,
    ],
  )
  const inputOptions: UseCommandScreenPopupAndViewOptions['input'] = useMemo(
    () => ({
      terminalRows: inputState.terminalRows,
      terminalColumns: inputState.terminalColumns,
      inputValue: inputState.inputValue,
      isPasteActive: inputState.isPasteActive,
      commandSelectionIndex: inputState.commandSelectionIndex,
      debugKeyLine: inputState.debugKeyLine,
      debugKeysEnabled: inputState.debugKeysEnabled,
      setTerminalSize: inputState.setTerminalSize,
      setInputValue: inputState.setInputValue,
      setPasteActive: inputState.setPasteActive,
      setCommandSelectionIndex: inputState.setCommandSelectionIndex,
      intentFilePath: inputState.intentFilePath,
      setIntentFilePath: inputState.setIntentFilePath,
      copyEnabled: inputState.copyEnabled,
      setCopyEnabled: inputState.setCopyEnabled,
      chatGptEnabled: inputState.chatGptEnabled,
      setChatGptEnabled: inputState.setChatGptEnabled,
      jsonOutputEnabled: inputState.jsonOutputEnabled,
      setJsonOutputEnabled: inputState.setJsonOutputEnabled,
      lastUserIntentRef: inputState.lastUserIntentRef,
      lastTypedIntentRef: inputState.lastTypedIntentRef,
      consumeSuppressedTextInputChange: inputState.consumeSuppressedTextInputChange,
      suppressNextInput: inputState.suppressNextInput,
      updateLastTypedIntent: inputState.updateLastTypedIntent,
      onDebugKeyEvent: inputState.onDebugKeyEvent,
    }),
    [
      inputState.terminalRows,
      inputState.terminalColumns,
      inputState.inputValue,
      inputState.isPasteActive,
      inputState.commandSelectionIndex,
      inputState.debugKeyLine,
      inputState.debugKeysEnabled,
      inputState.setTerminalSize,
      inputState.setInputValue,
      inputState.setPasteActive,
      inputState.setCommandSelectionIndex,
      inputState.intentFilePath,
      inputState.setIntentFilePath,
      inputState.copyEnabled,
      inputState.setCopyEnabled,
      inputState.chatGptEnabled,
      inputState.setChatGptEnabled,
      inputState.jsonOutputEnabled,
      inputState.setJsonOutputEnabled,
      inputState.lastUserIntentRef,
      inputState.lastTypedIntentRef,
      inputState.consumeSuppressedTextInputChange,
      inputState.suppressNextInput,
      inputState.updateLastTypedIntent,
      inputState.onDebugKeyEvent,
    ],
  )
  const popupOptions: UseCommandScreenPopupAndViewOptions['popup'] = useMemo(
    () => ({
      onPopupVisibilityChange,
      commandMenuSignal,
      helpOpen,
      reservedRows,
      ...(onOpenHelp ? { onOpenHelp } : {}),
    }),
    [onPopupVisibilityChange, commandMenuSignal, helpOpen, reservedRows, onOpenHelp],
  )
  const historyOptions: UseCommandScreenPopupAndViewOptions['history'] = useMemo(
    () => ({
      pushHistoryRef: historyAndTests.pushHistoryRef,
      pushHistoryProxy: historyAndTests.pushHistoryProxy,
      clearHistoryRef: historyAndTests.clearHistoryRef,
      scrollToRef: historyAndTests.scrollToRef,
      scrollToProxy: historyAndTests.scrollToProxy,
      closeTestPopupRef: historyAndTests.closeTestPopupRef,
      commandHistoryValues: historyAndTests.commandHistoryValues,
      addCommandHistoryEntry: historyAndTests.addCommandHistoryEntry,
      isTestCommandRunning: historyAndTests.isTestCommandRunning,
      lastTestFile: historyAndTests.lastTestFile,
      runTestsFromCommandProxy: historyAndTests.runTestsFromCommandProxy,
      onTestPopupSubmit: historyAndTests.onTestPopupSubmit,
    }),
    [
      historyAndTests.pushHistoryRef,
      historyAndTests.pushHistoryProxy,
      historyAndTests.clearHistoryRef,
      historyAndTests.scrollToRef,
      historyAndTests.scrollToProxy,
      historyAndTests.closeTestPopupRef,
      historyAndTests.commandHistoryValues,
      historyAndTests.addCommandHistoryEntry,
      historyAndTests.isTestCommandRunning,
      historyAndTests.lastTestFile,
      historyAndTests.runTestsFromCommandProxy,
      historyAndTests.onTestPopupSubmit,
    ],
  )
  const generationOptions: UseCommandScreenPopupAndViewOptions['generation'] = useMemo(
    () => ({
      currentModel: modelAndGeneration.currentModel,
      polishModelId: modelAndGeneration.polishModelId,
      currentTargetModel: modelAndGeneration.currentTargetModel,
      modelOptions: modelAndGeneration.modelOptions,
      providerStatuses: modelAndGeneration.providerStatuses,
      selectModel: modelAndGeneration.selectModel,
      selectPolishModel: modelAndGeneration.selectPolishModel,
      selectTargetModel: modelAndGeneration.selectTargetModel,
      isGenerating: modelAndGeneration.pipeline.isGenerating,
      runGeneration: modelAndGeneration.pipeline.runGeneration,
      runSeriesGeneration: modelAndGeneration.pipeline.runSeriesGeneration,
      statusChips: modelAndGeneration.pipeline.statusChips,
      isAwaitingRefinement: modelAndGeneration.pipeline.isAwaitingRefinement,
      submitRefinement: modelAndGeneration.pipeline.submitRefinement,
      awaitingInteractiveMode: modelAndGeneration.pipeline.awaitingInteractiveMode,
      tokenUsageRun: modelAndGeneration.pipeline.tokenUsageRun,
      tokenUsageBreakdown: modelAndGeneration.pipeline.tokenUsageBreakdown,
      latestContextOverflow: modelAndGeneration.pipeline.latestContextOverflow,
    }),
    [
      modelAndGeneration.currentModel,
      modelAndGeneration.polishModelId,
      modelAndGeneration.currentTargetModel,
      modelAndGeneration.modelOptions,
      modelAndGeneration.providerStatuses,
      modelAndGeneration.selectModel,
      modelAndGeneration.selectPolishModel,
      modelAndGeneration.selectTargetModel,
      modelAndGeneration.pipeline.isGenerating,
      modelAndGeneration.pipeline.runGeneration,
      modelAndGeneration.pipeline.runSeriesGeneration,
      modelAndGeneration.pipeline.statusChips,
      modelAndGeneration.pipeline.isAwaitingRefinement,
      modelAndGeneration.pipeline.submitRefinement,
      modelAndGeneration.pipeline.awaitingInteractiveMode,
      modelAndGeneration.pipeline.tokenUsageRun,
      modelAndGeneration.pipeline.tokenUsageBreakdown,
      modelAndGeneration.pipeline.latestContextOverflow,
    ],
  )
  const view = useCommandScreenPopupAndView({
    context: contextOptions,
    input: inputOptions,
    popup: popupOptions,
    history: historyOptions,
    generation: generationOptions,
  })
  const actions = useMemo(
    () => ({
      suppressNextInput: inputState.suppressNextInput,
    }),
    [inputState.suppressNextInput],
  )
  return useMemo(
    () => ({
      view,
      actions,
    }),
    [view, actions],
  )
}
</file>

<file path="src/tui/screens/command/hooks/useContextPopupGlue.ts">
import { useInput, type Key } from 'ink'
import { useStableCallback } from '../../../hooks/useStableCallback'
import type { CommandDescriptor, PopupState } from '../../../types'
import { useFilePopupGlue } from './context-popup-glue/useFilePopupGlue'
import { useImagePopupGlue } from './context-popup-glue/useImagePopupGlue'
import { usePdfPopupGlue } from './context-popup-glue/usePdfPopupGlue'
import { useSmartPopupGlue } from './context-popup-glue/useSmartPopupGlue'
import { useUrlPopupGlue } from './context-popup-glue/useUrlPopupGlue'
import { useVideoPopupGlue } from './context-popup-glue/useVideoPopupGlue'
export type UseContextPopupGlueOptions = {
  inputValue: string
  popupState: PopupState
  helpOpen: boolean
  isPopupOpen: boolean
  isCommandMode: boolean
  isCommandMenuActive: boolean
  isGenerating: boolean
  droppedFilePath: string | null
  files: string[]
  urls: string[]
  images: string[]
  videos: string[]
  pdfs: string[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  addFile: (value: string) => void
  removeFile: (index: number) => void
  addUrl: (value: string) => void
  removeUrl: (index: number) => void
  updateUrl: (index: number, value: string) => void
  addImage: (value: string) => void
  removeImage: (index: number) => void
  addVideo: (value: string) => void
  removeVideo: (index: number) => void
  addPdf: (value: string) => void
  removePdf: (index: number) => void
  toggleSmartContext: () => void
  setSmartRoot: (value: string) => void
  setInputValue: (value: string) => void
  setPopupState: (next: PopupState | ((prev: PopupState) => PopupState)) => void
  suppressNextInput: () => void
  notify: (message: string) => void
  pushHistory: (content: string, kind?: 'system' | 'user' | 'progress') => void
  addCommandHistoryEntry: (value: string) => void
  handleCommandSelection: (commandId: CommandDescriptor['id'], argsRaw?: string) => void
  consumeSuppressedTextInputChange: () => boolean
  isFilePath: (candidate: string) => boolean
}
export type UseContextPopupGlueResult = {
  // File
  filePopupSuggestions: string[]
  filePopupSuggestionSelectionIndex: number
  filePopupSuggestionsFocused: boolean
  onFilePopupDraftChange: (next: string) => void
  onAddFile: (value: string) => void
  onRemoveFile: (index: number) => void
  // URL
  onUrlPopupDraftChange: (next: string) => void
  onAddUrl: (value: string) => void
  onRemoveUrl: (index: number) => void
  // Image
  imagePopupSuggestions: string[]
  imagePopupSuggestionSelectionIndex: number
  imagePopupSuggestionsFocused: boolean
  onImagePopupDraftChange: (next: string) => void
  onAddImage: (value: string) => void
  onRemoveImage: (index: number) => void
  // Video
  videoPopupSuggestions: string[]
  videoPopupSuggestionSelectionIndex: number
  videoPopupSuggestionsFocused: boolean
  onVideoPopupDraftChange: (next: string) => void
  onAddVideo: (value: string) => void
  onRemoveVideo: (index: number) => void
  // PDF
  pdfPopupSuggestions: string[]
  pdfPopupSuggestionSelectionIndex: number
  pdfPopupSuggestionsFocused: boolean
  onPdfPopupDraftChange: (next: string) => void
  onAddPdf: (value: string) => void
  onRemovePdf: (index: number) => void
  // Smart
  smartPopupSuggestions: string[]
  smartPopupSuggestionSelectionIndex: number
  smartPopupSuggestionsFocused: boolean
  onSmartPopupDraftChange: (next: string) => void
  onSmartToggle: (nextEnabled: boolean) => void
  onSmartRootSubmit: (value: string) => void
}
export const useContextPopupGlue = ({
  inputValue,
  popupState,
  helpOpen,
  isPopupOpen,
  isCommandMode,
  isCommandMenuActive,
  isGenerating,
  droppedFilePath,
  files,
  urls,
  images,
  videos,
  pdfs,
  smartContextEnabled,
  smartContextRoot,
  addFile,
  removeFile,
  addUrl,
  removeUrl,
  updateUrl,
  addImage,
  removeImage,
  addVideo,
  removeVideo,
  addPdf,
  removePdf,
  toggleSmartContext,
  setSmartRoot,
  setInputValue,
  setPopupState,
  suppressNextInput,
  notify,
  pushHistory,
  addCommandHistoryEntry,
  handleCommandSelection,
  consumeSuppressedTextInputChange,
  isFilePath,
}: UseContextPopupGlueOptions): UseContextPopupGlueResult => {
  const fileGlue = useFilePopupGlue({
    popupState,
    files,
    setPopupState,
    pushHistory,
    addFile,
    removeFile,
    consumeSuppressedTextInputChange,
    isFilePath,
  })
  const urlGlue = useUrlPopupGlue({
    popupState,
    urls,
    setPopupState,
    pushHistory,
    addUrl,
    removeUrl,
    updateUrl,
    consumeSuppressedTextInputChange,
  })
  const imageGlue = useImagePopupGlue({
    popupState,
    images,
    setPopupState,
    pushHistory,
    addImage,
    removeImage,
    consumeSuppressedTextInputChange,
    isFilePath,
  })
  const videoGlue = useVideoPopupGlue({
    popupState,
    videos,
    setPopupState,
    pushHistory,
    addVideo,
    removeVideo,
    consumeSuppressedTextInputChange,
    isFilePath,
  })
  const pdfGlue = usePdfPopupGlue({
    popupState,
    pdfs,
    setPopupState,
    pushHistory,
    addPdf,
    removePdf,
    consumeSuppressedTextInputChange,
    isFilePath,
  })
  const smartGlue = useSmartPopupGlue({
    popupState,
    smartContextEnabled,
    smartContextRoot,
    setPopupState,
    notify,
    toggleSmartContext,
    setSmartRoot,
    consumeSuppressedTextInputChange,
  })
  const handleSeriesShortcut = useStableCallback((_input: string, key: Key) => {
    if (popupState || isCommandMenuActive || isCommandMode) {
      return
    }
    if (!key.tab || key.shift) {
      return
    }
    if (droppedFilePath) {
      fileGlue.onAddFile(droppedFilePath)
      suppressNextInput()
      setInputValue('')
      return
    }
    if (isGenerating) {
      pushHistory('Generation already running. Please wait.', 'system')
      return
    }
    const trimmedArgs = inputValue.trim()
    addCommandHistoryEntry(`/series${trimmedArgs ? ` ${trimmedArgs}` : ''}`)
    handleCommandSelection('series', inputValue)
  })
  useInput(handleSeriesShortcut, { isActive: !isPopupOpen && !helpOpen })
  return {
    ...fileGlue,
    ...urlGlue,
    ...imageGlue,
    ...videoGlue,
    ...pdfGlue,
    ...smartGlue,
  }
}
</file>

<file path="docs/prompt-maker-cli-tui-encyclopedia.md">
# Prompt Maker CLI + TUI Encyclopedia (Current Codebase)

This document is the **source-of-truth user + maintainer reference** for `@perceptron/prompt-maker-cli`’s Ink TUI and the underlying generate/test CLI workflows.

- Package: `@perceptron/prompt-maker-cli` (ESM)
- Node: `>=18` (`package.json`)
- Binary: `prompt-maker-cli` → `dist/index.js` (`package.json#bin`)

Where possible, each behavior links back to an implementation file under `src/`.

> Note: The previous version of this encyclopedia referenced an older repo layout (`apps/prompt-maker-cli/...`) and embedded line-ranges. This repo’s current layout is **single-package** at the root (`src/`, `dist/`).

## Table of Contents

- [1. CLI Entry Points & Modes](#1-cli-entry-points--modes)
- [2. Export Mode (CLI)](#2-export-mode-cli)
- [3. Compose Mode (CLI)](#3-compose-mode-cli)
- [4. Generate Mode (CLI)](#4-generate-mode-cli)
- [5. Context Ingestion](#5-context-ingestion)
- [6. Interactive Refinement (TTY and Transport)](#6-interactive-refinement-tty-and-transport)
- [7. Outputs, Streaming, Telemetry, and Persistence](#7-outputs-streaming-telemetry-and-persistence)
- [8. Test Runner (CLI)](#8-test-runner-cli)
- [9. TUI Overview](#9-tui-overview)
- [10. TUI Keybindings](#10-tui-keybindings)
- [11. TUI Commands (`/command` palette)](#11-tui-commands-command-palette)
- [12. TUI Themes](#12-tui-themes)
- [13. TUI Architecture Map (for maintainers)](#13-tui-architecture-map-for-maintainers)
- [14. TODOs / Known Quirks](#14-todos--known-quirks)

---

## 1. CLI Entry Points & Modes

Routing is implemented in `src/index.ts`.

### Commands and routing rules

`prompt-maker-cli` has five top-level modes:

- `ui`: Ink TUI
- `generate`: prompt generation pipeline (default)
- `test`: prompt test runner
- `export`: export a past generate payload from history
- `compose`: deterministic (non-LLM) prompt composition scaffold

Routing rules (`src/index.ts`):

- No args → `ui`
- First arg `ui` → `ui`
- First arg `test` → `test`
- First arg `export` → `export`
- First arg `compose` → `compose`
- First arg `generate` or `expand` → `generate` (alias)
- Anything else (including flags like `--json`) → `generate`

Help behavior:

- `prompt-maker-cli --help` shows **generate** help only.
- Use `prompt-maker-cli export --help` / `prompt-maker-cli compose --help` for subcommand help.

### Practical examples

```bash
# TUI (default)
prompt-maker-cli

# TUI explicitly
prompt-maker-cli ui

# Generate
prompt-maker-cli "Draft an onboarding prompt" --model gpt-4o-mini

# Generate (explicit subcommand)
prompt-maker-cli generate "Draft an onboarding prompt"

# Tests
prompt-maker-cli test
prompt-maker-cli test prompt-tests.yaml

# Export the last run from history
prompt-maker-cli export --format json --out runs/last-run.json

# Deterministic (non-LLM) composition scaffold
prompt-maker-cli compose --recipe recipes/triage.txt --input "Hello"
```

---

## 2. Export Mode (CLI)

Export pulls a previously-generated `GenerateJsonPayload` from the JSONL history log and writes it to disk as JSON or YAML.

Implementation: `src/export-command.ts` (history selection: `src/history/generate-history.ts`, file writer: `src/export/export-generate-payload.ts`).

### Usage

```bash
prompt-maker-cli export --format json --out runs/last-run.json
prompt-maker-cli export --format yaml --out runs/last-run.yaml

# Select an older entry from history
prompt-maker-cli export --from-history last:3 --format json --out runs/third-from-last.json
prompt-maker-cli export --from-history 10 --format yaml --out runs/tenth-from-last.yaml
```

### Flags

- `--from-history <selector>`: which entry to export from the history file (`last`, `last:N`, or `N`-th from end). Default: `last`.
- `--format json|yaml`: required output format.
- `--out <path>`: required output file path (directories are created).
- `--quiet`: suppresses human-readable **stderr** logs.

### Schema compatibility

- Export validates the selected history entry’s `schemaVersion`.
- If the selected entry is from a newer/older `schemaVersion` than the current binary supports, export fails with an actionable error message.

### Stdout/stderr

- Export does not print payloads to stdout. It writes the payload to `--out`.
- Success messages are printed to **stderr** unless `--quiet` is set.

---

## 3. Compose Mode (CLI)

Compose is a deterministic, non-LLM scaffold for building a prompt-like artifact from a "recipe" file plus an input string.

Implementation: `src/compose-command.ts`.

### Usage

```bash
prompt-maker-cli compose --recipe recipes/triage.txt --input "Summarize this diff"
```

### Output behavior (current scaffold)

- Reads the recipe file as plain text.
- Prints a deterministic composition to **stdout**:

```text
<recipe file contents>
---
<input>
```

Notes:

- This is scaffolding (no recipe semantics yet): it does not parse YAML, expand templates, or call any LLM.
- All successful output is written to **stdout** (safe for piping); errors are written to **stderr**.

---

## 4. Generate Mode (CLI)

Generate-mode argument parsing lives in `src/generate/args.ts` and the main pipeline is `src/generate/pipeline.ts`.

### Intent sources

Generate consumes intent from one of:

- Positional string: `prompt-maker-cli "..."`
- File: `--intent-file path/to/intent.md`
- `stdin`: pipe into the command

Implementation: `src/generate/intent.ts` (called from `src/generate/pipeline.ts`).

### Flags (authoritative)

The `--help` output for generate is produced by yargs in `src/generate/args.ts` and matches `node dist/index.js --help`.

| Flag                      |                  Type |       Default | Notes                                                                              |
| ------------------------- | --------------------: | ------------: | ---------------------------------------------------------------------------------- |
| `-f, --intent-file`       |                string |             - | Read intent from file                                                              |
| `--model`                 |                string |             - | Generation model override                                                          |
| `--target`                |                string |             - | Target/runtime model for optimization guidance (not included in prompt text)       |
| `--polish-model`          |                string |             - | Model used for polish pass                                                         |
| `-i, --interactive`       |               boolean |       `false` | Enables interactive refinement if a TTY is available                               |
| `--interactive-transport` |                string |             - | Local socket/pipe to drive refinements remotely                                    |
| `--polish`                |               boolean |       `false` | Run polish pass after generation                                                   |
| `--json`                  |               boolean |       `false` | Emit JSON payload to stdout (non-interactive only)                                 |
| `--stream`                |         `none\|jsonl` |        `none` | Emit JSONL event stream to stdout                                                  |
| `--quiet`                 |               boolean |       `false` | Suppress human-oriented output (banners/telemetry)                                 |
| `--progress`              |               boolean |        `true` | Show progress spinner (non-interactive only)                                       |
| `--copy`                  |               boolean |       `false` | Copy final prompt to clipboard                                                     |
| `--open-chatgpt`          |               boolean |       `false` | Open a ChatGPT URL prefilled with the prompt                                       |
| `--resume-last`           |               boolean |       `false` | Resume from the last history entry                                                 |
| `--resume`                |                string |             - | Resume from history selector (`last`, `last:N`, or `N`-th from end)                |
| `--resume-from`           |                string |             - | Resume from an exported payload file (`.json` / `.yaml` / `.yml`)                  |
| `--resume-mode`           | `strict\|best-effort` | `best-effort` | How to handle missing resumed context file paths                                   |
| `--context-template`      |                string |             - | Wrap final prompt using a named template                                           |
| `--show-context`          |               boolean |       `false` | Print resolved context files before generation                                     |
| `-c, --context`           |              string[] |          `[]` | File glob patterns (repeatable)                                                    |
| `--url`                   |              string[] |          `[]` | URL context entries (repeatable)                                                   |
| `--image`                 |              string[] |          `[]` | Image file paths (repeatable)                                                      |
| `--video`                 |              string[] |          `[]` | Video file paths (repeatable)                                                      |
| `--context-file`          |                string |             - | Write resolved context to a file                                                   |
| `--context-format`        |          `text\|json` |        `text` | Format used by `--show-context` and `--context-file`                               |
| `--smart-context`         |               boolean |       `false` | Auto-attach relevant local files via embeddings                                    |
| `--smart-context-root`    |                string |             - | Base directory for smart-context scan                                              |
| `--max-input-tokens`      |                number |             - | Enforce a maximum input budget (intent + system + text context)                    |
| `--max-context-tokens`    |                number |             - | Enforce a maximum budget for text context entries (file/url/smart)                 |
| `--context-overflow`      |                string |             - | Overflow handling: `fail`, `drop-smart`, `drop-url`, `drop-largest`, `drop-oldest` |

### Flag incompatibilities

Enforced in `src/generate/pipeline.ts`:

- `--json` **cannot** be combined with interactive modes (`--interactive` or `--interactive-transport`).
  - Error text: `--json cannot be combined with --interactive.`

### Common generate examples

```bash
# Minimal
prompt-maker-cli "Draft a confident onboarding-bot prompt" --model gpt-4o-mini

# Intent file + context + JSON output
prompt-maker-cli --intent-file drafts/intent.md \
  --context src/**/*.ts \
  --context "!src/**/__tests__/**" \
  --json > runs/intent.json

# Show exactly what gets embedded (text)
prompt-maker-cli "Explain this module" --context src/tui/**/*.ts --show-context

# Capture resolved context to a file (JSON array of {path, content})
prompt-maker-cli "Summarize" --url https://example.com \
  --context-file resolved-context.json --context-format json
```

---

## 5. Context Ingestion

### 3.1 File context (`--context`)

Implementation: `src/file-context.ts`.

- Globs are expanded with `fast-glob` using `{ dot: true }`.
- If no files match, the CLI prints a warning and continues with no file context.
- Each matched file is read as UTF-8; unreadable files are skipped with a warning.

Embedding format used in prompts (`src/file-context.ts`):

```xml
<file path="relative/or/absolute/path">
...file contents...
</file>
```

### 3.2 URL context (`--url`)

Implementation: `src/url-context.ts`.

- Only `http:` / `https:` URLs are accepted.
- HTML is downloaded with a timeout and a maximum size (`MAX_HTML_BYTES = 1MB`).
- HTML is converted to text via `html-to-text` (scripts/styles skipped).
- Each URL context entry is stored as a virtual file:
  - `path: url:https://example.com/...`

### 3.3 GitHub URL context

GitHub URLs are handled specially inside `src/url-context.ts` and resolved by `src/github-context.ts`.

Supported GitHub URL shapes (parsed in `src/github-context.ts`):

- Repository root: `https://github.com/<owner>/<repo>`
- Tree view: `https://github.com/<owner>/<repo>/tree/<ref>/<path?>`
- Blob view: `https://github.com/<owner>/<repo>/blob/<ref>/<path>`

Safety limits (`src/github-context.ts`):

- Max files: `MAX_GITHUB_FILES = 60`
- Max file size: `MAX_GITHUB_FILE_BYTES = 64KB`
- Skips common large/binary paths (node_modules, dist, lockfiles, archives, etc.)

GitHub files become virtual context entries:

- `path: github:<owner>/<repo>/<path>`

### 3.4 Smart context (`--smart-context`)

Implementation: `src/smart-context-service.ts` and `src/rag/vector-store.ts`.

Workflow:

1. Scan workspace for candidate files (default patterns):
   - `**/*.{ts,tsx,js,jsx,py,md,json}`
   - Ignored: `node_modules`, `dist`, `.git`, etc.
2. Only files smaller than 25KB are indexed (`MAX_EMBEDDING_FILE_SIZE`).
3. Index embeddings (cached on disk).
4. Search top K results (`k = 5`) relative to the user’s intent.
5. Read matching files and add them to context (skipping files already present).

Embeddings cache location (`src/rag/vector-store.ts`):

- `~/.config/prompt-maker-cli/embeddings_cache.json`

### 3.5 Images (`--image`)

Implementation: `src/image-loader.ts`.

- Supported extensions: `.png`, `.jpg`, `.jpeg`, `.webp`, `.gif`
- Max size: 20MB per image
- Images are embedded as base64 “image parts” (see `@prompt-maker/core` `ImagePart`).

### 3.6 Video (`--video`)

Implementation:

- Upload: `src/media-loader.ts` (Gemini Files API)
- Message parts: `src/prompt-generator/video-parts.ts`
- Model switching: `src/generate/pipeline.ts` + `src/generate/models.ts`

Behavior:

- Video files are uploaded to Gemini Files API and referenced by URI.
- Supported extensions (mime-inferred): `.mp4`, `.mov`, `.m4v`, `.webm`, `.avi`, `.mpeg`, `.mpg`, `.gif`.
- Requires `GEMINI_API_KEY` (or an explicit API key passed to the uploader).
- If videos are provided and the selected generation model is not Gemini, the CLI switches to a Gemini model that supports video input:
  - It prefers `gemini-2.5-pro` when available, otherwise falls back (see `src/generate/models.ts`).

### 3.7 Token budgets and context overflow

Token budgets are applied after all text context is resolved (files, URLs, smart context) and before generation begins.

Flags (or config equivalents) are consumed in `src/generate/pipeline.ts` and evaluated by `src/generate/context-budget.ts`:

- `--max-input-tokens`: caps total input tokens (`intentTokens + systemTokens + fileTokens`).
- `--max-context-tokens`: caps tokens reserved for text context entries (`fileTokens`).
- `--context-overflow`: chooses how to respond when `fileTokens` exceeds the allowed budget.

Notes:

- Budgets apply only to **text context entries** (`--context`, `--url`, `--smart-context`).
  - Images/videos are not included in the token budget model and are never trimmed by these strategies.
- If both `--max-input-tokens` and `--max-context-tokens` are set, the effective allowance for text context is:
  - `allowedFileTokens = min(maxContextTokens, maxInputTokens - intentTokens - systemTokens)`
- If budgets are set but `--context-overflow` is omitted, the default strategy is `fail`.
- If no budgets are set, overflow strategy is ignored and no trimming occurs.

Overflow strategies (`ContextOverflowStrategy` in `src/generate/types.ts`):

- `fail`: throw an error and abort generation.
- `drop-smart`: drop smart-context entries first, then drop remaining entries oldest-first.
- `drop-url`: drop URL entries first, then drop remaining entries oldest-first.
- `drop-largest`: drop the largest token entries first (ties break by age).
- `drop-oldest`: drop entries in original attachment order (oldest-first).

When trimming drops any text context entries:

- `contextPaths` in the final JSON payload is pruned to match the kept entries.
- A `context.overflow` stream event is emitted (see Streaming below).

---

## 6. Interactive Refinement (TTY and Transport)

Interactive logic is in `src/generate/interactive.ts`.

### 4.1 TTY interactive (`--interactive`)

- If `--interactive` is enabled and stdin/stdout are TTYs, the CLI uses `enquirer` prompts to ask whether to refine, and for the refinement instruction.
- If `--interactive` is enabled but no TTY is detected, the CLI prints a warning and proceeds as a normal non-interactive run.

### 4.2 Interactive transport (`--interactive-transport <path>`)

Implementation: `src/generate/interactive-transport.ts`.

- Creates a local `net.Server` on either:
  - Unix socket path (e.g. `/tmp/pmc.sock`), or
  - Windows named pipe path (must start with `\\.\pipe\...`).

The client sends **newline-delimited JSON** commands:

- Refine:
  - `{"type":"refine","instruction":"Make it shorter"}`
- Finish:
  - `{"type":"finish"}`

The transport stream also receives JSONL events emitted by the pipeline (even if `--stream` is `none`), because the transport is always attached as an event “tap”.

Constraint:

- `--json` cannot be used with `--interactive-transport` (enforced in `src/generate/pipeline.ts`).

---

## 7. Outputs, Streaming, Telemetry, and Persistence

### 5.1 JSON output (`--json`)

When `--json` is enabled, the CLI prints a JSON payload to **stdout**:

- Shape: `GenerateJsonPayload` (`src/generate/types.ts`)
- Includes: `schemaVersion`, `intent`, `model`, `targetModel`, `prompt`, optional `polishedPrompt`, `iterations`, `refinements`, `contextPaths`, timestamps, etc.

`--show-context` behavior when combined with `--json`:

- Context is printed to **stderr** (so stdout can remain machine-readable). Implementation: `src/generate/pipeline.ts`.

### 5.2 Streaming (`--stream jsonl`)

Implementation: `src/generate/stream.ts` and event types in `src/generate/types.ts`.

When enabled:

- The CLI writes newline-delimited JSON events to **stdout**.

Example (streaming + budgets):

```bash
prompt-maker-cli "Summarize these files" \
  --context src/**/*.ts \
  --stream jsonl \
  --progress=false \
  --max-context-tokens 8000 \
  --context-overflow drop-smart
```

- Event names include (non-exhaustive):
  - `progress.update`
  - `context.telemetry`
  - `context.overflow`
  - `upload.state`
  - `generation.iteration.start`
  - `generation.iteration.complete`
  - `interactive.state`
  - `interactive.awaiting`
  - `transport.*`
  - `generation.final`

#### `resume.loaded` event

Emitted early in a run when `--resume-last`, `--resume`, or `--resume-from` successfully loads a previous payload.

Shape (`src/generate/types.ts`):

- `event`: `"resume.loaded"`
- `timestamp`: ISO-8601 string
- `source`: `"history" | "file"`
- `reusedContextPaths`: subset of context paths that were successfully reloaded (today this is limited to `source: "file"` entries)
- `missingContextPaths`: context paths that could not be reused (missing files, plus any non-file sources like `url`/`smart`)

Notes:

- **Explicit `--context` overrides** resumed `contextPaths`.
- **Explicit flags override** resumed payload values (e.g. `--model` overrides `payload.model`).
- `--resume-mode best-effort` warns and continues if some file paths are missing.
- `--resume-mode strict` throws if any resumed `source: "file"` paths are missing.

#### `context.overflow` event

Emitted when token budgets are enabled and the CLI drops one or more text context entries to satisfy the budget.

Shape (`src/generate/types.ts`):

- `event`: `"context.overflow"`
- `timestamp`: ISO-8601 string
- `strategy`: `fail | drop-smart | drop-url | drop-largest | drop-oldest`
- `before`: token telemetry before trimming
- `after`: token telemetry after trimming
- `droppedPaths`: array of `{ path, source }` entries removed from context

Example JSONL line (formatted for readability):

```json
{
  "event": "context.overflow",
  "timestamp": "2026-01-04T16:33:17.000Z",
  "strategy": "drop-smart",
  "before": {
    "files": [
      { "path": "src/a.ts", "tokens": 1200 },
      { "path": "src/b.ts", "tokens": 900 }
    ],
    "intentTokens": 200,
    "fileTokens": 2100,
    "systemTokens": 700,
    "totalTokens": 3000
  },
  "after": {
    "files": [{ "path": "src/a.ts", "tokens": 1200 }],
    "intentTokens": 200,
    "fileTokens": 1200,
    "systemTokens": 700,
    "totalTokens": 2100
  },
  "droppedPaths": [{ "path": "src/b.ts", "source": "smart" }]
}
```

### 5.3 Token telemetry

Implementation: `src/generate/token-telemetry.ts`.

- Token telemetry is computed for:
  - the intent
  - each context file
  - the system prompt (+ optional meta instructions)
- A summary is printed in non-JSON, non-quiet mode.
- Telemetry is always emitted as a `context.telemetry` stream event and returned as `GeneratePipelineResult.telemetry`.

### 5.4 History logging (generate runs)

Implementation: `src/history-logger.ts`.

Every generate run appends JSONL to:

- `~/.config/prompt-maker-cli/history.jsonl`

### 5.5 Config resolution

Implementation: `src/config.ts`.

Config resolution order:

1. `PROMPT_MAKER_CLI_CONFIG=/path/to/config.json`
2. `~/.config/prompt-maker-cli/config.json`
3. `~/.prompt-maker-cli.json`

Config keys validated today (`src/config.ts`):

- Provider credentials:
  - `openaiApiKey`, `openaiBaseUrl`
  - `geminiApiKey`, `geminiBaseUrl`
- Prompt generator:
  - `promptGenerator.defaultModel`
  - `promptGenerator.defaultGeminiModel`
  - `promptGenerator.models[]` (custom model registry)
  - `promptGenerator.maxInputTokens` (budget default)
  - `promptGenerator.maxContextTokens` (budget default)
  - `promptGenerator.contextOverflowStrategy` (budget default)
- Templates:
  - `contextTemplates.<name>`
- TUI theming (persisted):
  - `theme`
  - `themeMode` (`light` / `dark` / `system`; `auto` is accepted as an alias for `system`)
- TUI resume defaults (persisted):
  - `resumeMode` (`best-effort` / `strict`)
  - `resumeSourceKind` (`history` / `file`)
- TUI export defaults (persisted):
  - `exportFormat` (`json` / `yaml`)
  - `exportOutDir` (directory path)

### 5.6 TUI-local persistence

TUI command history is stored separately from generate history:

- `~/.config/prompt-maker-cli/tui-history.json`

Implementation: `src/tui/command-history.ts`.

---

## 8. Test Runner (CLI)

Implementation: `src/test-command.ts`.

Usage:

```bash
prompt-maker-cli test
prompt-maker-cli test prompt-tests.yaml
```

- Default file: `prompt-tests.yaml` (repo root)
- Test suite format is validated by zod schema in `src/testing/test-schema.ts`.

---

## 9. TUI Overview

The Ink TUI is implemented under `src/tui/` and launched via `src/tui/index.tsx`.

### Launch

```bash
prompt-maker-cli
# or
prompt-maker-cli ui

# With interactive transport
prompt-maker-cli ui --interactive-transport /tmp/pmc.sock
```

TUI argument parsing is intentionally minimal (`src/tui/index.tsx`):

- Only `--interactive-transport` is parsed.
- `ui --help` is **not** implemented; passing `--help` will still attempt to run Ink.

TTY requirement:

- Ink requires raw-mode input; running the TUI without a real TTY will throw an Ink “Raw mode is not supported” error.

### Views

`AppContainer` (`src/tui/AppContainer.tsx`) has two views:

- Generate view (default)
- Test Runner view

---

## 10. TUI Keybindings

Authoritative sources:

- Global key router: `src/tui/app-container-keymap.ts`
- Help overlay text: `src/tui/help-config.ts`

### Global

- `Ctrl+G`: open command palette (and switch to Generate view if needed)
- `Ctrl+T`: switch to Test Runner view
- `/help`: open help overlay
- `Ctrl+C`: exit
- `Esc`: dismiss UI elements (never exits)

### Generate view

The Generate view is a “chat-like” screen:

- Type freeform text and press Enter to generate.
- Type `/` to enter command mode (command palette filtering).
- `↑/↓`: scroll history (when no popup/help is active)

Tab behavior (verified in `src/tui/screens/command/hooks/useContextPopupGlue.ts`):

- If the current input looks like a dropped absolute file path, `Tab` adds that file to context.
- Otherwise, `Tab` opens the `/series` flow.

### Test Runner view

Verified in `src/tui/screens/test-runner/TestRunnerScreen.tsx`:

- `Tab` / `Shift+Tab`: change focus
- `Enter` (file input): moves focus to actions
- `Enter` (actions): runs tests

---

## 11. TUI Commands (`/command` palette)

Commands are listed in the palette via `src/tui/config.ts`.

Execution is split:

- Popup-backed commands are handled by `src/tui/hooks/usePopupManager.ts`.
- Session commands like `/new` and `/reuse` are handled in the command screen layer (see `src/tui/screens/command/hooks/useSessionCommands.ts`).

### Command list and behavior

| Command                   | Args                     | Behavior                                                                                |
| ------------------------- | ------------------------ | --------------------------------------------------------------------------------------- |
| `/help`                   | -                        | Show keyboard shortcuts (help overlay)                                                  |
| `/model`                  | -                        | Open model picker popup (generation model)                                              |
| `/target`                 | -                        | Open target model picker popup                                                          |
| `/polish`                 | `off\|clear\|--clear`    | Clear polish model; otherwise open polish model picker                                  |
| `/intent`                 | `[path]`                 | Set intent file path; without args opens picker popup                                   |
| `/meta` / `/instructions` | `[text]`                 | Set meta instructions; without args opens editor popup                                  |
| `/new`                    | -                        | Reset session state (see `src/tui/screens/command/hooks/useSessionCommands.ts`)         |
| `/reuse`                  | -                        | Reset and reuse last prompt (see `src/tui/screens/command/hooks/useSessionCommands.ts`) |
| `/file`                   | -                        | Open file-context popup                                                                 |
| `/url`                    | `[url ...]`              | Add URLs directly (HTTP(S) only) or open popup                                          |
| `/smart`                  | `on\|off`                | Toggle smart context                                                                    |
| `/smart-root`             | `<path>\|--clear\|clear` | Set/clear smart context root (may auto-enable smart context)                            |
| `/image`                  | `[path]`                 | Attach an image path directly or open popup                                             |
| `/video`                  | `[path]`                 | Attach a video path directly or open popup                                              |
| `/copy`                   | `[on\|off]`              | Toggle auto-copy or open toggle popup                                                   |
| `/chatgpt`                | `[on\|off]`              | Toggle auto-open-chatgpt or open toggle popup                                           |
| `/json`                   | `[on\|off]`              | Toggle showing JSON payload in TUI history (blocked when transport active)              |
| `/tokens`                 | -                        | Open token breakdown popup                                                              |
| `/budgets`                | -                        | Configure token budgets and overflow strategy (popup)                                   |
| `/resume`                 | -                        | Resume generation from history or an exported payload (popup)                           |
| `/export`                 | -                        | Export a selected history payload to JSON/YAML (popup)                                  |
| `/settings`               | -                        | Open settings popup                                                                     |
| `/theme`                  | -                        | Open theme picker popup                                                                 |
| `/theme-mode`             | -                        | Open theme mode picker popup                                                            |
| `/reasoning` / `/why`     | -                        | Open last reasoning popup                                                               |
| `/history`                | -                        | Open command history popup                                                              |
| `/series`                 | `[draft text]`           | Start atomic prompt series flow (prefills from typed/last intent or intent file)        |
| `/test`                   | `[file]`                 | Run tests (with arg) or open test popup                                                 |
| `/exit`                   | -                        | Exit the app                                                                            |

Notes:

- `/theme-mode` currently **opens a popup**; it does not parse inline args (despite some older docs claiming `/theme-mode dark|light|system`). Implementation: `src/tui/hooks/usePopupManager.ts`.
- `/json` in the TUI controls whether the final JSON payload is shown in the history pane; it does not change the underlying generate-mode `--json` behavior.

---

## 12. TUI Themes

Theme implementation lives under `src/tui/theme/`.

### Theme discovery

Implementation: `src/tui/theme/theme-loader.ts`.

Themes are loaded from:

- Built-ins: `src/tui/theme/builtins/*`
- Global custom themes:
  - `~/.config/prompt-maker-cli/themes/*.json`
- Project themes (walk up from `process.cwd()`):
  - `<cwd-or-parent>/.prompt-maker-cli/themes/*.json`

Precedence (highest wins on name collision):

1. Project theme closest to the current working directory
2. Project themes in ancestor directories
3. Global themes
4. Built-in themes

If a custom theme overrides a built-in by name, the built-in label is kept.

### Theme JSON schema (enforced)

Validation is performed in `src/tui/theme/theme-loader.ts`.

- Required top-level keys:
  - `theme` (object)
  - `defs` (optional object)
- Required theme slots are enumerated in `src/tui/theme/theme-types.ts` (`REQUIRED_THEME_SLOTS`).
- Supported color value types:
  - string (hex, keyword, or reference)
  - number (ANSI 0–255)
  - variant object: `{ "dark": <value>, "light": <value> }`

Opencode compatibility:

- Some Opencode theme shapes are adapted on load (see `adaptOpencodeThemeJson` in `src/tui/theme/theme-loader.ts`).

### Persistence

Theme selection is persisted to CLI config (`src/config.ts`) via `updateCliThemeSettings`:

- `theme`: selected theme name
- `themeMode`: `dark` / `light` / `system`

Other TUI workflow defaults are also persisted to CLI config:

- Budgets (`promptGenerator.maxInputTokens`, `promptGenerator.maxContextTokens`, `promptGenerator.contextOverflowStrategy`)
- Resume defaults (`resumeMode`, `resumeSourceKind`)
- Export defaults (`exportFormat`, `exportOutDir`)

The theme provider loads and saves theme settings via `src/tui/theme/theme-settings-service.ts`.

---

## 13. TUI Architecture Map (for maintainers)

High-level module boundaries:

- Entrypoint + routing: `src/index.ts`
- Generate pipeline: `src/generate/pipeline.ts`
- Test runner: `src/test-command.ts`
- TUI entry: `src/tui/index.tsx`
- TUI app shell: `src/tui/AppContainer.tsx`
- Generate view (Command screen): `src/tui/screens/command/*`
- Test Runner view: `src/tui/screens/test-runner/*`

Key design invariants (reinforced by `AGENTS.md`):

- Input routing priority: Help overlay > Popups > Screen input > AppContainer global keys
- Reducers are kept pure and unit-testable (many tests live under `src/__tests__/tui/`)

---

## 14. TODOs / Known Quirks

These are verified behaviors that may surprise users:

- `prompt-maker-cli test --help` prints help but **still runs** the test suite afterwards.
  - Reason: `src/test-command.ts` parses args but does not special-case `--help`.
- `prompt-maker-cli ui --help` is not implemented; it will try to start Ink.
  - `src/tui/index.tsx` only parses `--interactive-transport`.
- `prompt-maker-cli --version` prints a version but does not exit the process.
  - If you run it without an intent, generate mode will then error with “Intent text is required…”.
</file>

<file path="src/generate/types.ts">
import type { FileContext } from '../file-context'
import type { UploadDetail, UploadState } from '../prompt-generator-service'
export type StreamMode = 'none' | 'jsonl'
export type ResumeMode = 'strict' | 'best-effort'
export type ContextOverflowStrategy =
  | 'fail'
  | 'drop-smart'
  | 'drop-url'
  | 'drop-largest'
  | 'drop-oldest'
export type GenerateArgs = {
  intent?: string
  intentFile?: string
  model?: string
  target?: string
  interactive: boolean
  copy: boolean
  openChatGpt: boolean
  polish: boolean
  polishModel?: string
  json: boolean
  quiet: boolean
  progress: boolean
  stream: StreamMode
  resumeLast?: boolean
  resume?: string
  resumeFrom?: string
  resumeMode?: ResumeMode
  maxInputTokens?: number
  maxContextTokens?: number
  contextOverflow?: ContextOverflowStrategy
  showContext: boolean
  contextTemplate?: string
  contextFile?: string
  interactiveTransport?: string
  contextFormat: 'text' | 'json'
  help: boolean
  context: string[]
  urls: string[]
  images: string[]
  video: string[]
  pdf: string[]
  metaInstructions?: string
  smartContext: boolean
  smartContextRoot?: string
  inlineIntentAfterInteractive?: boolean
}
export type ContextPathMetadata = {
  path: string
  source: ContextPathSource
}
type ContextPathSource = 'intent' | 'file' | 'url' | 'smart'
export const GENERATE_JSON_PAYLOAD_SCHEMA_VERSION = '1' as const
export type GenerateJsonPayload = {
  schemaVersion: typeof GENERATE_JSON_PAYLOAD_SCHEMA_VERSION
  intent: string
  model: string
  targetModel: string
  prompt: string
  reasoning?: string
  metaInstructions?: string
  refinements: string[]
  iterations: number
  interactive: boolean
  timestamp: string
  contextPaths: ContextPathMetadata[]
  images?: string[]
  videos?: string[]
  pdfs?: string[]
  outputPath?: string
  polishedPrompt?: string
  polishModel?: string
  contextTemplate?: string
  renderedPrompt?: string
}
export type GeneratePipelineResult = {
  payload: GenerateJsonPayload
  telemetry: TokenTelemetry
  generatedPrompt: string
  reasoning?: string
  polishedPrompt?: string
  finalPrompt: string
  iterations: number
  model: string
  contextPaths: ContextPathMetadata[]
}
export type GeneratePipelineOptions = {
  onStreamEvent?: (event: StreamEventInput) => void
  interactiveDelegate?: InteractiveDelegate
}
export type InteractiveDelegate = {
  getNextAction: (context: {
    iteration: number
    currentPrompt: string
  }) => Promise<{ type: 'refine'; instruction: string } | { type: 'finish' }>
}
export type TokenTelemetry = {
  files: FileTokenSummary[]
  intentTokens: number
  fileTokens: number
  systemTokens: number
  totalTokens: number
}
type FileTokenSummary = {
  path: string
  tokens: number
}
export type LoopContext = {
  intent: string
  refinements: string[]
  model: string
  targetModel: string
  fileContext: FileContext[]
  images: string[]
  videos: string[]
  pdfs: string[]
  metaInstructions: string
}
export type InteractiveMode = 'transport' | 'tty' | 'none'
export type ProgressScope = 'url' | 'smart' | 'generate' | 'polish' | 'generic'
type StreamEventBase<EventName extends string, Payload extends object> = {
  event: EventName
  timestamp: string
} & Payload
type ResumeLoadedStreamEvent = StreamEventBase<
  'resume.loaded',
  {
    source: 'history' | 'file'
    reusedContextPaths: GenerateJsonPayload['contextPaths']
    missingContextPaths: GenerateJsonPayload['contextPaths']
  }
>
type ContextTelemetryStreamEvent = StreamEventBase<
  'context.telemetry',
  { telemetry: TokenTelemetry }
>
type ContextOverflowStreamEvent = StreamEventBase<
  'context.overflow',
  {
    strategy: ContextOverflowStrategy
    before: TokenTelemetry
    after: TokenTelemetry
    droppedPaths: ContextPathMetadata[]
  }
>
type ProgressStreamEvent = StreamEventBase<
  'progress.update',
  {
    label: string
    state: 'start' | 'update' | 'stop'
    scope?: ProgressScope
  }
>
type UploadStreamEvent = StreamEventBase<
  'upload.state',
  { state: UploadState; detail: UploadDetail }
>
type GenerationIterationStartEvent = StreamEventBase<
  'generation.iteration.start',
  {
    iteration: number
    intent: string
    model: string
    interactive: boolean
    inputTokens: number
    refinements: string[]
    latestRefinement?: string
  }
>
type GenerationIterationCompleteEvent = StreamEventBase<
  'generation.iteration.complete',
  {
    iteration: number
    prompt: string
    tokens: number
    reasoningTokens?: number
  }
>
type InteractiveMilestoneStreamEvent = StreamEventBase<
  'interactive.state',
  {
    phase: 'start' | 'prompt' | 'refine' | 'complete'
    iteration: number
  }
>
type InteractiveAwaitingStreamEvent = StreamEventBase<
  'interactive.awaiting',
  { mode: InteractiveMode }
>
export type TransportListeningEvent = StreamEventBase<'transport.listening', { path: string }>
export type TransportClientConnectedEvent = StreamEventBase<
  'transport.client.connected',
  { status: 'connected' }
>
export type TransportClientDisconnectedEvent = StreamEventBase<
  'transport.client.disconnected',
  { status: 'disconnected' }
>
export type TransportEvent =
  | TransportListeningEvent
  | TransportClientConnectedEvent
  | TransportClientDisconnectedEvent
type GenerationFinalStreamEvent = StreamEventBase<
  'generation.final',
  { result: GenerateJsonPayload }
>
type StreamEvent =
  | ResumeLoadedStreamEvent
  | ContextTelemetryStreamEvent
  | ContextOverflowStreamEvent
  | ProgressStreamEvent
  | UploadStreamEvent
  | GenerationIterationStartEvent
  | GenerationIterationCompleteEvent
  | InteractiveMilestoneStreamEvent
  | InteractiveAwaitingStreamEvent
  | TransportEvent
  | GenerationFinalStreamEvent
export type StreamEventInput = {
  [EventName in StreamEvent['event']]: Omit<Extract<StreamEvent, { event: EventName }>, 'timestamp'>
}[StreamEvent['event']]
export type TransportLifecycleEventInput = Extract<
  StreamEventInput,
  {
    event: 'transport.listening' | 'transport.client.connected' | 'transport.client.disconnected'
  }
>
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenPopupAndView.types.ts">
import type { WriteStream } from 'node:tty'
import type { MutableRefObject } from 'react'
import type { NotifyOptions } from '../../../notifier'
import type { HistoryEntry, ModelOption, ProviderStatusMap } from '../../../types'
export type PushHistory = (
  content: string,
  kind?: HistoryEntry['kind'],
  format?: HistoryEntry['format'],
) => void
export type CommandContextOptions = {
  interactiveTransportPath?: string | undefined
  notify: (message: string, options?: NotifyOptions) => void
  stdout: WriteStream | undefined
  // context state
  files: string[]
  urls: string[]
  images: string[]
  videos: string[]
  pdfs: string[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  metaInstructions: string
  maxContextTokens: number | null
  maxInputTokens: number | null
  contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
  lastReasoning: string | null
  lastGeneratedPrompt: string | null
  // context dispatch
  addFile: (value: string) => void
  removeFile: (index: number) => void
  addUrl: (value: string) => void
  removeUrl: (index: number) => void
  updateUrl: (index: number, value: string) => void
  addImage: (value: string) => void
  removeImage: (index: number) => void
  addVideo: (value: string) => void
  removeVideo: (index: number) => void
  addPdf: (value: string) => void
  removePdf: (index: number) => void
  toggleSmartContext: () => void
  setSmartRoot: (value: string) => void
  setMetaInstructions: (value: string) => void
  setBudgets: (value: {
    maxContextTokens: number | null
    maxInputTokens: number | null
    contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
  }) => void
  resetContext: () => void
}
export type CommandInputOptions = {
  // screen state
  terminalRows: number
  terminalColumns: number
  inputValue: string
  isPasteActive: boolean
  commandSelectionIndex: number
  debugKeyLine: string | null
  debugKeysEnabled: boolean
  setTerminalSize: (rows: number, columns: number) => void
  setInputValue: (value: string | ((prev: string) => string)) => void
  setPasteActive: (active: boolean) => void
  setCommandSelectionIndex: (next: number | ((prev: number) => number)) => void
  // input local
  intentFilePath: string
  setIntentFilePath: (value: string) => void
  copyEnabled: boolean
  setCopyEnabled: (value: boolean) => void
  chatGptEnabled: boolean
  setChatGptEnabled: (value: boolean) => void
  jsonOutputEnabled: boolean
  setJsonOutputEnabled: (value: boolean) => void
  // refs
  lastUserIntentRef: MutableRefObject<string | null>
  lastTypedIntentRef: MutableRefObject<string>
  // suppression
  consumeSuppressedTextInputChange: () => boolean
  suppressNextInput: () => void
  updateLastTypedIntent: (next: string) => void
  onDebugKeyEvent: (
    event: import('../../../components/core/MultilineTextInput').DebugKeyEvent,
  ) => void
}
export type CommandPopupOptions = {
  onPopupVisibilityChange?: ((isOpen: boolean) => void) | undefined
  commandMenuSignal?: number | undefined
  helpOpen: boolean
  reservedRows: number
  onOpenHelp?: (() => void) | undefined
}
export type CommandHistoryOptions = {
  // history/test plumbing
  pushHistoryRef: MutableRefObject<PushHistory>
  pushHistoryProxy: PushHistory
  clearHistoryRef: MutableRefObject<() => void>
  scrollToRef: MutableRefObject<(row: number) => void>
  scrollToProxy: (row: number) => void
  closeTestPopupRef: MutableRefObject<() => void>
  commandHistoryValues: string[]
  addCommandHistoryEntry: (value: string) => void
  isTestCommandRunning: boolean
  lastTestFile: string | null
  runTestsFromCommandProxy: (value: string) => void
  onTestPopupSubmit: (value: string) => void
}
export type CommandGenerationOptions = {
  // model/generation
  currentModel: ModelOption['id']
  polishModelId: ModelOption['id'] | null
  currentTargetModel: ModelOption['id']
  modelOptions: ModelOption[]
  providerStatuses: ProviderStatusMap
  selectModel: (nextId: ModelOption['id']) => void
  selectPolishModel: (nextId: ModelOption['id'] | null) => void
  selectTargetModel: (nextId: ModelOption['id']) => void
  isGenerating: boolean
  runGeneration: (payload: {
    intent?: string
    intentFile?: string
    resume?:
      | { kind: 'history'; selector: string; mode: import('../../../types').ResumeMode }
      | { kind: 'file'; payloadPath: string; mode: import('../../../types').ResumeMode }
  }) => Promise<void>
  runSeriesGeneration: (intent: string) => void
  statusChips: string[]
  isAwaitingRefinement: boolean
  submitRefinement: (value: string) => void
  awaitingInteractiveMode:
    | import('../../../generation-pipeline-reducer').InteractiveAwaitingMode
    | null
  tokenUsageRun: import('../../../token-usage-store').TokenUsageRun | null
  tokenUsageBreakdown: import('../../../token-usage-store').TokenUsageBreakdown | null
  latestContextOverflow:
    | import('../../../generation-pipeline-reducer').ContextOverflowDetails
    | null
}
export type UseCommandScreenPopupAndViewOptions = {
  context: CommandContextOptions
  input: CommandInputOptions
  popup: CommandPopupOptions
  history: CommandHistoryOptions
  generation: CommandGenerationOptions
}
export type UseCommandScreenPopupAndViewResult = {
  transportMessage: string | null
  historyPaneProps: Parameters<
    typeof import('./useCommandScreenViewModel').useCommandScreenViewModel
  >[0]['panes']['history']
  popupAreaProps: ReturnType<
    typeof import('./useCommandScreenViewModel').useCommandScreenViewModel
  >['popupAreaProps']
  commandMenuPaneProps: Parameters<
    typeof import('./useCommandScreenViewModel').useCommandScreenViewModel
  >[0]['panes']['menu']
  commandInputProps: ReturnType<
    typeof import('./useCommandScreenViewModel').useCommandScreenViewModel
  >['commandInputProps']
}
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenPopupManager.ts">
import { useApp, useStdout } from 'ink'
import { useCallback, useEffect, useState } from 'react'
import { loadCliConfig } from '../../../../config'
import { usePopupManager } from '../../../hooks/usePopupManager'
import type { NotifyOptions } from '../../../notifier'
import { useTheme } from '../../../theme/theme-provider'
import type {
  HistoryEntry,
  ModelOption,
  PopupState,
  ResumeMode,
  ResumeSourceKind,
} from '../../../types'
const DEFAULT_TEST_FILE = 'prompt-tests.yaml'
type PushHistory = (
  content: string,
  kind?: HistoryEntry['kind'],
  format?: HistoryEntry['format'],
) => void
type UseCommandScreenPopupManagerOptions = {
  currentModel: ModelOption['id']
  currentTargetModel: ModelOption['id']
  modelOptions: readonly ModelOption[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  toggleSmartContext: () => void
  setSmartRoot: (value: string) => void
  urls: string[]
  addUrl: (value: string) => void
  images: string[]
  videos: string[]
  pdfs: string[]
  addImage: (value: string) => void
  addVideo: (value: string) => void
  addPdf: (value: string) => void
  lastTestFile: string | null
  interactiveTransportPath?: string | undefined
  isGenerating: boolean
  lastUserIntentRef: import('react').MutableRefObject<string | null>
  lastTypedIntentRef: import('react').MutableRefObject<string>
  pushHistoryProxy: PushHistory
  notify: (message: string, options?: NotifyOptions) => void
  setInputValue: (value: string | ((prev: string) => string)) => void
  runGeneration: (payload: {
    intent?: string
    intentFile?: string
    resume?:
      | { kind: 'history'; selector: string; mode: ResumeMode }
      | { kind: 'file'; payloadPath: string; mode: ResumeMode }
  }) => Promise<void>
  runSeriesGeneration: (intent: string) => void
  runTestsFromCommandProxy: (value: string) => void
  setCurrentModel: (value: ModelOption['id']) => void
  setCurrentTargetModel: (value: ModelOption['id']) => void
  setPolishModelId: (value: ModelOption['id'] | null) => void
  setCopyEnabled: (value: boolean) => void
  setChatGptEnabled: (value: boolean) => void
  setJsonOutputEnabled: (value: boolean) => void
  intentFilePath: string
  setIntentFilePath: (value: string) => void
  metaInstructions: string
  setMetaInstructions: (value: string) => void
  budgets: {
    maxContextTokens: number | null
    maxInputTokens: number | null
    contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
  }
  setBudgets: (value: {
    maxContextTokens: number | null
    maxInputTokens: number | null
    contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
  }) => void
  polishModelId: ModelOption['id'] | null
  copyEnabled: boolean
  chatGptEnabled: boolean
  jsonOutputEnabled: boolean
}
export type UseCommandScreenPopupManagerResult = {
  popupState: PopupState
  setPopupState: import('react').Dispatch<import('react').SetStateAction<PopupState>>
  actions: ReturnType<typeof usePopupManager>['actions']
  isPopupOpen: boolean
}
export const useCommandScreenPopupManager = ({
  currentModel,
  currentTargetModel,
  modelOptions,
  smartContextEnabled,
  smartContextRoot,
  toggleSmartContext,
  setSmartRoot,
  urls,
  addUrl,
  images,
  videos,
  pdfs,
  addImage,
  addVideo,
  addPdf,
  lastTestFile,
  interactiveTransportPath,
  isGenerating,
  lastUserIntentRef,
  lastTypedIntentRef,
  pushHistoryProxy,
  notify,
  setInputValue,
  runGeneration,
  runSeriesGeneration,
  runTestsFromCommandProxy,
  setCurrentModel,
  setCurrentTargetModel,
  setPolishModelId,
  setCopyEnabled,
  setChatGptEnabled,
  setJsonOutputEnabled,
  intentFilePath,
  setIntentFilePath,
  metaInstructions,
  setMetaInstructions,
  budgets,
  setBudgets,
  polishModelId,
  copyEnabled,
  chatGptEnabled,
  jsonOutputEnabled,
}: UseCommandScreenPopupManagerOptions): UseCommandScreenPopupManagerResult => {
  const { exit } = useApp()
  const { stdout } = useStdout()
  const clearScreen = useCallback(() => {
    if (stdout && stdout.isTTY) {
      stdout.write('\u001b[2J\u001b[H')
    }
  }, [stdout])
  const getLatestTypedIntent = useCallback(() => {
    const trimmed = lastTypedIntentRef.current.trim()
    return trimmed.length > 0 ? trimmed : null
  }, [lastTypedIntentRef])
  const syncTypedIntentRef = useCallback(
    (intent: string) => {
      lastTypedIntentRef.current = intent
    },
    [lastTypedIntentRef],
  )
  const { activeThemeName, mode, themes } = useTheme()
  const [resumeDefaults, setResumeDefaults] = useState<{
    sourceKind: ResumeSourceKind
    mode: ResumeMode
  }>({
    sourceKind: 'history',
    mode: 'best-effort',
  })
  const [exportDefaults, setExportDefaults] = useState<{
    format: 'json' | 'yaml'
    outDir: string | null
  }>({
    format: 'json',
    outDir: null,
  })
  useEffect(() => {
    let cancelled = false
    const hydrate = async (): Promise<void> => {
      const config = await loadCliConfig().catch(() => null)
      if (cancelled) {
        return
      }
      const sourceKind = config?.resumeSourceKind === 'file' ? 'file' : 'history'
      const resumeMode = config?.resumeMode === 'strict' ? 'strict' : 'best-effort'
      setResumeDefaults({ sourceKind, mode: resumeMode })
      const exportFormat = config?.exportFormat === 'yaml' ? 'yaml' : 'json'
      const exportOutDir = config?.exportOutDir?.trim() || null
      setExportDefaults({ format: exportFormat, outDir: exportOutDir })
    }
    void hydrate()
    return () => {
      cancelled = true
    }
  }, [])
  const popupManager = usePopupManager({
    currentModel,
    currentTargetModel,
    modelOptions,
    activeThemeName,
    themeMode: mode,
    themes: themes.map((theme) => ({ name: theme.name, label: theme.label })),
    smartContextEnabled,
    smartContextRoot,
    toggleSmartContext,
    setSmartRoot,
    urls,
    addUrl,
    images,
    videos,
    pdfs,
    addImage,
    addVideo,
    addPdf,
    lastTestFile,
    defaultTestFile: DEFAULT_TEST_FILE,
    ...(interactiveTransportPath ? { interactiveTransportPath } : {}),
    isGenerating,
    lastUserIntentRef,
    pushHistory: pushHistoryProxy,
    notify,
    setInputValue,
    runGeneration,
    runSeriesGeneration,
    runTestsFromCommand: runTestsFromCommandProxy,
    clearScreen,
    exitApp: exit,
    setCurrentModel,
    setCurrentTargetModel,
    setPolishModelId,
    setCopyEnabled,
    setChatGptEnabled,
    setJsonOutputEnabled,
    setIntentFilePath,
    intentFilePath,
    metaInstructions,
    setMetaInstructions,
    budgets,
    setBudgets,
    polishModelId,
    copyEnabled,
    chatGptEnabled,
    jsonOutputEnabled,
    getLatestTypedIntent,
    syncTypedIntentRef,
    resumeDefaults,
    setResumeDefaults,
    exportDefaults,
    setExportDefaults,
  })
  return {
    popupState: popupManager.popupState,
    setPopupState: popupManager.setPopupState,
    actions: popupManager.actions,
    isPopupOpen: popupManager.popupState !== null,
  }
}
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenBindings.ts">
import { POPUP_HEIGHTS } from '../../../config'
import { useStableCallback } from '../../../hooks/useStableCallback'
import { useCommandScreenChips } from './useCommandScreenChips'
import { useCommandScreenPopupBindings } from './useCommandScreenPopupBindings'
import { useCommandScreenPopupManager } from './useCommandScreenPopupManager'
import { useCommandScreenPopupVisibility } from './useCommandScreenPopupVisibility'
import { useCommandScreenShell } from './useCommandScreenShell'
import { useDroppedFilePath } from './useDroppedFilePath'
import type {
  PushHistory,
  UseCommandScreenPopupAndViewOptions,
} from './useCommandScreenPopupAndView.types'
export type UseCommandScreenBindingsResult = {
  popupManager: ReturnType<typeof useCommandScreenPopupManager>
  shell: ReturnType<typeof useCommandScreenShell>
  bindings: ReturnType<typeof useCommandScreenPopupBindings>
  enhancedStatusChips: string[]
}
export const useCommandScreenBindings = (
  options: UseCommandScreenPopupAndViewOptions,
): UseCommandScreenBindingsResult => {
  const { context, input, popup, history, generation } = options
  const popupManager = useCommandScreenPopupManager({
    currentModel: generation.currentModel,
    polishModelId: generation.polishModelId,
    currentTargetModel: generation.currentTargetModel,
    modelOptions: generation.modelOptions,
    smartContextEnabled: context.smartContextEnabled,
    smartContextRoot: context.smartContextRoot,
    toggleSmartContext: context.toggleSmartContext,
    setSmartRoot: context.setSmartRoot,
    urls: context.urls,
    addUrl: context.addUrl,
    images: context.images,
    videos: context.videos,
    pdfs: context.pdfs,
    addImage: context.addImage,
    addVideo: context.addVideo,
    addPdf: context.addPdf,
    lastTestFile: history.lastTestFile,
    ...(context.interactiveTransportPath
      ? { interactiveTransportPath: context.interactiveTransportPath }
      : {}),
    isGenerating: generation.isGenerating,
    lastUserIntentRef: input.lastUserIntentRef,
    lastTypedIntentRef: input.lastTypedIntentRef,
    pushHistoryProxy: history.pushHistoryProxy,
    notify: context.notify,
    setInputValue: input.setInputValue,
    runGeneration: generation.runGeneration,
    runSeriesGeneration: generation.runSeriesGeneration,
    runTestsFromCommandProxy: history.runTestsFromCommandProxy,
    setCurrentModel: generation.selectModel,
    setCurrentTargetModel: generation.selectTargetModel,
    setPolishModelId: generation.selectPolishModel,
    setCopyEnabled: input.setCopyEnabled,
    setChatGptEnabled: input.setChatGptEnabled,
    setJsonOutputEnabled: input.setJsonOutputEnabled,
    intentFilePath: input.intentFilePath,
    setIntentFilePath: input.setIntentFilePath,
    metaInstructions: context.metaInstructions,
    setMetaInstructions: context.setMetaInstructions,
    budgets: {
      maxContextTokens: context.maxContextTokens,
      maxInputTokens: context.maxInputTokens,
      contextOverflowStrategy: context.contextOverflowStrategy,
    },
    setBudgets: context.setBudgets,
    copyEnabled: input.copyEnabled,
    chatGptEnabled: input.chatGptEnabled,
    jsonOutputEnabled: input.jsonOutputEnabled,
  })
  history.closeTestPopupRef.current = () => {
    popupManager.setPopupState((prev) => (prev?.type === 'test' ? null : prev))
  }
  useCommandScreenPopupVisibility({
    isPopupOpen: popupManager.isPopupOpen,
    onPopupVisibilityChange: popup.onPopupVisibilityChange,
  })
  const pushHistory: PushHistory = useStableCallback((content, kind) => {
    history.pushHistoryRef.current(content, kind)
  })
  const droppedFilePath = useDroppedFilePath(input.inputValue)
  const shell = useCommandScreenShell({
    stdout: context.stdout,
    setTerminalSize: input.setTerminalSize,
    ...(context.interactiveTransportPath
      ? { interactiveTransportPath: context.interactiveTransportPath }
      : {}),
    terminalRows: input.terminalRows,
    inputValue: input.inputValue,
    debugKeyLine: input.debugKeyLine,
    debugKeysEnabled: input.debugKeysEnabled,
    helpOpen: popup.helpOpen,
    reservedRows: popup.reservedRows,
    popupState: popupManager.popupState,
    isPopupOpen: popupManager.isPopupOpen,
    setPopupState: popupManager.setPopupState,
    ...(popup.commandMenuSignal !== undefined
      ? { commandMenuSignal: popup.commandMenuSignal }
      : {}),
    commandSelectionIndex: input.commandSelectionIndex,
    setCommandSelectionIndex: input.setCommandSelectionIndex,
    isGenerating: generation.isGenerating,
    awaitingInteractiveMode: generation.awaitingInteractiveMode,
    files: context.files,
    urls: context.urls,
    lastGeneratedPrompt: context.lastGeneratedPrompt,
    resetContext: context.resetContext,
    lastUserIntentRef: input.lastUserIntentRef,
    lastTypedIntentRef: input.lastTypedIntentRef,
    setInputValue: input.setInputValue,
    setIntentFilePath: input.setIntentFilePath,
    setMetaInstructions: context.setMetaInstructions,
    scrollToRef: history.scrollToRef,
    clearHistoryRef: history.clearHistoryRef,
    pushHistoryRef: history.pushHistoryRef,
    scrollToProxy: history.scrollToProxy,
  })
  const { enhancedStatusChips } = useCommandScreenChips({
    currentModel: generation.currentModel,
    providerStatuses: generation.providerStatuses,
    statusChips: generation.statusChips,
    intentFilePath: input.intentFilePath,
    metaInstructions: context.metaInstructions,
  })
  const notify = useStableCallback((message: string) => {
    context.notify(message)
  })
  const bindings = useCommandScreenPopupBindings({
    input: {
      value: input.inputValue,
      setValue: input.setInputValue,
      setPasteActive: input.setPasteActive,
      consumeSuppressedTextInputChange: input.consumeSuppressedTextInputChange,
      suppressNextInput: input.suppressNextInput,
      updateLastTypedIntent: input.updateLastTypedIntent,
      intentFilePath: input.intentFilePath,
      lastUserIntentRef: input.lastUserIntentRef,
    },
    popup: {
      state: popupManager.popupState,
      setState: popupManager.setPopupState,
      isOpen: popupManager.isPopupOpen,
      helpOpen: popup.helpOpen,
      ...(popup.onOpenHelp ? { openHelp: popup.onOpenHelp } : {}),
      close: popupManager.actions.closePopup,
      actions: {
        handleCommandSelection: popupManager.actions.handleCommandSelection,
        handleModelPopupSubmit: popupManager.actions.handleModelPopupSubmit,
        applyToggleSelection: popupManager.actions.applyToggleSelection,
        handleIntentFileSubmit: popupManager.actions.handleIntentFileSubmit,
        handleResumeSubmit: popupManager.actions.handleResumeSubmit,
        handleExportSubmit: popupManager.actions.handleExportSubmit,
        handleBudgetsSubmit: popupManager.actions.handleBudgetsSubmit,
        handleSeriesIntentSubmit: popupManager.actions.handleSeriesIntentSubmit,
      },
    },
    menu: {
      isActive: shell.isCommandMenuActive,
      selectedCommandId: shell.selectedCommand?.id ?? null,
      argsRaw: shell.commandMenuArgsRaw,
      isCommandMode: shell.isCommandMode,
      actions: {
        handleNewCommand: shell.handleNewCommand,
        handleReuseCommand: shell.handleReuseCommand,
      },
    },
    generation: {
      isGenerating: generation.isGenerating,
      isAwaitingRefinement: generation.isAwaitingRefinement,
      submitRefinement: generation.submitRefinement,
      runGeneration: generation.runGeneration,
    },
    history: {
      pushHistory,
      addCommandHistoryEntry: history.addCommandHistoryEntry,
      commandHistoryValues: history.commandHistoryValues,
    },
    context: {
      droppedFilePath,
      files: context.files,
      urls: context.urls,
      images: context.images,
      videos: context.videos,
      pdfs: context.pdfs,
      smartContextEnabled: context.smartContextEnabled,
      smartContextRoot: context.smartContextRoot,
      addFile: context.addFile,
      removeFile: context.removeFile,
      addUrl: context.addUrl,
      removeUrl: context.removeUrl,
      updateUrl: context.updateUrl,
      addImage: context.addImage,
      removeImage: context.removeImage,
      addVideo: context.addVideo,
      removeVideo: context.removeVideo,
      addPdf: context.addPdf,
      removePdf: context.removePdf,
      toggleSmartContext: context.toggleSmartContext,
      setSmartRoot: context.setSmartRoot,
      notify,
      modelOptions: generation.modelOptions,
      lastReasoning: context.lastReasoning,
      terminalColumns: input.terminalColumns,
      reasoningPopupHeight: POPUP_HEIGHTS.reasoning,
    },
  })
  return {
    popupManager,
    shell,
    bindings,
    enhancedStatusChips,
  }
}
</file>

<file path="src/tui/popup-reducer.ts">
/*
 * Popup reducer (pure state transitions).
 *
 * This file intentionally contains *no Ink/React runtime imports*.
 *
 * Why a reducer?
 * - Popups behave like a small state machine: open, close, update fields, and
 *   sometimes receive async data (workspace scan suggestions).
 * - Reducers make transitions explicit and testable: (state, action) => nextState.
 *
 * Important behavior we preserve:
 * - `CommandScreen` and popup components still call `setPopupState(prev => ...)`.
 *   We model that as a `set` action that can optionally keep active scan state
 *   when the popup type does not change.
 * - Async scan results must not overwrite newer popups.
 */
import type {
  PopupState,
  ResumeMode,
  ResumeSourceKind,
  ResumeHistoryItem,
  ExportHistoryItem,
  ToggleField,
} from './types'
import type { ThemeMode } from './theme/theme-types'
// Lightweight replacement for React's SetStateAction type.
// Keeping it local avoids importing React into a pure helper.
export type SetStateAction<State> = State | ((prev: State) => State)
export type PopupScanKind = 'file' | 'image' | 'video' | 'pdf' | 'smart' | 'intent' | 'resume'
export type PopupManagerState = {
  popupState: PopupState
  activeScan: { kind: PopupScanKind; id: number } | null
}
export type PopupAction =
  | { type: 'set'; next: SetStateAction<PopupState> }
  | { type: 'close' }
  | {
      type: 'open-model'
      kind: 'generation' | 'polish' | 'target'
      query: string
      selectionIndex: number
    }
  | { type: 'open-toggle'; field: ToggleField; selectionIndex: number }
  | { type: 'open-file'; scanId: number }
  | { type: 'open-url' }
  | { type: 'open-image'; scanId: number }
  | { type: 'open-video'; scanId: number }
  | { type: 'open-pdf'; scanId: number }
  | { type: 'open-history' }
  | {
      type: 'open-resume'
      scanId: number | null
      sourceKind: ResumeSourceKind
      mode: ResumeMode
      payloadPathDraft: string
      historyItems: ResumeHistoryItem[]
      historySelectionIndex: number
      historyErrorMessage: string | null
    }
  | {
      type: 'open-export'
      format: 'json' | 'yaml'
      outPathDraft: string
      historyItems: ExportHistoryItem[]
      historySelectionIndex: number
      historyErrorMessage: string | null
    }
  | { type: 'open-smart'; scanId: number; draft: string }
  | { type: 'open-tokens' }
  | {
      type: 'open-budgets'
      maxContextTokens: number | null
      maxInputTokens: number | null
      contextOverflowStrategy: import('../config').ContextOverflowStrategy | null
    }
  | { type: 'open-settings' }
  | { type: 'open-theme'; selectionIndex: number; initialThemeName: string }
  | {
      type: 'open-theme-mode'
      selectionIndex: number
      initialMode: ThemeMode
    }
  | { type: 'open-reasoning'; scrollOffset: number }
  | { type: 'open-test'; draft: string }
  | { type: 'open-intent'; scanId: number; draft: string }
  | { type: 'open-instructions'; draft: string }
  | { type: 'open-series'; draft: string; hint: string }
  | { type: 'scan-suggestions-success'; kind: PopupScanKind; scanId: number; suggestions: string[] }
export const INITIAL_POPUP_MANAGER_STATE: PopupManagerState = { popupState: null, activeScan: null }
const buildFilePopupState = (): PopupState => ({
  type: 'file',
  draft: '',
  selectionIndex: 0,
  selectedFocused: false,
  suggestedItems: [],
  suggestedSelectionIndex: 0,
  suggestedFocused: false,
})
const buildImagePopupState = (): PopupState => ({
  type: 'image',
  draft: '',
  selectionIndex: 0,
  selectedFocused: false,
  suggestedItems: [],
  suggestedSelectionIndex: 0,
  suggestedFocused: false,
})
const buildVideoPopupState = (): PopupState => ({
  type: 'video',
  draft: '',
  selectionIndex: 0,
  selectedFocused: false,
  suggestedItems: [],
  suggestedSelectionIndex: 0,
  suggestedFocused: false,
})
const buildPdfPopupState = (): PopupState => ({
  type: 'pdf',
  draft: '',
  selectionIndex: 0,
  selectedFocused: false,
  suggestedItems: [],
  suggestedSelectionIndex: 0,
  suggestedFocused: false,
})
const buildSmartPopupState = (draft: string): PopupState => ({
  type: 'smart',
  draft,
  suggestedItems: [],
  suggestedSelectionIndex: 0,
  suggestedFocused: false,
})
const buildIntentPopupState = (draft: string): PopupState => ({
  type: 'intent',
  draft,
  suggestedItems: [],
  suggestedSelectionIndex: 0,
  suggestedFocused: false,
})
const applySuggestions = (
  popupState: PopupState,
  kind: PopupScanKind,
  suggestions: string[],
): PopupState => {
  if (kind === 'file' && popupState?.type === 'file') {
    return {
      ...popupState,
      suggestedItems: suggestions,
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    }
  }
  if (kind === 'image' && popupState?.type === 'image') {
    return {
      ...popupState,
      suggestedItems: suggestions,
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    }
  }
  if (kind === 'video' && popupState?.type === 'video') {
    return {
      ...popupState,
      suggestedItems: suggestions,
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    }
  }
  if (kind === 'pdf' && popupState?.type === 'pdf') {
    return {
      ...popupState,
      suggestedItems: suggestions,
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    }
  }
  if (kind === 'smart' && popupState?.type === 'smart') {
    return {
      ...popupState,
      suggestedItems: suggestions,
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    }
  }
  if (kind === 'intent' && popupState?.type === 'intent') {
    return {
      ...popupState,
      suggestedItems: suggestions,
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    }
  }
  if (kind === 'resume' && popupState?.type === 'resume') {
    return {
      ...popupState,
      suggestedItems: suggestions,
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    }
  }
  return popupState
}
export const popupReducer = (state: PopupManagerState, action: PopupAction): PopupManagerState => {
  switch (action.type) {
    case 'set': {
      const next = action.next
      const nextPopupState = typeof next === 'function' ? next(state.popupState) : next
      // Preserve ongoing scan state only if the popup type stays the same.
      // If we switch popup types (file -> smart), any in-flight scan becomes stale.
      const previousType = state.popupState?.type ?? null
      const nextType = nextPopupState?.type ?? null
      return {
        popupState: nextPopupState,
        activeScan: previousType === nextType ? state.activeScan : null,
      }
    }
    case 'close':
      return INITIAL_POPUP_MANAGER_STATE
    case 'open-model':
      return {
        popupState: {
          type: 'model',
          kind: action.kind,
          query: action.query,
          selectionIndex: action.selectionIndex,
        },
        activeScan: null,
      }
    case 'open-toggle':
      return {
        popupState: { type: 'toggle', field: action.field, selectionIndex: action.selectionIndex },
        activeScan: null,
      }
    case 'open-file':
      return { popupState: buildFilePopupState(), activeScan: { kind: 'file', id: action.scanId } }
    case 'open-url':
      return {
        popupState: {
          type: 'url',
          draft: '',
          selectionIndex: 0,
          selectedFocused: false,
          editingIndex: null,
        },
        activeScan: null,
      }
    case 'open-image':
      return {
        popupState: buildImagePopupState(),
        activeScan: { kind: 'image', id: action.scanId },
      }
    case 'open-video':
      return {
        popupState: buildVideoPopupState(),
        activeScan: { kind: 'video', id: action.scanId },
      }
    case 'open-pdf':
      return {
        popupState: buildPdfPopupState(),
        activeScan: { kind: 'pdf', id: action.scanId },
      }
    case 'open-history':
      return { popupState: { type: 'history', draft: '', selectionIndex: 0 }, activeScan: null }
    case 'open-resume':
      return {
        popupState: {
          type: 'resume',
          selectionIndex: 0,
          sourceKind: action.sourceKind,
          mode: action.mode,
          historyItems: action.historyItems,
          historySelectionIndex: action.historySelectionIndex,
          historyErrorMessage: action.historyErrorMessage,
          payloadPathDraft: action.payloadPathDraft,
          suggestedItems: [],
          suggestedSelectionIndex: 0,
          suggestedFocused: false,
        },
        activeScan: action.scanId === null ? null : { kind: 'resume', id: action.scanId },
      }
    case 'open-export':
      return {
        popupState: {
          type: 'export',
          selectionIndex: 0,
          historyItems: action.historyItems,
          historySelectionIndex: action.historySelectionIndex,
          historyErrorMessage: action.historyErrorMessage,
          format: action.format,
          outPathDraft: action.outPathDraft,
        },
        activeScan: null,
      }
    case 'open-smart':
      return {
        popupState: buildSmartPopupState(action.draft),
        activeScan: { kind: 'smart', id: action.scanId },
      }
    case 'open-tokens':
      return { popupState: { type: 'tokens' }, activeScan: null }
    case 'open-budgets':
      return {
        popupState: {
          type: 'budgets',
          selectionIndex: 0,
          maxContextTokensDraft:
            action.maxContextTokens === null ? '' : String(action.maxContextTokens),
          maxInputTokensDraft: action.maxInputTokens === null ? '' : String(action.maxInputTokens),
          contextOverflowStrategyDraft: action.contextOverflowStrategy ?? '',
          errorMessage: null,
        },
        activeScan: null,
      }
    case 'open-settings':
      return { popupState: { type: 'settings' }, activeScan: null }
    case 'open-theme':
      return {
        popupState: {
          type: 'theme',
          selectionIndex: action.selectionIndex,
          initialThemeName: action.initialThemeName,
        },
        activeScan: null,
      }
    case 'open-theme-mode':
      return {
        popupState: {
          type: 'themeMode',
          selectionIndex: action.selectionIndex,
          initialMode: action.initialMode,
        },
        activeScan: null,
      }
    case 'open-reasoning':
      return {
        popupState: { type: 'reasoning', scrollOffset: action.scrollOffset },
        activeScan: null,
      }
    case 'open-test':
      return { popupState: { type: 'test', draft: action.draft }, activeScan: null }
    case 'open-intent':
      return {
        popupState: buildIntentPopupState(action.draft),
        activeScan: { kind: 'intent', id: action.scanId },
      }
    case 'open-instructions':
      return { popupState: { type: 'instructions', draft: action.draft }, activeScan: null }
    case 'open-series':
      return {
        popupState: { type: 'series', draft: action.draft, hint: action.hint },
        activeScan: null,
      }
    case 'scan-suggestions-success': {
      if (state.activeScan?.kind !== action.kind || state.activeScan.id !== action.scanId) {
        return state
      }
      return {
        popupState: applySuggestions(state.popupState, action.kind, action.suggestions),
        activeScan: null,
      }
    }
    default:
      return state
  }
}
</file>

<file path="src/__tests__/usePopupManager.test.ts">
import { act, renderHook } from '@testing-library/react'
import { JSDOM } from 'jsdom'
import type { MutableRefObject } from 'react'
import { resetRecentSessionModelsForTests } from '../tui/model-session'
import { usePopupManager } from '../tui/hooks/usePopupManager'
import type { UsePopupManagerOptions } from '../tui/hooks/usePopupManager'
import type { ModelOption } from '../tui/types'
jest.mock('../tui/file-suggestions', () => ({
  discoverDirectorySuggestions: jest.fn(),
  discoverFileSuggestions: jest.fn(),
}))
jest.mock('node:fs/promises', () => ({
  readFile: jest.fn(),
}))
const dom = new JSDOM('<!doctype html><html><body></body></html>')
const globalScope = globalThis as typeof globalThis & {
  window: Window & typeof globalThis
  document: Document
  navigator: Navigator
}
globalScope.window = dom.window
globalScope.document = dom.window.document
globalScope.navigator = dom.window.navigator
beforeEach(() => {
  resetRecentSessionModelsForTests()
})
const defaultModelOptions: ModelOption[] = [
  {
    id: 'gpt-4o-mini',
    label: 'gpt-4o-mini',
    provider: 'openai',
    description: 'test',
    capabilities: [],
    source: 'builtin',
  },
]
const createOptions = (overrides: Partial<UsePopupManagerOptions> = {}): UsePopupManagerOptions => {
  const baseRef: MutableRefObject<string | null> = { current: null }
  const defaults: UsePopupManagerOptions = {
    currentModel: 'gpt-4o-mini',
    currentTargetModel: 'gpt-4o-mini',
    modelOptions: defaultModelOptions,
    activeThemeName: 'pm-dark',
    themeMode: 'dark',
    themes: [
      { name: 'pm-dark', label: 'Prompt Maker Dark' },
      { name: 'pm-light', label: 'Prompt Maker Light' },
    ],
    smartContextEnabled: false,
    smartContextRoot: null,
    toggleSmartContext: jest.fn(),
    setSmartRoot: jest.fn(),
    urls: [],
    addUrl: jest.fn(),
    images: [],
    videos: [],
    pdfs: [],
    addImage: jest.fn(),
    addVideo: jest.fn(),
    addPdf: jest.fn(),
    lastTestFile: null,
    defaultTestFile: 'prompt.test.ts',
    interactiveTransportPath: undefined,
    isGenerating: false,
    lastUserIntentRef: baseRef,
    pushHistory: jest.fn(),
    notify: jest.fn(),
    setInputValue: jest.fn(),
    runGeneration: jest.fn(async () => undefined),
    runSeriesGeneration: jest.fn(),
    runTestsFromCommand: jest.fn(),
    exitApp: jest.fn(),
    setCurrentModel: jest.fn(),
    setCurrentTargetModel: jest.fn(),
    setPolishModelId: jest.fn(),
    setCopyEnabled: jest.fn(),
    setChatGptEnabled: jest.fn(),
    setJsonOutputEnabled: jest.fn(),
    setIntentFilePath: jest.fn(),
    intentFilePath: '',
    metaInstructions: '',
    setMetaInstructions: jest.fn(),
    budgets: {
      maxContextTokens: null,
      maxInputTokens: null,
      contextOverflowStrategy: null,
    },
    setBudgets: jest.fn(),
    polishModelId: null,
    copyEnabled: false,
    chatGptEnabled: false,
    jsonOutputEnabled: false,
    getLatestTypedIntent: jest.fn(() => null),
    syncTypedIntentRef: jest.fn(),
    resumeDefaults: { sourceKind: 'history', mode: 'best-effort' },
    setResumeDefaults: jest.fn(),
    exportDefaults: { format: 'json', outDir: null },
    setExportDefaults: jest.fn(),
  }
  return { ...defaults, ...overrides }
}
type Deferred<T> = {
  promise: Promise<T>
  resolve: (value: T) => void
  reject: (reason?: unknown) => void
}
const createDeferred = <T>(): Deferred<T> => {
  let resolve: (value: T) => void = (_value) => undefined
  let reject: (reason?: unknown) => void = (_reason) => undefined
  const promise = new Promise<T>((promiseResolve, promiseReject) => {
    resolve = promiseResolve
    reject = promiseReject
  })
  return { promise, resolve, reject }
}
const fileSuggestions = jest.requireMock('../tui/file-suggestions') as {
  discoverDirectorySuggestions: jest.Mock
  discoverFileSuggestions: jest.Mock
}
const getFsMock = () =>
  jest.requireMock('node:fs/promises') as {
    readFile: jest.MockedFunction<(file: string, encoding: string) => Promise<string>>
  }
describe('usePopupManager theme popup', () => {
  it('opens theme popup with current selection', () => {
    const options = createOptions({
      activeThemeName: 'pm-light',
      themes: [
        { name: 'pm-dark', label: 'Prompt Maker Dark' },
        { name: 'pm-light', label: 'Prompt Maker Light' },
      ],
    })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openThemePopup()
    })
    expect(result.current.popupState).toEqual({
      type: 'theme',
      selectionIndex: 1,
      initialThemeName: 'pm-light',
    })
  })
})
describe('usePopupManager theme mode popup', () => {
  it('opens theme mode popup with current mode', () => {
    const options = createOptions({
      themeMode: 'system',
    })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openThemeModePopup()
    })
    expect(result.current.popupState).toEqual({
      type: 'themeMode',
      selectionIndex: 0,
      initialMode: 'system',
    })
  })
})
describe('usePopupManager file popup', () => {
  beforeEach(() => {
    resetRecentSessionModelsForTests()
    fileSuggestions.discoverFileSuggestions.mockReset()
  })
  it('initializes file popup with suggestion defaults', () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverFileSuggestions.mockReturnValue(deferred.promise)
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openFilePopup()
    })
    expect(fileSuggestions.discoverFileSuggestions).toHaveBeenCalledWith(
      expect.objectContaining({ limit: 5000 }),
    )
    expect(result.current.popupState).toEqual({
      type: 'file',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
  })
  it('populates file popup suggestions after scanning', async () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverFileSuggestions.mockReturnValue(deferred.promise)
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openFilePopup()
    })
    expect(fileSuggestions.discoverFileSuggestions).toHaveBeenCalledWith(
      expect.objectContaining({ limit: 5000 }),
    )
    await act(async () => {
      deferred.resolve(['src/index.ts', 'README.md'])
      await deferred.promise
    })
    expect(result.current.popupState).toEqual({
      type: 'file',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: ['src/index.ts', 'README.md'],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
  })
  it('logs a history entry when scanning fails', async () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverFileSuggestions.mockReturnValue(deferred.promise)
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openFilePopup()
    })
    await act(async () => {
      deferred.reject(new Error('boom'))
      try {
        await deferred.promise
      } catch {
        // ignored
      }
    })
    expect(options.pushHistory).toHaveBeenCalledWith(
      '[file] Failed to scan workspace: boom',
      'system',
    )
  })
})
describe('usePopupManager image popup', () => {
  beforeEach(() => {
    fileSuggestions.discoverFileSuggestions.mockReset()
  })
  it('initializes image popup with suggestion defaults', () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverFileSuggestions.mockReturnValue(deferred.promise)
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openImagePopup()
    })
    expect(result.current.popupState).toEqual({
      type: 'image',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
  })
  it('filters image popup suggestions after scanning', async () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverFileSuggestions.mockReturnValue(deferred.promise)
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openImagePopup()
    })
    await act(async () => {
      deferred.resolve(['src/index.ts', 'diagram.png', 'clip.mp4', 'photo.JPG'])
      await deferred.promise
    })
    expect(result.current.popupState).toEqual({
      type: 'image',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: ['diagram.png', 'photo.JPG'],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
  })
  it('attaches an image via command args', () => {
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('image', 'diagram.png')
    })
    expect(options.addImage).toHaveBeenCalledWith('diagram.png')
    expect(options.pushHistory).toHaveBeenCalledWith('[image] Attached: diagram.png', 'system')
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toBeNull()
  })
})
describe('usePopupManager video popup', () => {
  beforeEach(() => {
    fileSuggestions.discoverFileSuggestions.mockReset()
  })
  it('filters video popup suggestions after scanning', async () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverFileSuggestions.mockReturnValue(deferred.promise)
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openVideoPopup()
    })
    await act(async () => {
      deferred.resolve(['clip.mp4', 'diagram.png', 'movie.MOV', 'README.md'])
      await deferred.promise
    })
    expect(result.current.popupState).toEqual({
      type: 'video',
      draft: '',
      selectionIndex: 0,
      selectedFocused: false,
      suggestedItems: ['clip.mp4', 'movie.MOV'],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
  })
  it('attaches a video via command args', () => {
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('video', 'clip.mp4')
    })
    expect(options.addVideo).toHaveBeenCalledWith('clip.mp4')
    expect(options.pushHistory).toHaveBeenCalledWith('[video] Attached: clip.mp4', 'system')
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toBeNull()
  })
})
describe('usePopupManager smart-root popup', () => {
  beforeEach(() => {
    fileSuggestions.discoverDirectorySuggestions.mockReset()
  })
  it('initializes smart-root popup with suggestion defaults', () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverDirectorySuggestions.mockReturnValue(deferred.promise)
    const options = createOptions({ smartContextRoot: 'src' })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openSmartRootPopup()
    })
    expect(result.current.popupState).toEqual({
      type: 'smart',
      draft: 'src',
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
  })
  it('opens smart-root popup from command selection', () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverDirectorySuggestions.mockReturnValue(deferred.promise)
    const options = createOptions({ smartContextRoot: 'src' })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('smart-root')
    })
    expect(result.current.popupState).toEqual({
      type: 'smart',
      draft: 'src',
      suggestedItems: [],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
  })
  it('auto-enables smart context when setting a root via args', () => {
    const toggleSmartContext = jest.fn()
    const setSmartRoot = jest.fn()
    const options = createOptions({
      smartContextEnabled: false,
      toggleSmartContext,
      setSmartRoot,
    })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('smart-root', 'src')
    })
    expect(setSmartRoot).toHaveBeenCalledWith('src')
    expect(toggleSmartContext).toHaveBeenCalledTimes(1)
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toBeNull()
  })
  it('clears the root when running /smart off', () => {
    const toggleSmartContext = jest.fn()
    const setSmartRoot = jest.fn()
    const options = createOptions({
      smartContextEnabled: true,
      smartContextRoot: 'src',
      toggleSmartContext,
      setSmartRoot,
    })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('smart', 'off')
    })
    expect(setSmartRoot).toHaveBeenCalledWith('')
    expect(toggleSmartContext).toHaveBeenCalledTimes(1)
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toBeNull()
  })
  it('populates smart-root popup suggestions after scanning', async () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverDirectorySuggestions.mockReturnValue(deferred.promise)
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openSmartRootPopup()
    })
    await act(async () => {
      deferred.resolve(['src', 'apps/prompt-maker-cli'])
      await deferred.promise
    })
    expect(result.current.popupState).toEqual({
      type: 'smart',
      draft: '',
      suggestedItems: ['src', 'apps/prompt-maker-cli'],
      suggestedSelectionIndex: 0,
      suggestedFocused: false,
    })
  })
  it('logs a history entry when scanning fails for smart-root', async () => {
    const deferred = createDeferred<string[]>()
    fileSuggestions.discoverDirectorySuggestions.mockReturnValue(deferred.promise)
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openSmartRootPopup()
    })
    await act(async () => {
      deferred.reject(new Error('boom'))
      try {
        await deferred.promise
      } catch {
        // ignored
      }
    })
    expect(options.pushHistory).toHaveBeenCalledWith(
      '[smart] Failed to scan workspace: boom',
      'system',
    )
  })
})
describe('usePopupManager instructions command', () => {
  it('opens and saves meta instructions', () => {
    const options = createOptions({ metaInstructions: 'Be friendly' })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('instructions')
    })
    expect(result.current.popupState).toEqual({ type: 'instructions', draft: 'Be friendly' })
    act(() => {
      result.current.actions.handleInstructionsSubmit('Focus on security')
    })
    expect(options.setMetaInstructions).toHaveBeenCalledWith('Focus on security')
    expect(options.pushHistory).toHaveBeenCalledWith('[instr] Focus on security')
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toBeNull()
  })
  it('applies meta instructions from command args', () => {
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('instructions', 'Focus on security')
    })
    expect(options.setMetaInstructions).toHaveBeenCalledWith('Focus on security')
    expect(options.pushHistory).toHaveBeenCalledWith('[instr] Focus on security')
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toBeNull()
  })
})
describe('usePopupManager intent command', () => {
  it('applies intent file from command args', () => {
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('intent', '/tmp/intent.md')
    })
    expect(options.setIntentFilePath).toHaveBeenCalledWith('/tmp/intent.md')
    expect(options.pushHistory).toHaveBeenCalledWith('Intent file set to /tmp/intent.md')
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toBeNull()
  })
})
describe('usePopupManager exit command', () => {
  it('clears the screen before exiting', () => {
    const clearScreen = jest.fn()
    const exitApp = jest.fn()
    const options = createOptions({ clearScreen, exitApp })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('exit')
    })
    expect(options.pushHistory).toHaveBeenCalledWith('Exiting…', 'system')
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(clearScreen).toHaveBeenCalledTimes(1)
    expect(exitApp).toHaveBeenCalledTimes(1)
    const clearOrder = clearScreen.mock.invocationCallOrder[0]
    const exitOrder = exitApp.mock.invocationCallOrder[0]
    if (clearOrder === undefined || exitOrder === undefined) {
      throw new Error('Expected clearScreen and exitApp to be called')
    }
    expect(clearOrder).toBeLessThan(exitOrder)
  })
})
describe('usePopupManager tokens command', () => {
  it('opens the token usage popup', () => {
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('tokens')
    })
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toEqual({ type: 'tokens' })
  })
})
describe('usePopupManager reasoning command', () => {
  it('opens the reasoning popup', () => {
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('reasoning')
    })
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toEqual({ type: 'reasoning', scrollOffset: 0 })
  })
})
describe('usePopupManager series command', () => {
  beforeEach(() => {
    const fs = getFsMock()
    fs.readFile.mockReset()
  })
  it('prefills the series popup from command args', async () => {
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    await act(async () => {
      result.current.actions.handleCommandSelection('series', 'plan a feature')
      await Promise.resolve()
    })
    expect(options.pushHistory).toHaveBeenCalledWith(
      '[series] Using provided text as intent draft.',
      'system',
    )
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toEqual({
      type: 'series',
      draft: 'plan a feature',
      hint: 'Draft prefills from typed/last intent (or pass /series <intent>).',
    })
  })
  it('prefills the series popup from typed intent', async () => {
    const options = createOptions({ getLatestTypedIntent: jest.fn(() => 'typed intent') })
    const { result } = renderHook(() => usePopupManager(options))
    await act(async () => {
      result.current.actions.handleCommandSelection('series')
      await Promise.resolve()
    })
    expect(options.pushHistory).toHaveBeenCalledWith(
      '[series] Using typed intent as draft.',
      'system',
    )
    expect(result.current.popupState).toEqual({
      type: 'series',
      draft: 'typed intent',
      hint: 'Draft prefills from typed/last intent (or pass /series <intent>).',
    })
  })
  it('prefills the series popup from the last run intent', async () => {
    const lastUserIntentRef: MutableRefObject<string | null> = { current: 'last intent' }
    const options = createOptions({ lastUserIntentRef })
    const { result } = renderHook(() => usePopupManager(options))
    await act(async () => {
      result.current.actions.handleCommandSelection('series')
      await Promise.resolve()
    })
    expect(options.pushHistory).toHaveBeenCalledWith(
      '[series] Reusing last intent as draft.',
      'system',
    )
    expect(result.current.popupState).toEqual({
      type: 'series',
      draft: 'last intent',
      hint: 'Draft prefills from typed/last intent (or pass /series <intent>).',
    })
  })
  it('loads the series popup draft from an intent file when empty', async () => {
    const fs = getFsMock()
    fs.readFile.mockResolvedValueOnce('intent from file')
    const options = createOptions({ intentFilePath: '/tmp/intent.md' })
    const { result } = renderHook(() => usePopupManager(options))
    await act(async () => {
      result.current.actions.handleCommandSelection('series')
      await Promise.resolve()
    })
    expect(fs.readFile).toHaveBeenCalledWith('/tmp/intent.md', 'utf8')
    expect(options.pushHistory).toHaveBeenCalledWith(
      '[series] Loaded draft from intent file intent.md.',
      'system',
    )
    expect(options.syncTypedIntentRef).toHaveBeenCalledWith('intent from file')
    expect(result.current.popupState).toEqual({
      type: 'series',
      draft: 'intent from file',
      hint: 'Loaded from intent file intent.md',
    })
  })
})
describe('usePopupManager test command', () => {
  it('logs a hint when running /test with args', () => {
    const options = createOptions()
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('test', 'prompt-tests.yaml')
    })
    expect(options.pushHistory).toHaveBeenCalledWith(
      '[tests] Running /test prompt-tests.yaml',
      'system',
    )
    expect(options.runTestsFromCommand).toHaveBeenCalledWith('prompt-tests.yaml')
  })
})
describe('usePopupManager quick toggles', () => {
  it('opens polish model picker without arguments', () => {
    const options = createOptions({ polishModelId: null })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('polish')
    })
    expect(result.current.popupState).toEqual({
      type: 'model',
      kind: 'polish',
      query: '',
      selectionIndex: 0,
    })
  })
  it('sets polish model when submitting selection', () => {
    const options = createOptions({ polishModelId: null })
    const { result } = renderHook(() => usePopupManager(options))
    const option = defaultModelOptions[0]
    if (!option) {
      throw new Error('Expected a default model option')
    }
    act(() => {
      result.current.actions.handleCommandSelection('polish')
    })
    act(() => {
      result.current.actions.handleModelPopupSubmit(option)
    })
    expect(options.setPolishModelId).toHaveBeenCalledWith('gpt-4o-mini')
    expect(options.notify).toHaveBeenCalledWith(
      'Selected polish model: gpt-4o-mini (gpt-4o-mini)',
      { kind: 'info' },
    )
    expect(options.setInputValue).toHaveBeenCalledWith('')
  })
  it('clears polish model when submitting null', () => {
    const options = createOptions({ polishModelId: 'gpt-4o-mini' })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('polish')
    })
    act(() => {
      result.current.actions.handleModelPopupSubmit(null)
    })
    expect(options.setPolishModelId).toHaveBeenCalledWith(null)
  })
  it('accepts explicit on/off arguments for copy', () => {
    const options = createOptions({ copyEnabled: true })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('copy', 'off')
    })
    expect(options.setCopyEnabled).toHaveBeenCalledWith(false)
    expect(options.pushHistory).toHaveBeenCalledWith('Copy disabled')
  })
  it('opens the toggle popup when chatgpt args are invalid', () => {
    const options = createOptions({ chatGptEnabled: false })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('chatgpt', 'maybe')
    })
    expect(result.current.popupState).toEqual({
      type: 'toggle',
      field: 'chatgpt',
      selectionIndex: 1,
    })
  })
  it('toggles json output with no args when allowed', () => {
    const options = createOptions({ jsonOutputEnabled: false })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('json')
    })
    expect(options.setJsonOutputEnabled).toHaveBeenCalledWith(true)
    expect(options.notify).toHaveBeenCalledWith('JSON output is ON (payload shown in history)', {
      kind: 'info',
    })
    expect(options.pushHistory).not.toHaveBeenCalled()
  })
  it('blocks json toggling when interactive transport is active', () => {
    const options = createOptions({ interactiveTransportPath: '/tmp/socket' })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('json')
    })
    expect(options.setJsonOutputEnabled).not.toHaveBeenCalled()
    expect(options.pushHistory).toHaveBeenCalledWith(
      'JSON output is unavailable while interactive transport is enabled.',
      'system',
    )
    expect(options.setInputValue).toHaveBeenCalledWith('')
  })
  it('accepts explicit json arguments', () => {
    const options = createOptions({ jsonOutputEnabled: true })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.handleCommandSelection('json', 'off')
    })
    expect(options.setJsonOutputEnabled).toHaveBeenCalledWith(false)
    expect(options.notify).toHaveBeenCalledWith('JSON output is OFF (payload hidden)', {
      kind: 'warning',
    })
    expect(options.pushHistory).not.toHaveBeenCalled()
  })
})
describe('usePopupManager model popup', () => {
  it('notifies on model selection without history', () => {
    const modelOptions: ModelOption[] = [
      {
        id: 'gpt-4o-mini',
        label: 'GPT-4o Mini',
        provider: 'openai',
        description: 'test',
        capabilities: [],
        source: 'builtin',
      },
      {
        id: 'gemini-1.5-pro',
        label: 'Gemini 1.5 Pro',
        provider: 'gemini',
        description: 'test',
        capabilities: [],
        source: 'builtin',
      },
    ]
    const options = createOptions({ currentModel: 'gpt-4o-mini', modelOptions })
    const { result } = renderHook(() => usePopupManager(options))
    act(() => {
      result.current.actions.openModelPopup()
    })
    act(() => {
      result.current.actions.handleModelPopupSubmit(modelOptions[1])
    })
    expect(options.setCurrentModel).toHaveBeenCalledWith('gemini-1.5-pro')
    expect(options.notify).toHaveBeenCalledWith('Selected model: Gemini 1.5 Pro (gemini-1.5-pro)', {
      kind: 'info',
    })
    expect(options.pushHistory).not.toHaveBeenCalled()
    expect(options.setInputValue).toHaveBeenCalledWith('')
    expect(result.current.popupState).toBeNull()
  })
})
</file>

<file path="src/tui/hooks/useGenerationPipeline.ts">
import path from 'node:path'
import { useCallback, useEffect, useMemo, useReducer, useRef } from 'react'
import { createBufferedHistoryWriter } from './buffered-history-writer'
import {
  buildIterationCompleteHistoryMessages,
  buildJsonPayloadHistoryMessages,
  extractValidationSection,
  formatCompactTokens,
  getHistoryWrapWidth,
  wrapTextForHistory,
} from './generation-history-formatters'
import {
  prepareSeriesOutputDir,
  type WriteSeriesArtifactsResult,
  writeSeriesArtifacts,
} from './series-artifacts-io'
import { useLatestRef } from './useLatestRef'
import {
  INITIAL_GENERATION_PIPELINE_STATE,
  generationPipelineReducer,
  type InteractiveAwaitingMode,
} from '../generation-pipeline-reducer'
import {
  maybeCopyToClipboard,
  maybeOpenChatGpt,
  runGeneratePipeline,
  type GenerateArgs,
  type GeneratePipelineOptions,
  type GeneratePipelineResult,
  type InteractiveDelegate,
  type StreamEventInput,
} from '../../generate-command'
import { evaluateContextBudget, type ContextEntry } from '../../generate/context-budget'
import { resolveGeminiVideoModel } from '../../generate/models'
import { buildTokenTelemetry } from '../../generate/token-telemetry'
import { generatePromptSeries, isGemini } from '../../prompt-generator-service'
import type { PromptGenerationRequest, SeriesResponse } from '../../prompt-generator-service'
import { resolveFileContext } from '../../file-context'
import { resolveSmartContextFiles } from '../../smart-context-service'
import { resolveUrlContext } from '../../url-context'
import type { UploadStateChange } from '../../prompt-generator-service'
import { MODEL_PROVIDER_LABELS } from '../../model-providers'
import { checkModelProviderStatus } from '../provider-status'
import type { TokenUsageStore } from '../token-usage-store'
import type { BudgetSettings } from '../budget-settings'
import type { NotifyOptions } from '../notifier'
import type { HistoryEntry, ProviderStatus, ResumeMode } from '../types'
export type UseGenerationPipelineOptions = {
  pushHistory: (
    content: string,
    kind?: HistoryEntry['kind'],
    format?: HistoryEntry['format'],
  ) => void
  notify?: (message: string, options?: NotifyOptions) => void
  files: string[]
  urls: string[]
  images: string[]
  videos: string[]
  pdfs: string[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  metaInstructions: string
  budgets: BudgetSettings
  currentModel: string
  targetModel?: string
  interactiveTransportPath?: string | undefined
  terminalColumns: number
  polishModelId: string | null
  jsonOutputEnabled: boolean
  copyEnabled: boolean
  chatGptEnabled: boolean
  isTestCommandRunning: boolean
  onProviderStatusUpdate?: (status: ProviderStatus) => void
  tokenUsageStore?: TokenUsageStore
  onReasoningUpdate?: (reasoning: string | null) => void
  onLastGeneratedPromptUpdate?: (prompt: string) => void
}
export const useGenerationPipeline = ({
  pushHistory,
  notify,
  files,
  urls,
  images,
  videos,
  pdfs,
  smartContextEnabled,
  smartContextRoot,
  metaInstructions,
  budgets,
  currentModel,
  targetModel,
  interactiveTransportPath,
  terminalColumns,
  polishModelId,
  jsonOutputEnabled,
  copyEnabled,
  chatGptEnabled,
  isTestCommandRunning,
  onProviderStatusUpdate,
  tokenUsageStore,
  onReasoningUpdate,
  onLastGeneratedPromptUpdate,
}: UseGenerationPipelineOptions) => {
  const [pipelineState, dispatch] = useReducer(
    generationPipelineReducer,
    INITIAL_GENERATION_PIPELINE_STATE,
  )
  const {
    isGenerating,
    statusMessage,
    isAwaitingRefinement,
    awaitingInteractiveMode,
    latestTelemetry,
  } = pipelineState
  const normalizedMetaInstructions = metaInstructions.trim()
  // “Stale closure” explanation (plain-English):
  // React callbacks capture the variables that were in scope when they were created.
  // If we keep a callback stable (so we don’t recreate it every render), it would
  // otherwise keep using old values.
  //
  // Example: if `handleStreamEvent` closed over an old `terminalColumns`, it would
  // keep wrapping text to the wrong width after the terminal is resized.
  //
  // Solution used here: keep the callback stable, but read changing values from refs
  // (kept fresh via useLatestRef).
  const pushHistoryRef = useLatestRef(pushHistory)
  const tokenUsageStoreRef = useLatestRef(tokenUsageStore)
  const terminalColumnsRef = useLatestRef(terminalColumns)
  const interactiveTransportPathRef = useLatestRef(interactiveTransportPath)
  const notifyRef = useLatestRef(notify)
  const activeRunIdRef = useRef<string | null>(null)
  const lastGeneratedPromptUpdateRef = useLatestRef<((prompt: string) => void) | null>(
    onLastGeneratedPromptUpdate ?? null,
  )
  type PendingRefinement = {
    requestId: number
    resolveText: (text: string) => void
  }
  const pendingRefinementRef = useRef<PendingRefinement | null>(null)
  const refinementRequestIdRef = useRef(0)
  const isGeneratingRef = useLatestRef(isGenerating)
  const transportAwaitingHintShownRef = useRef(false)
  const setAwaitingInteractiveMode = useCallback(
    (nextMode: InteractiveAwaitingMode | null, nextStatusMessage?: string): void => {
      dispatch({
        type: 'set-awaiting-interactive',
        awaitingInteractiveMode: nextMode,
        ...(nextStatusMessage ? { statusMessage: nextStatusMessage } : {}),
      })
    },
    [],
  )
  const setAwaitingRefinement = useCallback((next: boolean): void => {
    dispatch({ type: 'set-awaiting-refinement', isAwaitingRefinement: next })
  }, [])
  const setLatestTelemetry = useCallback(
    (telemetry: GeneratePipelineResult['telemetry'] | null) => {
      dispatch({ type: 'set-telemetry', telemetry })
    },
    [],
  )
  const setStatusMessage = useCallback((message: string): void => {
    dispatch({ type: 'set-status', statusMessage: message })
  }, [])
  const submitRefinement = useCallback(
    (text: string): void => {
      const pending = pendingRefinementRef.current
      if (!pending) {
        return
      }
      pendingRefinementRef.current = null
      setAwaitingRefinement(false)
      pending.resolveText(text)
    },
    [setAwaitingRefinement],
  )
  useEffect(() => {
    if (isGenerating) {
      return
    }
    // Cleanup is important:
    // - Without it, a pending refinement promise could keep the UI in a “waiting” state.
    // - It also prevents “runaway updates” after generation stops.
    submitRefinement('')
    setAwaitingRefinement(false)
    setAwaitingInteractiveMode(null)
    transportAwaitingHintShownRef.current = false
  }, [isGenerating, setAwaitingInteractiveMode, setAwaitingRefinement, submitRefinement])
  useEffect(() => {
    return () => {
      submitRefinement('')
    }
  }, [submitRefinement])
  const bufferedHistory = useMemo(
    () =>
      createBufferedHistoryWriter({
        push: (content, kind, format) => {
          pushHistoryRef.current(content, kind, format)
        },
      }),
    [pushHistoryRef],
  )
  useEffect(() => {
    if (!isGenerating) {
      bufferedHistory.flush()
    }
  }, [bufferedHistory, isGenerating])
  useEffect(() => {
    return () => {
      bufferedHistory.flush()
    }
  }, [bufferedHistory])
  const handleStreamEvent = useCallback(
    (event: StreamEventInput) => {
      const tokenUsageStoreLatest = tokenUsageStoreRef.current
      const wrapWidth = getHistoryWrapWidth(terminalColumnsRef.current)
      switch (event.event) {
        case 'progress.update': {
          const scope = event.scope ? `[${event.scope}] ` : ''
          const message = `${scope}${event.label} (${event.state})`
          bufferedHistory.pushBuffered(message, 'progress')
          setStatusMessage(message)
          return
        }
        case 'upload.state': {
          const action = event.state === 'start' ? 'Uploading' : 'Uploaded'
          const message = `${action} ${event.detail.kind}: ${event.detail.filePath}`
          const notifyLatest = notifyRef.current
          if (notifyLatest) {
            notifyLatest(message, {
              kind: 'progress',
              autoDismissMs: event.state === 'start' ? 6000 : 2600,
            })
            return
          }
          bufferedHistory.pushBuffered(message, 'progress')
          return
        }
        case 'generation.iteration.start':
          bufferedHistory.pushBuffered(`Iteration ${event.iteration} started`, 'progress')
          return
        case 'generation.iteration.complete': {
          const reasoningTokens = event.reasoningTokens ?? 0
          const activeRunId = activeRunIdRef.current
          if (activeRunId && tokenUsageStoreLatest) {
            tokenUsageStoreLatest.recordIteration(activeRunId, {
              iteration: event.iteration,
              promptTokens: event.tokens,
              reasoningTokens,
            })
          }
          bufferedHistory.pushManyBuffered(
            buildIterationCompleteHistoryMessages({
              iteration: event.iteration,
              tokens: event.tokens,
              ...(event.reasoningTokens !== undefined
                ? { reasoningTokens: event.reasoningTokens }
                : {}),
              prompt: event.prompt,
              wrapWidth,
            }),
          )
          return
        }
        case 'resume.loaded': {
          dispatch({ type: 'set-resume-loaded', details: event })
          const reusedCount = event.reusedContextPaths.length
          const missingCount = event.missingContextPaths.length
          const message = `Resume loaded (${event.source}) · reused ${reusedCount} · missing ${missingCount}`
          bufferedHistory.pushBuffered(message, 'progress')
          const notifyLatest = notifyRef.current
          if (notifyLatest) {
            notifyLatest(message, { kind: missingCount > 0 ? 'warning' : 'info' })
          }
          return
        }
        case 'context.overflow': {
          dispatch({ type: 'set-context-overflow', details: event })
          const droppedCount = event.droppedPaths.length
          const message = `Context overflow (${event.strategy}) · dropped ${droppedCount}`
          bufferedHistory.pushBuffered(message, 'system')
          const notifyLatest = notifyRef.current
          if (notifyLatest) {
            notifyLatest(message, { kind: 'warning' })
          }
          return
        }
        case 'context.telemetry': {
          const telemetry = event.telemetry
          setLatestTelemetry(telemetry)
          const activeRunId = activeRunIdRef.current
          if (activeRunId && tokenUsageStoreLatest) {
            tokenUsageStoreLatest.recordTelemetry(activeRunId, telemetry)
          }
          bufferedHistory.pushBuffered(
            `Telemetry · total ${telemetry.totalTokens} · intent ${telemetry.intentTokens} · files ${telemetry.fileTokens} · system ${telemetry.systemTokens}`,
            'progress',
          )
          return
        }
        case 'generation.final':
          setAwaitingInteractiveMode(null)
          bufferedHistory.pushBuffered('Generation stream finalized.', 'progress')
          return
        case 'transport.listening':
          bufferedHistory.pushBuffered(`Transport listening on ${event.path}`, 'progress')
          return
        case 'transport.client.connected':
          bufferedHistory.pushBuffered('Transport client connected.', 'progress')
          return
        case 'transport.client.disconnected':
          bufferedHistory.pushBuffered('Transport client disconnected.', 'progress')
          return
        case 'interactive.awaiting': {
          const normalizedMode =
            event.mode === 'transport' || event.mode === 'tty' ? event.mode : null
          const waitingMessage =
            normalizedMode === 'transport'
              ? 'Waiting for interactive transport input…'
              : 'Waiting for interactive input…'
          // One dispatch updates both mode + message.
          setAwaitingInteractiveMode(normalizedMode, waitingMessage)
          bufferedHistory.pushBuffered(waitingMessage, 'progress')
          const transportPath = interactiveTransportPathRef.current
          if (
            normalizedMode === 'transport' &&
            transportPath &&
            !transportAwaitingHintShownRef.current
          ) {
            bufferedHistory.pushBuffered(
              'Tip: connect a client and send refine/finish to continue.',
              'system',
            )
            transportAwaitingHintShownRef.current = true
          }
          return
        }
        case 'interactive.state': {
          const message = `Interactive ${event.phase}`
          setAwaitingInteractiveMode(null, message)
          bufferedHistory.pushBuffered(
            `Interactive ${event.phase} (iteration ${event.iteration})`,
            'progress',
          )
          return
        }
        default:
          return
      }
    },
    [
      bufferedHistory,
      interactiveTransportPathRef,
      notifyRef,
      terminalColumnsRef,
      tokenUsageStoreRef,
      setAwaitingInteractiveMode,
      setLatestTelemetry,
      setStatusMessage,
    ],
  )
  const interactiveDelegate: InteractiveDelegate = useMemo(
    () => ({
      getNextAction: async ({ iteration }) => {
        if (!isGeneratingRef.current) {
          return { type: 'finish' }
        }
        refinementRequestIdRef.current += 1
        const requestId = refinementRequestIdRef.current
        if (pendingRefinementRef.current) {
          submitRefinement('')
        }
        setAwaitingRefinement(true)
        pushHistoryRef.current(
          `Refine the prompt above (iteration ${iteration}): describe changes or press Enter on empty line to finish.`,
          'system',
        )
        try {
          return await new Promise<{ type: 'refine'; instruction: string } | { type: 'finish' }>(
            (resolve) => {
              pendingRefinementRef.current = {
                requestId,
                resolveText: (submittedText: string) => {
                  const trimmed = submittedText.trim()
                  if (!isGeneratingRef.current) {
                    resolve({ type: 'finish' })
                    return
                  }
                  if (!trimmed) {
                    pushHistoryRef.current('Interactive refinement complete.', 'system')
                    resolve({ type: 'finish' })
                    return
                  }
                  pushHistoryRef.current(`> [refine] ${trimmed}`, 'user')
                  resolve({ type: 'refine', instruction: trimmed })
                },
              }
            },
          )
        } finally {
          if (pendingRefinementRef.current?.requestId === requestId) {
            pendingRefinementRef.current = null
          }
          if (refinementRequestIdRef.current === requestId) {
            setAwaitingRefinement(false)
          }
        }
      },
    }),
    [isGeneratingRef, pushHistoryRef, submitRefinement, setAwaitingRefinement],
  )
  const onProviderStatusUpdateRef = useLatestRef(onProviderStatusUpdate)
  const ensureProviderReady = useCallback(
    async (modelId: string): Promise<boolean> => {
      try {
        const status = await checkModelProviderStatus(modelId)
        onProviderStatusUpdateRef.current?.(status)
        if (status.status === 'ok') {
          return true
        }
        const providerLabel = MODEL_PROVIDER_LABELS[status.provider]
        pushHistoryRef.current(
          `Generation aborted: ${providerLabel} unavailable (${status.message}).`,
          'system',
        )
        return false
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown provider check error.'
        pushHistoryRef.current(`Generation aborted: provider check failed (${message}).`, 'system')
        return false
      }
    },
    [onProviderStatusUpdateRef, pushHistoryRef],
  )
  const runGeneration = useCallback(
    async (intentInput: {
      intent?: string
      intentFile?: string
      resume?:
        | { kind: 'history'; selector: string; mode: ResumeMode }
        | { kind: 'file'; payloadPath: string; mode: ResumeMode }
    }) => {
      const trimmedIntent = intentInput.intent?.trim() ?? ''
      const trimmedIntentFile = intentInput.intentFile?.trim() ?? ''
      const resume = intentInput.resume
      if (!trimmedIntent && !trimmedIntentFile && !resume) {
        pushHistoryRef.current('No intent provided. Enter text or set an intent file.', 'system')
        return
      }
      const normalizedModel = currentModel.trim() || 'gpt-4o-mini'
      const generationModel =
        videos.length > 0 && !isGemini(normalizedModel)
          ? await resolveGeminiVideoModel()
          : normalizedModel
      if (generationModel !== normalizedModel) {
        pushHistoryRef.current(
          `Switching to ${generationModel} to support video input.`,
          'progress',
        )
      }
      const normalizedTargetModel = (targetModel ?? '').trim() || generationModel
      const normalizedPolishModel = (polishModelId ?? '').trim()
      const polishEnabled = normalizedPolishModel.length > 0
      const providerReady = await ensureProviderReady(generationModel)
      if (!providerReady) {
        return
      }
      if (polishEnabled && normalizedPolishModel !== generationModel) {
        const polishProviderReady = await ensureProviderReady(normalizedPolishModel)
        if (!polishProviderReady) {
          return
        }
      }
      activeRunIdRef.current = tokenUsageStoreRef.current
        ? tokenUsageStoreRef.current.startRun(generationModel)
        : null
      setLatestTelemetry(null)
      onReasoningUpdate?.(null)
      dispatch({ type: 'generation-start', statusMessage: 'Preparing generation…' })
      transportAwaitingHintShownRef.current = false
      pushHistoryRef.current('Starting generation…')
      let stopStatusMessage: string | undefined
      try {
        const transportPath = interactiveTransportPathRef.current
        const usesTransportInteractive = Boolean(transportPath)
        const usesTuiInteractiveDelegate = !usesTransportInteractive && !jsonOutputEnabled
        const shouldIgnoreContextForResume = Boolean(resume)
        const args: GenerateArgs = {
          interactive: usesTransportInteractive || usesTuiInteractiveDelegate,
          copy: false,
          openChatGpt: false,
          polish: polishEnabled,
          json: jsonOutputEnabled,
          quiet: true,
          progress: false,
          stream: 'none',
          showContext: false,
          contextFormat: 'text',
          help: false,
          context: shouldIgnoreContextForResume ? [] : [...files],
          urls: shouldIgnoreContextForResume ? [] : [...urls],
          images: [...images],
          video: [...videos],
          pdf: [...pdfs],
          smartContext: shouldIgnoreContextForResume ? false : smartContextEnabled,
          model: generationModel,
          target: normalizedTargetModel,
          ...(budgets.maxInputTokens !== null ? { maxInputTokens: budgets.maxInputTokens } : {}),
          ...(budgets.maxContextTokens !== null
            ? { maxContextTokens: budgets.maxContextTokens }
            : {}),
          ...(budgets.contextOverflowStrategy !== null
            ? { contextOverflow: budgets.contextOverflowStrategy }
            : {}),
        }
        if (normalizedMetaInstructions) {
          args.metaInstructions = normalizedMetaInstructions
        }
        if (trimmedIntentFile) {
          args.intentFile = trimmedIntentFile
        } else if (trimmedIntent) {
          args.intent = trimmedIntent
        }
        if (resume) {
          args.resumeMode = resume.mode
          if (resume.kind === 'history') {
            args.resume = resume.selector
          } else {
            args.resumeFrom = resume.payloadPath
          }
        }
        if (polishEnabled) {
          args.polishModel = normalizedPolishModel
        }
        if (!shouldIgnoreContextForResume && smartContextEnabled && smartContextRoot) {
          args.smartContextRoot = smartContextRoot
        }
        if (transportPath) {
          args.interactiveTransport = transportPath
        }
        const options: GeneratePipelineOptions = {
          onStreamEvent: handleStreamEvent,
          ...(usesTuiInteractiveDelegate ? { interactiveDelegate } : {}),
        }
        const result: GeneratePipelineResult = await runGeneratePipeline(args, options)
        onReasoningUpdate?.(result.reasoning ?? null)
        setStatusMessage('Finalizing prompt…')
        const wrapWidth = getHistoryWrapWidth(terminalColumnsRef.current)
        const iterationLabel = result.iterations ? ` · ${result.iterations} iterations` : ''
        pushHistoryRef.current(`Final prompt (${result.model}${iterationLabel}):`, 'system')
        wrapTextForHistory(result.finalPrompt, wrapWidth).forEach((line) => {
          pushHistoryRef.current(line, 'system', 'markdown')
        })
        lastGeneratedPromptUpdateRef.current?.(result.finalPrompt)
        if (result.telemetry) {
          setLatestTelemetry(result.telemetry)
          const activeRunId = activeRunIdRef.current
          if (activeRunId && tokenUsageStoreRef.current) {
            tokenUsageStoreRef.current.recordTelemetry(activeRunId, result.telemetry)
          }
          pushHistoryRef.current(
            `Telemetry · total ${result.telemetry.totalTokens} · intent ${result.telemetry.intentTokens} · files ${result.telemetry.fileTokens} · system ${result.telemetry.systemTokens}`,
            'system',
          )
        }
        if (jsonOutputEnabled) {
          const wrapWidth = getHistoryWrapWidth(terminalColumnsRef.current)
          bufferedHistory.pushManyBuffered(
            buildJsonPayloadHistoryMessages(result.payload, wrapWidth),
          )
        }
        if (copyEnabled) {
          await maybeCopyToClipboard(true, result.finalPrompt, false)
          pushHistoryRef.current('Copied prompt to clipboard.', 'system')
        }
        if (chatGptEnabled) {
          await maybeOpenChatGpt(true, result.finalPrompt, false)
          pushHistoryRef.current('Opened ChatGPT with generated prompt.', 'system')
        }
        stopStatusMessage = 'Complete'
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown generation error.'
        pushHistoryRef.current(`Generation failed: ${message}`)
        onReasoningUpdate?.(null)
        stopStatusMessage = 'Failed'
      } finally {
        submitRefinement('')
        dispatch({
          type: 'generation-stop',
          ...(stopStatusMessage ? { statusMessage: stopStatusMessage } : {}),
        })
      }
    },
    [
      chatGptEnabled,
      copyEnabled,
      currentModel,
      targetModel,
      files,
      urls,
      images,
      videos,
      polishModelId,
      jsonOutputEnabled,
      smartContextEnabled,
      budgets,
      smartContextRoot,
      normalizedMetaInstructions,
      interactiveTransportPathRef,
      pushHistoryRef,
      bufferedHistory,
      terminalColumnsRef,
      tokenUsageStoreRef,
      handleStreamEvent,
      interactiveDelegate,
      submitRefinement,
      ensureProviderReady,
      lastGeneratedPromptUpdateRef,
      onReasoningUpdate,
      setLatestTelemetry,
      setStatusMessage,
    ],
  )
  const runSeriesGeneration = useCallback(
    async (intent: string) => {
      let generationModel = currentModel.trim() || 'gpt-4o-mini'
      if (videos.length > 0 && !isGemini(generationModel)) {
        generationModel = await resolveGeminiVideoModel()
        pushHistoryRef.current(
          `[series] Switching to ${generationModel} to support video input.`,
          'progress',
        )
      }
      const runtimeTargetModel = (targetModel ?? '').trim() || generationModel
      const providerReady = await ensureProviderReady(generationModel)
      if (!providerReady) {
        return
      }
      dispatch({ type: 'generation-start', statusMessage: 'Series: resolving context…' })
      pushHistoryRef.current('[series] Starting series generation…', 'progress')
      const prepareDirResult = await prepareSeriesOutputDir(intent)
      const seriesDir = prepareDirResult.seriesDir
      let canWriteFiles = prepareDirResult.canWriteFiles
      if (!canWriteFiles) {
        const message = prepareDirResult.errorMessage ?? 'Unknown filesystem error.'
        pushHistoryRef.current(
          `[series] Failed to prepare output directory: ${message}`,
          'progress',
        )
      }
      try {
        let contextEntries: ContextEntry[] = (
          await resolveFileContext(Array.from(files) as string[])
        ).map((entry) => ({ ...entry, source: 'file' }))
        if (contextEntries.length > 0) {
          pushHistoryRef.current(
            `[series] Added ${contextEntries.length} file context entr${contextEntries.length === 1 ? 'y' : 'ies'}.`,
            'progress',
          )
        }
        if (urls.length > 0) {
          pushHistoryRef.current(`[series] Fetching ${urls.length} URL source(s)…`, 'progress')
          try {
            const urlFiles = await resolveUrlContext(urls, {
              onProgress: (message: string) => {
                pushHistoryRef.current(`[series] ${message}`, 'progress')
                setStatusMessage(`Series: ${message}`)
              },
            })
            if (urlFiles.length > 0) {
              contextEntries = [
                ...contextEntries,
                ...urlFiles.map((entry) => ({ ...entry, source: 'url' as const })),
              ]
            }
          } catch (error) {
            const message = error instanceof Error ? error.message : 'Unknown URL context error.'
            pushHistoryRef.current(`[series] URL context failed: ${message}`, 'progress')
          }
        }
        if (smartContextEnabled) {
          if (notify) {
            notify('Smart context: resolving…', { kind: 'progress' })
          } else {
            pushHistoryRef.current('[series] Resolving smart context…', 'progress')
          }
          try {
            const smartFiles = await resolveSmartContextFiles(
              intent,
              contextEntries.map(({ path, content }) => ({ path, content })),
              (message: string) => {
                if (notify) {
                  notify(`Smart context: ${message}`, { kind: 'progress' })
                } else {
                  pushHistoryRef.current(`[series] ${message}`, 'progress')
                }
                setStatusMessage(`Series: ${message}`)
              },
              smartContextRoot ?? undefined,
            )
            if (smartFiles.length > 0) {
              contextEntries = [
                ...contextEntries,
                ...smartFiles.map((entry) => ({ ...entry, source: 'smart' as const })),
              ]
            }
          } catch (error) {
            const message = error instanceof Error ? error.message : 'Unknown smart context error.'
            pushHistoryRef.current(`[series] Smart context failed: ${message}`, 'progress')
          }
        }
        const budgetEvaluation = evaluateContextBudget({
          intentText: intent,
          metaInstructions: normalizedMetaInstructions,
          contextEntries,
          ...(budgets.maxInputTokens !== null ? { maxInputTokens: budgets.maxInputTokens } : {}),
          ...(budgets.maxContextTokens !== null
            ? { maxContextTokens: budgets.maxContextTokens }
            : {}),
          ...(budgets.contextOverflowStrategy !== null
            ? { strategy: budgets.contextOverflowStrategy }
            : {}),
          buildTelemetry: (intentText, files, metaInstructions) =>
            buildTokenTelemetry(intentText, files, metaInstructions),
        })
        setLatestTelemetry(budgetEvaluation.after)
        if (budgetEvaluation.droppedEntries.length > 0) {
          contextEntries = budgetEvaluation.keptEntries
          const droppedCount = budgetEvaluation.droppedEntries.length
          dispatch({
            type: 'set-context-overflow',
            details: {
              event: 'context.overflow',
              strategy: budgetEvaluation.strategy ?? 'fail',
              before: budgetEvaluation.before,
              after: budgetEvaluation.after,
              droppedPaths: budgetEvaluation.droppedPaths,
            },
          })
          pushHistoryRef.current(
            `[series] Context overflow (${budgetEvaluation.strategy ?? 'fail'}) · dropped ${droppedCount}`,
            'system',
          )
        }
        pushHistoryRef.current(
          `[series] Context ready (${contextEntries.length} file(s)).`,
          'progress',
        )
        const handleUploadState: UploadStateChange = (state, detail) => {
          const action = state === 'start' ? 'Uploading' : 'Uploaded'
          const message = `${action} ${detail.kind}: ${detail.filePath}`
          if (notify) {
            notify(message, { kind: 'progress', autoDismissMs: state === 'start' ? 6000 : 2600 })
            return
          }
          pushHistoryRef.current(`[series] ${message}`, 'progress')
        }
        const request: PromptGenerationRequest = {
          intent,
          model: generationModel,
          targetModel: runtimeTargetModel,
          fileContext: contextEntries.map(({ path, content }) => ({ path, content })),
          images: [...images],
          videos: [...videos],
          pdfs: [...pdfs],
          onUploadStateChange: handleUploadState,
          onSeriesRepairAttempt: ({ attempt, maxAttempts, validationError }) => {
            const normalizedError = validationError.replace(/\s+/g, ' ').trim()
            const shortError =
              normalizedError.length > 140 ? `${normalizedError.slice(0, 137)}…` : normalizedError
            pushHistoryRef.current(
              `[series] Validation failed; attempting automatic repair (${attempt}/${maxAttempts})… Reason: ${shortError}`,
              'progress',
            )
            if (process.env.DEBUG || process.env.VERBOSE) {
              pushHistoryRef.current(
                `[series][debug] Full validation error: ${normalizedError}`,
                'progress',
              )
            }
          },
          ...(normalizedMetaInstructions ? { metaInstructions: normalizedMetaInstructions } : {}),
        }
        setStatusMessage('Series: generating…')
        const series: SeriesResponse = await generatePromptSeries(request)
        const totalPrompts = 1 + series.atomicPrompts.length
        let writeResult: WriteSeriesArtifactsResult | null = null
        if (canWriteFiles) {
          try {
            writeResult = await writeSeriesArtifacts(seriesDir, series)
            writeResult.errors.forEach((entry) => {
              pushHistoryRef.current(
                `[series] Failed to write ${entry.fileName}: ${entry.message}`,
                'progress',
              )
            })
          } catch (error) {
            canWriteFiles = false
            const message = error instanceof Error ? error.message : 'Unknown filesystem error.'
            pushHistoryRef.current(
              `[series] Failed to write series artifacts: ${message}`,
              'progress',
            )
          }
        }
        pushHistoryRef.current('[series] Overview ready.', 'progress')
        series.atomicPrompts.forEach((step, index) => {
          const stepNumber = index + 1
          const validationSection = extractValidationSection(step.content)
          if (validationSection) {
            pushHistoryRef.current(
              `[Step ${stepNumber}: ${step.title}] Validation section:\n${validationSection}`,
              'system',
            )
            return
          }
          pushHistoryRef.current(
            `[Step ${stepNumber}: ${step.title}] (no Validation section found)`,
            'system',
          )
        })
        if (canWriteFiles) {
          const relativeDir = path.relative(process.cwd(), seriesDir) || seriesDir
          const writtenCount = writeResult?.writtenCount ?? 0
          pushHistoryRef.current(
            `[Series] Saved ${writtenCount}/${totalPrompts} prompts to ${relativeDir}`,
            'system',
          )
        } else {
          pushHistoryRef.current(`[Series] Generated ${totalPrompts} prompts (not saved)`, 'system')
        }
        dispatch({ type: 'generation-stop', statusMessage: 'Series complete' })
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown series generation error.'
        pushHistoryRef.current(`[series] Failed: ${message}`, 'progress')
        dispatch({ type: 'generation-stop', statusMessage: 'Series failed' })
      } finally {
        dispatch({ type: 'generation-stop' })
      }
    },
    [
      currentModel,
      targetModel,
      files,
      urls,
      images,
      videos,
      smartContextEnabled,
      smartContextRoot,
      normalizedMetaInstructions,
      pushHistoryRef,
      notify,
      ensureProviderReady,
      setStatusMessage,
    ],
  )
  const statusChips = useMemo(() => {
    const effectiveStatusMessage = isGenerating
      ? statusMessage
      : isTestCommandRunning
        ? 'Running tests'
        : statusMessage
    const statusChip = `[status:${effectiveStatusMessage}]`
    const normalizedTarget = (targetModel ?? '').trim() || currentModel
    const chips = [statusChip, `[${currentModel}]`, `[target:${normalizedTarget}]`]
    if (latestTelemetry) {
      chips.push(`[tokens:${formatCompactTokens(latestTelemetry.totalTokens)}]`)
    }
    const normalizedPolishModel = (polishModelId ?? '').trim()
    if (normalizedPolishModel) {
      chips.push(`[polish:${normalizedPolishModel}]`)
    }
    chips.push(`[copy:${copyEnabled ? 'on' : 'off'}]`)
    chips.push(`[chatgpt:${chatGptEnabled ? 'on' : 'off'}]`)
    chips.push(`[json:${jsonOutputEnabled ? 'on' : 'off'}]`)
    chips.push(`[files:${files.length}]`)
    chips.push(`[urls:${urls.length}]`)
    chips.push(`[smart:${smartContextEnabled ? 'on' : 'off'}]`)
    chips.push(`[tests:${isTestCommandRunning ? 'running' : 'idle'}]`)
    if (smartContextRoot) {
      chips.push(`[root:${smartContextRoot}]`)
    }
    return chips
  }, [
    isGenerating,
    statusMessage,
    currentModel,
    targetModel,
    latestTelemetry,
    polishModelId,
    copyEnabled,
    chatGptEnabled,
    jsonOutputEnabled,
    files.length,
    urls.length,
    smartContextEnabled,
    smartContextRoot,
    isTestCommandRunning,
  ])
  return {
    isGenerating,
    statusMessage,
    runGeneration,
    runSeriesGeneration,
    statusChips,
    latestContextOverflow: pipelineState.latestContextOverflow,
    isAwaitingRefinement,
    submitRefinement,
    awaitingInteractiveMode,
  }
}
</file>

<file path="src/tui/hooks/usePopupManager.ts">
import fs from 'node:fs/promises'
import path from 'node:path'
import { useCallback, useMemo, useReducer, useRef } from 'react'
import {
  JSON_INTERACTIVE_ERROR,
  mapPopupCommandSelection,
  type PopupManagerCommandStep,
} from './popup-manager/command-mapping'
import { createPopupScanOrchestrator } from './popup-manager/scan-orchestrator'
import {
  INITIAL_POPUP_MANAGER_STATE,
  popupReducer,
  type PopupAction,
  type SetStateAction,
} from '../popup-reducer'
import {
  updateCliExportSettings,
  updateCliPromptGeneratorSettings,
  updateCliResumeSettings,
} from '../../config'
import { TOGGLE_LABELS } from '../config'
import { parseBudgetSettingsDraft } from '../budget-settings'
import {
  scanFileSuggestions,
  scanImageSuggestions,
  scanIntentSuggestions,
  scanPdfSuggestions,
  scanSmartSuggestions,
  scanVideoSuggestions,
} from './popup-scans'
import type { NotifyOptions } from '../notifier'
import type { ThemeMode } from '../theme/theme-types'
import { loadGeneratePayloadFromFile } from '../../generate/payload-io'
import { GENERATE_JSON_PAYLOAD_SCHEMA_VERSION } from '../../generate/types'
import { writeGeneratePayloadExport } from '../../export/export-generate-payload'
import {
  loadGenerateHistoryPickerItems,
  loadGeneratePayloadFromHistory,
} from '../../history/generate-history'
import { buildModelPopupOptions } from '../model-popup-options'
import { loadResumeHistoryItems } from '../resume-history'
import { getRecentSessionModels, recordRecentSessionModel } from '../model-session'
import type {
  CommandDescriptor,
  HistoryEntry,
  ModelOption,
  PopupState,
  ResumeMode,
  ResumeSourceKind,
  ToggleField,
} from '../types'
export type PopupManagerActions = {
  openModelPopup: () => void
  openPolishModelPopup: () => void
  openTargetModelPopup: () => void
  openTogglePopup: (field: ToggleField) => void
  openFilePopup: () => void
  openUrlPopup: () => void
  openImagePopup: () => void
  openVideoPopup: () => void
  openPdfPopup: () => void
  openHistoryPopup: () => void
  openResumePopup: () => void
  openExportPopup: () => void
  openSmartRootPopup: () => void
  openTokensPopup: () => void
  openBudgetsPopup: () => void
  openSettingsPopup: () => void
  openThemePopup: () => void
  openThemeModePopup: () => void
  openReasoningPopup: () => void
  openTestPopup: () => void
  openIntentPopup: () => void
  openInstructionsPopup: () => void
  openSeriesPopup: (initialDraft?: string, hintOverride?: string) => void
  closePopup: () => void
  handleCommandSelection: (commandId: CommandDescriptor['id'], argsRaw?: string) => void
  handleModelPopupSubmit: (option: ModelOption | null | undefined) => void
  applyToggleSelection: (field: ToggleField, value: boolean) => void
  handleIntentFileSubmit: (value: string) => void
  handleInstructionsSubmit: (value: string) => void
  handleBudgetsSubmit: () => void
  handleResumeSubmit: () => void
  handleExportSubmit: () => void
  handleSeriesIntentSubmit: (value: string) => void
}
export type ThemeOption = {
  name: string
  label: string
}
export type UsePopupManagerOptions = {
  currentModel: ModelOption['id']
  currentTargetModel: ModelOption['id']
  modelOptions: readonly ModelOption[]
  activeThemeName: string
  themeMode: ThemeMode
  themes: readonly ThemeOption[]
  smartContextEnabled: boolean
  smartContextRoot: string | null
  toggleSmartContext: () => void
  setSmartRoot: (value: string) => void
  urls: string[]
  addUrl: (value: string) => void
  images: string[]
  videos: string[]
  pdfs: string[]
  addImage: (value: string) => void
  addVideo: (value: string) => void
  addPdf: (value: string) => void
  lastTestFile: string | null
  defaultTestFile: string
  interactiveTransportPath?: string | undefined
  isGenerating: boolean
  lastUserIntentRef: React.MutableRefObject<string | null>
  pushHistory: (content: string, kind?: HistoryEntry['kind']) => void
  notify: (message: string, options?: NotifyOptions) => void
  setInputValue: (value: string) => void
  runGeneration: (payload: {
    intent?: string
    intentFile?: string
    resume?:
      | { kind: 'history'; selector: string; mode: ResumeMode }
      | { kind: 'file'; payloadPath: string; mode: ResumeMode }
  }) => Promise<void>
  runSeriesGeneration: (intent: string) => void
  runTestsFromCommand: (value: string) => void
  clearScreen?: () => void
  exitApp: () => void
  setCurrentModel: (value: ModelOption['id']) => void
  setCurrentTargetModel: (value: ModelOption['id']) => void
  setPolishModelId: (value: ModelOption['id'] | null) => void
  setCopyEnabled: (value: boolean) => void
  setChatGptEnabled: (value: boolean) => void
  setJsonOutputEnabled: (value: boolean) => void
  setIntentFilePath: (value: string) => void
  intentFilePath: string
  metaInstructions: string
  setMetaInstructions: (value: string) => void
  budgets: {
    maxContextTokens: number | null
    maxInputTokens: number | null
    contextOverflowStrategy: import('../../config').ContextOverflowStrategy | null
  }
  setBudgets: (value: {
    maxContextTokens: number | null
    maxInputTokens: number | null
    contextOverflowStrategy: import('../../config').ContextOverflowStrategy | null
  }) => void
  polishModelId: ModelOption['id'] | null
  copyEnabled: boolean
  chatGptEnabled: boolean
  jsonOutputEnabled: boolean
  getLatestTypedIntent: () => string | null
  syncTypedIntentRef: (intent: string) => void
  resumeDefaults: {
    sourceKind: ResumeSourceKind
    mode: ResumeMode
  }
  setResumeDefaults: (value: { sourceKind: ResumeSourceKind; mode: ResumeMode }) => void
  exportDefaults: {
    format: 'json' | 'yaml'
    outDir: string | null
  }
  setExportDefaults: (value: { format: 'json' | 'yaml'; outDir: string | null }) => void
}
/*
 * Popup state management for the Ink TUI.
 *
 * This hook wires UI actions (open/close/submit) to a pure reducer:
 * `apps/prompt-maker-cli/src/tui/popup-reducer.ts`.
 *
 * Keeping the reducer in a separate module lets us unit test popup transitions
 * without a TTY and keeps this hook focused on effects (async scans, commands).
 */
const POPUP_SUGGESTION_SCAN_LIMIT = 5000
export const usePopupManager = ({
  currentModel,
  currentTargetModel,
  modelOptions,
  activeThemeName,
  themeMode,
  themes,
  smartContextEnabled,
  smartContextRoot,
  toggleSmartContext,
  setSmartRoot,
  urls,
  addUrl,
  images,
  videos,
  pdfs,
  addImage,
  addVideo,
  addPdf,
  lastTestFile,
  defaultTestFile,
  interactiveTransportPath,
  isGenerating,
  lastUserIntentRef,
  pushHistory,
  notify,
  setInputValue,
  runGeneration,
  runSeriesGeneration,
  runTestsFromCommand,
  clearScreen,
  exitApp,
  setCurrentModel,
  setCurrentTargetModel,
  setPolishModelId,
  setCopyEnabled,
  setChatGptEnabled,
  setJsonOutputEnabled,
  setIntentFilePath,
  intentFilePath,
  metaInstructions,
  setMetaInstructions,
  budgets,
  setBudgets,
  polishModelId,
  copyEnabled,
  chatGptEnabled,
  jsonOutputEnabled,
  getLatestTypedIntent,
  syncTypedIntentRef,
  resumeDefaults,
  setResumeDefaults,
  exportDefaults,
  setExportDefaults,
}: UsePopupManagerOptions): {
  popupState: PopupState
  setPopupState: React.Dispatch<React.SetStateAction<PopupState>>
  actions: PopupManagerActions
} => {
  const scanIdRef = useRef(0)
  const [popupManagerState, dispatch] = useReducer(popupReducer, INITIAL_POPUP_MANAGER_STATE)
  const popupState = popupManagerState.popupState
  const { runSuggestionScan } = useMemo(
    () => createPopupScanOrchestrator({ scanIdRef, dispatch, pushHistory }),
    [dispatch, pushHistory],
  )
  // Compatibility shim: keeps the existing `setPopupState(prev => ...)` call sites working.
  // Internally we treat it as a reducer action.
  const setPopupState = useCallback<React.Dispatch<SetStateAction<PopupState>>>((next) => {
    dispatch({ type: 'set', next } satisfies PopupAction)
  }, [])
  const closePopup = useCallback(() => {
    dispatch({ type: 'close' })
  }, [])
  const openModelPopup = useCallback(() => {
    const recentModelIds = getRecentSessionModels()
    const { options } = buildModelPopupOptions({ query: '', modelOptions, recentModelIds })
    const defaultIndex = Math.max(
      0,
      options.findIndex((option) => option.id === currentModel),
    )
    dispatch({ type: 'open-model', kind: 'generation', query: '', selectionIndex: defaultIndex })
  }, [currentModel, modelOptions])
  const openPolishModelPopup = useCallback(() => {
    const recentModelIds = getRecentSessionModels()
    const { options } = buildModelPopupOptions({ query: '', modelOptions, recentModelIds })
    const selectedId = polishModelId ?? currentModel
    const defaultIndex = Math.max(
      0,
      options.findIndex((option) => option.id === selectedId),
    )
    dispatch({ type: 'open-model', kind: 'polish', query: '', selectionIndex: defaultIndex })
  }, [currentModel, modelOptions, polishModelId])
  const openTargetModelPopup = useCallback(() => {
    const recentModelIds = getRecentSessionModels()
    const { options } = buildModelPopupOptions({ query: '', modelOptions, recentModelIds })
    const defaultIndex = Math.max(
      0,
      options.findIndex((option) => option.id === currentTargetModel),
    )
    dispatch({ type: 'open-model', kind: 'target', query: '', selectionIndex: defaultIndex })
  }, [currentTargetModel, modelOptions])
  const openTogglePopup = useCallback(
    (field: ToggleField) => {
      const currentValue =
        field === 'copy' ? copyEnabled : field === 'chatgpt' ? chatGptEnabled : jsonOutputEnabled
      dispatch({
        type: 'open-toggle',
        field,
        selectionIndex: currentValue ? 0 : 1,
      })
    },
    [copyEnabled, chatGptEnabled, jsonOutputEnabled],
  )
  const openFilePopup = useCallback(() => {
    runSuggestionScan({
      kind: 'file',
      open: (scanId) => ({ type: 'open-file', scanId }),
      scan: () => scanFileSuggestions({ cwd: process.cwd(), limit: POPUP_SUGGESTION_SCAN_LIMIT }),
    })
  }, [runSuggestionScan])
  const openUrlPopup = useCallback(() => {
    dispatch({ type: 'open-url' })
  }, [])
  const openImagePopup = useCallback(() => {
    runSuggestionScan({
      kind: 'image',
      open: (scanId) => ({ type: 'open-image', scanId }),
      scan: () => scanImageSuggestions({ cwd: process.cwd(), limit: POPUP_SUGGESTION_SCAN_LIMIT }),
    })
  }, [runSuggestionScan])
  const openVideoPopup = useCallback(() => {
    runSuggestionScan({
      kind: 'video',
      open: (scanId) => ({ type: 'open-video', scanId }),
      scan: () => scanVideoSuggestions({ cwd: process.cwd(), limit: POPUP_SUGGESTION_SCAN_LIMIT }),
    })
  }, [runSuggestionScan])
  const openPdfPopup = useCallback(() => {
    runSuggestionScan({
      kind: 'pdf',
      open: (scanId) => ({ type: 'open-pdf', scanId }),
      scan: () => scanPdfSuggestions({ cwd: process.cwd(), limit: POPUP_SUGGESTION_SCAN_LIMIT }),
    })
  }, [runSuggestionScan])
  const openHistoryPopup = useCallback(() => {
    dispatch({ type: 'open-history' })
  }, [])
  const openResumePopup = useCallback(() => {
    const resumeDefaultsSnapshot = resumeDefaults
    runSuggestionScan({
      kind: 'resume',
      open: (scanId) => ({
        type: 'open-resume',
        scanId,
        sourceKind: resumeDefaultsSnapshot.sourceKind,
        mode: resumeDefaultsSnapshot.mode,
        payloadPathDraft: '',
        historyItems: [],
        historySelectionIndex: 0,
        historyErrorMessage: null,
      }),
      scan: () => scanFileSuggestions({ cwd: process.cwd(), limit: POPUP_SUGGESTION_SCAN_LIMIT }),
    })
    const hydrate = async (): Promise<void> => {
      const historyResult = await loadResumeHistoryItems({ limit: 30 })
      setPopupState((prev) => {
        if (prev?.type !== 'resume') {
          return prev
        }
        return {
          ...prev,
          historyItems: historyResult.ok ? historyResult.items : [],
          historyErrorMessage: historyResult.ok ? null : historyResult.errorMessage,
        }
      })
    }
    void hydrate()
  }, [resumeDefaults, runSuggestionScan, setPopupState])
  const openExportPopup = useCallback(() => {
    const exportDefaultsSnapshot = exportDefaults
    const fileName = `prompt-export.${exportDefaultsSnapshot.format}`
    const outPathDraft = exportDefaultsSnapshot.outDir
      ? path.join(exportDefaultsSnapshot.outDir, fileName)
      : fileName
    dispatch({
      type: 'open-export',
      format: exportDefaultsSnapshot.format,
      outPathDraft,
      historyItems: [],
      historySelectionIndex: 0,
      historyErrorMessage: null,
    })
    const hydrate = async (): Promise<void> => {
      const historyResult = await loadGenerateHistoryPickerItems({ limit: 30 })
      setPopupState((prev) => {
        if (prev?.type !== 'export') {
          return prev
        }
        return {
          ...prev,
          historyItems: historyResult.ok ? historyResult.items : [],
          historyErrorMessage: historyResult.ok ? null : historyResult.errorMessage,
        }
      })
    }
    void hydrate()
  }, [exportDefaults, setPopupState])
  const openSmartRootPopup = useCallback(() => {
    const draft = smartContextRoot ?? ''
    runSuggestionScan({
      kind: 'smart',
      open: (scanId) => ({ type: 'open-smart', scanId, draft }),
      scan: () => scanSmartSuggestions({ cwd: process.cwd(), limit: POPUP_SUGGESTION_SCAN_LIMIT }),
    })
  }, [runSuggestionScan, smartContextRoot])
  const openTokensPopup = useCallback(() => {
    dispatch({ type: 'open-tokens' })
  }, [])
  const openBudgetsPopup = useCallback(() => {
    dispatch({
      type: 'open-budgets',
      maxContextTokens: budgets.maxContextTokens,
      maxInputTokens: budgets.maxInputTokens,
      contextOverflowStrategy: budgets.contextOverflowStrategy,
    })
  }, [budgets.contextOverflowStrategy, budgets.maxContextTokens, budgets.maxInputTokens])
  const openSettingsPopup = useCallback(() => {
    dispatch({ type: 'open-settings' })
  }, [])
  const openThemePopup = useCallback(() => {
    const selectionIndex = Math.max(
      0,
      themes.findIndex((theme) => theme.name === activeThemeName),
    )
    dispatch({ type: 'open-theme', selectionIndex, initialThemeName: activeThemeName })
  }, [activeThemeName, themes])
  const openThemeModePopup = useCallback(() => {
    const selectionIndex = themeMode === 'system' ? 0 : themeMode === 'dark' ? 1 : 2
    dispatch({ type: 'open-theme-mode', selectionIndex, initialMode: themeMode })
  }, [themeMode])
  const openReasoningPopup = useCallback(() => {
    dispatch({ type: 'open-reasoning', scrollOffset: 0 })
  }, [])
  const openTestPopup = useCallback(() => {
    dispatch({ type: 'open-test', draft: lastTestFile ?? defaultTestFile })
  }, [defaultTestFile, lastTestFile])
  const openIntentPopup = useCallback(() => {
    runSuggestionScan({
      kind: 'intent',
      open: (scanId) => ({ type: 'open-intent', scanId, draft: intentFilePath }),
      scan: () => scanIntentSuggestions({ cwd: process.cwd(), limit: POPUP_SUGGESTION_SCAN_LIMIT }),
    })
  }, [intentFilePath, runSuggestionScan])
  const openInstructionsPopup = useCallback(() => {
    dispatch({ type: 'open-instructions', draft: metaInstructions })
  }, [metaInstructions])
  const openSeriesPopup = useCallback(
    (initialDraft?: string, hintOverride?: string) => {
      const trimmedIntentFile = intentFilePath.trim()
      const defaultHint = trimmedIntentFile
        ? 'Draft prefills from typed/last intent; if empty, loads the intent file.'
        : 'Draft prefills from typed/last intent (or pass /series <intent>).'
      dispatch({
        type: 'open-series',
        draft: initialDraft ?? '',
        hint: hintOverride ?? defaultHint,
      })
    },
    [intentFilePath],
  )
  const applyModelSelection = useCallback(
    (option?: ModelOption) => {
      if (!option) {
        return
      }
      recordRecentSessionModel(option.id)
      setCurrentModel(option.id)
      notify(`Selected model: ${option.label} (${option.id})`, { kind: 'info' })
      setInputValue('')
      closePopup()
    },
    [closePopup, notify, setCurrentModel, setInputValue],
  )
  const applyTargetModelSelection = useCallback(
    (option?: ModelOption) => {
      if (!option) {
        return
      }
      recordRecentSessionModel(option.id)
      setCurrentTargetModel(option.id)
      notify(`Selected target model: ${option.label} (${option.id})`, { kind: 'info' })
      setInputValue('')
      closePopup()
    },
    [closePopup, notify, setCurrentTargetModel, setInputValue],
  )
  const applyPolishModelSelection = useCallback(
    (option: ModelOption | null | undefined) => {
      if (option === null) {
        setPolishModelId(null)
        notify('Polish disabled', { kind: 'warning' })
        setInputValue('')
        closePopup()
        return
      }
      if (!option) {
        return
      }
      recordRecentSessionModel(option.id)
      setPolishModelId(option.id)
      notify(`Selected polish model: ${option.label} (${option.id})`, { kind: 'info' })
      setInputValue('')
      closePopup()
    },
    [closePopup, notify, setInputValue, setPolishModelId],
  )
  const handleModelPopupSubmit = useCallback(
    (option: ModelOption | null | undefined) => {
      if (popupState?.type === 'model') {
        if (popupState.kind === 'target') {
          applyTargetModelSelection(option ?? undefined)
          return
        }
        if (popupState.kind === 'polish') {
          applyPolishModelSelection(option)
          return
        }
      }
      applyModelSelection(option ?? undefined)
    },
    [applyModelSelection, applyPolishModelSelection, applyTargetModelSelection, popupState],
  )
  const applyToggleSelection = useCallback(
    (field: ToggleField, value: boolean) => {
      // Guardrail: JSON output and interactive transport both want to “own” stdout.
      if (field === 'json' && value && interactiveTransportPath) {
        pushHistory(JSON_INTERACTIVE_ERROR, 'system')
        setInputValue('')
        closePopup()
        return
      }
      if (field === 'json') {
        setJsonOutputEnabled(value)
        notify(
          value
            ? 'JSON output is ON (payload shown in history)'
            : 'JSON output is OFF (payload hidden)',
          { kind: value ? 'info' : 'warning' },
        )
        setInputValue('')
        closePopup()
        return
      }
      const message = `${TOGGLE_LABELS[field]} ${value ? 'enabled' : 'disabled'}`
      if (field === 'copy') {
        setCopyEnabled(value)
      } else {
        setChatGptEnabled(value)
      }
      pushHistory(message)
      setInputValue('')
      closePopup()
    },
    [
      closePopup,
      interactiveTransportPath,
      notify,
      pushHistory,
      setChatGptEnabled,
      setCopyEnabled,
      setInputValue,
      setJsonOutputEnabled,
    ],
  )
  const handleIntentFileSubmit = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      setIntentFilePath(trimmed)
      pushHistory(
        trimmed ? `Intent file set to ${trimmed}` : 'Intent file cleared; using typed intent.',
      )
      setInputValue('')
      closePopup()
    },
    [closePopup, pushHistory, setInputValue, setIntentFilePath],
  )
  const handleInstructionsSubmit = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      setMetaInstructions(trimmed)
      pushHistory(trimmed ? `[instr] ${trimmed}` : '[instr] cleared')
      setInputValue('')
      closePopup()
    },
    [closePopup, pushHistory, setInputValue, setMetaInstructions],
  )
  const handleBudgetsSubmit = useCallback(() => {
    if (popupState?.type !== 'budgets') {
      return
    }
    const parsed = parseBudgetSettingsDraft({
      maxContextTokensDraft: popupState.maxContextTokensDraft,
      maxInputTokensDraft: popupState.maxInputTokensDraft,
      contextOverflowStrategyDraft: popupState.contextOverflowStrategyDraft,
    })
    if (!parsed.ok) {
      setPopupState((prev) =>
        prev?.type === 'budgets' ? { ...prev, errorMessage: parsed.errorMessage } : prev,
      )
      return
    }
    const persist = async (): Promise<void> => {
      try {
        await updateCliPromptGeneratorSettings({
          maxContextTokens: parsed.settings.maxContextTokens,
          maxInputTokens: parsed.settings.maxInputTokens,
          contextOverflowStrategy: parsed.settings.contextOverflowStrategy,
        })
        setBudgets(parsed.settings)
        const enabled =
          parsed.settings.maxContextTokens !== null || parsed.settings.maxInputTokens !== null
        const summary = enabled
          ? `Budgets saved · input=${parsed.settings.maxInputTokens ?? 'unset'} · context=${parsed.settings.maxContextTokens ?? 'unset'} · overflow=${parsed.settings.contextOverflowStrategy ?? 'fail'}`
          : 'Budgets cleared'
        pushHistory(`[budgets] ${summary}`, 'system')
        notify(summary, { kind: enabled ? 'info' : 'warning' })
        setInputValue('')
        closePopup()
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown config write error.'
        setPopupState((prev) =>
          prev?.type === 'budgets'
            ? { ...prev, errorMessage: `Failed to save budgets: ${message}` }
            : prev,
        )
        notify(`Failed to save budgets: ${message}`, { kind: 'error' })
      }
    }
    void persist()
  }, [closePopup, notify, popupState, pushHistory, setBudgets, setInputValue, setPopupState])
  const handleResumeSubmit = useCallback(() => {
    if (popupState?.type !== 'resume') {
      return
    }
    const sourceKind = popupState.sourceKind
    const mode = popupState.mode
    const persistDefaults = async (): Promise<void> => {
      try {
        await updateCliResumeSettings({ resumeMode: mode, resumeSourceKind: sourceKind })
        setResumeDefaults({ sourceKind, mode })
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown config write error.'
        notify(`Failed to save resume defaults: ${message}`, { kind: 'error' })
      }
    }
    void persistDefaults()
    if (sourceKind === 'history') {
      const selected = popupState.historyItems[popupState.historySelectionIndex]
      if (!selected) {
        const message = popupState.historyErrorMessage ?? 'No resumable history entries found.'
        pushHistory(`[resume] ${message}`, 'system')
        notify(message, { kind: 'warning' })
        return
      }
      pushHistory(`> /resume ${selected.selector} (${mode})`, 'user')
      setInputValue('')
      closePopup()
      const intentFileOverride = intentFilePath.trim()
      void runGeneration({
        ...(intentFileOverride ? { intentFile: intentFileOverride } : {}),
        resume: { kind: 'history', selector: selected.selector, mode },
      })
      return
    }
    const payloadPath = popupState.payloadPathDraft.trim()
    if (!payloadPath) {
      const message = 'Resume-from file path is required.'
      pushHistory(`[resume] ${message}`, 'system')
      notify(message, { kind: 'warning' })
      return
    }
    const validateAndRun = async (): Promise<void> => {
      try {
        await loadGeneratePayloadFromFile(payloadPath)
      } catch (error) {
        const rawMessage = error instanceof Error ? error.message : 'Unknown payload error.'
        const schemaHint = rawMessage.includes('schemaVersion')
          ? ` This prompt-maker-cli supports schemaVersion=${GENERATE_JSON_PAYLOAD_SCHEMA_VERSION}; try upgrading/downgrading prompt-maker-cli or re-exporting/regenerating the payload with a matching version.`
          : ''
        const message = `${rawMessage}${schemaHint}`
        pushHistory(`[resume] ${message}`, 'system')
        notify(message, { kind: 'error' })
        return
      }
      pushHistory(`> /resume-from ${payloadPath} (${mode})`, 'user')
      setInputValue('')
      closePopup()
      const intentFileOverride = intentFilePath.trim()
      await runGeneration({
        ...(intentFileOverride ? { intentFile: intentFileOverride } : {}),
        resume: { kind: 'file', payloadPath, mode },
      })
    }
    void validateAndRun()
  }, [
    closePopup,
    intentFilePath,
    notify,
    popupState,
    pushHistory,
    runGeneration,
    setInputValue,
    setResumeDefaults,
  ])
  const handleExportSubmit = useCallback(() => {
    if (popupState?.type !== 'export') {
      return
    }
    const format = popupState.format
    const outPath = popupState.outPathDraft.trim()
    const selected = popupState.historyItems[popupState.historySelectionIndex]
    if (!selected) {
      const message = popupState.historyErrorMessage ?? 'No history entries available for export.'
      pushHistory(`[export] ${message}`, 'system')
      notify(message, { kind: 'warning' })
      return
    }
    if (!outPath) {
      const message = 'Export output path is required.'
      pushHistory(`[export] ${message}`, 'system')
      notify(message, { kind: 'warning' })
      return
    }
    const persistDefaults = async (): Promise<void> => {
      try {
        const resolvedOutPath = path.resolve(process.cwd(), outPath)
        const outDir = path.dirname(resolvedOutPath)
        await updateCliExportSettings({ exportFormat: format, exportOutDir: outDir })
        setExportDefaults({ format, outDir })
      } catch (error) {
        const message = error instanceof Error ? error.message : 'Unknown config write error.'
        notify(`Failed to save export defaults: ${message}`, { kind: 'error' })
      }
    }
    void persistDefaults()
    const exportFromHistory = async (): Promise<void> => {
      try {
        const payload = await loadGeneratePayloadFromHistory({ selector: selected.selector })
        const { absolutePath } = await writeGeneratePayloadExport({
          payload,
          format,
          outPath,
        })
        const relative = path.relative(process.cwd(), absolutePath)
        const displayPath = relative && !relative.startsWith('..') ? relative : absolutePath
        pushHistory(`> /export ${selected.selector} (${format})`, 'user')
        pushHistory(`[export] Exported ${format.toUpperCase()} → ${displayPath}`, 'system')
        notify(`Exported ${format.toUpperCase()} → ${displayPath}`, { kind: 'info' })
        setInputValue('')
        closePopup()
      } catch (error) {
        const rawMessage = error instanceof Error ? error.message : 'Unknown export error.'
        const normalized = rawMessage.replace(/\s+/g, ' ').trim()
        const shortMessage = normalized.length > 220 ? `${normalized.slice(0, 217)}…` : normalized
        pushHistory(`[export] Export failed: ${shortMessage}`, 'system')
        notify(`Export failed: ${shortMessage}`, { kind: 'error' })
      }
    }
    void exportFromHistory()
  }, [closePopup, notify, popupState, pushHistory, setExportDefaults, setInputValue])
  const handleSeriesIntentSubmit = useCallback(
    (value: string) => {
      const trimmed = value.trim()
      if (!trimmed) {
        pushHistory('Series intent cannot be empty.', 'system')
        return
      }
      lastUserIntentRef.current = trimmed
      syncTypedIntentRef(trimmed)
      pushHistory(`> /series ${trimmed}`, 'user')
      setInputValue('')
      closePopup()
      void runSeriesGeneration(trimmed)
    },
    [
      closePopup,
      lastUserIntentRef,
      pushHistory,
      runSeriesGeneration,
      setInputValue,
      syncTypedIntentRef,
    ],
  )
  const runCommandSteps = useCallback(
    (steps: readonly PopupManagerCommandStep[]): void => {
      const pushHistoryEntry = (
        step: Extract<PopupManagerCommandStep, { type: 'push-history' }>,
      ): void => {
        if (step.kind) {
          pushHistory(step.message, step.kind)
          return
        }
        pushHistory(step.message)
      }
      for (const step of steps) {
        switch (step.type) {
          case 'open-popup':
            switch (step.popup) {
              case 'model':
                openModelPopup()
                break
              case 'target':
                openTargetModelPopup()
                break
              case 'polish':
                openPolishModelPopup()
                break
              case 'toggle': {
                const field = step.field
                if (!field) {
                  throw new Error('Expected toggle field in command mapping.')
                }
                openTogglePopup(field)
                break
              }
              case 'file':
                openFilePopup()
                break
              case 'url':
                openUrlPopup()
                break
              case 'image':
                openImagePopup()
                break
              case 'video':
                openVideoPopup()
                break
              case 'pdf':
                openPdfPopup()
                break
              case 'history':
                openHistoryPopup()
                break
              case 'resume':
                openResumePopup()
                break
              case 'export':
                openExportPopup()
                break
              case 'smart-root':
                openSmartRootPopup()
                break
              case 'tokens':
                openTokensPopup()
                break
              case 'budgets':
                openBudgetsPopup()
                break
              case 'settings':
                openSettingsPopup()
                break
              case 'theme':
                openThemePopup()
                break
              case 'theme-mode':
                openThemeModePopup()
                break
              case 'reasoning':
                openReasoningPopup()
                break
              case 'test':
                openTestPopup()
                break
              case 'intent':
                openIntentPopup()
                break
              case 'instructions':
                openInstructionsPopup()
                break
            }
            break
          case 'apply-toggle':
            applyToggleSelection(step.field, step.value)
            break
          case 'clear-polish':
            applyPolishModelSelection(null)
            break
          case 'add-url':
            addUrl(step.value)
            break
          case 'add-image':
            addImage(step.value)
            break
          case 'add-video':
            addVideo(step.value)
            break
          case 'add-pdf':
            addPdf(step.value)
            break
          case 'toggle-smart-context':
            toggleSmartContext()
            break
          case 'set-smart-root':
            setSmartRoot(step.value)
            break
          case 'set-intent-file':
            setIntentFilePath(step.value)
            break
          case 'set-meta-instructions':
            setMetaInstructions(step.value)
            break
          case 'push-history':
            pushHistoryEntry(step)
            break
          case 'notify':
            notify(step.message, { kind: step.kind })
            break
          case 'set-input':
            setInputValue(step.value)
            break
          case 'close-popup':
            closePopup()
            break
          case 'clear-screen':
            clearScreen?.()
            break
          case 'exit-app':
            exitApp()
            break
          case 'run-tests':
            void runTestsFromCommand(step.value)
            break
        }
      }
    },
    [
      addImage,
      addUrl,
      addVideo,
      applyPolishModelSelection,
      applyToggleSelection,
      clearScreen,
      closePopup,
      exitApp,
      notify,
      openFilePopup,
      openHistoryPopup,
      openResumePopup,
      openExportPopup,
      openImagePopup,
      openInstructionsPopup,
      openIntentPopup,
      openModelPopup,
      openPolishModelPopup,
      openReasoningPopup,
      openSettingsPopup,
      openSmartRootPopup,
      openTargetModelPopup,
      openTestPopup,
      openThemeModePopup,
      openThemePopup,
      openTogglePopup,
      openTokensPopup,
      openUrlPopup,
      openVideoPopup,
      pushHistory,
      runTestsFromCommand,
      setInputValue,
      setIntentFilePath,
      setMetaInstructions,
      setSmartRoot,
      toggleSmartContext,
    ],
  )
  const runSeriesCommand = useCallback(
    (trimmedArgs: string): void => {
      const handle = async (): Promise<void> => {
        if (isGenerating) {
          pushHistory('Generation already running. Please wait.', 'system')
          return
        }
        const latestTypedIntent = getLatestTypedIntent()
        const typedDraft = latestTypedIntent?.trim() ?? ''
        let initialDraft = trimmedArgs || typedDraft || lastUserIntentRef.current || ''
        let hintOverride: string | undefined
        if (trimmedArgs) {
          pushHistory('[series] Using provided text as intent draft.', 'system')
        } else if (typedDraft) {
          pushHistory('[series] Using typed intent as draft.', 'system')
        } else if (lastUserIntentRef.current) {
          pushHistory('[series] Reusing last intent as draft.', 'system')
        }
        if (!initialDraft) {
          const trimmedIntentFile = intentFilePath.trim()
          if (trimmedIntentFile) {
            try {
              const raw = await fs.readFile(trimmedIntentFile, 'utf8')
              const fileIntent = raw.trim()
              if (fileIntent) {
                initialDraft = fileIntent
                const fileLabel = path.basename(trimmedIntentFile)
                pushHistory(`[series] Loaded draft from intent file ${fileLabel}.`, 'system')
                hintOverride = `Loaded from intent file ${fileLabel}`
                syncTypedIntentRef(fileIntent)
              } else {
                pushHistory(
                  `[series] Intent file ${trimmedIntentFile} is empty; please add content.`,
                  'system',
                )
              }
            } catch (error) {
              const message = error instanceof Error ? error.message : 'Unknown intent file error.'
              pushHistory(
                `[series] Failed to read intent file ${trimmedIntentFile}: ${message}`,
                'system',
              )
            }
          }
        }
        if (!initialDraft) {
          pushHistory('[series] No intent found; enter one in the popup.', 'system')
        }
        openSeriesPopup(initialDraft, hintOverride)
        setInputValue('')
      }
      void handle()
    },
    [
      getLatestTypedIntent,
      intentFilePath,
      isGenerating,
      lastUserIntentRef,
      openSeriesPopup,
      pushHistory,
      setInputValue,
      syncTypedIntentRef,
    ],
  )
  const handleCommandSelection = useCallback(
    (commandId: CommandDescriptor['id'], argsRaw?: string) => {
      const result = mapPopupCommandSelection({
        commandId,
        argsRaw,
        context: {
          copyEnabled,
          chatGptEnabled,
          jsonOutputEnabled,
          interactiveTransportPath,
          urls,
          images,
          videos,
          pdfs,
          smartContextEnabled,
          smartContextRoot,
        },
      })
      if (result.kind === 'series') {
        runSeriesCommand(result.trimmedArgs)
        return
      }
      runCommandSteps(result.steps)
    },
    [
      chatGptEnabled,
      copyEnabled,
      images,
      interactiveTransportPath,
      jsonOutputEnabled,
      runCommandSteps,
      runSeriesCommand,
      smartContextEnabled,
      smartContextRoot,
      urls,
      videos,
      pdfs,
    ],
  )
  // Memoizing the actions object keeps `actions` referentially stable.
  // This reduces avoidable rerenders in components that receive `actions`.
  const actions = useMemo<PopupManagerActions>(
    () => ({
      openModelPopup,
      openPolishModelPopup,
      openTargetModelPopup,
      openTogglePopup,
      openFilePopup,
      openUrlPopup,
      openImagePopup,
      openVideoPopup,
      openPdfPopup,
      openHistoryPopup,
      openResumePopup,
      openExportPopup,
      openSmartRootPopup,
      openTokensPopup,
      openBudgetsPopup,
      openSettingsPopup,
      openThemePopup,
      openThemeModePopup,
      openReasoningPopup,
      openTestPopup,
      openIntentPopup,
      openInstructionsPopup,
      openSeriesPopup,
      closePopup,
      handleCommandSelection,
      handleModelPopupSubmit,
      applyToggleSelection,
      handleIntentFileSubmit,
      handleInstructionsSubmit,
      handleBudgetsSubmit,
      handleResumeSubmit,
      handleExportSubmit,
      handleSeriesIntentSubmit,
    }),
    [
      openModelPopup,
      openPolishModelPopup,
      openTargetModelPopup,
      openTogglePopup,
      openFilePopup,
      openUrlPopup,
      openImagePopup,
      openVideoPopup,
      openPdfPopup,
      openHistoryPopup,
      openResumePopup,
      openExportPopup,
      openSmartRootPopup,
      openTokensPopup,
      openBudgetsPopup,
      openSettingsPopup,
      openThemePopup,
      openThemeModePopup,
      openReasoningPopup,
      openTestPopup,
      openIntentPopup,
      openInstructionsPopup,
      openSeriesPopup,
      closePopup,
      handleCommandSelection,
      handleModelPopupSubmit,
      applyToggleSelection,
      handleIntentFileSubmit,
      handleInstructionsSubmit,
      handleBudgetsSubmit,
      handleResumeSubmit,
      handleExportSubmit,
      handleSeriesIntentSubmit,
    ],
  )
  return {
    popupState,
    setPopupState,
    actions,
  }
}
</file>

<file path="src/tui/screens/command/hooks/usePopupKeyboardShortcuts.ts">
import { useInput, type Key } from 'ink'
import type { Dispatch, SetStateAction } from 'react'
import { useStableCallback } from '../../../hooks/useStableCallback'
import type { HistoryEntry, ModelOption, PopupState } from '../../../types'
import { handleEscapeOnlyPopupShortcuts } from './popup-shortcuts/escape-only-popup-shortcuts'
import { handleHistoryPopupShortcuts } from './popup-shortcuts/history-popup-shortcuts'
import { handleIntentPopupShortcuts } from './popup-shortcuts/intent-popup-shortcuts'
import { handleResumePopupShortcuts } from './popup-shortcuts/resume-popup-shortcuts'
import { handleExportPopupShortcuts } from './popup-shortcuts/export-popup-shortcuts'
import { handleModelPopupShortcuts } from './popup-shortcuts/model-popup-shortcuts'
import { handleReasoningPopupShortcuts } from './popup-shortcuts/reasoning-popup-shortcuts'
import { handleSmartPopupShortcuts } from './popup-shortcuts/smart-popup-shortcuts'
import { handleBudgetsPopupShortcuts } from './popup-shortcuts/budgets-popup-shortcuts'
import { handleSuggestedSelectedListPopupShortcuts } from './popup-shortcuts/suggested-selected-list-popup-shortcuts'
import {
  handleThemeModePopupShortcuts,
  handleThemePopupShortcuts,
} from './popup-shortcuts/theme-popup-shortcuts'
import { handleTogglePopupShortcuts } from './popup-shortcuts/toggle-popup-shortcuts'
import { handleUrlPopupShortcuts } from './popup-shortcuts/url-popup-shortcuts'
export type UsePopupKeyboardShortcutsOptions = {
  popupState: PopupState
  helpOpen: boolean
  setPopupState: Dispatch<SetStateAction<PopupState>>
  closePopup: () => void
  model: {
    options: ModelOption[]
    onSubmit: (option: ModelOption | null | undefined) => void
  }
  toggle: {
    applySelection: (field: 'copy' | 'chatgpt' | 'json', value: boolean) => void
  }
  theme: {
    count: number
    onConfirm: () => void
    onCancel: () => void
  }
  themeMode: {
    count: number
    onConfirm: () => void
    onCancel: () => void
  }
  budgets: {
    onSubmit: () => void
  }
  file: {
    items: string[]
    suggestions: string[]
    onAdd: (value: string) => void
    onRemove: (index: number) => void
  }
  url: {
    items: string[]
    onRemove: (index: number) => void
  }
  image: {
    items: string[]
    suggestions: string[]
    onAdd: (value: string) => void
    onRemove: (index: number) => void
  }
  video: {
    items: string[]
    suggestions: string[]
    onAdd: (value: string) => void
    onRemove: (index: number) => void
  }
  pdf: {
    items: string[]
    suggestions: string[]
    onAdd: (value: string) => void
    onRemove: (index: number) => void
  }
  history: {
    items: string[]
  }
  resume: {
    onSubmit: () => void
  }
  export: {
    onSubmit: () => void
  }
  smart: {
    suggestions: string[]
    contextRoot: string | null
    onRootSubmit: (value: string) => void
  }
  intent: {
    suggestions: string[]
    onFileSubmit: (value: string) => void
  }
  reasoning: {
    lines: HistoryEntry[]
    visibleRows: number
  }
}
export const usePopupKeyboardShortcuts = ({
  popupState,
  helpOpen,
  setPopupState,
  closePopup,
  model,
  toggle,
  theme,
  themeMode,
  budgets,
  file,
  url,
  image,
  video,
  pdf,
  history,
  resume,
  export: exportActions,
  smart,
  intent,
  reasoning,
}: UsePopupKeyboardShortcutsOptions): void => {
  const isActive = popupState !== null && !helpOpen
  const handlePopupKey = useStableCallback((input: string, key: Key) => {
    if (!popupState) {
      return
    }
    switch (popupState.type) {
      case 'model':
        handleModelPopupShortcuts({
          popupState,
          input,
          key,
          options: model.options,
          setPopupState,
          closePopup,
          onModelPopupSubmit: model.onSubmit,
        })
        return
      case 'toggle':
        handleTogglePopupShortcuts({
          popupState,
          key,
          setPopupState,
          closePopup,
          applyToggleSelection: toggle.applySelection,
        })
        return
      case 'theme':
        handleThemePopupShortcuts({
          popupState,
          key,
          themeCount: theme.count,
          setPopupState,
          onThemeConfirm: theme.onConfirm,
          onThemeCancel: theme.onCancel,
        })
        return
      case 'themeMode':
        handleThemeModePopupShortcuts({
          popupState,
          key,
          optionCount: themeMode.count,
          setPopupState,
          onThemeModeConfirm: themeMode.onConfirm,
          onThemeModeCancel: themeMode.onCancel,
        })
        return
      case 'budgets':
        handleBudgetsPopupShortcuts({
          popupState,
          key,
          setPopupState,
          closePopup,
          onBudgetsSubmit: budgets.onSubmit,
        })
        return
      case 'file':
        handleSuggestedSelectedListPopupShortcuts({
          popupType: 'file',
          popupState,
          input,
          key,
          itemsLength: file.items.length,
          suggestions: file.suggestions,
          setPopupState,
          closePopup,
          onRemove: file.onRemove,
          onSelectSuggestion: file.onAdd,
        })
        return
      case 'url':
        handleUrlPopupShortcuts({
          popupState,
          input,
          key,
          urls: url.items,
          setPopupState,
          closePopup,
          onRemoveUrl: url.onRemove,
        })
        return
      case 'image':
        handleSuggestedSelectedListPopupShortcuts({
          popupType: 'image',
          popupState,
          input,
          key,
          itemsLength: image.items.length,
          suggestions: image.suggestions,
          setPopupState,
          closePopup,
          onRemove: image.onRemove,
          onSelectSuggestion: image.onAdd,
        })
        return
      case 'video':
        handleSuggestedSelectedListPopupShortcuts({
          popupType: 'video',
          popupState,
          input,
          key,
          itemsLength: video.items.length,
          suggestions: video.suggestions,
          setPopupState,
          closePopup,
          onRemove: video.onRemove,
          onSelectSuggestion: video.onAdd,
        })
        return
      case 'pdf':
        handleSuggestedSelectedListPopupShortcuts({
          popupType: 'pdf',
          popupState,
          input,
          key,
          itemsLength: pdf.items.length,
          suggestions: pdf.suggestions,
          setPopupState,
          closePopup,
          onRemove: pdf.onRemove,
          onSelectSuggestion: pdf.onAdd,
        })
        return
      case 'history':
        handleHistoryPopupShortcuts({
          popupState,
          key,
          itemCount: history.items.length,
          setPopupState,
          closePopup,
        })
        return
      case 'resume':
        handleResumePopupShortcuts({
          popupState,
          key,
          setPopupState,
          closePopup,
          onResumeSubmit: resume.onSubmit,
        })
        return
      case 'export':
        handleExportPopupShortcuts({
          popupState,
          key,
          setPopupState,
          closePopup,
          onExportSubmit: exportActions.onSubmit,
        })
        return
      case 'smart':
        handleSmartPopupShortcuts({
          popupState,
          input,
          key,
          suggestions: smart.suggestions,
          smartContextRoot: smart.contextRoot,
          setPopupState,
          closePopup,
          onSmartRootSubmit: smart.onRootSubmit,
        })
        return
      case 'tokens':
      case 'settings':
      case 'instructions':
      case 'series':
      case 'test':
        handleEscapeOnlyPopupShortcuts(key, closePopup)
        return
      case 'reasoning':
        handleReasoningPopupShortcuts({
          popupState,
          key,
          lineCount: reasoning.lines.length,
          visibleRows: reasoning.visibleRows,
          setPopupState,
          closePopup,
        })
        return
      case 'intent':
        handleIntentPopupShortcuts({
          popupState,
          key,
          suggestions: intent.suggestions,
          setPopupState,
          closePopup,
          onIntentFileSubmit: intent.onFileSubmit,
        })
        return
      default: {
        const exhaustive: never = popupState
        return exhaustive
      }
    }
  })
  useInput(handlePopupKey, { isActive })
}
</file>

<file path="src/tui/config.ts">
export const COMMAND_DESCRIPTORS = [
  { id: 'exit', label: 'Exit', description: 'Exit the app (/exit)' },
  { id: 'help', label: 'Help', description: 'Show keyboard shortcuts (/help)' },
  { id: 'model', label: 'Model', description: 'Switch the generation model' },
  {
    id: 'target',
    label: 'Target',
    description:
      'Switch the runtime model used for optimization (/target; not included in prompt text)',
  },
  {
    id: 'intent',
    label: 'Intent File',
    description: 'Use a file for the intent text (/intent path/to/file.md)',
  },
  {
    id: 'instructions',
    label: 'Meta Instructions',
    description: 'Add optional meta guidance (/meta <text>)',
    aliases: ['meta'] as const,
  },
  {
    id: 'new',
    label: 'New',
    description: 'Reset session state (/new)',
  },
  {
    id: 'reuse',
    label: 'Reuse',
    description: 'Reset and reuse last prompt (/reuse)',
  },
  { id: 'file', label: 'File', description: 'Attach file context' },
  { id: 'url', label: 'URL', description: 'Add/manage URL context (/url [url1 url2 ...])' },
  { id: 'smart', label: 'Smart', description: 'Toggle smart context (/smart on|off)' },
  {
    id: 'smart-root',
    label: 'Smart Context Root',
    description: 'Set/clear smart context root (/smart-root path or /smart-root --clear)',
  },
  { id: 'image', label: 'Image', description: 'Attach reference images' },
  { id: 'video', label: 'Video', description: 'Attach reference videos' },
  { id: 'pdf', label: 'PDF', description: 'Attach reference PDFs' },
  { id: 'polish', label: 'Polish', description: 'Select a model for prompt polishing' },
  {
    id: 'series',
    label: 'Series',
    description:
      'Atomic prompt series (Tab) · standalone prompts; prefilled from typed/last intent or intent file',
  },
  { id: 'copy', label: 'Copy', description: 'Auto-copy final prompt' },
  { id: 'chatgpt', label: 'ChatGPT', description: 'Open ChatGPT automatically' },
  { id: 'json', label: 'JSON', description: 'Toggle JSON payload in history (/json on|off)' },
  { id: 'tokens', label: 'Tokens', description: 'Show token usage breakdown (/tokens)' },
  {
    id: 'budgets',
    label: 'Budgets',
    description: 'Configure token budgets and overflow strategy (/budgets)',
  },
  { id: 'settings', label: 'Settings', description: 'Show current settings (/settings)' },
  { id: 'theme', label: 'Theme', description: 'Switch TUI theme (/theme)' },
  {
    id: 'theme-mode',
    label: 'Theme Mode',
    description: 'Switch theme mode (/theme-mode dark|light|system)',
  },
  {
    id: 'reasoning',
    label: 'Reasoning',
    description: 'Show last model reasoning (/reasoning or /why)',
    aliases: ['why'] as const,
  },
  { id: 'history', label: 'History', description: 'Browse command/intent history (/history)' },
  {
    id: 'resume',
    label: 'Resume',
    description: 'Resume generation from history or an exported payload (/resume)',
  },
  {
    id: 'export',
    label: 'Export',
    description: 'Export a selected history payload to JSON/YAML (/export)',
  },
  { id: 'test', label: 'Test', description: 'Run prompt tests (/test prompt-tests.yaml)' },
] as const
export const TOGGLE_LABELS = {
  copy: 'Copy',
  chatgpt: 'ChatGPT',
  json: 'JSON',
} as const
export const POPUP_HEIGHTS = {
  model: 16,
  toggle: 6,
  file: 16,
  url: 12,
  image: 16,
  video: 16,
  pdf: 16,
  history: 16,
  resume: 18,
  export: 18,
  smart: 12,
  tokens: 16,
  budgets: 14,
  settings: 14,
  theme: 16,
  themeMode: 8,
  reasoning: 18,
  test: 7,
  intent: 9,
  instructions: 7,
  series: 8,
} as const
</file>

<file path="src/tui/types.ts">
import type { ContextOverflowStrategy } from '../config'
import type { ModelProvider } from '../model-providers'
import type { COMMAND_DESCRIPTORS, POPUP_HEIGHTS, TOGGLE_LABELS } from './config'
import type { ThemeMode } from './theme/theme-types'
export type CommandDescriptor = (typeof COMMAND_DESCRIPTORS)[number]
export type ToggleField = keyof typeof TOGGLE_LABELS
export type PopupKind = keyof typeof POPUP_HEIGHTS
export type ModelOption = {
  id: string
  label: string
  provider: ModelProvider
  description: string
  capabilities: string[]
  default?: boolean
  notes?: string
  source: 'builtin' | 'config' | 'discovered'
}
export type ProviderStatus = {
  provider: ModelProvider
  status: 'ok' | 'missing' | 'error'
  message: string
}
export type ProviderStatusMap = Record<ModelProvider, ProviderStatus>
export type ResumeSourceKind = 'history' | 'file'
export type ResumeMode = 'best-effort' | 'strict'
export type ResumeHistoryItem = {
  selector: string
  title: string
  detail: string
}
export type ExportHistoryItem = {
  selector: string
  title: string
  detail: string
  schemaVersion: string
  supported: boolean
}
export type PopupState =
  | {
      type: 'model'
      kind: 'generation' | 'polish' | 'target'
      query: string
      selectionIndex: number
    }
  | { type: 'toggle'; field: ToggleField; selectionIndex: number }
  | {
      type: 'file'
      draft: string
      selectionIndex: number
      selectedFocused: boolean
      suggestedItems: string[]
      suggestedSelectionIndex: number
      suggestedFocused: boolean
    }
  | {
      type: 'url'
      draft: string
      selectionIndex: number
      selectedFocused: boolean
      editingIndex: number | null
    }
  | {
      type: 'image'
      draft: string
      selectionIndex: number
      selectedFocused: boolean
      suggestedItems: string[]
      suggestedSelectionIndex: number
      suggestedFocused: boolean
    }
  | {
      type: 'video'
      draft: string
      selectionIndex: number
      selectedFocused: boolean
      suggestedItems: string[]
      suggestedSelectionIndex: number
      suggestedFocused: boolean
    }
  | {
      type: 'pdf'
      draft: string
      selectionIndex: number
      selectedFocused: boolean
      suggestedItems: string[]
      suggestedSelectionIndex: number
      suggestedFocused: boolean
    }
  | { type: 'history'; draft: string; selectionIndex: number }
  | {
      type: 'resume'
      selectionIndex: number
      sourceKind: ResumeSourceKind
      mode: ResumeMode
      historyItems: ResumeHistoryItem[]
      historySelectionIndex: number
      historyErrorMessage: string | null
      payloadPathDraft: string
      suggestedItems: string[]
      suggestedSelectionIndex: number
      suggestedFocused: boolean
    }
  | {
      type: 'export'
      selectionIndex: number
      historyItems: ExportHistoryItem[]
      historySelectionIndex: number
      historyErrorMessage: string | null
      format: 'json' | 'yaml'
      outPathDraft: string
    }
  | {
      type: 'smart'
      draft: string
      suggestedItems: string[]
      suggestedSelectionIndex: number
      suggestedFocused: boolean
    }
  | { type: 'tokens' }
  | {
      type: 'budgets'
      selectionIndex: number
      maxContextTokensDraft: string
      maxInputTokensDraft: string
      contextOverflowStrategyDraft: ContextOverflowStrategy | ''
      errorMessage: string | null
    }
  | { type: 'settings' }
  | { type: 'theme'; selectionIndex: number; initialThemeName: string }
  | {
      type: 'themeMode'
      selectionIndex: number
      initialMode: ThemeMode
    }
  | { type: 'reasoning'; scrollOffset: number }
  | { type: 'test'; draft: string }
  | {
      type: 'intent'
      draft: string
      suggestedItems: string[]
      suggestedSelectionIndex: number
      suggestedFocused: boolean
    }
  | { type: 'instructions'; draft: string }
  | { type: 'series'; draft: string; hint?: string }
  | null
export type HistoryEntry = {
  id: string
  content: string
  kind: 'user' | 'system' | 'progress'
  format?: 'markdown'
}
</file>

<file path="src/tui/screens/command/components/PopupArea.tsx">
/*
 * PopupArea
 *
 * Presentational component: renders whichever popup is currently active.
 *
 * This keeps the large popup JSX switch out of `CommandScreen` so the main
 * screen file is mostly orchestration.
 */
import type { ComponentProps } from 'react'
import { ListPopup } from '../../../components/popups/ListPopup'
import { ModelPopup } from '../../../components/popups/ModelPopup'
import { SmartPopup } from '../../../components/popups/SmartPopup'
import { BudgetsPopup } from '../../../components/popups/BudgetsPopup'
import { TokenUsagePopup } from '../../../components/popups/TokenUsagePopup'
import { SettingsPopup } from '../../../components/popups/SettingsPopup'
import { ReasoningPopup } from '../../../components/popups/ReasoningPopup'
import { TestPopup } from '../../../components/popups/TestPopup'
import { TogglePopup } from '../../../components/popups/TogglePopup'
import { IntentFilePopup } from '../../../components/popups/IntentFilePopup'
import { InstructionsPopup } from '../../../components/popups/InstructionsPopup'
import { SeriesIntentPopup } from '../../../components/popups/SeriesIntentPopup'
import { ThemePickerPopup } from '../../../components/popups/ThemePickerPopup'
import { ThemeModePopup } from '../../../components/popups/ThemeModePopup'
import { ResumePopup } from '../../../components/popups/ResumePopup'
import { ExportPopup } from '../../../components/popups/ExportPopup'
import type { HistoryEntry, ModelOption, PopupState, ProviderStatusMap } from '../../../types'
import type { TokenUsageBreakdown, TokenUsageRun } from '../../../token-usage-store'
export type PopupAreaProps = {
  popupState: PopupState
  helpOpen: boolean
  overlayHeight: number
  // Model popup
  modelPopupOptions: ModelOption[]
  modelPopupSelection: number
  modelPopupRecentCount: number
  providerStatuses: ProviderStatusMap
  onModelPopupQueryChange: (next: string) => void
  onModelPopupSubmit: (option: ModelOption | null | undefined) => void
  // Toggle popup
  // (toggle popup is self-contained; selection is stored in `popupState`)
  // File popup
  files: string[]
  filePopupSuggestions: string[]
  filePopupSuggestionSelectionIndex: number
  filePopupSuggestionsFocused: boolean
  onFilePopupDraftChange: (next: string) => void
  onAddFile: (value: string) => void
  // URL popup
  urls: string[]
  onUrlPopupDraftChange: (next: string) => void
  onAddUrl: (value: string) => void
  // Image popup
  images: string[]
  imagePopupSuggestions: string[]
  imagePopupSuggestionSelectionIndex: number
  imagePopupSuggestionsFocused: boolean
  onImagePopupDraftChange: (next: string) => void
  onAddImage: (value: string) => void
  // Video popup
  videos: string[]
  videoPopupSuggestions: string[]
  videoPopupSuggestionSelectionIndex: number
  videoPopupSuggestionsFocused: boolean
  onVideoPopupDraftChange: (next: string) => void
  onAddVideo: (value: string) => void
  // PDF popup
  pdfs: string[]
  pdfPopupSuggestions: string[]
  pdfPopupSuggestionSelectionIndex: number
  pdfPopupSuggestionsFocused: boolean
  onPdfPopupDraftChange: (next: string) => void
  onAddPdf: (value: string) => void
  // History popup
  historyPopupItems: string[]
  onHistoryPopupDraftChange: (next: string) => void
  onHistoryPopupSubmit: (value: string) => void
  // Resume popup
  onResumePayloadPathDraftChange: (next: string) => void
  onResumeSubmit: () => void
  // Export popup
  onExportOutPathDraftChange: (next: string) => void
  onExportSubmit: () => void
  // Intent popup
  intentPopupSuggestions: string[]
  intentPopupSuggestionSelectionIndex: number
  intentPopupSuggestionsFocused: boolean
  onIntentPopupDraftChange: (next: string) => void
  onIntentFileSubmit: (value: string) => void
  // Instructions
  onInstructionsDraftChange: (next: string) => void
  onInstructionsSubmit: (value: string) => void
  // Series
  isGenerating: boolean
  onSeriesDraftChange: (next: string) => void
  onSeriesSubmit: (value: string) => void
  // Test
  isTestCommandRunning: boolean
  onTestDraftChange: (next: string) => void
  onTestSubmit: (value: string) => void
  // Tokens
  tokenUsageRun: TokenUsageRun | null
  tokenUsageBreakdown: TokenUsageBreakdown | null
  maxContextTokens: number | null
  maxInputTokens: number | null
  contextOverflowStrategy: import('../../../../config').ContextOverflowStrategy | null
  latestContextOverflow:
    | import('../../../generation-pipeline-reducer').ContextOverflowDetails
    | null
  // Budgets
  onBudgetsMaxContextTokensDraftChange: (next: string) => void
  onBudgetsMaxInputTokensDraftChange: (next: string) => void
  onBudgetsSubmit: () => void
  // Settings
  statusChips: string[]
  // Reasoning
  reasoningPopupLines: HistoryEntry[]
  reasoningPopupVisibleRows: number
  // Smart context
  smartContextEnabled: boolean
  smartContextRoot: string | null
  smartPopupSuggestions: string[]
  smartPopupSuggestionSelectionIndex: number
  smartPopupSuggestionsFocused: boolean
  onSmartPopupDraftChange: (next: string) => void
  onSmartRootSubmit: (value: string) => void
}
type NonNullPopupState = Exclude<PopupState, null>
type PopupStateFor<T extends NonNullPopupState['type']> = Extract<NonNullPopupState, { type: T }>
const renderModelPopup = (props: PopupAreaProps, popupState: PopupStateFor<'model'>) => {
  const title =
    popupState.kind === 'target'
      ? 'Select target model'
      : popupState.kind === 'polish'
        ? 'Select polish model'
        : 'Select model'
  const viewModel = {
    title,
    query: popupState.query,
    options: props.modelPopupOptions,
    selectedIndex: props.modelPopupSelection,
    recentCount: props.modelPopupRecentCount,
    maxHeight: props.overlayHeight,
    providerStatuses: props.providerStatuses,
    onQueryChange: props.onModelPopupQueryChange,
    onSubmit: props.onModelPopupSubmit,
  } satisfies ComponentProps<typeof ModelPopup>
  return <ModelPopup {...viewModel} />
}
const renderTogglePopup = (_props: PopupAreaProps, popupState: PopupStateFor<'toggle'>) => {
  const viewModel = {
    field: popupState.field,
    selectionIndex: popupState.selectionIndex,
  } satisfies ComponentProps<typeof TogglePopup>
  return <TogglePopup {...viewModel} />
}
const renderFilePopup = (props: PopupAreaProps, popupState: PopupStateFor<'file'>) => {
  const viewModel = {
    title: 'File Context',
    placeholder: 'src/**/*.ts',
    draft: popupState.draft,
    items: props.files,
    selectedIndex: popupState.selectionIndex,
    selectedFocused: popupState.selectedFocused,
    layout: 'selected-first',
    emptyLabel: 'No file globs added',
    instructions:
      "Enter add · ↑/↓ focus list · Del/Backspace remove · Tab suggestions · Esc close\nfzf: ^start $end 'exact",
    suggestedItems: props.filePopupSuggestions,
    suggestedSelectionIndex: props.filePopupSuggestionSelectionIndex,
    suggestedFocused: props.filePopupSuggestionsFocused,
    maxHeight: props.overlayHeight,
    onDraftChange: props.onFilePopupDraftChange,
    onSubmitDraft: props.onAddFile,
  } satisfies ComponentProps<typeof ListPopup>
  return <ListPopup {...viewModel} />
}
const renderUrlPopup = (props: PopupAreaProps, popupState: PopupStateFor<'url'>) => {
  const viewModel = {
    title: 'URL Context',
    placeholder: 'https://github.com/...',
    draft: popupState.draft,
    items: props.urls,
    selectedIndex: popupState.selectionIndex,
    selectedFocused: popupState.selectedFocused,
    emptyLabel: 'No URLs added',
    instructions:
      popupState.editingIndex === null
        ? 'Enter add (space/comma ok) · ↑/↓ focus list · e edit · Del remove · Esc close'
        : 'Editing… Enter save · Esc cancel · Del remove',
    onDraftChange: props.onUrlPopupDraftChange,
    onSubmitDraft: props.onAddUrl,
  } satisfies ComponentProps<typeof ListPopup>
  return <ListPopup {...viewModel} />
}
const renderImagePopup = (props: PopupAreaProps, popupState: PopupStateFor<'image'>) => {
  const viewModel = {
    title: 'Images',
    placeholder: 'path/to/image.png',
    draft: popupState.draft,
    items: props.images,
    selectedIndex: popupState.selectionIndex,
    selectedFocused: popupState.selectedFocused,
    layout: 'selected-first',
    emptyLabel: 'No images attached',
    instructions:
      "Enter add · ↑/↓ focus list · Del/Backspace remove · Tab suggestions · Esc close\nfzf: ^start $end 'exact",
    suggestedItems: props.imagePopupSuggestions,
    suggestedSelectionIndex: props.imagePopupSuggestionSelectionIndex,
    suggestedFocused: props.imagePopupSuggestionsFocused,
    maxHeight: props.overlayHeight,
    onDraftChange: props.onImagePopupDraftChange,
    onSubmitDraft: props.onAddImage,
  } satisfies ComponentProps<typeof ListPopup>
  return <ListPopup {...viewModel} />
}
const renderVideoPopup = (props: PopupAreaProps, popupState: PopupStateFor<'video'>) => {
  const viewModel = {
    title: 'Videos',
    placeholder: 'path/to/video.mp4',
    draft: popupState.draft,
    items: props.videos,
    selectedIndex: popupState.selectionIndex,
    selectedFocused: popupState.selectedFocused,
    layout: 'selected-first',
    emptyLabel: 'No videos attached',
    instructions:
      "Enter add · ↑/↓ focus list · Del/Backspace remove · Tab suggestions · Esc close\nfzf: ^start $end 'exact",
    suggestedItems: props.videoPopupSuggestions,
    suggestedSelectionIndex: props.videoPopupSuggestionSelectionIndex,
    suggestedFocused: props.videoPopupSuggestionsFocused,
    maxHeight: props.overlayHeight,
    onDraftChange: props.onVideoPopupDraftChange,
    onSubmitDraft: props.onAddVideo,
  } satisfies ComponentProps<typeof ListPopup>
  return <ListPopup {...viewModel} />
}
const renderPdfPopup = (props: PopupAreaProps, popupState: PopupStateFor<'pdf'>) => {
  const viewModel = {
    title: 'PDFs',
    placeholder: 'path/to/document.pdf',
    draft: popupState.draft,
    items: props.pdfs,
    selectedIndex: popupState.selectionIndex,
    selectedFocused: popupState.selectedFocused,
    layout: 'selected-first',
    emptyLabel: 'No PDFs attached',
    instructions:
      "Enter add · ↑/↓ focus list · Del/Backspace remove · Tab suggestions · Esc close\nfzf: ^start $end 'exact",
    suggestedItems: props.pdfPopupSuggestions,
    suggestedSelectionIndex: props.pdfPopupSuggestionSelectionIndex,
    suggestedFocused: props.pdfPopupSuggestionsFocused,
    maxHeight: props.overlayHeight,
    onDraftChange: props.onPdfPopupDraftChange,
    onSubmitDraft: props.onAddPdf,
  } satisfies ComponentProps<typeof ListPopup>
  return <ListPopup {...viewModel} />
}
const renderHistoryPopup = (props: PopupAreaProps, popupState: PopupStateFor<'history'>) => {
  const viewModel = {
    title: 'History',
    placeholder: 'Type to filter…',
    draft: popupState.draft,
    items: props.historyPopupItems,
    selectedIndex: popupState.selectionIndex,
    selectedFocused: true,
    emptyLabel: 'No history entries',
    instructions: '↑/↓ select · Enter paste · Esc close',
    onDraftChange: props.onHistoryPopupDraftChange,
    onSubmitDraft: props.onHistoryPopupSubmit,
  } satisfies ComponentProps<typeof ListPopup>
  return <ListPopup {...viewModel} />
}
const renderResumePopup = (props: PopupAreaProps, popupState: PopupStateFor<'resume'>) => {
  const viewModel = {
    selectionIndex: popupState.selectionIndex,
    sourceKind: popupState.sourceKind,
    mode: popupState.mode,
    historyItems: popupState.historyItems,
    historySelectionIndex: popupState.historySelectionIndex,
    historyErrorMessage: popupState.historyErrorMessage,
    payloadPathDraft: popupState.payloadPathDraft,
    suggestedItems: popupState.suggestedItems,
    suggestedSelectionIndex: popupState.suggestedSelectionIndex,
    suggestedFocused: popupState.suggestedFocused,
    onPayloadPathChange: props.onResumePayloadPathDraftChange,
    onSubmit: props.onResumeSubmit,
  } satisfies ComponentProps<typeof ResumePopup>
  return <ResumePopup {...viewModel} />
}
const renderExportPopup = (props: PopupAreaProps, popupState: PopupStateFor<'export'>) => {
  const viewModel = {
    selectionIndex: popupState.selectionIndex,
    format: popupState.format,
    outPathDraft: popupState.outPathDraft,
    historyItems: popupState.historyItems,
    historySelectionIndex: popupState.historySelectionIndex,
    historyErrorMessage: popupState.historyErrorMessage,
    onOutPathChange: props.onExportOutPathDraftChange,
    onSubmit: props.onExportSubmit,
  } satisfies ComponentProps<typeof ExportPopup>
  return <ExportPopup {...viewModel} />
}
const renderIntentPopup = (props: PopupAreaProps, popupState: PopupStateFor<'intent'>) => {
  const viewModel = {
    draft: popupState.draft,
    suggestions: props.intentPopupSuggestions,
    suggestedSelectionIndex: props.intentPopupSuggestionSelectionIndex,
    suggestedFocused: props.intentPopupSuggestionsFocused,
    maxHeight: props.overlayHeight,
    onDraftChange: props.onIntentPopupDraftChange,
    onSubmitDraft: props.onIntentFileSubmit,
  } satisfies ComponentProps<typeof IntentFilePopup>
  return <IntentFilePopup {...viewModel} />
}
const renderSmartPopup = (props: PopupAreaProps, popupState: PopupStateFor<'smart'>) => {
  const viewModel = {
    savedRoot: props.smartContextRoot,
    draft: popupState.draft,
    suggestedItems: props.smartPopupSuggestions,
    suggestedSelectionIndex: props.smartPopupSuggestionSelectionIndex,
    suggestedFocused: props.smartPopupSuggestionsFocused,
    maxHeight: props.overlayHeight,
    onDraftChange: props.onSmartPopupDraftChange,
    onSubmitRoot: props.onSmartRootSubmit,
  } satisfies ComponentProps<typeof SmartPopup>
  return <SmartPopup {...viewModel} />
}
const renderInstructionsPopup = (
  props: PopupAreaProps,
  popupState: PopupStateFor<'instructions'>,
) => {
  const viewModel = {
    draft: popupState.draft,
    onDraftChange: props.onInstructionsDraftChange,
    onSubmitDraft: props.onInstructionsSubmit,
  } satisfies ComponentProps<typeof InstructionsPopup>
  return <InstructionsPopup {...viewModel} />
}
const renderSeriesPopup = (props: PopupAreaProps, popupState: PopupStateFor<'series'>) => {
  const viewModel = {
    draft: popupState.draft,
    hint: popupState.hint,
    isRunning: props.isGenerating,
    onDraftChange: props.onSeriesDraftChange,
    onSubmitDraft: props.onSeriesSubmit,
  } satisfies ComponentProps<typeof SeriesIntentPopup>
  return <SeriesIntentPopup {...viewModel} />
}
const renderTestPopup = (props: PopupAreaProps, popupState: PopupStateFor<'test'>) => {
  const viewModel = {
    draft: popupState.draft,
    isRunning: props.isTestCommandRunning,
    onDraftChange: props.onTestDraftChange,
    onSubmitDraft: props.onTestSubmit,
  } satisfies ComponentProps<typeof TestPopup>
  return <TestPopup {...viewModel} />
}
const renderReasoningPopup = (props: PopupAreaProps, popupState: PopupStateFor<'reasoning'>) => {
  const viewModel = {
    lines: props.reasoningPopupLines,
    visibleRows: props.reasoningPopupVisibleRows,
    scrollOffset: popupState.scrollOffset,
  } satisfies ComponentProps<typeof ReasoningPopup>
  return <ReasoningPopup {...viewModel} />
}
const renderTokenUsagePopup = (props: PopupAreaProps) => {
  const viewModel = {
    run: props.tokenUsageRun,
    breakdown: props.tokenUsageBreakdown,
    budgets: {
      maxContextTokens: props.maxContextTokens,
      maxInputTokens: props.maxInputTokens,
      contextOverflowStrategy: props.contextOverflowStrategy,
    },
    latestContextOverflow: props.latestContextOverflow,
  } satisfies ComponentProps<typeof TokenUsagePopup>
  return <TokenUsagePopup {...viewModel} />
}
const renderBudgetsPopup = (props: PopupAreaProps, popupState: PopupStateFor<'budgets'>) => {
  const viewModel = {
    selectionIndex: popupState.selectionIndex,
    maxContextTokensDraft: popupState.maxContextTokensDraft,
    maxInputTokensDraft: popupState.maxInputTokensDraft,
    contextOverflowStrategyDraft: popupState.contextOverflowStrategyDraft,
    errorMessage: popupState.errorMessage,
    onMaxContextTokensChange: props.onBudgetsMaxContextTokensDraftChange,
    onMaxInputTokensChange: props.onBudgetsMaxInputTokensDraftChange,
    onSubmit: props.onBudgetsSubmit,
  } satisfies ComponentProps<typeof BudgetsPopup>
  return <BudgetsPopup {...viewModel} />
}
const renderSettingsPopup = (props: PopupAreaProps) => {
  const viewModel = {
    chips: props.statusChips,
  } satisfies ComponentProps<typeof SettingsPopup>
  return <SettingsPopup {...viewModel} />
}
const renderThemePopup = (props: PopupAreaProps, popupState: PopupStateFor<'theme'>) => {
  const viewModel = {
    selectionIndex: popupState.selectionIndex,
    initialThemeName: popupState.initialThemeName,
    maxHeight: props.overlayHeight,
  } satisfies ComponentProps<typeof ThemePickerPopup>
  return <ThemePickerPopup {...viewModel} />
}
const renderThemeModePopup = (_props: PopupAreaProps, popupState: PopupStateFor<'themeMode'>) => {
  const viewModel = {
    selectionIndex: popupState.selectionIndex,
    initialMode: popupState.initialMode,
  } satisfies ComponentProps<typeof ThemeModePopup>
  return <ThemeModePopup {...viewModel} />
}
export const PopupArea = (props: PopupAreaProps) => {
  const { popupState, helpOpen } = props
  if (popupState === null || helpOpen) {
    return null
  }
  switch (popupState.type) {
    case 'model':
      return renderModelPopup(props, popupState)
    case 'toggle':
      return renderTogglePopup(props, popupState)
    case 'file':
      return renderFilePopup(props, popupState)
    case 'url':
      return renderUrlPopup(props, popupState)
    case 'image':
      return renderImagePopup(props, popupState)
    case 'video':
      return renderVideoPopup(props, popupState)
    case 'pdf':
      return renderPdfPopup(props, popupState)
    case 'history':
      return renderHistoryPopup(props, popupState)
    case 'resume':
      return renderResumePopup(props, popupState)
    case 'export':
      return renderExportPopup(props, popupState)
    case 'intent':
      return renderIntentPopup(props, popupState)
    case 'smart':
      return renderSmartPopup(props, popupState)
    case 'instructions':
      return renderInstructionsPopup(props, popupState)
    case 'series':
      return renderSeriesPopup(props, popupState)
    case 'test':
      return renderTestPopup(props, popupState)
    case 'reasoning':
      return renderReasoningPopup(props, popupState)
    case 'tokens':
      return renderTokenUsagePopup(props)
    case 'budgets':
      return renderBudgetsPopup(props, popupState)
    case 'settings':
      return renderSettingsPopup(props)
    case 'theme':
      return renderThemePopup(props, popupState)
    case 'themeMode':
      return renderThemeModePopup(props, popupState)
    default: {
      const _exhaustive: never = popupState
      return null
    }
  }
}
</file>

<file path="src/tui/screens/command/hooks/useCommandScreenPopupBindings.ts">
import { useMemo } from 'react'
import type { CommandDescriptor, HistoryEntry, ModelOption, PopupState } from '../../../types'
import { useStableCallback } from '../../../hooks/useStableCallback'
import { useMiscPopupDraftHandlers } from './useMiscPopupDraftHandlers'
import { useModelPopupData } from './useModelPopupData'
import { usePopupKeyboardShortcuts } from './usePopupKeyboardShortcuts'
import { useReasoningPopup } from './useReasoningPopup'
import { useThemePopupGlue } from './useThemePopupGlue'
import { useThemeModePopupGlue } from './useThemeModePopupGlue'
import {
  useCommandScreenPasteBindings,
  type UseCommandScreenPasteBindingsOptions,
} from './useCommandScreenPasteBindings'
import {
  useCommandScreenContextPopupBindings,
  type UseCommandScreenContextPopupBindingsOptions,
} from './useCommandScreenContextPopupBindings'
import {
  useCommandScreenHistoryIntentPopupBindings,
  type UseCommandScreenHistoryIntentPopupBindingsOptions,
} from './useCommandScreenHistoryIntentPopupBindings'
import {
  useCommandScreenSubmitBindings,
  type UseCommandScreenSubmitBindingsOptions,
} from './useCommandScreenSubmitBindings'
type SetPopupState = import('react').Dispatch<import('react').SetStateAction<PopupState>>
export type UseCommandScreenPopupBindingsOptions = {
  input: {
    value: string
    setValue: (value: string | ((prev: string) => string)) => void
    setPasteActive: (active: boolean) => void
    consumeSuppressedTextInputChange: () => boolean
    suppressNextInput: () => void
    updateLastTypedIntent: (next: string) => void
    intentFilePath: string
    lastUserIntentRef: import('react').MutableRefObject<string | null>
  }
  popup: {
    state: PopupState
    setState: SetPopupState
    isOpen: boolean
    helpOpen: boolean
    openHelp?: () => void
    close: () => void
    actions: {
      handleCommandSelection: (commandId: CommandDescriptor['id'], argsRaw?: string) => void
      handleModelPopupSubmit: (option: ModelOption | null | undefined) => void
      applyToggleSelection: (field: 'copy' | 'chatgpt' | 'json', value: boolean) => void
      handleIntentFileSubmit: (value: string) => void
      handleResumeSubmit: () => void
      handleExportSubmit: () => void
      handleSeriesIntentSubmit: (value: string) => void
      handleBudgetsSubmit: () => void
    }
  }
  menu: {
    isActive: boolean
    selectedCommandId: CommandDescriptor['id'] | null
    argsRaw: string
    isCommandMode: boolean
    actions: {
      handleNewCommand: (argsRaw: string) => void
      handleReuseCommand: () => void
    }
  }
  generation: {
    isGenerating: boolean
    isAwaitingRefinement: boolean
    submitRefinement: (value: string) => void
    runGeneration: (payload: {
      intent?: string
      intentFile?: string
      resume?:
        | { kind: 'history'; selector: string; mode: import('../../../types').ResumeMode }
        | { kind: 'file'; payloadPath: string; mode: import('../../../types').ResumeMode }
    }) => Promise<void>
  }
  history: {
    pushHistory: (
      content: string,
      kind?: HistoryEntry['kind'],
      format?: HistoryEntry['format'],
    ) => void
    addCommandHistoryEntry: (value: string) => void
    commandHistoryValues: string[]
  }
  context: {
    droppedFilePath: string | null
    files: string[]
    urls: string[]
    images: string[]
    videos: string[]
    pdfs: string[]
    smartContextEnabled: boolean
    smartContextRoot: string | null
    addFile: (value: string) => void
    removeFile: (index: number) => void
    addUrl: (value: string) => void
    removeUrl: (index: number) => void
    updateUrl: (index: number, value: string) => void
    addImage: (value: string) => void
    removeImage: (index: number) => void
    addVideo: (value: string) => void
    removeVideo: (index: number) => void
    addPdf: (value: string) => void
    removePdf: (index: number) => void
    toggleSmartContext: () => void
    setSmartRoot: (value: string) => void
    notify: (message: string) => void
    modelOptions: ModelOption[]
    lastReasoning: string | null
    terminalColumns: number
    reasoningPopupHeight: number
  }
}
export type UseCommandScreenPopupBindingsResult = {
  input: {
    tokenLabel: (token: string) => string | null
    onChange: (next: string) => void
  }
  submit: {
    onSubmit: (value: string) => void
    onSeriesSubmit: (value: string) => void
  }
  popup: {
    model: {
      options: ModelOption[]
      recentCount: number
      selection: number
      onQueryChange: (next: string) => void
    }
    history: {
      items: string[]
      onDraftChange: (next: string) => void
      onSubmit: (value: string) => void
    }
    intent: {
      suggestions: string[]
      suggestionSelectionIndex: number
      suggestionsFocused: boolean
      onDraftChange: (next: string) => void
    }
    context: {
      file: {
        suggestions: string[]
        suggestionSelectionIndex: number
        suggestionsFocused: boolean
        onDraftChange: (next: string) => void
        onAdd: (value: string) => void
        onRemove: (index: number) => void
      }
      url: {
        onDraftChange: (next: string) => void
        onAdd: (value: string) => void
        onRemove: (index: number) => void
      }
      image: {
        suggestions: string[]
        suggestionSelectionIndex: number
        suggestionsFocused: boolean
        onDraftChange: (next: string) => void
        onAdd: (value: string) => void
        onRemove: (index: number) => void
      }
      video: {
        suggestions: string[]
        suggestionSelectionIndex: number
        suggestionsFocused: boolean
        onDraftChange: (next: string) => void
        onAdd: (value: string) => void
        onRemove: (index: number) => void
      }
      pdf: {
        suggestions: string[]
        suggestionSelectionIndex: number
        suggestionsFocused: boolean
        onDraftChange: (next: string) => void
        onAdd: (value: string) => void
        onRemove: (index: number) => void
      }
      smart: {
        suggestions: string[]
        suggestionSelectionIndex: number
        suggestionsFocused: boolean
        onDraftChange: (next: string) => void
        onRootSubmit: (value: string) => void
      }
    }
    misc: {
      onSeriesDraftChange: (next: string) => void
      onInstructionsDraftChange: (next: string) => void
      onTestDraftChange: (next: string) => void
      onBudgetsMaxContextTokensDraftChange: (next: string) => void
      onBudgetsMaxInputTokensDraftChange: (next: string) => void
      onResumePayloadPathDraftChange: (next: string) => void
      onExportOutPathDraftChange: (next: string) => void
    }
    reasoning: {
      lines: HistoryEntry[]
      visibleRows: number
    }
  }
}
export const useCommandScreenPopupBindings = (
  options: UseCommandScreenPopupBindingsOptions,
): UseCommandScreenPopupBindingsResult => {
  const notify = useStableCallback((message: string) => {
    options.context.notify(message)
  })
  const pushHistory = useStableCallback(
    (content: string, kind: HistoryEntry['kind'] = 'system', format?: HistoryEntry['format']) => {
      options.history.pushHistory(content, kind, format)
    },
  )
  const paste = useCommandScreenPasteBindings({
    inputValue: options.input.value,
    popupState: options.popup.state,
    helpOpen: options.popup.helpOpen,
    setInputValue: options.input.setValue,
    setPasteActive: options.input.setPasteActive,
    consumeSuppressedTextInputChange: options.input.consumeSuppressedTextInputChange,
    suppressNextInput: options.input.suppressNextInput,
    updateLastTypedIntent: options.input.updateLastTypedIntent,
  } satisfies UseCommandScreenPasteBindingsOptions)
  const context = useCommandScreenContextPopupBindings({
    inputValue: options.input.value,
    popupState: options.popup.state,
    helpOpen: options.popup.helpOpen,
    isPopupOpen: options.popup.isOpen,
    isCommandMode: options.menu.isCommandMode,
    isCommandMenuActive: options.menu.isActive,
    isGenerating: options.generation.isGenerating,
    droppedFilePath: options.context.droppedFilePath,
    files: options.context.files,
    urls: options.context.urls,
    images: options.context.images,
    videos: options.context.videos,
    pdfs: options.context.pdfs,
    smartContextEnabled: options.context.smartContextEnabled,
    smartContextRoot: options.context.smartContextRoot,
    addFile: options.context.addFile,
    removeFile: options.context.removeFile,
    addUrl: options.context.addUrl,
    removeUrl: options.context.removeUrl,
    updateUrl: options.context.updateUrl,
    addImage: options.context.addImage,
    removeImage: options.context.removeImage,
    addVideo: options.context.addVideo,
    removeVideo: options.context.removeVideo,
    addPdf: options.context.addPdf,
    removePdf: options.context.removePdf,
    toggleSmartContext: options.context.toggleSmartContext,
    setSmartRoot: options.context.setSmartRoot,
    setInputValue: options.input.setValue,
    setPopupState: options.popup.setState,
    suppressNextInput: options.input.suppressNextInput,
    notify,
    pushHistory,
    addCommandHistoryEntry: options.history.addCommandHistoryEntry,
    handleCommandSelection: options.popup.actions.handleCommandSelection,
    consumeSuppressedTextInputChange: options.input.consumeSuppressedTextInputChange,
  } satisfies UseCommandScreenContextPopupBindingsOptions)
  const historyAndIntent = useCommandScreenHistoryIntentPopupBindings({
    popupState: options.popup.state,
    setPopupState: options.popup.setState,
    closePopup: options.popup.close,
    setInputValue: options.input.setValue,
    consumeSuppressedTextInputChange: options.input.consumeSuppressedTextInputChange,
    suppressNextInput: options.input.suppressNextInput,
    commandHistoryValues: options.history.commandHistoryValues,
  } satisfies UseCommandScreenHistoryIntentPopupBindingsOptions)
  const { modelPopupOptions, modelPopupRecentCount, modelPopupSelection } = useModelPopupData({
    popupState: options.popup.state,
    modelOptions: options.context.modelOptions,
  })
  const { reasoningPopupVisibleRows, reasoningPopupLines } = useReasoningPopup({
    lastReasoning: options.context.lastReasoning,
    terminalColumns: options.context.terminalColumns,
    popupHeight: options.context.reasoningPopupHeight,
  })
  const themePopup = useThemePopupGlue({
    popupState: options.popup.state,
    setPopupState: options.popup.setState,
    closePopup: options.popup.close,
  })
  const themeModePopup = useThemeModePopupGlue({
    popupState: options.popup.state,
    setPopupState: options.popup.setState,
    closePopup: options.popup.close,
  })
  usePopupKeyboardShortcuts({
    popupState: options.popup.state,
    helpOpen: options.popup.helpOpen,
    setPopupState: options.popup.setState,
    closePopup: options.popup.close,
    model: {
      options: modelPopupOptions,
      onSubmit: options.popup.actions.handleModelPopupSubmit,
    },
    toggle: {
      applySelection: options.popup.actions.applyToggleSelection,
    },
    theme: {
      count: themePopup.themeCount,
      onConfirm: themePopup.onThemeConfirm,
      onCancel: themePopup.onThemeCancel,
    },
    themeMode: {
      count: themeModePopup.optionCount,
      onConfirm: themeModePopup.onConfirm,
      onCancel: themeModePopup.onCancel,
    },
    budgets: {
      onSubmit: options.popup.actions.handleBudgetsSubmit,
    },
    file: {
      items: options.context.files,
      suggestions: context.filePopupSuggestions,
      onAdd: context.onAddFile,
      onRemove: context.onRemoveFile,
    },
    url: {
      items: options.context.urls,
      onRemove: context.onRemoveUrl,
    },
    image: {
      items: options.context.images,
      suggestions: context.imagePopupSuggestions,
      onAdd: context.onAddImage,
      onRemove: context.onRemoveImage,
    },
    video: {
      items: options.context.videos,
      suggestions: context.videoPopupSuggestions,
      onAdd: context.onAddVideo,
      onRemove: context.onRemoveVideo,
    },
    pdf: {
      items: options.context.pdfs,
      suggestions: context.pdfPopupSuggestions,
      onAdd: context.onAddPdf,
      onRemove: context.onRemovePdf,
    },
    history: {
      items: historyAndIntent.history.historyPopupItems,
    },
    resume: {
      onSubmit: options.popup.actions.handleResumeSubmit,
    },
    export: {
      onSubmit: options.popup.actions.handleExportSubmit,
    },
    smart: {
      suggestions: context.smartPopupSuggestions,
      contextRoot: options.context.smartContextRoot,
      onRootSubmit: context.onSmartRootSubmit,
    },
    intent: {
      suggestions: historyAndIntent.intent.intentPopupSuggestions,
      onFileSubmit: options.popup.actions.handleIntentFileSubmit,
    },
    reasoning: {
      lines: reasoningPopupLines,
      visibleRows: reasoningPopupVisibleRows,
    },
  })
  const submit = useCommandScreenSubmitBindings({
    popupState: options.popup.state,
    isAwaitingRefinement: options.generation.isAwaitingRefinement,
    submitRefinement: options.generation.submitRefinement,
    isCommandMenuActive: options.menu.isActive,
    selectedCommandId: options.menu.selectedCommandId,
    commandMenuArgsRaw: options.menu.argsRaw,
    isCommandMode: options.menu.isCommandMode,
    intentFilePath: options.input.intentFilePath,
    isGenerating: options.generation.isGenerating,
    expandInputForSubmit: paste.expandInputForSubmit,
    setInputValue: options.input.setValue,
    pushHistory,
    addCommandHistoryEntry: options.history.addCommandHistoryEntry,
    runGeneration: options.generation.runGeneration,
    handleCommandSelection: options.popup.actions.handleCommandSelection,
    handleNewCommand: options.menu.actions.handleNewCommand,
    handleReuseCommand: options.menu.actions.handleReuseCommand,
    lastUserIntentRef: options.input.lastUserIntentRef,
    handleSeriesIntentSubmit: options.popup.actions.handleSeriesIntentSubmit,
    ...(options.popup.openHelp ? { openHelp: options.popup.openHelp } : {}),
  } satisfies UseCommandScreenSubmitBindingsOptions)
  const miscDraftHandlers = useMiscPopupDraftHandlers({
    setPopupState: options.popup.setState,
    consumeSuppressedTextInputChange: options.input.consumeSuppressedTextInputChange,
  })
  const input = useMemo(
    () => ({
      tokenLabel: paste.tokenLabel,
      onChange: paste.handleInputChange,
    }),
    [paste.tokenLabel, paste.handleInputChange],
  )
  const submitGroup = useMemo(
    () => ({
      onSubmit: submit.handleSubmit,
      onSeriesSubmit: submit.onSeriesSubmit,
    }),
    [submit.handleSubmit, submit.onSeriesSubmit],
  )
  const popup = useMemo(
    () => ({
      model: {
        options: modelPopupOptions,
        recentCount: modelPopupRecentCount,
        selection: modelPopupSelection,
        onQueryChange: miscDraftHandlers.onModelPopupQueryChange,
      },
      history: {
        items: historyAndIntent.history.historyPopupItems,
        onDraftChange: historyAndIntent.history.onHistoryPopupDraftChange,
        onSubmit: historyAndIntent.history.onHistoryPopupSubmit,
      },
      intent: {
        suggestions: historyAndIntent.intent.intentPopupSuggestions,
        suggestionSelectionIndex: historyAndIntent.intent.intentPopupSuggestionSelectionIndex,
        suggestionsFocused: historyAndIntent.intent.intentPopupSuggestionsFocused,
        onDraftChange: historyAndIntent.intent.onIntentPopupDraftChange,
      },
      context: {
        file: {
          suggestions: context.filePopupSuggestions,
          suggestionSelectionIndex: context.filePopupSuggestionSelectionIndex,
          suggestionsFocused: context.filePopupSuggestionsFocused,
          onDraftChange: context.onFilePopupDraftChange,
          onAdd: context.onAddFile,
          onRemove: context.onRemoveFile,
        },
        url: {
          onDraftChange: context.onUrlPopupDraftChange,
          onAdd: context.onAddUrl,
          onRemove: context.onRemoveUrl,
        },
        image: {
          suggestions: context.imagePopupSuggestions,
          suggestionSelectionIndex: context.imagePopupSuggestionSelectionIndex,
          suggestionsFocused: context.imagePopupSuggestionsFocused,
          onDraftChange: context.onImagePopupDraftChange,
          onAdd: context.onAddImage,
          onRemove: context.onRemoveImage,
        },
        video: {
          suggestions: context.videoPopupSuggestions,
          suggestionSelectionIndex: context.videoPopupSuggestionSelectionIndex,
          suggestionsFocused: context.videoPopupSuggestionsFocused,
          onDraftChange: context.onVideoPopupDraftChange,
          onAdd: context.onAddVideo,
          onRemove: context.onRemoveVideo,
        },
        pdf: {
          suggestions: context.pdfPopupSuggestions,
          suggestionSelectionIndex: context.pdfPopupSuggestionSelectionIndex,
          suggestionsFocused: context.pdfPopupSuggestionsFocused,
          onDraftChange: context.onPdfPopupDraftChange,
          onAdd: context.onAddPdf,
          onRemove: context.onRemovePdf,
        },
        smart: {
          suggestions: context.smartPopupSuggestions,
          suggestionSelectionIndex: context.smartPopupSuggestionSelectionIndex,
          suggestionsFocused: context.smartPopupSuggestionsFocused,
          onDraftChange: context.onSmartPopupDraftChange,
          onRootSubmit: context.onSmartRootSubmit,
        },
      },
      misc: {
        onSeriesDraftChange: miscDraftHandlers.onSeriesDraftChange,
        onInstructionsDraftChange: miscDraftHandlers.onInstructionsDraftChange,
        onTestDraftChange: miscDraftHandlers.onTestDraftChange,
        onBudgetsMaxContextTokensDraftChange:
          miscDraftHandlers.onBudgetsMaxContextTokensDraftChange,
        onBudgetsMaxInputTokensDraftChange: miscDraftHandlers.onBudgetsMaxInputTokensDraftChange,
        onResumePayloadPathDraftChange: miscDraftHandlers.onResumePayloadPathDraftChange,
        onExportOutPathDraftChange: miscDraftHandlers.onExportOutPathDraftChange,
      },
      reasoning: {
        lines: reasoningPopupLines,
        visibleRows: reasoningPopupVisibleRows,
      },
    }),
    [
      modelPopupOptions,
      modelPopupRecentCount,
      modelPopupSelection,
      miscDraftHandlers.onModelPopupQueryChange,
      historyAndIntent.history.historyPopupItems,
      historyAndIntent.history.onHistoryPopupDraftChange,
      historyAndIntent.history.onHistoryPopupSubmit,
      historyAndIntent.intent.intentPopupSuggestions,
      historyAndIntent.intent.intentPopupSuggestionSelectionIndex,
      historyAndIntent.intent.intentPopupSuggestionsFocused,
      historyAndIntent.intent.onIntentPopupDraftChange,
      context.filePopupSuggestions,
      context.filePopupSuggestionSelectionIndex,
      context.filePopupSuggestionsFocused,
      context.onFilePopupDraftChange,
      context.onAddFile,
      context.onRemoveFile,
      context.onUrlPopupDraftChange,
      context.onAddUrl,
      context.onRemoveUrl,
      context.imagePopupSuggestions,
      context.imagePopupSuggestionSelectionIndex,
      context.imagePopupSuggestionsFocused,
      context.onImagePopupDraftChange,
      context.onAddImage,
      context.onRemoveImage,
      context.videoPopupSuggestions,
      context.videoPopupSuggestionSelectionIndex,
      context.videoPopupSuggestionsFocused,
      context.onVideoPopupDraftChange,
      context.onAddVideo,
      context.onRemoveVideo,
      context.pdfPopupSuggestions,
      context.pdfPopupSuggestionSelectionIndex,
      context.pdfPopupSuggestionsFocused,
      context.onPdfPopupDraftChange,
      context.onAddPdf,
      context.onRemovePdf,
      context.smartPopupSuggestions,
      context.smartPopupSuggestionSelectionIndex,
      context.smartPopupSuggestionsFocused,
      context.onSmartPopupDraftChange,
      context.onSmartRootSubmit,
      miscDraftHandlers.onSeriesDraftChange,
      miscDraftHandlers.onInstructionsDraftChange,
      miscDraftHandlers.onTestDraftChange,
      miscDraftHandlers.onBudgetsMaxContextTokensDraftChange,
      miscDraftHandlers.onBudgetsMaxInputTokensDraftChange,
      reasoningPopupLines,
      reasoningPopupVisibleRows,
    ],
  )
  return useMemo(
    () => ({
      input,
      submit: submitGroup,
      popup,
    }),
    [input, submitGroup, popup],
  )
}
</file>

<file path="README.md">
# Prompt Maker CLI

Terminal-first prompt generator with a built-in Ink TUI. It turns rough intent + optional context (files, URLs, smart context, images, videos, PDFs) into a structured **prompt contract**, with optional polishing, streaming telemetry, and automatic history logging.

Highlights:

- **Generate workflow**: run from flags or from the TUI.
- **Context ingestion**: file globs (`--context`), URLs (`--url`, including GitHub trees), optional smart context (`--smart-context`), and media (`--image`, `--video`, `--pdf`).
- **Auditable runs**: token telemetry, token budgets (optional trimming), structured stream events (`--stream jsonl`), and JSONL history (`~/.config/prompt-maker-cli/history.jsonl`) with export + resume workflows.
- **TUI-first**: `prompt-maker-cli` with no args launches the TUI.

The project is **TUI-first**:

- `prompt-maker-cli` with **no args** launches the TUI.
- Use the same pipelines via flags (`generate`) or tests (`test`) when you need automation.

## Table of Contents

- [Diagram Index](#diagram-index)
- [Quickstart](#quickstart)
- [CLI Modes and Routing](#cli-modes-and-routing)
- [TUI Mode (recommended)](#tui-mode-recommended)
  - [Launch](#launch)
  - [Keybindings](#keybindings)
  - [Input Routing Invariant](#input-routing-invariant)
  - [Command Palette (`/commands`)](#command-palette-commands)
  - [Series Generation (“atomic prompts”)](#series-generation-atomic-prompts)
  - [TUI Theming](#tui-theming)
- [Generate Mode (CLI)](#generate-mode-cli)
  - [Common Workflows](#common-workflows)
  - [Key Flags](#key-flags)
  - [Conceptual Architecture](#conceptual-architecture)
  - [Intent Intake](#intent-intake)
  - [Context Ingestion](#context-ingestion)
  - [Remote URL and GitHub Context](#remote-url-and-github-context)
  - [Smart Context (Local RAG)](#smart-context-local-rag)
  - [Media Attachments](#media-attachments)
  - [Streaming Events and Telemetry](#streaming-events-and-telemetry)
  - [Interactive Refinement](#interactive-refinement)
  - [Polish Pass](#polish-pass)
  - [Output Artifacts](#output-artifacts)
- [Prompt Tests](#prompt-tests)
- [TUI Architecture and UX Model](#tui-architecture-and-ux-model)
- [Provider Configuration](#provider-configuration)
- [Outputs and Persistence](#outputs-and-persistence)
- [Development](#development)
- [Maintainer References](#maintainer-references)
- [License](#license)

## Diagram Index

All diagrams are GitHub Mermaid compatible and grounded in:

- `docs/neovim-plugin-integration.md` (pipeline, streaming events, transports, context/media details)
- `docs/tui-design.md` (TUI architecture, UX invariants)
- This README’s CLI/TUI behavior sections

CLI + Generate Pipeline:

- [D1. CLI Routing Flow](#d1-cli-routing-flow) — how `ui` / `generate` / `test` are selected.
- [D2. Generate Pipeline Overview](#d2-generate-pipeline-overview) — end-to-end generate workflow, including interactive and polish.
- [D3. `--stream jsonl` Event Timeline](#d3---stream-jsonl-event-timeline) — typical JSONL event emission order.
- [D4. `--interactive-transport` Lifecycle](#d4---interactive-transport-lifecycle) — socket/pipe setup, commands, mirrored events.
- [D5. Interactive Phase State Machine](#d5-interactive-phase-state-machine) — phases emitted via `interactive.state`.

Context + Media:

- [D6. Context Ingestion Decision Tree](#d6-context-ingestion-decision-tree) — how sources merge and when failures warn vs fail.
- [D7. Smart Context Workflow](#d7-smart-context-workflow) — scan → cache/index → top‑k → de-dupe append.
- [D8. URL + GitHub Context Resolution](#d8-url--github-context-resolution) — remote fetch behavior and safety limits.
- [D9. Media Handling (Images + Videos + PDFs)](#d9-media-handling-images--videos--pdfs) — image parts, video uploads, and PDF attachment behavior.

TUI:

- [D10. TUI Architecture Map](#d10-tui-architecture-map) — AppContainer + screens + popups + keymaps.
- [D11. TUI Input Routing Priority](#d11-tui-input-routing-priority) — help overlay > popup > screen > globals.

Other:

- [D12. `/series` Generation and Filesystem Outputs](#d12-series-generation-and-filesystem-outputs) — atomic prompt series, write-on-best-effort.
- [D13. Theme Resolution Precedence](#d13-theme-resolution-precedence) — theme search order and overriding.
- [D14. Prompt Test Runner Flow](#d14-prompt-test-runner-flow) — `test` mode and the TUI Test Runner view.

---

## Quickstart

From repo root:

```bash
npm ci
npm run build

# TUI (default if no args)
npm start

# Generate from a one-shot intent
npm start -- "Draft a confident onboarding-bot spec" --model gpt-4o-mini

# Explicit commands
npm start -- ui
npm start -- test prompt-tests.yaml

# After at least one generate run, export the last payload
npm start -- export --format json --out runs/last-run.json

# Deterministic (non-LLM) composition scaffold
npm start -- compose --recipe docs/tui-design.md --input "Draft a summary" > composed.txt
```

Global install from a local checkout:

```bash
npm install -g .

# Now you can run:
prompt-maker-cli
```

## CLI modes and routing

`prompt-maker-cli` has five top-level modes:

- `ui`: Ink TUI
- `generate` (default for non-`ui`/`test`/`export`/`compose` args): prompt generation pipeline
- `test`: prompt test runner
- `export`: export a past generate payload from history
- `compose`: deterministic (non-LLM) prompt composition scaffold

Routing rules:

- No args → `ui`
- First arg `ui` → `ui`
- First arg `test` → `test`
- First arg `export` → `export`
- First arg `compose` → `compose`
- First arg `generate` or `expand` → `generate` (`expand` is an alias)
- Anything else (including flags like `--json`) → `generate`

Help behavior:

- `prompt-maker-cli --help` shows **generate** help only.
- Use `prompt-maker-cli export --help` / `prompt-maker-cli compose --help` for subcommand help.

### D1. CLI Routing Flow

Shows how argv maps to top-level modes.

```mermaid
flowchart TD
  A[Start: argv] --> B{No args?}
  B -- Yes --> UI[ui]
  B -- No --> C[First token]

  C --> T{"first == 'test'?"}
  T -- Yes --> TEST[test]
  T -- No --> U{"first == 'ui'?"}
  U -- Yes --> UI
  U -- No --> E{"first == 'export'?"}
  E -- Yes --> EXP[export]
  E -- No --> P{first == 'compose'?}
  P -- Yes --> COM[compose]
  P -- No --> G{first == 'generate' or 'expand'?}
  G -- Yes --> GEN[generate]
  G -- No --> F{first startsWith '-'?}
  F -- Yes --> GEN
  F -- No --> GEN
```

## TUI mode (recommended)

The Ink TUI is the fastest way to iterate on prompts interactively:

- One “generate” view with a scrollable history + a single input bar.
- Command palette + popups for models, context, settings, and themes.
- Built-in Test Runner view.
- Session-oriented UX (reuse last prompt, view token breakdown, show last reasoning, etc.).

Reference: `docs/tui-design.md`.

### Launch

```bash
# Default: the TUI (no args)
prompt-maker-cli

# Explicit entry
prompt-maker-cli ui

# Start TUI with an interactive transport socket/pipe
prompt-maker-cli ui --interactive-transport /tmp/pmc.sock
```

Important caveats:

- **TTY required**: Ink needs raw-mode input. Running the TUI without a real TTY will throw an Ink “Raw mode is not supported” error.
- **Platform/terminal support**: the TUI has only been tested on **macOS** using the **Kitty** terminal app. Running on Windows (or in other terminals) might pose problems.
- **Minimal arg parsing**: `ui` only parses `--interactive-transport`. `prompt-maker-cli ui --help` is not implemented.
- **FZF matching**: the TUI uses the npm `fzf` library for fuzzy matching (bundled via `npm install`; no separate `fzf` binary is required).

### Keybindings

Global controls:

- `Ctrl+G`: open command palette (Generate view)
- `Ctrl+T`: switch to Test Runner view
- `/help`: open help overlay
- `Ctrl+C` or `/exit`: exit
- `Esc`: dismiss popups/menus (never exits)

Generate view:

- Type normal text + `Enter`: generate
- Type `/`: enter command mode (filters the palette)
- `↑/↓`: scroll history (when no help/popup is active)
- `Tab`:
  - If the input looks like a dropped absolute file path, it adds that file as context.
  - Otherwise, it opens the `/series` flow.

Test Runner view:

- `Tab` / `Shift+Tab`: change focus
- `Enter`:
  - in file input: moves focus to actions
  - in actions: runs tests

### Input routing invariant

Key handling priority (highest wins):

1. Help overlay
2. Popup input
3. Active screen input
4. AppContainer global keys

This prevents “fallthrough” where one key triggers multiple layers.

### D11. TUI Input Routing Priority

Shows the key handling priority order as an explicit routing gate.

```mermaid
flowchart TD
  K[Keypress] --> H{Help overlay open?}
  H -- Yes --> HNDH["Help overlay handles key<br>(suppress everything else)"]
  H -- No --> P{Popup open?}

  P -- Yes --> HNDP["Active popup handles key<br>(no fallthrough)"]
  P -- No --> S{Active screen handles key?}

  S -- Yes --> HNDS["Screen handles key<br>(e.g., history scroll, submit)"]
  S -- No --> G{Global keybind?}

  G -- Yes --> HNDG["AppContainer global key<br>(exit, toggle views)"]
  G -- No --> IGN[Ignored]
```

### Command palette (`/commands`)

Open the palette with `Ctrl+G` or type `/` in the Generate input.

Commands:

| Command                   | Args                     | What it does                                                                                        |
| ------------------------- | ------------------------ | --------------------------------------------------------------------------------------------------- |
| `/help`                   | -                        | Show keyboard shortcuts (help overlay)                                                              |
| `/model`                  | -                        | Pick the generation model (popup)                                                                   |
| `/target`                 | -                        | Pick the target/runtime model (recorded; not included in prompt text) (popup)                       |
| `/polish`                 | `off\|clear\|--clear`    | Clear polish model, or pick the polish model (popup)                                                |
| `/intent`                 | `[path]`                 | Set an intent file path, or pick a file (popup)                                                     |
| `/meta` / `/instructions` | `[text]`                 | Set meta instructions, or open the meta editor (popup)                                              |
| `/new`                    | -                        | Reset session state                                                                                 |
| `/reuse`                  | -                        | Reset session and reuse last prompt                                                                 |
| `/file`                   | -                        | Add local file context (popup)                                                                      |
| `/url`                    | `[url ...]`              | Add one or more HTTP(S) URLs inline, or open the URL popup                                          |
| `/smart`                  | `on\|off`                | Toggle smart context                                                                                |
| `/smart-root`             | `<path>\|--clear\|clear` | Set/clear smart-context scan root (may auto-enable smart context)                                   |
| `/image`                  | `[path]`                 | Add an image path inline, or open the image popup                                                   |
| `/video`                  | `[path]`                 | Add a video path inline, or open the video popup                                                    |
| `/copy`                   | `[on\|off]`              | Toggle auto-copy, or open the toggle popup                                                          |
| `/chatgpt`                | `[on\|off]`              | Toggle auto-open-ChatGPT, or open the toggle popup                                                  |
| `/json`                   | `[on\|off]`              | Toggle showing the JSON payload in history (TUI-only; blocked when interactive transport is active) |
| `/tokens`                 | -                        | Show token breakdown (popup)                                                                        |
| `/budgets`                | -                        | Configure token budgets and overflow strategy (popup)                                               |
| `/resume`                 | -                        | Resume generation from history or an exported payload (popup)                                       |
| `/export`                 | -                        | Export a selected history payload to JSON/YAML (popup)                                              |
| `/settings`               | -                        | Show settings (popup)                                                                               |
| `/theme`                  | -                        | Theme picker (preview with arrows, `Enter` confirm, `Esc` cancel) (popup)                           |
| `/theme-mode`             | -                        | Theme mode picker (popup)                                                                           |
| `/reasoning` / `/why`     | -                        | Show last model reasoning (popup, when available)                                                   |
| `/history`                | -                        | Show command history (popup)                                                                        |
| `/series`                 | `[draft text]`           | Generate a set of standalone “atomic prompts”                                                       |
| `/test`                   | `[file]`                 | Run prompt tests (with a file arg) or open the test popup                                           |
| `/exit`                   | -                        | Exit the app                                                                                        |

Notes:

- `/theme-mode` opens a popup (it does not parse inline args).
- `/json` inside the TUI only toggles whether a JSON payload is shown in the history pane; it does not enable generate-mode `--json`.
- `/json` is blocked when `prompt-maker-cli ui --interactive-transport ...` is active.

#### Workflow popups (`/budgets`, `/resume`, `/export`)

These workflow commands open popups (no inline args parsing) and persist defaults back into CLI config (`src/config.ts`):

- `/budgets`: sets token budgets + overflow strategy.
  - Persists: `promptGenerator.maxInputTokens`, `promptGenerator.maxContextTokens`, `promptGenerator.contextOverflowStrategy`
  - Defaulting: when budgets are enabled and overflow is unset, effective overflow defaults to `fail` (`src/tui/budget-settings.ts`).
- `/resume`: resumes from a selected history entry or an exported payload file.
  - Persists: `resumeMode` (`best-effort` | `strict`), `resumeSourceKind` (`history` | `file`)
  - Behavior: resume runs can start with an empty typed intent (the resumed payload supplies intent) (`src/tui/hooks/useGenerationPipeline.ts`).
  - Context reuse: only `source:"file"` context paths are reusable; `url`/`smart` are treated as missing (`src/generate/pipeline.ts`).
- `/export`: exports a selected history payload to JSON/YAML.
  - Persists: `exportFormat` (`json` | `yaml`), `exportOutDir` (directory path)
  - Schema gating: exporting a history entry with an unsupported `schemaVersion` fails with an actionable error (`src/history/generate-history.ts`).

### Series generation (“atomic prompts”)

`/series` (or `Tab`) generates:

- `00-overview.md`
- One file per atomic prompt step (e.g. `01-...md`, `02-...md`, ...)

Artifacts are written under:

- `generated/series/<timestamp>-<intent-slug>/`

If the output directory cannot be created (permissions, read-only filesystem, etc.), the series still generates but won’t be saved.

### D12. `/series` Generation and Filesystem Outputs

Shows the `/series` happy path and the best-effort write behavior.

<details>
<summary>Diagram</summary>

```mermaid
flowchart TD
  A["User triggers /series<br>(or presses Tab)"] --> B["Collect draft text<br>(or use current input)"]
  B --> C["Generate atomic prompt steps<br>(no cross references)"]
  C --> D[Render files<br>00-overview + 01..N steps]

  D --> E{Can create output dir?<br> generated/series/<timestamp>-<slug>/}
  E -- Yes --> W[Write markdown files]

  E -- No --> NW[Skip writes; keep results in-session]

  W --> DONE[Series ready]
  NW --> DONE
```

</details>

### TUI theming

Inside the TUI:

- `/theme` opens the theme picker.
- `/theme-mode` switches appearance mode (`dark`, `light`, or `system`).

Theme settings (and some workflow defaults like budgets/resume/export) persist to CLI config.

Theme JSON format:

- Top-level `theme` object (required)
- Top-level `defs` object (optional)
- Canonical schema: `src/tui/theme/theme-types.ts`

#### Custom themes

Theme files are plain JSON. The theme name is the filename without `.json`.

- **Global** (per-user): `~/.config/prompt-maker-cli/themes/*.json`
- **Project-local**: `.prompt-maker-cli/themes/*.json`
  - Parent directories are also scanned (walking up from the current working directory).

Quick start:

```bash
# Global install
mkdir -p ~/.config/prompt-maker-cli/themes
cp src/tui/theme/examples/ocean-example.json ~/.config/prompt-maker-cli/themes/ocean.json

# Or project-local
mkdir -p .prompt-maker-cli/themes
cp src/tui/theme/examples/ocean-example.json .prompt-maker-cli/themes/ocean.json
```

Restart the TUI, then run `/theme` and select `ocean`.

#### Theme precedence

If multiple themes share the same name:

1. Project-local themes in the nearest directory to your CWD (highest precedence)
2. Project-local themes in ancestor directories
3. Global themes (`~/.config/prompt-maker-cli/themes`)
4. Built-in themes (lowest precedence)

### D13. Theme Resolution Precedence

Shows the lookup order used when multiple themes share a name.

<details>
<summary>Diagram</summary>

```mermaid
flowchart TD
  A["Resolve theme name<br>(from config: theme)"] --> B[Search sources by precedence]

  B --> P1["1) Project-local themes<br>nearest .prompt-maker-cli/themes"]
  P1 --> M1{Found theme with name?}
  M1 -- Yes --> USE[Use that theme]
  M1 -- No --> P2["2) Project-local themes<br>ancestor directories"]

  P2 --> M2{Found?}
  M2 -- Yes --> USE
  M2 -- No --> G["3) Global themes<br>~/.config/prompt-maker-cli/themes"]

  G --> M3{Found?}
  M3 -- Yes --> USE
  M3 -- No --> BI["4) Built-in themes"]

  BI --> USE
```

</details>

#### Theme mode (`system`)

`system` is intentionally pragmatic:

- If `TERM_BACKGROUND` is set to `light`/`dark`, it is used.
- Else, we try to infer from `COLORFGBG`.
- If no reliable signal is present, we deterministically fall back to `dark`.
- Config also accepts `themeMode: "auto"` as an alias for `"system"`.

---

## Generate mode (CLI)

Generate consumes intent from one of:

- Positional string: `prompt-maker-cli "..."`
- File: `--intent-file path/to/intent.md`
- `stdin`: pipe into the command

### Common workflows

```bash
# Inline intent + context files + copy to clipboard
prompt-maker-cli "Draft a confident onboarding-bot spec" \
  --model gpt-4o-mini \
  --context docs/spec/**/*.md \
  --image assets/wireframe.png \
  --copy

# URL context (web pages or GitHub)
prompt-maker-cli "Summarize the docs" \
  --url https://example.com/docs \
  --url https://github.com/example/repo/tree/main/docs

# Smart context (local embeddings) + scoped scan root
prompt-maker-cli "Explain this module" \
  --smart-context \
  --smart-context-root src

# JSON payload capture (non-interactive only)
prompt-maker-cli --intent-file drafts/travel.md --json > runs/travel.json

# Export last payload from history (portable JSON/YAML)
prompt-maker-cli export --format json --out runs/last-run.json

# Resume from an exported payload (reuses prompt + refinements; best-effort on missing files)
prompt-maker-cli "" --resume-from runs/last-run.json --resume-mode best-effort --quiet --stream jsonl --progress=false

# Stream progress/events as JSONL (use --quiet to avoid mixing text output)
prompt-maker-cli "Summarize" --stream jsonl --quiet > runs/events.jsonl
```

### Key flags

| Flag / Input                                | Purpose                                                                               |
| ------------------------------------------- | ------------------------------------------------------------------------------------- |
| `<intent>` / `--intent-file <path>` / stdin | Provide the rough intent text. Pipe stdin when automating.                            |
| `-c, --context <glob>` (repeatable)         | Attach local file(s) to the request; globs use `fast-glob` (`dot: true`).             |
| `--url <https://...>` (repeatable)          | Download remote pages or GitHub content and attach as virtual context.                |
| `--smart-context`                           | Attach additional relevant local files via embeddings search.                         |
| `--smart-context-root <path>`               | Limit smart-context scanning to a specific directory (default: CWD).                  |
| `--image <path>` (repeatable)               | Attach reference images (PNG/JPG/JPEG/WEBP/GIF, ≤20MB).                               |
| `--video <path>` (repeatable)               | Attach reference videos (Gemini only; non-Gemini models are auto-switched).           |
| `--pdf <path>` (repeatable)                 | Attach reference PDFs (Gemini native; OpenAI best-effort fallback).                   |
| `--model <name>`                            | Override the generation model used by the CLI.                                        |
| `--target <name>`                           | Target/runtime model recorded in JSON/history (not included in prompt text).          |
| `-i, --interactive`                         | Enable a TTY refinement loop (requires a TTY).                                        |
| `--interactive-transport <path>`            | Listen on a Unix socket / Windows named pipe for `refine`/`finish` commands.          |
| `--resume-last`                             | Resume from the last JSONL history entry.                                             |
| `--resume <selector>`                       | Resume from history selector (`last`, `last:N`, or `N`-th from end).                  |
| `--resume-from <path>`                      | Resume from an exported payload file (`.json` / `.yaml` / `.yml`).                    |
| `--resume-mode strict\|best-effort`         | How to handle missing resumed context paths.                                          |
| `--polish`, `--polish-model <name>`         | Run the finishing pass and optionally choose a different model.                       |
| `--json`                                    | Emit machine-readable JSON (non-interactive only).                                    |
| `--stream none\|jsonl`                      | Emit newline-delimited JSON events to stdout.                                         |
| `--max-input-tokens <n>`                    | Cap total input tokens (intent + system + text context).                              |
| `--max-context-tokens <n>`                  | Cap tokens reserved for text context entries (file/url/smart).                        |
| `--context-overflow <strategy>`             | Overflow handling: `fail`, `drop-smart`, `drop-url`, `drop-largest`, `drop-oldest`.   |
| `--quiet`                                   | Suppress human-oriented output (banners/telemetry/boxed prompts).                     |
| `--progress/--no-progress`                  | Enable/disable progress spinners (spinners are disabled during interactive sessions). |
| `--show-context`                            | Print resolved context payload before generation.                                     |
| `--context-file <path>`                     | Write resolved context to disk (same format as `--show-context`).                     |
| `--context-format text\|json`               | Choose how `--show-context`/`--context-file` render the context payload.              |
| `--context-template <name>`                 | Wrap the final prompt using a named template (built-in: `nvim`).                      |
| `--copy`, `--open-chatgpt`                  | Copy/open the final artifact for quick sharing.                                       |

Notes:

- `--json` cannot be combined with interactive refinement (`--interactive` or `--interactive-transport`).
- `--show-context` prints to **stderr** when `--json` is enabled (so stdout stays machine-readable).
- `--stream jsonl` is designed for machine consumption; for clean JSONL output on stdout use `--quiet` and avoid other human-output flags.
- Token budgets apply only to **text** context entries (`--context`, `--url`, `--smart-context`); images/videos/pdfs are not trimmed by these strategies.
- `DEBUG=1` or `VERBOSE=1` prints the model’s `reasoning` (if provided) to stderr.

### Conceptual architecture

Generate mode assembles a “prompt contract” from:

- **Intent** text
- **Resolved context** (local files, remote URLs/GitHub, optional smart context)
- **Optional media** parts (images, videos)
- **Model choices** (generation model and recorded target model)

Then it iterates (optionally interactively), optionally runs a polish pass, emits events, and writes history.

### D2. Generate Pipeline Overview

Summarizes the full “generate” path, including interactive refinement and the polish pass.

<details>
<summary>Diagram</summary>

```mermaid
flowchart TD
  A[Start: generate invocation] --> I["Intent intake<br>(positional | --intent-file | stdin)"]
  I --> V{Intent valid?<br>UTF-8, no NUL, <=512KB}
  V -- No --> FATAL["Fail fast<br>(intent required / invalid)"]
  V -- Yes --> C[Resolve context blocks]

  C --> CF[Local file context<br>--context globs]
  C --> CU["Remote URL context<br>--url http(s)"]
  C --> CG[GitHub URL expansion<br>--url github.com/...]
  C --> CS[Optional smart context<br>--smart-context]
  C --> CM[Optional media<br>--image / --video / --pdf]

  CF --> DEDUPE[De-dupe / merge context paths]
  CU --> DEDUPE
  CG --> DEDUPE
  CS --> DEDUPE
  CM --> DEDUPE

  DEDUPE --> T["Token telemetry<br>(intent + context)"]
   T --> BUDGET{Token budgets enabled?<br>--max-* or config}

   BUDGET -- No --> G1[Generation iteration 1<br>PromptGeneratorService.generatePrompt]
   BUDGET -- Yes --> TRIM{Context exceeds budget?}
   TRIM -- No --> G1
   TRIM -- Yes --> PRUNE["Trim text context per strategy<br>(context.overflow)"]
   PRUNE --> G1

  G1 --> INT{Interactive?<br>--interactive or --interactive-transport}
  INT -- No --> DONEITER[Stop refining]

  INT -- Yes --> LOOP["Interactive loop<br>(transport or TTY)"]
  LOOP --> GREF["Generation iteration N<br>(buildRefinementMessage)"]
  GREF --> LOOP
  LOOP --> DONEITER

  DONEITER --> P{Polish enabled?<br>--polish}
  P -- Yes --> POL["polishPrompt<br>(optional --polish-model)"]
  P -- No --> SKIP[Skip polish]

  POL --> OUT["Assemble output payload<br>(render template, history append)"]
  SKIP --> OUT

  OUT --> FINAL["Deliver artifacts<br>(prompt + renderedPrompt + JSON/history)"]
```

</details>

---

### Intent intake

Intent sources (exactly one must yield content):

- Inline argument
- `--intent-file <path>`
- stdin

Intent file constraints (used by integrations as guardrails):

- Must be UTF‑8 text
- Must not contain NUL bytes
- Size cap: 512 KB

### Context ingestion

Context can be attached from:

- Local file globs: `--context` (repeatable)
- Remote URLs: `--url` (repeatable)
- Smart context: `--smart-context` (+ `--smart-context-root`)
- Media: `--image`, `--video`, `--pdf`

#### D6. Context Ingestion Decision Tree

Shows how sources are resolved, merged, and how errors are classified (warnings vs fatal).

<details>
<summary>Diagram</summary>

```mermaid
flowchart TD
  A[Begin context resolution] --> B["Resolve user intent<br>(positional | --intent-file | stdin)"]

  B --> C{Has non-empty intent text?}
  C -- No --> F[Fail: intent text required]
  C -- Yes --> D[Initialize context list]

  D --> E1[Expand --context globs<br>fast-glob dot:true]
  E1 --> E1M{Any matches?}
  E1M -- No --> W1["Warn: glob matched nothing<br>(non-fatal)"]
  E1M -- Yes --> E1R[Read files<br>attach as <file path="...">]

  D --> E2["Resolve --url entries<br>http(s) only"]
  E2 --> E2P{Protocol ok?}
  E2P -- No --> W2["Warn: non-http(s) URL<br>(skip)"]
  E2P -- Yes --> E2D[Download <=1MB<br>HTML -> text]
  E2D --> E2OK{Fetch ok?}
  E2OK -- No --> W3["Warn: URL fetch failed<br>(non-fatal)"]
  E2OK -- Yes --> E2R[Attach as virtual file<br>path: url:<url>]

  D --> E3["Recognize GitHub URLs<br>(blob/tree/repo root)"]
  E3 --> E3R["Expand with limits<br><=60 files, <=64KB each"]
  E3R --> E3OK{Fetch ok?}
  E3OK -- No --> W4["Warn: GitHub fetch failed<br>(non-fatal)"]
  E3OK -- Yes --> E3A["Attach expanded files<br>(ignore dist, node_modules, lockfiles..."]

  D --> E4{--smart-context enabled?}
  E4 -- No --> SKIPSC[Skip smart context]
  E4 -- Yes --> SC[Scan/index/search local files<br><=25KB]

  D --> E5[Media attachments<br>--image / --video / --pdf]
  E5 --> E5W[Unsupported/oversize -> warn + skip]

  E1R --> MERGE["De-dupe context paths<br>(avoid duplicates across sources)"]
  E2R --> MERGE
  E3A --> MERGE
  SC --> MERGE
  SKIPSC --> MERGE
  E5W --> MERGE

  MERGE --> DONE["Context ready<br>(used for telemetry + generation)"]
```

</details>

### Remote URL and GitHub context

- `--url` supports `http:`/`https:` only.
- Each URL becomes a virtual context file (`path: url:<url>`).
- HTML pages are capped at 1MB and converted to text (scripts/styles stripped).
- GitHub URLs are recognized for repo root, `tree`, and `blob` forms.
- GitHub safety limits:
  - ≤60 files
  - ≤64KB each
  - ignore lists (e.g., `node_modules`, `dist`, lockfiles, archives)
- Optional `GITHUB_TOKEN` improves rate limits.

#### D8. URL + GitHub Context Resolution

Summarizes remote context paths and the GitHub expansion safeguards.

<details>
<summary>Diagram</summary>

```mermaid
flowchart TD
  A[--url <value>] --> B{Is github.com URL?}

  B -- No --> U[Generic URL fetch]

  U --> U1{"Protocol http(s)?"}
  U1 -- No --> UW[Warn + skip]
  U1 -- Yes --> U2[Download <=1MB]
  U2 --> U3{HTML?}
  U3 -- Yes --> U4["Convert HTML to text<br>(html-to-text)"]
  U3 -- No --> U5[Use response body as text]
  U4 --> UA[Attach virtual file<br>path: url:<url>]
  U5 --> UA

  B -- Yes --> G[GitHub resolver]

  G --> G1["Detect form<br>repo root | tree | blob"]
  G1 --> G2[Enumerate targets<br>apply ignore lists]

  G2 --> G3[Fetch contents]

  G3 --> G4{Within safety limits?<br><=60 files, <=64KB each}
  G4 -- No --> GL["Stop at limits<br>(skip extras)"]
  G4 -- Yes --> GA["Attach expanded files<br>(as virtual context entries)"]

  GL --> DONE[Context additions complete]
  GA --> DONE
  UA --> DONE
```

</details>

### Smart context (Local RAG)

Smart context attaches additional relevant local files based on embeddings.

- Trigger: `--smart-context` (optionally with `--smart-context-root <dir>`)
- Scan glob: `**/*.{ts,tsx,js,jsx,py,md,json}` (excludes `node_modules`, build outputs, lockfiles, git metadata)
- File cap: skip files larger than 25KB
- Cache: `~/.config/prompt-maker-cli/embeddings_cache.json`
- Select: top‑k results (default 5) against the intent string
- De-dupe: do not append files already attached via user context

#### D7. Smart Context Workflow

Shows the scan/index/search loop and where caching and de-dupe happen.

<details>
<summary>Diagram</summary>

```mermaid
flowchart TD
  A["--smart-context enabled] --> B[Choose scan root<br>(--smart-context-root or CWD)"]
  B --> C[Scan files<br>fast-glob include extensions]

  C --> D[Filter<br>exclude node_modules/dist/...<br>skip >25KB]
  D --> E["Index embeddings<br>(cache by SHA256)"]
  E --> F[Search top-k<br>against intent text]

  F --> G[Read selected files]

  G --> H[Append unique files<br>skip ones already in user context]
  H --> DONE[Smart context ready<br>merged into generation context]
```

</details>

### Media attachments

- **Images** (`--image`): PNG/JPG/JPEG/WEBP/GIF up to 20 MB. Encoded into image parts.
- **Videos** (`--video`): Gemini-only.
  - If any `--video` is present and the requested model is not Gemini, the CLI auto-switches to a Gemini model (`gemini-3-pro-preview` or configured default).
  - Uploads use Google’s Files API and poll until the file becomes `ACTIVE`.
  - Upload progress emits stream events.
- **PDFs** (`--pdf`): Gemini native + OpenAI text extraction.
  - Gemini: uploaded via Files API and sent as `application/pdf` `fileData` parts.
  - OpenAI: extracts text from PDFs in-process (uses `pdfjs-dist`, no external binaries). Configure `PROMPT_MAKER_PDF_MAX_PAGES`, `PROMPT_MAKER_PDF_MAX_TEXT_CHARS`, and `PROMPT_MAKER_PDF_MAX_STREAMS`.

#### D9. Media Handling (Images + Videos + PDFs)

Shows validation, model switching for videos, and PDF attachment behavior.

<details>
<summary>Diagram</summary>

```mermaid
flowchart TD
  A[Media flags present?] --> B{--image provided?}

  B -- Yes --> I1[Validate image<br>ext + <=20MB]
  I1 --> IOK{Valid?}
  IOK -- No --> IW[Warn + skip]
  IOK -- Yes --> I2["Read + Base64 encode<br>(build image parts)"]

  B -- No --> I0[No image work]

  A --> C{--video provided?}
  C -- No --> V0[No video work]

  C -- Yes --> V1{Is selected model Gemini?}
  V1 -- No --> VSW[Auto-switch generation model<br>-> Gemini video-capable default]
  V1 -- Yes --> V2["Upload video<br>(GoogleAIFileManager)"]

  VSW --> V2
  V2 --> V3["Poll until ACTIVE<br>(or fail with error)"]
  V3 --> VOK{Upload ok?}
  VOK -- No --> VW["Warn/Fail (upload error)<br>(run continues where possible)"]
  VOK -- Yes --> V4[Attach video reference<br>into request parts]

  A --> P{--pdf provided?}
  P -- No --> P0[No PDF work]
  P -- Yes --> P1{Is selected model Gemini?}
  P1 -- Yes --> P2["Upload PDF<br>(GoogleAIFileManager)"]
  P2 --> P3["Poll until ACTIVE<br>(or fail with error)"]
  P3 --> POK{Upload ok?}
  POK -- No --> PW["Fail (upload error)"]
  POK -- Yes --> P4[Attach PDF reference<br>into request parts]
  P1 -- No --> P5["Extract PDF text<br>(in-process) + attach as text"]

  I2 --> DONE[Media ready]
  IW --> DONE
  I0 --> DONE
  V0 --> DONE
  V4 --> DONE
  VW --> DONE
  P0 --> DONE
  P4 --> DONE
  P5 --> DONE
  PW --> DONE
```

</details>

### Streaming events and telemetry

`--stream none|jsonl` controls whether events are printed to stdout.

- `--stream jsonl` emits newline-delimited JSON events to stdout.
- Even with `--stream none`, the interactive transport acts as an always-on “tap” that receives the same events.

Event types (integration-relevant highlights):

- `resume.loaded` (emitted early when resuming from history or a payload file)
- `context.telemetry` (intent/system/context/total token counts)
- `context.overflow` (emitted when budgets force text-context trimming)
- `progress.update` (machine-readable progress)
- `upload.state` (image/video/pdf upload start/finish)
- `generation.iteration.start` / `generation.iteration.complete`
- `interactive.state` / `interactive.awaiting`
- `transport.listening`, `transport.client.connected`, `transport.client.disconnected`
- `generation.final` (final JSON payload)

#### D3. `--stream jsonl` Event Timeline

Shows a typical event sequence (actual events depend on flags and context sources).

<details>
<summary>Diagram</summary>

```mermaid
sequenceDiagram
  autonumber
  participant User
  participant CLI as prompt-maker-cli
  participant Stream as stdout (--stream jsonl)

  User->>CLI: start generate (intent + flags)

  opt Budgets enabled and overflow
    CLI->>Stream: context.overflow
  end

  CLI->>Stream: context.telemetry

  alt Remote context present
    CLI->>Stream: progress.update (scope=url start/update/stop)
  end

  alt Smart context enabled
    CLI->>Stream: progress.update (scope=smart start/update/stop)
  end

  alt Media present
    CLI->>Stream: upload.state (start per image/video)
    CLI->>Stream: progress.update (label "Uploading..." updates)
    CLI->>Stream: upload.state (finish per image/video)
  end

  CLI->>Stream: generation.iteration.start (iteration=1)
  CLI->>Stream: generation.iteration.complete (iteration=1)

  opt Interactive refinement enabled
    CLI->>Stream: interactive.state (phase=start)
    CLI->>Stream: interactive.awaiting (mode=tty|transport)
    loop for each refinement
      CLI->>Stream: interactive.state (phase=refine)
      CLI->>Stream: generation.iteration.start (iteration=N)
      CLI->>Stream: generation.iteration.complete (iteration=N)
      CLI->>Stream: interactive.awaiting (mode=tty|transport)
    end
    CLI->>Stream: interactive.state (phase=complete)
  end

  opt Polish enabled
    CLI->>Stream: progress.update (scope=polish start/update/stop)
  end

  CLI->>Stream: generation.final (result payload)
  CLI-->>User: human output (unless --quiet)
```

</details>

### Interactive refinement

Two interactive mechanisms exist:

- `--interactive` (TTY): uses terminal prompts to ask whether to refine and to collect the next refinement instruction.
  - If `--interactive` is set but no TTY is detected, the CLI warns and proceeds non-interactively.
- `--interactive-transport <path>`: accepts newline-delimited JSON commands over a local socket/pipe.
  - Unix: pass a socket path like `/tmp/pmc.sock`.
  - Windows: pass a named pipe path like `\\.\pipe\pmc`.

Commands are newline-delimited JSON:

```json
{"type":"refine","instruction":"Make it shorter"}
{"type":"finish"}
```

The transport also receives the same JSONL stream events as an always-on event “tap” (even if `--stream` is `none`).

#### D4. `--interactive-transport` Lifecycle

Shows how a client drives refinement and how events mirror back over the same connection.

<details>
<summary>Diagram</summary>

```mermaid
sequenceDiagram
  autonumber
  participant CLI as prompt-maker-cli
  participant Sock as Socket/Pipe (server)
  participant Client as Integration client

  CLI->>Sock: bind + listen
  CLI->>Client: transport.listening (event tap)

  Client->>Sock: connect
  CLI->>Client: transport.client.connected

  CLI->>Client: (mirrored stream events)\ncontext.telemetry, context.overflow, progress.update, ...

  loop Refinement cycle
    Client->>Sock: {"type":"refine","instruction":"..."}\n(newline-delimited JSON)
    CLI->>Client: interactive.state (phase=refine)
    CLI->>Client: generation.iteration.start
    CLI->>Client: generation.iteration.complete
    CLI->>Client: interactive.awaiting (mode=transport)
  end

  Client->>Sock: {"type":"finish"}
  CLI->>Client: interactive.state (phase=complete)
  CLI->>Client: generation.final

  Client-->>Sock: disconnect
  CLI->>Client: transport.client.disconnected
  CLI->>Sock: cleanup socket on exit
```

</details>

#### D5. Interactive Phase State Machine

Shows the high-level phases exposed via events during refinement.

<details>
<summary>Diagram</summary>

```mermaid
stateDiagram-v2
  [*] --> start

  start: interactive.state<br>phase=start
  start --> prompt

  prompt: interactive.state<br>phase=prompt
  prompt --> awaiting

  awaiting: interactive.awaiting<br>mode=tty|transport
  awaiting --> refine: receive instruction
  awaiting --> complete: finish requested

  refine: interactive.state<br>phase=refine
  refine --> awaiting: iteration complete

  complete: interactive.state<br>phase=complete
  complete --> [*]
```

</details>

### Polish pass

`--polish` triggers a finishing pass (`polishPrompt`) that focuses on formatting fidelity.

- Polish model defaults to the generation model.
- Override with `--polish-model` (or `PROMPT_MAKER_POLISH_MODEL`).

### Output artifacts

A completed run can produce:

- `prompt`: the final generated prompt text (untemplated)
- `polishedPrompt`: present when `--polish` is used
- `renderedPrompt`: present when `--context-template <name>` is applied
- `generation.final` stream event: includes the full payload (`GenerateJsonPayload`)
- History append: `~/.config/prompt-maker-cli/history.jsonl`

Practical integration rule (used by editor integrations):

- Prefer `renderedPrompt` if present.
- Else prefer `polishedPrompt` if present.
- Else use `prompt`.

<details>
<summary>Reference: output payload fields (GenerateJsonPayload)</summary>

From `docs/neovim-plugin-integration.md`, the JSON payload includes (high-level):

- `intent`
- `model` (generation model)
- `targetModel` (recorded runtime model)
- `prompt`
- `refinements`, `iterations`, `interactive`
- `timestamp`
- `contextPaths`
- optional `outputPath`
- optional `polishedPrompt`, `polishModel`
- optional `contextTemplate`, `renderedPrompt`

</details>

---

## Prompt tests

```bash
prompt-maker-cli test
# or
prompt-maker-cli test prompt-tests.yaml
```

- Default file: `prompt-tests.yaml` (repo root)
- TUI Test Runner view: `Ctrl+T`
- Quirk: `prompt-maker-cli test --help` prints help but still runs the suite

### D14. Prompt Test Runner Flow

Shows CLI and TUI entry points converging on the same runner.

<details>
<summary>Diagram</summary>

```mermaid
flowchart TD
  A["Start: prompt-maker-cli test [file]"] --> B{File provided?}
  B -- No --> D[Use default<br>prompt-tests.yaml]
  B -- Yes --> C[Use provided YAML path]

  C --> E[Load and validate YAML suite]
  D --> E

  E --> F["Run cases<br>(using src/test-command.ts runner)"]
  F --> G[Report results<br>stdout/stderr UI]

  subgraph TUI[Test Runner view]
    T1[Ctrl+T opens Test Runner] --> T2[Edit file path input]
    T2 --> T3["Run tests action<br>(Enter)"]
    T3 --> F
  end
```

</details>

---

## TUI architecture and UX model

The TUI is organized around an `AppContainer` that selects the active screen and enforces input routing invariants.

- Screens:
  - Generate screen: `src/tui/screens/command/CommandScreen.tsx`
  - Test Runner screen: `src/tui/screens/test-runner/TestRunnerScreen.tsx`
- Popups:
  - Popup transitions managed by `src/tui/popup-reducer.ts` (pure reducer)
  - Side effects and orchestration managed by `src/tui/hooks/usePopupManager.ts`
- Keybinds:
  - Defined by `src/tui/app-container-keymap.ts`

Reference: `docs/tui-design.md`.

### D10. TUI Architecture Map

Shows how routing, screens, and popups fit together conceptually.

<details>
<summary>Diagram</summary>

```mermaid
flowchart TD
  subgraph Entry[Entry]
    EP[src/index.ts<br>CLI router] -->|ui| TUI[src/tui/index.tsx<br>parse --interactive-transport]
    TUI --> AC[AppContainer<br>src/tui/AppContainer.tsx]
  end

  subgraph Global[Global layers]
    KEYMAP[Global keymap<br>src/tui/app-container-keymap.ts]
    HELP["Help overlay<br>('?' toggles)"]
  end

  subgraph Screens[Screens]
    CS["CommandScreen (Generate)<br>src/tui/screens/command/CommandScreen.tsx"]
    TS[TestRunnerScreen<br>src/tui/screens/test-runner/TestRunnerScreen.tsx]
  end

  subgraph Popups[Popups + palette]
    PR["popup-reducer<br>src/tui/popup-reducer.ts<br>(pure)"]

    PM["usePopupManager<br>src/tui/hooks/usePopupManager.ts<br>(effects)"]
    PAL["Command palette<br>(COMMAND_DESCRIPTORS in src/tui/config.ts)"]
  end

  AC -->|active view| CS
  AC -->|active view| TS

  AC --> HELP
  AC --> KEYMAP

  CS --> PAL
  PAL --> PR
  PR --> PM
  PM --> CS
```

</details>

---

## Provider configuration

Credentials and defaults can come from env vars or a config file.

Config path resolution:

- `PROMPT_MAKER_CLI_CONFIG=/path/to/config.json` (highest precedence)
- `~/.config/prompt-maker-cli/config.json`
- `~/.prompt-maker-cli.json`

Example config:

```json
{
  "openaiApiKey": "sk-...",
  "geminiApiKey": "gk-...",
  "promptGenerator": {
    "defaultModel": "gpt-4o-mini",
    "defaultGeminiModel": "gemini-2.5-pro",
    "maxInputTokens": 12000,
    "maxContextTokens": 8000,
    "contextOverflowStrategy": "drop-smart"
  },
  "contextTemplates": {
    "scratch": "# Scratch\n\n{{prompt}}"
  },
  "theme": "ocean",
  "themeMode": "system",
  "resumeMode": "best-effort",
  "resumeSourceKind": "history",
  "exportFormat": "json",
  "exportOutDir": "runs"
}
```

Env vars override config keys:

- `OPENAI_API_KEY` (and optional `OPENAI_BASE_URL`)
- `GEMINI_API_KEY` (and optional `GEMINI_BASE_URL`)
- Optional: `GITHUB_TOKEN` (for GitHub URL context rate limits)

---

## Outputs and persistence

- Generate-run history (JSONL): `~/.config/prompt-maker-cli/history.jsonl`
- TUI command history: `~/.config/prompt-maker-cli/tui-history.json`
- TUI workflow defaults persisted to config: budgets (`promptGenerator.*`), resume (`resumeMode`, `resumeSourceKind`), export (`exportFormat`, `exportOutDir`)
- Token telemetry:
  - Printed as a summary in non-`--quiet` runs
  - Always emitted as a `context.telemetry` JSONL stream event
  - When budgets force trimming, a `context.overflow` event is also emitted

---

## Development

```bash
npm ci
npm run build
npm start

# watch + restart
npm run dev -- ui
npm run dev -- "Draft a confident onboarding-bot spec" --model gemini-1.5-flash

npm run typecheck
npm test
npm run format
```

---

## Maintainer references

- `docs/prompt-maker-cli-tui-encyclopedia.md` (authoritative behavior reference)
- `docs/tui-design.md` (UX goals + input routing invariant)
- `docs/neovim-plugin-integration.md` (generate pipeline + streaming events + interactive transports)
- `src/tui/DEVELOPER_NOTE.md` (architecture notes; some paths are historical)

---

## License

MIT
</file>

</files>
